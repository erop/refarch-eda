{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"IBM Automation Event-Driven Architecture Introduction Getting started with Event-Driven Architectures. >> Concepts of EDA Delve into the foundational concepts of Event-Driven Architectures. >> Advantages of EDA Explore some of the many benefits of Event-Driven Architectures. >> Patterns in EDA Complex design patterns solved with Event-Driven Architectures. >> Methodology Master Event-Driven Architectures methodologies from start to finish. >> Technology Leverage the technologies behind Event-Driven Architectures. >> Scenarios & Use Cases Walk through real-world use cases of event-driven applications. >>","title":"Home"},{"location":"tracking/","text":"Track refactoring \u00b6 From one page fix all links, and consider migration done when links point to page ported to new docs-mk folder. The target page may not be done. to move \u00b6 Done \u00b6 - introduction - Overview: introduction/overview/index.md - Reference Architecture: introduction/reference-architecture/index.md - Business Use Cases: introduction/usecases/index.md - Target Audiences: introduction/target-audiences/index.md - Learning Journey: - Get started (101 content): journey/101/index.md - Get started (201 content): journey/201/index.md - Concepts: - Terms & Definitions: concepts/terms-and-definitions/index.md - Event streaming versus Queuing: concepts/events-versus-messages/index.md - Fit for purpose: concepts/fit-to-purpose/index.md - Service mesh: concepts/service-mesh/index.md - Agile Integration: concepts/integration/index.md - Devising the data models: concepts/model/index.md - Flow Architecture: concepts/flow-architectures.md - Advantages of EDA: - Microservice decoupling: advantages/microservice/index.md - Reactive systems: advantages/reactive/index.md - Resiliency: advantages/resiliency/index.md - Scalability: advantages/scalability/index.md - Patterns in EDA: - Introduction: patterns/intro/index.md - Event Sourcing: patterns/event-sourcing/index.md - CQRS: patterns/cqrs/index.md - Saga: patterns/saga/index.md - Dead Letter Queue: patterns/dlq/index.md - Data Intensive App: patterns/data-pipeline/index.md - Near real-time analytics: patterns/realtime-analytics/index.md - API management: patterns/api-mgt/index.md - Situational decision: patterns/cep/index.md - Technology: - Kafka Overview: technology/kafka-overview/index.md - Event Streams: technology/event-streams/index.md - Kafka FAQ: technology/faq/index.md - Avro Schema: technology/avro-schemas/index.md - MQ in EDA context: technology/mq/index.md - Kafka Consumers: technology/kafka-consumers/index.md - Kafka Producers: technology/kafka-producers/index.md - Mirror Maker 2: technology/kafka-mirrormaker/index.md - Security: technology/security/index.md - Apache Flink: technology/flink/index.md - Spring cloud: technology/spring/index.md - Methodology: - Event Storming: methodology/event-storming/index.md - Domain-Driven Design: methodology/domain-driven-design/index.md - Data lineage: methodology/data-lineage/index.md - Data Intensive App Development: methodology/data-intensive/index.md - governance: methodolgy/governance/index.md - use cases: - Event-driven solution GitOps: use-cases/gitops/index.md - Deploy Event-Streams: technology/event-streams/es-cp4i/index.md - Kafka Connect - S3: use-cases/connect-s3/index.md - Kafka Connect - COS: use-cases/connect-cos/index.md - Kafka Connect - jdbc: use-cases/connect-jdbc/index.md - Kafka Connect - MQ: use-cases/connect-mq/index.md - Kafka Connect - Rabbitmq: use-cases/connect-rabbitmq /index.md - DB2 - CDC Debezium - Outbox: use-cases/db2-debezium/index.md - Mirror maker 2 labs: use-cases/kafka-mm2/index.md Scenarios: Overview: scenarios/overview/ Reefer Shipment Solution: https://ibm-cloud-architecture.github.io/refarch-kc/ Vaccine at Scale: https://ibm-cloud-architecture.github.io/vaccine-solution-main/ Near real-time Inventory: https://ibm-cloud-architecture.github.io/eda-rt-inventory-gitops moved content to verify links . \u00b6 Patterns in EDA: Topic Replication: patterns/topic-replication/index.md use cases - api management Technology streams Kafka Streams: technology/kafka-streams/index.md To revisit the structure and content \u00b6 Pattern > Topic replication with DR and MM2 Scenario: kafka-mm2, reefer, SAGA with MQ Orchestration: scenarios/saga-orchestration/","title":"Track refactoring"},{"location":"tracking/#track-refactoring","text":"From one page fix all links, and consider migration done when links point to page ported to new docs-mk folder. The target page may not be done.","title":"Track refactoring"},{"location":"tracking/#to-move","text":"","title":"to move"},{"location":"tracking/#done","text":"- introduction - Overview: introduction/overview/index.md - Reference Architecture: introduction/reference-architecture/index.md - Business Use Cases: introduction/usecases/index.md - Target Audiences: introduction/target-audiences/index.md - Learning Journey: - Get started (101 content): journey/101/index.md - Get started (201 content): journey/201/index.md - Concepts: - Terms & Definitions: concepts/terms-and-definitions/index.md - Event streaming versus Queuing: concepts/events-versus-messages/index.md - Fit for purpose: concepts/fit-to-purpose/index.md - Service mesh: concepts/service-mesh/index.md - Agile Integration: concepts/integration/index.md - Devising the data models: concepts/model/index.md - Flow Architecture: concepts/flow-architectures.md - Advantages of EDA: - Microservice decoupling: advantages/microservice/index.md - Reactive systems: advantages/reactive/index.md - Resiliency: advantages/resiliency/index.md - Scalability: advantages/scalability/index.md - Patterns in EDA: - Introduction: patterns/intro/index.md - Event Sourcing: patterns/event-sourcing/index.md - CQRS: patterns/cqrs/index.md - Saga: patterns/saga/index.md - Dead Letter Queue: patterns/dlq/index.md - Data Intensive App: patterns/data-pipeline/index.md - Near real-time analytics: patterns/realtime-analytics/index.md - API management: patterns/api-mgt/index.md - Situational decision: patterns/cep/index.md - Technology: - Kafka Overview: technology/kafka-overview/index.md - Event Streams: technology/event-streams/index.md - Kafka FAQ: technology/faq/index.md - Avro Schema: technology/avro-schemas/index.md - MQ in EDA context: technology/mq/index.md - Kafka Consumers: technology/kafka-consumers/index.md - Kafka Producers: technology/kafka-producers/index.md - Mirror Maker 2: technology/kafka-mirrormaker/index.md - Security: technology/security/index.md - Apache Flink: technology/flink/index.md - Spring cloud: technology/spring/index.md - Methodology: - Event Storming: methodology/event-storming/index.md - Domain-Driven Design: methodology/domain-driven-design/index.md - Data lineage: methodology/data-lineage/index.md - Data Intensive App Development: methodology/data-intensive/index.md - governance: methodolgy/governance/index.md - use cases: - Event-driven solution GitOps: use-cases/gitops/index.md - Deploy Event-Streams: technology/event-streams/es-cp4i/index.md - Kafka Connect - S3: use-cases/connect-s3/index.md - Kafka Connect - COS: use-cases/connect-cos/index.md - Kafka Connect - jdbc: use-cases/connect-jdbc/index.md - Kafka Connect - MQ: use-cases/connect-mq/index.md - Kafka Connect - Rabbitmq: use-cases/connect-rabbitmq /index.md - DB2 - CDC Debezium - Outbox: use-cases/db2-debezium/index.md - Mirror maker 2 labs: use-cases/kafka-mm2/index.md Scenarios: Overview: scenarios/overview/ Reefer Shipment Solution: https://ibm-cloud-architecture.github.io/refarch-kc/ Vaccine at Scale: https://ibm-cloud-architecture.github.io/vaccine-solution-main/ Near real-time Inventory: https://ibm-cloud-architecture.github.io/eda-rt-inventory-gitops","title":"Done"},{"location":"tracking/#moved-content-to-verify-links","text":"Patterns in EDA: Topic Replication: patterns/topic-replication/index.md use cases - api management Technology streams Kafka Streams: technology/kafka-streams/index.md","title":"moved content to verify links ."},{"location":"tracking/#to-revisit-the-structure-and-content","text":"Pattern > Topic replication with DR and MM2 Scenario: kafka-mm2, reefer, SAGA with MQ Orchestration: scenarios/saga-orchestration/","title":"To revisit the structure and content"},{"location":"additional-reading/","text":"Event-Driven Architecture \u00b6 Event-Driven Architecture Learning Journey via IBM Skills Gateway , provides a navigable guide through this GitBook and associated assets. IBM Event Streams \u00b6 IBM Event Streams presentation Validating Event Streams deployment with sample app. Install IBM Event Streams on Red Hat OpenShift Kafka \u00b6 Start by reading Kafka introduction - a must read! Another introduction from Confluent, one of the main contributors of the open source. Planning event streams installation Develop Stream Application using Kafka Tutorial on access control, user authentication and authorization from IBM. IBM Developer article - learn kafka Using Kafka Connect to connect to enterprise MQ systems - Andrew Schofield Does Apache Kafka do ACID transactions? - Andrew Schofield Spark and Kafka with direct stream, and persistence considerations and best practices Example in scala for processing Tweets with Kafka Streams Microservices and event-driven patterns \u00b6 API for declaring messaging handlers using Reactive Streams Microservice patterns - Chris Richardson Stream Analytics \u00b6 IBM Streams Samples Getting started with IBM Streaming Analytics on IBM Cloud Integration \u00b6 Interesting article on the evolving hybrid integration reference architecture : How to ensure your integration landscape keeps pace with digital transformation. Companion web site for hybrid integration reference architecture Conferences, Talks, and Sessions \u00b6 IBM THINK 2020 - From Monolithic Application to API Centric and Event-Driven Microservices \u2013 the Cloud Journey of a Leading Health Care Organization IBM THINK 2020 - Change the Way You Integrate Applications with IBM Cloud Pak for Integration IBM THINK 2020 - Modernize Integration to Unlock Data and Applications Securely While Lowering Costs Kafka Summit 2016 - San Francisco Kafka Summit 2017 - New York Kafka Summit 2017 - San Francisco Kafka Summit 2018 - San Francisco Kafka Summit 2019 - San Francisco Kafka Summit 2019 - London Kafka Summit 2020 - Virtual Kafka Summit 2022 - Recap","title":"Additional reading"},{"location":"additional-reading/#event-driven-architecture","text":"Event-Driven Architecture Learning Journey via IBM Skills Gateway , provides a navigable guide through this GitBook and associated assets.","title":"Event-Driven Architecture"},{"location":"additional-reading/#ibm-event-streams","text":"IBM Event Streams presentation Validating Event Streams deployment with sample app. Install IBM Event Streams on Red Hat OpenShift","title":"IBM Event Streams"},{"location":"additional-reading/#kafka","text":"Start by reading Kafka introduction - a must read! Another introduction from Confluent, one of the main contributors of the open source. Planning event streams installation Develop Stream Application using Kafka Tutorial on access control, user authentication and authorization from IBM. IBM Developer article - learn kafka Using Kafka Connect to connect to enterprise MQ systems - Andrew Schofield Does Apache Kafka do ACID transactions? - Andrew Schofield Spark and Kafka with direct stream, and persistence considerations and best practices Example in scala for processing Tweets with Kafka Streams","title":"Kafka"},{"location":"additional-reading/#microservices-and-event-driven-patterns","text":"API for declaring messaging handlers using Reactive Streams Microservice patterns - Chris Richardson","title":"Microservices and event-driven patterns"},{"location":"additional-reading/#stream-analytics","text":"IBM Streams Samples Getting started with IBM Streaming Analytics on IBM Cloud","title":"Stream Analytics"},{"location":"additional-reading/#integration","text":"Interesting article on the evolving hybrid integration reference architecture : How to ensure your integration landscape keeps pace with digital transformation. Companion web site for hybrid integration reference architecture","title":"Integration"},{"location":"additional-reading/#conferences-talks-and-sessions","text":"IBM THINK 2020 - From Monolithic Application to API Centric and Event-Driven Microservices \u2013 the Cloud Journey of a Leading Health Care Organization IBM THINK 2020 - Change the Way You Integrate Applications with IBM Cloud Pak for Integration IBM THINK 2020 - Modernize Integration to Unlock Data and Applications Securely While Lowering Costs Kafka Summit 2016 - San Francisco Kafka Summit 2017 - New York Kafka Summit 2017 - San Francisco Kafka Summit 2018 - San Francisco Kafka Summit 2019 - San Francisco Kafka Summit 2019 - London Kafka Summit 2020 - Virtual Kafka Summit 2022 - Recap","title":"Conferences, Talks, and Sessions"},{"location":"advantages/microservice/","text":"Updated 10/07/2021 As we have seen in the introduction, modern business application needs to responds to events in real time, as the event happen, so it can deliver better user experiences and apply business rule on those events. The key is to be able to act quickly on those facts. Acting may involve computing analytics or machine trained models. On top of that a modern cloud native application needs to be reactive, responsive by adopting the reactive manifesto . We can also claim they are becoming intelligence by integrating rule engine and predictive scoring / AI capabilities. When adopting microservice implementation approach, the bounded context is defined with events and aggregates or main business entity. So each microservice is responsible to manage the operations of creating, updating and reading the data from a main business entity. This clear separation leads to exchange data between services, they may used to be integrated in the same monolytic application before. A web application, single page app (SPA), accesses the different microservices using RESTful API, to get the different data views it needs, or to post new data elements to one of the service. The following diagram illustrates a simple view of the microservice challenges: When the user interface exposes entry form to get data for one of the business entity, it calls a REST end point with a HTTP POST operation, then data are saved to data store: document oriented database or SQL based RDBMS. When a microservice (1) needs to access data from another service, then it calls another end point via an HTTP GET. A coupling is still existing, at the data schema definition level: a change to the data model, from the source microservice, impacts the service contract and so all the callers. This may be acceptable when there is few microservices, but could become a real pain when the number increase. When the microservice dependencies grows in size and complexity, as illustrated by the following figure from Jack Kleeman's Monzo study , we can see the coupling impact, which lead to impacting time to deliver new function and cost to maintain such complexity. Finally, imagine we need to join data coming from two different services to address an urgent business request? Who will implement the join, service A or B? May be the simplest is to add a service C and implement the join: it will call two API end points, and try to reconcile data using primary keys on both business entities. With event-driven microservices, the communication point becomes the Pub/Sub layer of the event backbone. By adopting an event-based approach for intercommunication between microservices, the microservices applications are naturally responsive (event-driven). This approach enhances the loose coupling nature of microservices because it decouples producers and consumers. The figure below illustrates, that microservices A and B produces facts about their business entities, and their life cycle to topic in the pub/sub event backbone: The microservice C consumes those facts to build it own projection or view for supporting the join query. When adopting technology like Kafka as messaging backbone, the data sharing is done via an event log, which can be kept for a very long time period, and is replayable to improve resilience. These event style characteristics are increasingly important considerations when you develop microservices style applications. In practical terms microservices applications are a combination of synchronous API-driven, and asynchronous event-driven communication styles. There is something important to add, is that coupling by the data, still existing but in a less impactful manner. Messaging structures are defined with JSON schema or Avro schema and managed inside a Schema registry, so Kafka-based applications can get their data contract. The following figure presents a potential structure for event-driven microservice: APIs are defined using microprofile OpenAPI annotations in one or more JAXRS resource classes. Those APIs can then be managed within an API management product as IBM API Connect. The rest of the application structure reflects the DDD approach of onion architecture. The business logic is in its own layer with DDD aggregate, ubiquitous language, services, business rules, etc\u2026 The repository layer supports persisting those aggregates to an external document-oriented or SQL-based database. As most of the new microservices are message-driven, we are adding a messaging layer that may use queues or topics. Use queue for request/response exactly once delivery and topic for sharing facts in append log. In Java, the Microprofile Reactive Messaging is used to define the different publishing channels, being queue, topic, or both. From the JSON or Avro schema defining the messages or events structure, developers can build an AsyncAPI specification which may also be managed by an API product.","title":"Microservice decoupling"},{"location":"advantages/reactive/","text":"This chapter describes how event-driven architecture addresses the implementation of reactive systems and presents the most recent technologies to implement such event-driven responsive solutions. Overview \u00b6 Cloud native \u00b6 A buzzy set of words which has some concrete requirements. The 12 factors app defines the needs for modern business service apps or webapps to be a good citizen in cloud deployment: Codebase : One codebase tracked in version control, many deploys. Dependencies : Explicitly declare and isolate dependencies. Config : store config in the environment. Backing services : Treat backing services as attached resources. Build, release, run : Strictly separate build and run stages. Processes : Execute the app as one or more stateless (which may be challenged in case of event streaming and real-time analytics) processes Port binding : Export services via port binding. Concurrency : Scale out via the process model. Disposability : Maximize the robustness with fast startup and graceful shutdown. Dev/prod parity : Keep development, staging, and production as similar as possible. Logs : Treat your logs as event streams. Admin processes : Run admin/management tasks as one-off processes. Achieving cloud native is not an easy task. Avoid vendor locking by using specific vendor services to keep data, messaging... Hybrid cloud applications run on multiple clouds, mixing private and public clouds, to reduce response time and prevent global unavailability as recently demonstrated by AWS outtages. Kubernetes helps delivering an abstraction on top of cloud providers, and uses them as infrastructure as a services, and bring your own platform. Kubernetes facilitates reactive systems through responsiveness and resilience. Distributed systems \u00b6 From Leslie Lamport's definition : A distributed system is one in which the failure of a computer you didn\u2019t even know existed can render your own computer unusable. we need to design application for transient, intermittent, or complete failure and resilience. Complex exchanges involving multiple services cannot expect all the participants and the network to be operational for the complete duration of that exchange. Always ask ourselves: How would you detect failures? How would you handle them gracefully? When designing a distributed solution, we need to keep in mind that the CAP theorem prevents a distributed data store from simultaneously providing more than two out of the three guarantees of Consistency, Availability and Partition tolerance. With simple synchronous calls between services leads to time coupling and enforces programming in sequence. The code must gracefully handle faulty responses and the absence of response. Quarkus/Vert.x has a router that can be intercepted to simulate communication loss, wrong response or delay, this is vertically helpful to test for failure. See Clement Escoffier's code sample . Combining time-out and retries, is a common development practice, but we can\u2019t assume the service was not invoked and retrying may reprocess the same request multiple times. Service needs to support idempotence: multiple requests with same content, results in the same state and same output. Idempotence can be implemented using unique identifiers added to the payload so consumer can identify same request. But server applications need to keep state of those ids, which means using Storage, which is under the CAP theorem. We may include an in-memory data grid such as Infinispan or Hazelcast, an inventory service such as Apache ZooKeeper, a distributed cache as Redis. Reactive systems \u00b6 Modern business applications embrace the strong need to be responsive, bringing immediate response and feedbacks to the end user or system acting on it at the moment it needed. Modern solution based on microservices needs to support load increase and failure and developers are adopting the reactive manifesto and use modern programming libraries and software to support the manifesto characteristics. The reactive manifesto defines four characteristics modern cloud native application needs to support: Responsive : deliver a consistent quality of service to end users or systems, react quickly and consistently to events happening in the system. Elastic : The system stays responsive under varying workload, it can scale up and down the resource utilization depending of the load to the system. Resilient : stay responsive in the face of failure, this is a key characteristics. It implies distributed systems. Message driven : the underlying behavior is to have an asynchronous message driven backbone, to enable loose coupling of the application components by exchanging asynchronous messages to minimize or isolate the negative effects of resource contention, coherency delays and inter-service communication network latency. It is the base to support the other reactive characteristics. It also helps for isolation and support location transparency. Reactive architecture is an architecture approach aims to use asynchronous messaging or event driven architecture to build Responsive, Resilient and Elastic systems. Relying on message passing enables the responsive characteristics and more, like flow control by monitoring the messages in the system and applying backpressure when necessary. Under the \"reactive\" terms we can see two important caveats: Reactive systems is a group of application components which can heal and scale automatically. It address data consistency, cross domain communication, orchestration, failure, recovery... Reactive programming is a subset of asynchronous programming and a paradigm where the availability of new information drives the logic forward rather than having control flow driven by a thread-of-execution. This is the adoption of non-blocking IO and event-based model. The following figure illustrates well how those two paradigms work together to deliver business value: We recommend to go over this excellent IBM article on defining reactive to go deeper into those concepts. Commands and Events \u00b6 Those two concepts are very fundamental to implement well distributed applications. Commands : represent action a user or system wants to perform. HTTP APIs pass commands. Commands are sent to a specific service and result is sent back to the caller. Events : are actions that have successfully completed. An event represents a fact. They are immutable. By looking at your solution in terms of commands and events, you focus on the behavior, workflow, instead of the structure. Events are wrapped into Messages. But Commands can also being passed via messaging and asynchronous communication. Most likely strong consistency is needed and queuing systems are used as message brokers. Is it for me? \u00b6 We have learnt from years of point to point microservice implementations, that embrassing asynchronous communication helps a lot to support scaling, integration, coupling and failover. So adopting reactive design and implementation may look complex at first but is becoming a necessity in the long run. In e-commerce, a lot of monolithic applications were redesigned to adopt reactive manifesto characteristics to support scaling the business needs and respond to sporadic demand. In the world of big data, collecting, aggregating, applying real time analytics, decisions and AI need to scale and respond to events at the time of occurence. EDA and reactive systems \u00b6 The adoption of event driven microservice implementation fits well into the reactive manifesto, where most of the work presented in this git repository started by adopting Kafka as event backbone, it is too reductor to think EDA is just Kafka. EDA supports reactive systems at large, and developing event-driven microservice should use reactive libraries to support non-blocking IO and event bus for inter process communication. Also microservices is part of the game, functions / serverless are also in scope and with serverless 2.0, Knative eventing is one of the new kid in the play. The manifesto stipulates \"message driven\" while EDA is about events and commands. Events represent unmmutable data and facts about what happened, and components subscribe to those event streams. Command demands the consumer to process the content data sent and gives an answer. Both are sent via messages, and transported and managed by brokers. For sure we define event-driven implementations to cover both. And we should not be purist and opinionated about messaging versus eventing: it will not make any sense to say: you are using queue to exchange message while we produce events to topic. The following figure illustrates the combination of synchronous communication, sending commands to reactive system, supported by reactive applications link together via messaging. With messaging applications can scale horizontally giving the elastic need of the reactive manifesto. They are also more resilient as messages are kept until consumed or for a long period of time and consumer can restart from where they were in the ordered log. Reactive systems are not only exchanging messages. Sending and receiving messages must be done efficiently and Reactive promotes the use of nonblocking I/Os. Which leads to reactive programming and supporting libraries, like Vert.x, Mutiny, reactive messaging... Technology review \u00b6 Concurrency \u00b6 The following figure illustrates the traditional Java multi-threading approach to handle request and access I/Os on a two CPUs computer. When the second thread starts working on the IO the CPU is locked and the CPU yellow is supporting 2 threads (1 and 3) On public clouds, the blocking I/O approach inflates your monthly bill; on private clouds, it reduces the deployment density. Non blocking IO framework or library adopts the reactor pattern where requests are internally asynchronous events processed, in order, by an event loop running in one thread, and handlers (or callbacks) are used to process the response. The above figure and next one are coming from Clement Escoffier's book Building reactive microservice in Java . In multi CPUs, cores and threading computer, the reactors can run in parallel, with one event loop per core: With non-blocking I/O the I/O operation can be allocated to another pool and the allocation of CPU to thread is well balanced: Vert.X is the open source library to build such non-blocking I/O app, and using vertices to support scalable concurrent processor, which executes one event loop thread. vertices communicate asynchronously via an event bus. Reactive programming \u00b6 Reactive programming is about observing asynchronous streams . Streams can be seen as a pipe in which events flow. We observe the events flowing\u2014such as items, failures, completion, cancellations\u2014and implement side effects. You structure your code around streams and build chains of transformation, also called pipeline. Reactive programming libraries offer countless operators that let you create, combine, filter, and transform the object emitted by streams. One important consideration is the speed of items processing by the consumer. If it is too slow compare to the producer, there will be a big proble, that can be solved efficiency by using backprassure protocol. Reactive Streams is such backpressure protocol. It defines the concept of Subscriber who requests the Publisher a certain amount of items. The consumer controls the flow, as when items are received and processed, consumers can ask for more. Producer is not strongly coupled to the consumer as they participate together to the stream processing. Producer uses a Subscription object to act as a contract between the participants. different libraries support the Reactive Streams protocol, like Mutiny, RxJava, Project Reactor... Vert.x \u00b6 We do not need to reintroduce Vert.X , but with the large adoption of Quarkus to develop new JVM based microservice, Vert.x is an important library to understand. The main concepts used are: An application would typically be composed of multiple vertices running in the same Vert.x instance and communicate with each other using events via the event bus . vertices remain dormant until they receive a message or event. Message handling is ideally asynchronous, messages are queued to the event bus, and control is returned to the sender Regular vertices are executed in the event loop Worker vertices are not executed on the event loop, which means they can execute blocking code Two event loops per CPU core thread No thread coordination mechanisms to manipulate a verticle state A verticle can be deployed several times as part of a container for example Event bus is used by different vertices to communicate through asynchronous message passing (JSON) (point to point, pub / sub, req / resp) We can have in memory event bus or clustered cross nodes, managed by Vert.x with a TCP protocol Quarkus HTTP support is based on a non-blocking and reactive engine (Vert.x and Netty). All the HTTP requests your application receive are handled by event loops (IO Thread) and then are routed towards the code that manages the request. Depending on the destination, it can invoke the code managing the request on a worker thread (Servlet, Jax-RS) or use the IO Thread (reactive route). (Images src: quarkus.io) The application code should be written in a non-blocking manner using SmallRye Mutiny or RsJava libraries. So when interactive with different services using kafka as an inter service communication layer the producer and consumer are handlers and the internal processing can be schematized as: Vert.x and reactive messaging applications may be combined to byild reactive systems to support the reactive manifesto: To achieve resilience and responsiveness , microservice can scale vertically using vertices inside the JVM and horizontally via pod scaling capability within a Kubernetes cluster. The inter-communication between vertices is done via event bus and managed by the Vert.x library using virtual addresses, service discovery and event bus. Internal service communication (even cross pods) and external cross service boundary are message driven . Using Kafka, they are also durable improving resilience and recovery. Kubernetes enforces part of the reactive manifesto at the container level: elasticity and resilience with automatic pod recovery and scheduling. MicroProfile reactive messaging \u00b6 The MicroProfile Reactive messaging specification aims to deliver applications embracing the characteristics of reactive systems as stated by reactive manifesto. It enables non-blocking, asynchronous message passing between services, giving them the ability to scale, fail, and evolve independently. To summarize the main concepts, developer declares channels as way to get incoming or outgoing messages between CDI Java beans and connector to external message brokers: The potential matching declarations for the connector, for the above figure, may look like below: # Kafka connector to items topic mapped to the item-channel mp.messaging.incoming.item-channel.connector = smallrye-kafka mp.messaging.incoming.item-channel.topic = items For code which defines the 'outhoing' and 'incoming' message processing see this quarkus guide , the EDA quickstart code templates for producer and consumer public class OrderService { @Channel ( \"orders\" ) public Emitter < OrderEvent > eventProducer ; public OrderEntity createOrder ( OrderEntity order ) { try { // build orderPayload based on cloudevent from order entity Message < OrderEvent > record = KafkaRecord . of ( order . getOrderID (), orderPayload ); eventProducer . send ( record ); logger . info ( \"order created event sent for \" + order . getOrderID ()); } catch ( Exception e ) { e . printStackTrace (); } } JMS and message driven bean, were the messaging APIs to asynchronously communicate with other applications. They support transaction and so it is an heavier protocol to use. They do not support asynchronous IO. When building microservices, the CQRS and event-sourcing patterns provide an answer to the data sharing between microservices. Reactive Messaging can also be used as the foundation to CQRS and Event-Sourcing mechanisms. A lot of EDA repositories demonstrating EDA concepts are using microprofile 3.0 Reactive Messaging, as it simplifies the implementation to produce or consume messages to messaging middleware like Kafka. When you use @Incoming and @Outgoing annotations, the runtime framework (Open Liberty or Quarkus) creates a Reactive Streams component for each method and joins them up by matching the channel names. A simple guide from Quarkus web site with integration with Kafka. Open Liberty supports this specification implementation . Mutiny \u00b6 Mutiny is a modern reactive programming library to provide more natural, readable reactive code. It supports asynchrony, non-blocking programming and streams, events, back-pressure and data flows. With Mutiny both Uni and Multi expose event-driven APIs: you express what you want to do upon a given event (success, failure, etc.). These APIs are divided into groups (types of operations) to make it more expressive and avoid having 100s of methods attached to a single class. This section of the product documentation goes over some examples on how to use Uni/ Multi. AMQP \u00b6 Advanced Message Queueing Protocol is an international standard for interoperability between messaging middlewares. IBM MQ supports AMQP client via specific AMQP channel. Clients can connect to the queue manager and send / receive messages to / from queue. Knative eventing \u00b6 Knative is Kubernetes based platform to develop serverless. Major value proposition is a simplified deployment syntax with automated scale-to-zero and scale-out based on HTTP load. Knative consists of the following components: Eventing - Management and delivery of events Serving - Request-driven compute that can scale to zero See the RedHat Knative cookbook for a simple tutorial. Code samples \u00b6 Vert.x kafka client Experiences writing a reactive Kafka application More... \u00b6 Reactive Systems Explained - Book from Grace Jansen - Peter Gollmar Reactive Java Modules show how to build an event-driven, streams-optimized Kafka-based Java application. You will use the Eclipse MicroProfile Reactive Messaging API and Open Liberty to build it and then you'll learn how to test it in true-to-production environments by using containers with MicroShed testing. Resiliency discussion in IBM architecture center Clement Escoffier's book Building reactive microservice in Java","title":"Reactive systems"},{"location":"advantages/reactive/#overview","text":"","title":"Overview"},{"location":"advantages/reactive/#cloud-native","text":"A buzzy set of words which has some concrete requirements. The 12 factors app defines the needs for modern business service apps or webapps to be a good citizen in cloud deployment: Codebase : One codebase tracked in version control, many deploys. Dependencies : Explicitly declare and isolate dependencies. Config : store config in the environment. Backing services : Treat backing services as attached resources. Build, release, run : Strictly separate build and run stages. Processes : Execute the app as one or more stateless (which may be challenged in case of event streaming and real-time analytics) processes Port binding : Export services via port binding. Concurrency : Scale out via the process model. Disposability : Maximize the robustness with fast startup and graceful shutdown. Dev/prod parity : Keep development, staging, and production as similar as possible. Logs : Treat your logs as event streams. Admin processes : Run admin/management tasks as one-off processes. Achieving cloud native is not an easy task. Avoid vendor locking by using specific vendor services to keep data, messaging... Hybrid cloud applications run on multiple clouds, mixing private and public clouds, to reduce response time and prevent global unavailability as recently demonstrated by AWS outtages. Kubernetes helps delivering an abstraction on top of cloud providers, and uses them as infrastructure as a services, and bring your own platform. Kubernetes facilitates reactive systems through responsiveness and resilience.","title":"Cloud native"},{"location":"advantages/reactive/#distributed-systems","text":"From Leslie Lamport's definition : A distributed system is one in which the failure of a computer you didn\u2019t even know existed can render your own computer unusable. we need to design application for transient, intermittent, or complete failure and resilience. Complex exchanges involving multiple services cannot expect all the participants and the network to be operational for the complete duration of that exchange. Always ask ourselves: How would you detect failures? How would you handle them gracefully? When designing a distributed solution, we need to keep in mind that the CAP theorem prevents a distributed data store from simultaneously providing more than two out of the three guarantees of Consistency, Availability and Partition tolerance. With simple synchronous calls between services leads to time coupling and enforces programming in sequence. The code must gracefully handle faulty responses and the absence of response. Quarkus/Vert.x has a router that can be intercepted to simulate communication loss, wrong response or delay, this is vertically helpful to test for failure. See Clement Escoffier's code sample . Combining time-out and retries, is a common development practice, but we can\u2019t assume the service was not invoked and retrying may reprocess the same request multiple times. Service needs to support idempotence: multiple requests with same content, results in the same state and same output. Idempotence can be implemented using unique identifiers added to the payload so consumer can identify same request. But server applications need to keep state of those ids, which means using Storage, which is under the CAP theorem. We may include an in-memory data grid such as Infinispan or Hazelcast, an inventory service such as Apache ZooKeeper, a distributed cache as Redis.","title":"Distributed systems"},{"location":"advantages/reactive/#reactive-systems","text":"Modern business applications embrace the strong need to be responsive, bringing immediate response and feedbacks to the end user or system acting on it at the moment it needed. Modern solution based on microservices needs to support load increase and failure and developers are adopting the reactive manifesto and use modern programming libraries and software to support the manifesto characteristics. The reactive manifesto defines four characteristics modern cloud native application needs to support: Responsive : deliver a consistent quality of service to end users or systems, react quickly and consistently to events happening in the system. Elastic : The system stays responsive under varying workload, it can scale up and down the resource utilization depending of the load to the system. Resilient : stay responsive in the face of failure, this is a key characteristics. It implies distributed systems. Message driven : the underlying behavior is to have an asynchronous message driven backbone, to enable loose coupling of the application components by exchanging asynchronous messages to minimize or isolate the negative effects of resource contention, coherency delays and inter-service communication network latency. It is the base to support the other reactive characteristics. It also helps for isolation and support location transparency. Reactive architecture is an architecture approach aims to use asynchronous messaging or event driven architecture to build Responsive, Resilient and Elastic systems. Relying on message passing enables the responsive characteristics and more, like flow control by monitoring the messages in the system and applying backpressure when necessary. Under the \"reactive\" terms we can see two important caveats: Reactive systems is a group of application components which can heal and scale automatically. It address data consistency, cross domain communication, orchestration, failure, recovery... Reactive programming is a subset of asynchronous programming and a paradigm where the availability of new information drives the logic forward rather than having control flow driven by a thread-of-execution. This is the adoption of non-blocking IO and event-based model. The following figure illustrates well how those two paradigms work together to deliver business value: We recommend to go over this excellent IBM article on defining reactive to go deeper into those concepts.","title":"Reactive systems"},{"location":"advantages/reactive/#commands-and-events","text":"Those two concepts are very fundamental to implement well distributed applications. Commands : represent action a user or system wants to perform. HTTP APIs pass commands. Commands are sent to a specific service and result is sent back to the caller. Events : are actions that have successfully completed. An event represents a fact. They are immutable. By looking at your solution in terms of commands and events, you focus on the behavior, workflow, instead of the structure. Events are wrapped into Messages. But Commands can also being passed via messaging and asynchronous communication. Most likely strong consistency is needed and queuing systems are used as message brokers.","title":"Commands and Events"},{"location":"advantages/reactive/#is-it-for-me","text":"We have learnt from years of point to point microservice implementations, that embrassing asynchronous communication helps a lot to support scaling, integration, coupling and failover. So adopting reactive design and implementation may look complex at first but is becoming a necessity in the long run. In e-commerce, a lot of monolithic applications were redesigned to adopt reactive manifesto characteristics to support scaling the business needs and respond to sporadic demand. In the world of big data, collecting, aggregating, applying real time analytics, decisions and AI need to scale and respond to events at the time of occurence.","title":"Is it for me?"},{"location":"advantages/reactive/#eda-and-reactive-systems","text":"The adoption of event driven microservice implementation fits well into the reactive manifesto, where most of the work presented in this git repository started by adopting Kafka as event backbone, it is too reductor to think EDA is just Kafka. EDA supports reactive systems at large, and developing event-driven microservice should use reactive libraries to support non-blocking IO and event bus for inter process communication. Also microservices is part of the game, functions / serverless are also in scope and with serverless 2.0, Knative eventing is one of the new kid in the play. The manifesto stipulates \"message driven\" while EDA is about events and commands. Events represent unmmutable data and facts about what happened, and components subscribe to those event streams. Command demands the consumer to process the content data sent and gives an answer. Both are sent via messages, and transported and managed by brokers. For sure we define event-driven implementations to cover both. And we should not be purist and opinionated about messaging versus eventing: it will not make any sense to say: you are using queue to exchange message while we produce events to topic. The following figure illustrates the combination of synchronous communication, sending commands to reactive system, supported by reactive applications link together via messaging. With messaging applications can scale horizontally giving the elastic need of the reactive manifesto. They are also more resilient as messages are kept until consumed or for a long period of time and consumer can restart from where they were in the ordered log. Reactive systems are not only exchanging messages. Sending and receiving messages must be done efficiently and Reactive promotes the use of nonblocking I/Os. Which leads to reactive programming and supporting libraries, like Vert.x, Mutiny, reactive messaging...","title":"EDA and reactive systems"},{"location":"advantages/reactive/#technology-review","text":"","title":"Technology review"},{"location":"advantages/reactive/#concurrency","text":"The following figure illustrates the traditional Java multi-threading approach to handle request and access I/Os on a two CPUs computer. When the second thread starts working on the IO the CPU is locked and the CPU yellow is supporting 2 threads (1 and 3) On public clouds, the blocking I/O approach inflates your monthly bill; on private clouds, it reduces the deployment density. Non blocking IO framework or library adopts the reactor pattern where requests are internally asynchronous events processed, in order, by an event loop running in one thread, and handlers (or callbacks) are used to process the response. The above figure and next one are coming from Clement Escoffier's book Building reactive microservice in Java . In multi CPUs, cores and threading computer, the reactors can run in parallel, with one event loop per core: With non-blocking I/O the I/O operation can be allocated to another pool and the allocation of CPU to thread is well balanced: Vert.X is the open source library to build such non-blocking I/O app, and using vertices to support scalable concurrent processor, which executes one event loop thread. vertices communicate asynchronously via an event bus.","title":"Concurrency"},{"location":"advantages/reactive/#reactive-programming","text":"Reactive programming is about observing asynchronous streams . Streams can be seen as a pipe in which events flow. We observe the events flowing\u2014such as items, failures, completion, cancellations\u2014and implement side effects. You structure your code around streams and build chains of transformation, also called pipeline. Reactive programming libraries offer countless operators that let you create, combine, filter, and transform the object emitted by streams. One important consideration is the speed of items processing by the consumer. If it is too slow compare to the producer, there will be a big proble, that can be solved efficiency by using backprassure protocol. Reactive Streams is such backpressure protocol. It defines the concept of Subscriber who requests the Publisher a certain amount of items. The consumer controls the flow, as when items are received and processed, consumers can ask for more. Producer is not strongly coupled to the consumer as they participate together to the stream processing. Producer uses a Subscription object to act as a contract between the participants. different libraries support the Reactive Streams protocol, like Mutiny, RxJava, Project Reactor...","title":"Reactive programming"},{"location":"advantages/reactive/#vertx","text":"We do not need to reintroduce Vert.X , but with the large adoption of Quarkus to develop new JVM based microservice, Vert.x is an important library to understand. The main concepts used are: An application would typically be composed of multiple vertices running in the same Vert.x instance and communicate with each other using events via the event bus . vertices remain dormant until they receive a message or event. Message handling is ideally asynchronous, messages are queued to the event bus, and control is returned to the sender Regular vertices are executed in the event loop Worker vertices are not executed on the event loop, which means they can execute blocking code Two event loops per CPU core thread No thread coordination mechanisms to manipulate a verticle state A verticle can be deployed several times as part of a container for example Event bus is used by different vertices to communicate through asynchronous message passing (JSON) (point to point, pub / sub, req / resp) We can have in memory event bus or clustered cross nodes, managed by Vert.x with a TCP protocol Quarkus HTTP support is based on a non-blocking and reactive engine (Vert.x and Netty). All the HTTP requests your application receive are handled by event loops (IO Thread) and then are routed towards the code that manages the request. Depending on the destination, it can invoke the code managing the request on a worker thread (Servlet, Jax-RS) or use the IO Thread (reactive route). (Images src: quarkus.io) The application code should be written in a non-blocking manner using SmallRye Mutiny or RsJava libraries. So when interactive with different services using kafka as an inter service communication layer the producer and consumer are handlers and the internal processing can be schematized as: Vert.x and reactive messaging applications may be combined to byild reactive systems to support the reactive manifesto: To achieve resilience and responsiveness , microservice can scale vertically using vertices inside the JVM and horizontally via pod scaling capability within a Kubernetes cluster. The inter-communication between vertices is done via event bus and managed by the Vert.x library using virtual addresses, service discovery and event bus. Internal service communication (even cross pods) and external cross service boundary are message driven . Using Kafka, they are also durable improving resilience and recovery. Kubernetes enforces part of the reactive manifesto at the container level: elasticity and resilience with automatic pod recovery and scheduling.","title":"Vert.x"},{"location":"advantages/reactive/#microprofile-reactive-messaging","text":"The MicroProfile Reactive messaging specification aims to deliver applications embracing the characteristics of reactive systems as stated by reactive manifesto. It enables non-blocking, asynchronous message passing between services, giving them the ability to scale, fail, and evolve independently. To summarize the main concepts, developer declares channels as way to get incoming or outgoing messages between CDI Java beans and connector to external message brokers: The potential matching declarations for the connector, for the above figure, may look like below: # Kafka connector to items topic mapped to the item-channel mp.messaging.incoming.item-channel.connector = smallrye-kafka mp.messaging.incoming.item-channel.topic = items For code which defines the 'outhoing' and 'incoming' message processing see this quarkus guide , the EDA quickstart code templates for producer and consumer public class OrderService { @Channel ( \"orders\" ) public Emitter < OrderEvent > eventProducer ; public OrderEntity createOrder ( OrderEntity order ) { try { // build orderPayload based on cloudevent from order entity Message < OrderEvent > record = KafkaRecord . of ( order . getOrderID (), orderPayload ); eventProducer . send ( record ); logger . info ( \"order created event sent for \" + order . getOrderID ()); } catch ( Exception e ) { e . printStackTrace (); } } JMS and message driven bean, were the messaging APIs to asynchronously communicate with other applications. They support transaction and so it is an heavier protocol to use. They do not support asynchronous IO. When building microservices, the CQRS and event-sourcing patterns provide an answer to the data sharing between microservices. Reactive Messaging can also be used as the foundation to CQRS and Event-Sourcing mechanisms. A lot of EDA repositories demonstrating EDA concepts are using microprofile 3.0 Reactive Messaging, as it simplifies the implementation to produce or consume messages to messaging middleware like Kafka. When you use @Incoming and @Outgoing annotations, the runtime framework (Open Liberty or Quarkus) creates a Reactive Streams component for each method and joins them up by matching the channel names. A simple guide from Quarkus web site with integration with Kafka. Open Liberty supports this specification implementation .","title":"MicroProfile reactive messaging"},{"location":"advantages/reactive/#mutiny","text":"Mutiny is a modern reactive programming library to provide more natural, readable reactive code. It supports asynchrony, non-blocking programming and streams, events, back-pressure and data flows. With Mutiny both Uni and Multi expose event-driven APIs: you express what you want to do upon a given event (success, failure, etc.). These APIs are divided into groups (types of operations) to make it more expressive and avoid having 100s of methods attached to a single class. This section of the product documentation goes over some examples on how to use Uni/ Multi.","title":"Mutiny"},{"location":"advantages/reactive/#amqp","text":"Advanced Message Queueing Protocol is an international standard for interoperability between messaging middlewares. IBM MQ supports AMQP client via specific AMQP channel. Clients can connect to the queue manager and send / receive messages to / from queue.","title":"AMQP"},{"location":"advantages/reactive/#knative-eventing","text":"Knative is Kubernetes based platform to develop serverless. Major value proposition is a simplified deployment syntax with automated scale-to-zero and scale-out based on HTTP load. Knative consists of the following components: Eventing - Management and delivery of events Serving - Request-driven compute that can scale to zero See the RedHat Knative cookbook for a simple tutorial.","title":"Knative eventing"},{"location":"advantages/reactive/#code-samples","text":"Vert.x kafka client Experiences writing a reactive Kafka application","title":"Code samples"},{"location":"advantages/reactive/#more","text":"Reactive Systems Explained - Book from Grace Jansen - Peter Gollmar Reactive Java Modules show how to build an event-driven, streams-optimized Kafka-based Java application. You will use the Eclipse MicroProfile Reactive Messaging API and Open Liberty to build it and then you'll learn how to test it in true-to-production environments by using containers with MicroShed testing. Resiliency discussion in IBM architecture center Clement Escoffier's book Building reactive microservice in Java","title":"More..."},{"location":"advantages/resiliency/","text":"Resiliency \u00b6 The reduction in inter-dependency between applications that is enabled in an Event-Driven architecture enables increased resiliency. If services fail, they can restart autonomously and, subsequently, recover events and replay them if needed. Their ability to self-heal means that the functionality of the whole system is less reliant on certain services being immediately available. We are detailing how consumer offset management works and how to rebuild data projection after recovery in the Kafka Consumer article . Reduced coupling between services means they do not need to have any knowledge of the services to which they produce or from whom they consume. There are a number of advantages to this. For example, even if a service goes down, events will still be produced or consumed once it has recovered, known as 'Guaranteed Delivery'. For instance, let's say we run a shipping company that operates a fleet of container ships. The containers themselves could be smart IoT devices, in that they collect data about the health of the container (temperature, position etc). At the vessel level, we can use edge computing with local event backbone to do some simple aggregations and correlations before sending those data back at regular intervals to a central onshore monitoring platform. If the ship network goes offline and the refrigerator containers can not send back the data, it can still be collected and will be sent once the service is available again. We have resilience between data centers. Here is diagram illustrating those concepts with some underlying technologies. Applications on the right, run in a data center or cloud provider, and receive aggregate data coming from the Kafka cluster running on the vessel. The topic data replication is done via Mirror Maker 2 . A second level of real time analytics could compute aggregates between all the vessels sailing over seas. If the connection is lost the mirroring will get the records when reconnecting. On the vessel level, multiple brokers ensure high availability, and replication cross broker ensures data resilience. Real time analytic components can scale horizontally, even when computing global aggregate by using kafka streams capability of Ktable and store .","title":"Resiliency"},{"location":"advantages/resiliency/#resiliency","text":"The reduction in inter-dependency between applications that is enabled in an Event-Driven architecture enables increased resiliency. If services fail, they can restart autonomously and, subsequently, recover events and replay them if needed. Their ability to self-heal means that the functionality of the whole system is less reliant on certain services being immediately available. We are detailing how consumer offset management works and how to rebuild data projection after recovery in the Kafka Consumer article . Reduced coupling between services means they do not need to have any knowledge of the services to which they produce or from whom they consume. There are a number of advantages to this. For example, even if a service goes down, events will still be produced or consumed once it has recovered, known as 'Guaranteed Delivery'. For instance, let's say we run a shipping company that operates a fleet of container ships. The containers themselves could be smart IoT devices, in that they collect data about the health of the container (temperature, position etc). At the vessel level, we can use edge computing with local event backbone to do some simple aggregations and correlations before sending those data back at regular intervals to a central onshore monitoring platform. If the ship network goes offline and the refrigerator containers can not send back the data, it can still be collected and will be sent once the service is available again. We have resilience between data centers. Here is diagram illustrating those concepts with some underlying technologies. Applications on the right, run in a data center or cloud provider, and receive aggregate data coming from the Kafka cluster running on the vessel. The topic data replication is done via Mirror Maker 2 . A second level of real time analytics could compute aggregates between all the vessels sailing over seas. If the connection is lost the mirroring will get the records when reconnecting. On the vessel level, multiple brokers ensure high availability, and replication cross broker ensures data resilience. Real time analytic components can scale horizontally, even when computing global aggregate by using kafka streams capability of Ktable and store .","title":"Resiliency"},{"location":"advantages/scalability/","text":"Scalability \u00b6 Event-Driven architectures are highly scalable. The use of an event-driven backbone allows for the addition (and removal) of consumers based on the number of messages waiting to be consumed from a topic. This is good for architectures where in-stream transformation is needed, like data science workflows. Messages can be consumed and transformed extremely fast, which is advantageous for processes where millisecond decision making is necessary. When using a system like Kafka, the data (in a given topic) is partitioned by the broker, which allows for parallel processing of the data for consumpton. Consumers are usually assigned to one partition. As well as scaling up, there is also the ability to scale down (even to zero). When scaling up and down happens autonomously, promoting energy and cost efficiency, it is referred to as 'elasticity'.","title":"Scalability"},{"location":"advantages/scalability/#scalability","text":"Event-Driven architectures are highly scalable. The use of an event-driven backbone allows for the addition (and removal) of consumers based on the number of messages waiting to be consumed from a topic. This is good for architectures where in-stream transformation is needed, like data science workflows. Messages can be consumed and transformed extremely fast, which is advantageous for processes where millisecond decision making is necessary. When using a system like Kafka, the data (in a given topic) is partitioned by the broker, which allows for parallel processing of the data for consumpton. Consumers are usually assigned to one partition. As well as scaling up, there is also the ability to scale down (even to zero). When scaling up and down happens autonomously, promoting energy and cost efficiency, it is referred to as 'elasticity'.","title":"Scalability"},{"location":"concepts/flow-architectures/","text":"** Added 1/21/2022 From the James Urquhart's book: Flow architecture and personal studies. As more and more of our businesses \u201cgo digital\u201d, the groundwork is in place to fundamentally change how real-time data is exchanged across organization boundaries. Data from different sources can be combined to create a holistic view of a business situation. Flow is networked software integration that is event-driven, loosely coupled, and highly adaptable and extensible. Value is created by interacting with the flow, and not just the data movement. Since the beginning of IT as an industry, we are digitizing and automating the exchanges of value, and we spend a lot of time and money to execute key transactions with less human intervention. However, most of the integrations we execute across organizational boundaries today are not in real time. Today, most, perhaps all\u2014digital financial transactions in the world economy still rely on batch processing at some point in the course of settlement. There is no consistent and agreed-upon mechanism for exchanging signals for immediate action across companies or industries. It is still extremely rare for a company to make real-time data available for unknown consumers to process at will. This is why modern event-driven architecture (EDA) will enable profound changes in the way companies integrate. EDAs are highly decoupled architectures, meaning there are very few mutual dependencies between the parties at both ends of the exchange. 1- Flow characteristics \u00b6 Consumer applications requests data streams through self-service interfaces, and get the data continuously. Producers maintain control of relevant information to transmit and when to transmit. Event packages information of data state changes, with timestamp and unique ID. The context included with the transmitted data allows the consumer to better understand the nature of that data. CloudEvent helps defining such context. The transmission of a series of events between two parties is called an event stream . The more streams there are from more sources, the more flow consumers will be drawn to those streams and the more experimentation may be done. Over time, organizations will find new ways to tie activities together to generate new value. Composable architectures allow the developer to assemble fine grained parts using consistent mechanisms for both inputting data and consuming the output. In contextual architectures , the environment provides specific contexts in which integration can happen. Developer must know a lot about the data that is available, the mechanism by which the data will be passed, the rules for coding and deploying the software. EDA provides a much more composable and evolutionary approach for building event and data streams. 2- Business motivations \u00b6 Do digital transformation to improve customer experiences. Customers expect their data to be used in a way that is valuable to them, not just to the vendors. Sharing data between organizations can lead to new business opportunities. This is one of the pilard of Society 5.0. The Japan government defined Society 5.0 as \"A human-centered society that balances economic advancement with the resolution of social problems by a system that highly integrates cyberspace and physical space\". Improve process automation, to drive efficiencies and profitability. The most limiting constraint in the process hides any improvements made to other steps. Finding constraints is where value stream mapping shines: it uses lead time (queue time) and actual time to do the work. EDA will help to get time stamp and data for steps in the process that are not completely in scope of a business process: may be cross business boundaries. Extract innovative value from data streams. Innovation as better solution for existing problem, or as new solution to emerging problems. To improve process time, software needs accurate data at the time to process the work. As business evolve, having a rigid protocol to get the data, impacts process time. A business will need to experiment with new data sources when they are available and potentially relevant to their business. Stream processing improves interoperability (exchange data) Innovation is not adaptation. Companies must adapt constantly just to survive, like adding features on a product to pace with competition. Digital transformation aimed at avoiding competitive disruption is not innovation. As the number of stream options grows, more and more business capabilities will be defined in terms of stream processing. This will drive developers to find easier ways to discover, connect to, and process streams. Enabler for flow adoption \u00b6 Lowering the cost of stream processing : Integration costs dominate modern IT budgets. For many integrations, the cost of creating interaction between systems is simply too high for what little value is gained. With common interfaces and protocols that enable flows, the integration cost will be lower and people will find new uses for streaming that will boost the overall demand for streaming technologies. The Jevons paradox at work Increasing the flexibility in composing data flows : \"pipe\" data streams from one processing system to another through common interfaces and protocols. Creating and utilizing a rich market ecosystem around key streams . The equities markets have all moved entirely to electronic forms of executing their marketplaces. Health-care data streams for building services around patient data. Refrigerators streaming data to grocery delivery services. Flow must be secure (producers maintain control over who can access their events), agile (change schema definitions), timely (Data must arrive in a time frame that is appropriate for the context to which it is being applied), manageable and retain a memory of its past. Serverless, stream processing, machine learning, will create alternative to batch processing. 3- Market \u00b6 SOA has brought challenges for adoption and scaling. Many applications have their own interfaces and even protocols to expose their functionality, so most integrations need protocol and data model translations. The adoption of queues and adaptors to do data and protocol translation was a scalable solution. Extending this central layer of adaptation was the Enterprise Service Bus, with intelligent pipes / flows. Message queues and ESBs are important to the development of streaming architectures but to support scalability and address complexity more decoupling is needed between producers and consumers. For IoT MQTT is the standard for messaging protocols in a lightweight pub/sub transport protocol. MQTT supports 3 service levels: 0 - at most once, 1- at least once, 2 - exactly once. It allows for messaging between device to cloud and cloud to device. It supports for persistent sessions reduces the time to reconnect the client with the broker. The MQTT broker manages a list of topics, which enable it to identify groups of subscribers interested in a collection of messages. For event processing three type of engines: Functions (including low-code or no-code processors): AWS lambda, Knative eventing, Flink, Storm. Mendix and Vantiq have event-driven low code platform. log-based event streaming platforms : Apache Kafka, Apache Pulsar, AWS Kinesis, and Microsoft Azure Event Hubs. Topic becomes a system of record, as an event-sourcing pattern implementation. real-time stateful systems : Digital twins are software agents supporting the problem domain in a stateful manner. Behavior is supported by code or rules, and relationship between agents. Agents can monitor the overall system state. Swim.ai builds its model dynamically from the event stream and provides built-in machine learning capabilities that enable both continuous learning and high performance model execution. Mainstream adoption of flow itself will be five to ten years from now (2020). Flow will have to prove that it meets security criteria for everything from electronic payments, to health-care data, to classified information. The CNCF\u2019s CloudEvents specification, for instance, strongly suggests payloads be encrypted. There is no single approach to defining an event with encryption explicitly supported that can be read by any event-consuming application (MQTT, AMQP, have different encryption and TLS add more for TCP connection). Consumers need assurances that the data they receive in an event is valid and accurate, a practice known as data provenance . Data provenance is defined as \u201ca record of the inputs, entities, systems, and processes that influence data of interest, providing a historical record of the data and its origins\" Provenance has to be maintained by the producer as a checksum number created by parsing the event data, and encrypted by the producer's key. CloudEvent has metadata about the message. When sent to Kafka they are immutable record. Now the traceability of the consumers in kafka world is a major challenge. Blockchain may also be used to track immutable record with network parties attest its accuracy. Applying the concept of data loose value over time, it is important to act on data as early as possible, close to creation time. After a period of time data becomes less valuable. Two time factors are important in this data processing: latency (time to deliver data to consumers) and retention (time to keep data). For latency try to reduce the number of network segment between producer and consumers. Considering edge computing as a way to bring event processing close to the source. The event processing add time to the end to end latency. Considering constraining the processing time frame. Retention is a problem linked to the business requirements, and we need to assess for each topic how long an event is still valuable for the consumers. Not keeping enough events will impact correctness of consumer state, projection views... keeping for too long, increase the cost of storage, but also the time to rebuild data projection. Finally, producers will want to trust that only authorized consumers are using the events they produce. Also it may be possible to imagine a way to control the intellectual property of the data so producer can keep its ownership. Data consumption should be done via payment like we do with music subscription. Flow patterns: \u00b6 Collector pattern \u00b6 The Collector pattern is a pattern in which a single consumer subscribes to topics from multiple producers. Distributor pattern \u00b6 Each event in a stream is distributed to multiple consumers. It could be a hard problem to solve when doing it across geographically distributed systems. Edge computing can be used to distribute streaming endpoints closer to the consumers that need those streams. Alternate is to move the event processing close to the source. For many Distributor use cases, partitioning the data by region is probably smart, and flow interfaces will need to take this into account. Signal pattern \u00b6 The Signal pattern is a general pattern that represents functions that exchange data between actors based on a distinct function or process, in can be seen as a traffic cop. It supports multiple producers and multiple consumers. The signal pattern is supported by multiple event processing each handling one aspect of the event processing. Stream processing may route event streams between several distributed edge computing services as well as core shared services, but then we need management layer to get global view of the systems. They need to be integrated into observability tool. But the \"single pane of glass\" is often a lure as distributed systems require distributed decision-making. More local solutions are more agile, flexible and better address local problems for improved resilience. One of the challenge of complex adaptive systems is that any agent participating in the system has difficulty seeing how the system as a whole operates, because of its limited connections to other neighbor agents. Facilitator pattern \u00b6 A specialized form of Signal pattern, facilitator is a \"broker\" to match producers' events to consumers' demands. It is like matching sellers with buyers. 4- Identifying flow in your business \u00b6 The classical usee case categories: Addressing and discovery : In modern complex systems environments, multiple systems need to be informed of the new entity, be able to utilize to assign work to it. Addressing and discovery happens across organizational boundaries (for example in real-time inventory SKU is used to identify item for both supplier and retailers). To seek such use cases, look at tracking problems like who or what is involved in a problem domain that is difficult to scale. With event stream centric approach, A&D is done via a registry service used by new agents to indicate their existence, and the service publishes an event to a topic to broadcast the information about the new agent. A second option is to use a discovery service to watch specific event stream for certain transmissions that indicate the presence of an agent. Swim.ai continuously process and analyze streaming data in concert with contextual data to inform business-critical, operational decisions. See also SwimOS or Apache Flink. Command and control : sources are connected to key decision-making and action-taking services to complete a business task. So they are everywhere in any business. A typical example of such use case, is the Supervisory Control And Data Acquisition, used in manufacturing, or energy production. Try to ask: where does the organization depend on timely responses to changes in state? C&C can be supported by centralized control with events come from multiple sources to stateless or stateful services, to apply real-time analysis and decision-making algorithms to those streams. Output events are published to sinks for future processing. Scaling with a centralized control approach is not straightforward, as getting the right events to the right processing instances can be a challenge. Also when we need the compute global aggregates by looking at the state of various agents in the systems is more complex, as it needs to integrate with stateful stream processing. Actions can be triggered by state changes, triggers that fire at specific times, or even API requests from other applications or services. An alternate is to use distributed control, like applying the decision-making logic at the edge. Query and observability: querying or monitoring individual agents or specific groups of agents. The problem is to locate the right agent target of the query, and get current state or history from that agent. Telemetry and analytics: focuses on understanding systems behavior, and get real-time big data insights (e.g. Click streams). Need to assess which insights require understanding the emerging behavior of a system of agents emitting vast amounts of data. Interesting presentations: Voxxed Athens 2018 - Eventing, Serverless, and the Extensible Enterprise by Clemens Vasters 5- Model Flow \u00b6 Use Event storming to build a timeline of events that are required to complete a complex task, and to get and understanding of the people, systems, commands and policies that affect the event flow. The Event Storming process is a highly interactive endeavor that :brings subject matter experts together to identify the flow of events in a process or system Identify the business activities that you would like to model in terms of event flow Begin by asking the group to identify the events that are interesting and/or required for that business activity Place events along a timeline from earliest action to latest action Capture: The real-world influences on the flow, such as the human users or external systems that produce or consume events What commands may initiate an event What policies are activated when an event takes place. A policy usually initiates a new command. Events always result in a policy action unless the event is generating output. What are the outputs from the event flow. When designing the solution assess: When the event is simply to broadcast facts for consumption by interested parties. The producer contract is simply to promise to send events as soon as they are available. If consumers can come and go, and experiment with the consumption of a stream with little risk of consequences if they choose not to continue When event is part of an interaction around an intent, requiring a conversation with the consumer Is the event a stand-alone communication, discrete, or is it only useful in a context that includes a series of events. Series applications are where log-based queueing shines Is the processing involve one simple action per event, or is there a group of related actions, a workflow, required to complete processing When building a flow-ready application for messaging, the \u201ctrunk-limb-branch-leaf\u201d pattern is a critical tool to consider: use edge computing to distribute decision-making close to the related groups of agents, computing local aggregates, and propagate to larger more central flows. Using messaging middleware to manage interaction between agents, to isolate message distribution to just the needed servers and agents, and propagate aggregates to the trunk, greatly reducing traffic between the original agents and the core. Another consideration is to assess if the consumers need to filter events from a unique topic before doing its own processing, in this case the event payload may include metadata and URL to get the payload. If the metadata indicates an action is required, the consumer can then call the data retrieval URL. Whether or not you include payload data depends a bit on the volume of events being published and the security and latency requirements of your application. Log-based queues can play the role of \u201csystem of record\u201d for both event values and sequence, especially for systems that need both the most recent event and an understanding of the recent history of events received For single action processing, serverless, knative eventing are technologies to consider. Solution needs to route events to the appropriate processor. But if your event processing needs require maintaining accurate state for the elements sending events then stateful streaming platform are better fit. For workflow, modern solutions, simplify creating and managing process definitions independent of the actions taken in that process. It supports for stepping an event through multiple interdependent actions. Workflow may require to wait for another related event occurs or a human completes his action. 6- Today landscape \u00b6 Standards are important for flow: TLS, WebSockets, and HTTP from IETF, MQTT and AMQP from OASIS, CloudEvents and the Serverless Working Group from CNCF Open sources projects : Apache Kafka and Apache Pulse for log-based queueing Apache Beam, Flink, Heron, Nifi, Samza, and Storm for stream processing Apache Druid as a \u201cstream-native\u201d database gRPC may play a key role in any future flow interface NATS.io, a cloud-native messaging platform Argo, a Kubernetes-based workflow manager that theoretically could act as the core of an event-driven process automation bus Opportunities: Data provenance and security for payloads passed between disparate parties Tracking event data distribution across the world wide flow. Where does the data generated by an event end up being consumed or processed? Platforms for coordinating event processing, aggregation, and synchronization between core data center event processors, edge computing environments, and end-user or IoT devices Monetization mechanisms for all types of event and messaging streams The adoption of a technology is not the delivery that makes it valuable, but the ecosystem that consumes it. Look at existing streams and determine how to add value for the consumers of that stream. Can you automate valuable insights and analytics in real time for customers with shared needs? Would it be possible to recast the stream in another format for an industry that is currently using a different standard to consume that form of data?","title":"Flow Architecture"},{"location":"concepts/flow-architectures/#1-flow-characteristics","text":"Consumer applications requests data streams through self-service interfaces, and get the data continuously. Producers maintain control of relevant information to transmit and when to transmit. Event packages information of data state changes, with timestamp and unique ID. The context included with the transmitted data allows the consumer to better understand the nature of that data. CloudEvent helps defining such context. The transmission of a series of events between two parties is called an event stream . The more streams there are from more sources, the more flow consumers will be drawn to those streams and the more experimentation may be done. Over time, organizations will find new ways to tie activities together to generate new value. Composable architectures allow the developer to assemble fine grained parts using consistent mechanisms for both inputting data and consuming the output. In contextual architectures , the environment provides specific contexts in which integration can happen. Developer must know a lot about the data that is available, the mechanism by which the data will be passed, the rules for coding and deploying the software. EDA provides a much more composable and evolutionary approach for building event and data streams.","title":"1- Flow characteristics"},{"location":"concepts/flow-architectures/#2-business-motivations","text":"Do digital transformation to improve customer experiences. Customers expect their data to be used in a way that is valuable to them, not just to the vendors. Sharing data between organizations can lead to new business opportunities. This is one of the pilard of Society 5.0. The Japan government defined Society 5.0 as \"A human-centered society that balances economic advancement with the resolution of social problems by a system that highly integrates cyberspace and physical space\". Improve process automation, to drive efficiencies and profitability. The most limiting constraint in the process hides any improvements made to other steps. Finding constraints is where value stream mapping shines: it uses lead time (queue time) and actual time to do the work. EDA will help to get time stamp and data for steps in the process that are not completely in scope of a business process: may be cross business boundaries. Extract innovative value from data streams. Innovation as better solution for existing problem, or as new solution to emerging problems. To improve process time, software needs accurate data at the time to process the work. As business evolve, having a rigid protocol to get the data, impacts process time. A business will need to experiment with new data sources when they are available and potentially relevant to their business. Stream processing improves interoperability (exchange data) Innovation is not adaptation. Companies must adapt constantly just to survive, like adding features on a product to pace with competition. Digital transformation aimed at avoiding competitive disruption is not innovation. As the number of stream options grows, more and more business capabilities will be defined in terms of stream processing. This will drive developers to find easier ways to discover, connect to, and process streams.","title":"2- Business motivations"},{"location":"concepts/flow-architectures/#enabler-for-flow-adoption","text":"Lowering the cost of stream processing : Integration costs dominate modern IT budgets. For many integrations, the cost of creating interaction between systems is simply too high for what little value is gained. With common interfaces and protocols that enable flows, the integration cost will be lower and people will find new uses for streaming that will boost the overall demand for streaming technologies. The Jevons paradox at work Increasing the flexibility in composing data flows : \"pipe\" data streams from one processing system to another through common interfaces and protocols. Creating and utilizing a rich market ecosystem around key streams . The equities markets have all moved entirely to electronic forms of executing their marketplaces. Health-care data streams for building services around patient data. Refrigerators streaming data to grocery delivery services. Flow must be secure (producers maintain control over who can access their events), agile (change schema definitions), timely (Data must arrive in a time frame that is appropriate for the context to which it is being applied), manageable and retain a memory of its past. Serverless, stream processing, machine learning, will create alternative to batch processing.","title":"Enabler for flow adoption"},{"location":"concepts/flow-architectures/#3-market","text":"SOA has brought challenges for adoption and scaling. Many applications have their own interfaces and even protocols to expose their functionality, so most integrations need protocol and data model translations. The adoption of queues and adaptors to do data and protocol translation was a scalable solution. Extending this central layer of adaptation was the Enterprise Service Bus, with intelligent pipes / flows. Message queues and ESBs are important to the development of streaming architectures but to support scalability and address complexity more decoupling is needed between producers and consumers. For IoT MQTT is the standard for messaging protocols in a lightweight pub/sub transport protocol. MQTT supports 3 service levels: 0 - at most once, 1- at least once, 2 - exactly once. It allows for messaging between device to cloud and cloud to device. It supports for persistent sessions reduces the time to reconnect the client with the broker. The MQTT broker manages a list of topics, which enable it to identify groups of subscribers interested in a collection of messages. For event processing three type of engines: Functions (including low-code or no-code processors): AWS lambda, Knative eventing, Flink, Storm. Mendix and Vantiq have event-driven low code platform. log-based event streaming platforms : Apache Kafka, Apache Pulsar, AWS Kinesis, and Microsoft Azure Event Hubs. Topic becomes a system of record, as an event-sourcing pattern implementation. real-time stateful systems : Digital twins are software agents supporting the problem domain in a stateful manner. Behavior is supported by code or rules, and relationship between agents. Agents can monitor the overall system state. Swim.ai builds its model dynamically from the event stream and provides built-in machine learning capabilities that enable both continuous learning and high performance model execution. Mainstream adoption of flow itself will be five to ten years from now (2020). Flow will have to prove that it meets security criteria for everything from electronic payments, to health-care data, to classified information. The CNCF\u2019s CloudEvents specification, for instance, strongly suggests payloads be encrypted. There is no single approach to defining an event with encryption explicitly supported that can be read by any event-consuming application (MQTT, AMQP, have different encryption and TLS add more for TCP connection). Consumers need assurances that the data they receive in an event is valid and accurate, a practice known as data provenance . Data provenance is defined as \u201ca record of the inputs, entities, systems, and processes that influence data of interest, providing a historical record of the data and its origins\" Provenance has to be maintained by the producer as a checksum number created by parsing the event data, and encrypted by the producer's key. CloudEvent has metadata about the message. When sent to Kafka they are immutable record. Now the traceability of the consumers in kafka world is a major challenge. Blockchain may also be used to track immutable record with network parties attest its accuracy. Applying the concept of data loose value over time, it is important to act on data as early as possible, close to creation time. After a period of time data becomes less valuable. Two time factors are important in this data processing: latency (time to deliver data to consumers) and retention (time to keep data). For latency try to reduce the number of network segment between producer and consumers. Considering edge computing as a way to bring event processing close to the source. The event processing add time to the end to end latency. Considering constraining the processing time frame. Retention is a problem linked to the business requirements, and we need to assess for each topic how long an event is still valuable for the consumers. Not keeping enough events will impact correctness of consumer state, projection views... keeping for too long, increase the cost of storage, but also the time to rebuild data projection. Finally, producers will want to trust that only authorized consumers are using the events they produce. Also it may be possible to imagine a way to control the intellectual property of the data so producer can keep its ownership. Data consumption should be done via payment like we do with music subscription.","title":"3- Market"},{"location":"concepts/flow-architectures/#flow-patterns","text":"","title":"Flow patterns:"},{"location":"concepts/flow-architectures/#collector-pattern","text":"The Collector pattern is a pattern in which a single consumer subscribes to topics from multiple producers.","title":"Collector pattern"},{"location":"concepts/flow-architectures/#distributor-pattern","text":"Each event in a stream is distributed to multiple consumers. It could be a hard problem to solve when doing it across geographically distributed systems. Edge computing can be used to distribute streaming endpoints closer to the consumers that need those streams. Alternate is to move the event processing close to the source. For many Distributor use cases, partitioning the data by region is probably smart, and flow interfaces will need to take this into account.","title":"Distributor pattern"},{"location":"concepts/flow-architectures/#signal-pattern","text":"The Signal pattern is a general pattern that represents functions that exchange data between actors based on a distinct function or process, in can be seen as a traffic cop. It supports multiple producers and multiple consumers. The signal pattern is supported by multiple event processing each handling one aspect of the event processing. Stream processing may route event streams between several distributed edge computing services as well as core shared services, but then we need management layer to get global view of the systems. They need to be integrated into observability tool. But the \"single pane of glass\" is often a lure as distributed systems require distributed decision-making. More local solutions are more agile, flexible and better address local problems for improved resilience. One of the challenge of complex adaptive systems is that any agent participating in the system has difficulty seeing how the system as a whole operates, because of its limited connections to other neighbor agents.","title":"Signal pattern"},{"location":"concepts/flow-architectures/#facilitator-pattern","text":"A specialized form of Signal pattern, facilitator is a \"broker\" to match producers' events to consumers' demands. It is like matching sellers with buyers.","title":"Facilitator pattern"},{"location":"concepts/flow-architectures/#4-identifying-flow-in-your-business","text":"The classical usee case categories: Addressing and discovery : In modern complex systems environments, multiple systems need to be informed of the new entity, be able to utilize to assign work to it. Addressing and discovery happens across organizational boundaries (for example in real-time inventory SKU is used to identify item for both supplier and retailers). To seek such use cases, look at tracking problems like who or what is involved in a problem domain that is difficult to scale. With event stream centric approach, A&D is done via a registry service used by new agents to indicate their existence, and the service publishes an event to a topic to broadcast the information about the new agent. A second option is to use a discovery service to watch specific event stream for certain transmissions that indicate the presence of an agent. Swim.ai continuously process and analyze streaming data in concert with contextual data to inform business-critical, operational decisions. See also SwimOS or Apache Flink. Command and control : sources are connected to key decision-making and action-taking services to complete a business task. So they are everywhere in any business. A typical example of such use case, is the Supervisory Control And Data Acquisition, used in manufacturing, or energy production. Try to ask: where does the organization depend on timely responses to changes in state? C&C can be supported by centralized control with events come from multiple sources to stateless or stateful services, to apply real-time analysis and decision-making algorithms to those streams. Output events are published to sinks for future processing. Scaling with a centralized control approach is not straightforward, as getting the right events to the right processing instances can be a challenge. Also when we need the compute global aggregates by looking at the state of various agents in the systems is more complex, as it needs to integrate with stateful stream processing. Actions can be triggered by state changes, triggers that fire at specific times, or even API requests from other applications or services. An alternate is to use distributed control, like applying the decision-making logic at the edge. Query and observability: querying or monitoring individual agents or specific groups of agents. The problem is to locate the right agent target of the query, and get current state or history from that agent. Telemetry and analytics: focuses on understanding systems behavior, and get real-time big data insights (e.g. Click streams). Need to assess which insights require understanding the emerging behavior of a system of agents emitting vast amounts of data. Interesting presentations: Voxxed Athens 2018 - Eventing, Serverless, and the Extensible Enterprise by Clemens Vasters","title":"4- Identifying flow in your business"},{"location":"concepts/flow-architectures/#5-model-flow","text":"Use Event storming to build a timeline of events that are required to complete a complex task, and to get and understanding of the people, systems, commands and policies that affect the event flow. The Event Storming process is a highly interactive endeavor that :brings subject matter experts together to identify the flow of events in a process or system Identify the business activities that you would like to model in terms of event flow Begin by asking the group to identify the events that are interesting and/or required for that business activity Place events along a timeline from earliest action to latest action Capture: The real-world influences on the flow, such as the human users or external systems that produce or consume events What commands may initiate an event What policies are activated when an event takes place. A policy usually initiates a new command. Events always result in a policy action unless the event is generating output. What are the outputs from the event flow. When designing the solution assess: When the event is simply to broadcast facts for consumption by interested parties. The producer contract is simply to promise to send events as soon as they are available. If consumers can come and go, and experiment with the consumption of a stream with little risk of consequences if they choose not to continue When event is part of an interaction around an intent, requiring a conversation with the consumer Is the event a stand-alone communication, discrete, or is it only useful in a context that includes a series of events. Series applications are where log-based queueing shines Is the processing involve one simple action per event, or is there a group of related actions, a workflow, required to complete processing When building a flow-ready application for messaging, the \u201ctrunk-limb-branch-leaf\u201d pattern is a critical tool to consider: use edge computing to distribute decision-making close to the related groups of agents, computing local aggregates, and propagate to larger more central flows. Using messaging middleware to manage interaction between agents, to isolate message distribution to just the needed servers and agents, and propagate aggregates to the trunk, greatly reducing traffic between the original agents and the core. Another consideration is to assess if the consumers need to filter events from a unique topic before doing its own processing, in this case the event payload may include metadata and URL to get the payload. If the metadata indicates an action is required, the consumer can then call the data retrieval URL. Whether or not you include payload data depends a bit on the volume of events being published and the security and latency requirements of your application. Log-based queues can play the role of \u201csystem of record\u201d for both event values and sequence, especially for systems that need both the most recent event and an understanding of the recent history of events received For single action processing, serverless, knative eventing are technologies to consider. Solution needs to route events to the appropriate processor. But if your event processing needs require maintaining accurate state for the elements sending events then stateful streaming platform are better fit. For workflow, modern solutions, simplify creating and managing process definitions independent of the actions taken in that process. It supports for stepping an event through multiple interdependent actions. Workflow may require to wait for another related event occurs or a human completes his action.","title":"5- Model Flow"},{"location":"concepts/flow-architectures/#6-today-landscape","text":"Standards are important for flow: TLS, WebSockets, and HTTP from IETF, MQTT and AMQP from OASIS, CloudEvents and the Serverless Working Group from CNCF Open sources projects : Apache Kafka and Apache Pulse for log-based queueing Apache Beam, Flink, Heron, Nifi, Samza, and Storm for stream processing Apache Druid as a \u201cstream-native\u201d database gRPC may play a key role in any future flow interface NATS.io, a cloud-native messaging platform Argo, a Kubernetes-based workflow manager that theoretically could act as the core of an event-driven process automation bus Opportunities: Data provenance and security for payloads passed between disparate parties Tracking event data distribution across the world wide flow. Where does the data generated by an event end up being consumed or processed? Platforms for coordinating event processing, aggregation, and synchronization between core data center event processors, edge computing environments, and end-user or IoT devices Monetization mechanisms for all types of event and messaging streams The adoption of a technology is not the delivery that makes it valuable, but the ecosystem that consumes it. Look at existing streams and determine how to add value for the consumers of that stream. Can you automate valuable insights and analytics in real time for customers with shared needs? Would it be possible to recast the stream in another format for an industry that is currently using a different standard to consume that form of data?","title":"6- Today landscape"},{"location":"concepts/events-versus-messages/","text":"Info Updated 6/18/2022 Consider queue system. like IBM MQ, for: Exactly once delivery, and to participate into two phase commit transaction Asynchronous request / reply communication: the semantic of the communication is for one component to ask a second command to do something on its data. This is a command pattern with delay on the response. Recall messages in queue are kept until consumer(s) got them. Consider streaming system, like Kafka, as pub/sub and persistence system for: Publish events as immutable facts of what happened in an application Get continuous visibility of the data Streams Keep data once consumed, for future consumers, for replay-ability Scale horizontally the message consumption Events and Messages \u00b6 There is a long history of messaging in IT systems. You can easily see an event-driven solution and events in the context of messaging systems and messages. However, there are different characteristics that are worth considering: Messaging: Messages transport a payload and messages are persisted until consumed. Message consumers are typically directly targeted and related to the producer who cares that the message has been delivered and processed. Events: Events are persisted as a replayable stream history. Event consumers are not tied to the producer. An event is a record of something that has happened and so can't be changed. (You can't change history.) Messaging versus event streaming \u00b6 We recommend reading this article and this one , to get insight on messaging (focusing on operations / actions to be performed by a system or service) versus events (focusing on the state / facts of a system with no knowledge of the downstream processing). To summarize messaging (like MQ) are to support: Transient Data: data is only stored until a consumer has processed the message, or it expires. Request / reply most of the time. Targeted reliable delivery: targeted to the entity that will process the request or receive the response. Reliable with transaction support. Time Coupled producers and consumers: consumers can subscribe to queue, but message can be remove after a certain time or when all subscribers got message. The coupling is still loose at the data model level and interface definition level. For events: Stream History: consumers are interested in historic events, not just the most recent. Scalable Consumption: A single event is consumed by many consumers with limited impact as the number of consumers grow. Immutable Data Loosely coupled / decoupled producers and consumers: strong time decoupling as consumer may come at anytime. Some coupling at the message definition level, but schema management best practices and schema registry reduce frictions. See also the MQ in an event-driven solution context article See this code (Store sale simulator) to produce messages to different middleware: RabbitMQ, IBM MQ or Kafka.","title":"Event streaming versus Queuing"},{"location":"concepts/events-versus-messages/#events-and-messages","text":"There is a long history of messaging in IT systems. You can easily see an event-driven solution and events in the context of messaging systems and messages. However, there are different characteristics that are worth considering: Messaging: Messages transport a payload and messages are persisted until consumed. Message consumers are typically directly targeted and related to the producer who cares that the message has been delivered and processed. Events: Events are persisted as a replayable stream history. Event consumers are not tied to the producer. An event is a record of something that has happened and so can't be changed. (You can't change history.)","title":"Events and Messages"},{"location":"concepts/events-versus-messages/#messaging-versus-event-streaming","text":"We recommend reading this article and this one , to get insight on messaging (focusing on operations / actions to be performed by a system or service) versus events (focusing on the state / facts of a system with no knowledge of the downstream processing). To summarize messaging (like MQ) are to support: Transient Data: data is only stored until a consumer has processed the message, or it expires. Request / reply most of the time. Targeted reliable delivery: targeted to the entity that will process the request or receive the response. Reliable with transaction support. Time Coupled producers and consumers: consumers can subscribe to queue, but message can be remove after a certain time or when all subscribers got message. The coupling is still loose at the data model level and interface definition level. For events: Stream History: consumers are interested in historic events, not just the most recent. Scalable Consumption: A single event is consumed by many consumers with limited impact as the number of consumers grow. Immutable Data Loosely coupled / decoupled producers and consumers: strong time decoupling as consumer may come at anytime. Some coupling at the message definition level, but schema management best practices and schema registry reduce frictions. See also the MQ in an event-driven solution context article See this code (Store sale simulator) to produce messages to different middleware: RabbitMQ, IBM MQ or Kafka.","title":"Messaging versus event streaming"},{"location":"concepts/fit-to-purpose/","text":"Updated 06/18/2022 In this note we want to list some of the main criteria to consider and assess during an event-driven architecture establishment work or during the continuous application governance. This is not fully exhaustive, but give good foundations for analysis and study. Fit for purpose practices should be done under a bigger program about application development governance and data governance. Cloud native applications \u00b6 With the adoption of cloud native and microservice applications (the 12 factors app), the followings need to be addressed: Responsiveness with elastic scaling and resilience to failure. Which leads to adopt the ' reactive manifesto' and consider messaging as a way to communicate between apps. Elastic also may lead to multi-cloud deployments. Address data sharing using a push model to improve decoupling, and performance. Instead of having each service using REST endpoints to pull the data from other services, each service pushes the change to their main business entity state to a event backbone. Each future service in need for those data, pulls from the messaging system. Adopting common patterns like command query responsibility seggregation to help implementing complex queries, joining different business entities owned by different microservices, event sourcing to build logs of what happend, transactional outbox and SAGA for long running transaction. Addressing data eventual consistency to propagate change to other components versus ACID transaction. Support \"always-on\" approach with the deployment to multiple data centers (at least three) being active/active and being able to propagate data in all data centers. Supporting all or part of those requirements will lead to the adoption of event-driven microservices and architecture. Motivation for data streaming \u00b6 The central value propositions of data stream are to: lower the cost of integrating via event streams, use event streams to signal state changes in near-real time replay the past to build data projection Applying the concept of data loose value over time, it is important to act on data as early as possible, close to creation time. After a period of time data becomes less valuable. Two time factors are important in this data processing: latency (time to deliver data to consumers) retention (time to keep data). For latency try to reduce the number of network segment between producer and consumers. Considering edge computing as a way to bring event processing close to the source. The event processing adds time to the end to end latency. Considering constraining the processing time frame. Retention is a problem linked to the business requirements, and we need to assess for each topic how long an event is still valuable for the consumers. Not keeping enough events will impact correctness of consumer state, projection views... keeping for too long, increase the cost of storage, but also the time to rebuild data projection. Modern data pipeline \u00b6 As new business applications need to react to events in real time, the adoption of event backbone is really part of the IT toolbox. Modern IT architecture encompasses the adoption of new data hub, where all the data about a 'customer', for example, is accessible in one event backbone. Therefore, it is natural to assess the data movement strategy and assess how to offload some of those ETL jobs running at night, by adopting real time data ingestion. We detailed the new architecture in this modern data lake article, so from a fit for purpose point of view, we need to assess the scope of existing ETL jobs, and refector to streaming logic that can be incorporated into different logs/ topics. With Event Backbone like Kafka, any consumer can join the data log consumption at any point of time, within the retention period. By moving the ETL logic to a streaming application, we do not need to wait for the next morning to get important metrics. MQ Versus Kafka \u00b6 We already addressed the differences between Queueing and Streaming in this chapter . Now in term of technologies we can quickly highlight the followings: Kafka characteristics \u00b6 Keep message for long period of time, messages are not deleted once consumed Suited for high volume and low latency processing Support pub/sub model only Messages are ordered in a topic/partition but not cross partitions Stores and replicates events published to a topic, remove on expired period or on disk space constraint Messages are removed from file system independent of applications Topic can have multiple partitions to make consumer processing parallel. Not supporting two phase commits / XA transaction, but message can be produced with local transaction Multi-region architecture requires data replication across regions with Mirror Maker 2 Applications (producers, consumers, or streaming) are going to a central cluster. Cluster can support multi availability zones But also support extended cluster to go over different regions if those regions have low latency network Scales horizontally, by adding more nodes non-standard API but rich library to support the main programming language. But support also HTTP bridge or proxy to get message sent via HTTP when there is a problem on the broker it takes a lot of time to recover and it impacts all consumers Kafka is easy to setup with kubernetes deployment with real operator, is more difficult to manage with bare metal deployment. Cluster and topics definition can be managed with Gitops and automatically instantiated in new k8s cluster Consumer and producers are build and deployed with simple CI/CD pipeline MQ characteristics \u00b6 Best suited for point-to-point communication Supports horizontal scaling and high volume processing Supports event mesh , where Brokers can be deployed in different environments and serve the async applications locally, improving communication, performance, placement and scalability. The local transaction supports to write or read a message from a queue is a strength for scalability as if a consumer is not able to process the messagem another one will do. In Kafka, one partition is assigned to a consumer, and may be reassigned to a new consumer after failure, which leads to a very costly work of partition rebalancing. Participates to two-phase commit transaction Exactly one delivery with strong consistency Integrate with Mainframes: transactional applications on mainframe Support JMS for JEE applications Support AMQP for lighter protocol Messages are removed after consumption, they stayed persisted until consumed by all subscribers Strong coupling with subscribers, producer knows its consumers Supports MQ brokers in cluster with leader and followers. Support replication between brokers Support message priority Support dynamic queue creation. Typical case it replay to queue. Support much more queue per broker so it is easier to scale. Easily containerized and managed with Kubernetes operators. Direct product feature comparison \u00b6 Kafka IBM MQ Kafka is a pub/sub engine with streams and connectors MQ is a queue,or pub/sub engine All topics are persistent Queues and topics can be persistent or non persistent All subscribers are durable Subscribers can be durable or non durable Adding brokers to requires little work (changing a configuration file) Adding QMGRs requires some work (Add the QMGRs to the cluster, add cluster channels. Queues and Topics need to be added to the cluster.) Topics can be spread across brokers (partitions) with a command Queues and topics can be spread across a cluster by adding them to clustered QMGRs Producers and Consumers are aware of changes made to the cluster All MQ clients require a CCDT file to know of changes if not using a gateway QMGR Can have n number of replication partitions Can have 2 replicas (RDQM) of a QMGR, Multi Instance QMGRs Simple load balancing Load balancing can be simple or more complex using weights and affinity Can reread messages Cannot reread messages that have been already processed All clients connect using a single connection method MQ has Channels which allow different clients to connect, each having the ability to have different security requirements Data Streams processing built in, using Kafka topic for efficiency Stream processing is not built in, but using third party libraries, like MicroProfile Reactive Messaging, ReactiveX, etc. Has connection security, authentication security, and ACLs (read/write to Topic) Has connection security, channel security, authentication security, message security/encryption, ACLs for each Object, third party plugins (Channel Exits) Built on Java, so can run on any platform that support Java 8+ Latest native on AIX, IBM i, Linux systems, Solaris, Windows, z/OS, run as Container Monitoring by using statistics provided by Kafka CLI, open source tools, Prometheus Monitoring using PCF API, MQ Explorer, MQ CLI (runmqsc), Third Party Tools (Tivoli, CA APM, Help Systems, Open Source, etc) Migrating from MQ to Kafka \u00b6 When the real conditions as listed above are met, architects may assess if it makes sense to migrate MQ application to Kafka. Most of the time the investment is not justified. Modern MQ supports the same DevOps and deployment pattern as other cloud native applications. JEE or mainframe applications use MQ in transaction to avoid duplicate messages or loss of messages. Supporting exactly once delivery in Kafka needs some configuration and participation of producer and consumers: far more complex to implement. We recommend adopting the two messaging capabilities for any business applications. Kafka Streams vs Apache Flink \u00b6 Once we have setup data streams, we need technology to support near real-time analytics and complex event processing. Historically, analytics performed on static data was done using batch reporting techniques. However, if insights have to be derived in near real-time, event-driven architectures help to analyse and look for patterns within events. Apache Flink (2016) is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. It is considered to be superior to Apache Spark and Hadoop. It supports batch and graph processing and complex event processing. The major stream processing features offered by Flink are: Support for event time and out of order streams: use event time for consistent results Consistency, fault tolerance, and high availability: guarantees consistent state updates in the presence of failures and consistent data movement between selected sources and sinks Low latency and high throughput: tune the latency-throughput trade off, making the system suitable for both high-throughput data ingestion and transformations, as well as ultra low latency (millisecond range) applications. Expressive and easy-to-use APIs in Scala and Java: map, reduce, join, with window, split,... Easy to implement the business logic using Function. Support for sessions and unaligned windows: Flink completely decouples windowing from fault tolerance, allowing for richer forms of windows, such as sessions. Connectors and integration points: Kafka, Kinesis, Queue, Database, Devices... Developer productivity and operational simplicity: Start in IDE to develop and deploy and deploy to Kubernetes, Yarn , Mesos or containerized Support batch processing Includes Complex Event Processing capabilities Here is simple diagram of Flink architecture from the Flink web site: See this technology summary . See also this article from Confluent about comparing Flink with Kafka Streams.","title":"Fit for purpose"},{"location":"concepts/fit-to-purpose/#cloud-native-applications","text":"With the adoption of cloud native and microservice applications (the 12 factors app), the followings need to be addressed: Responsiveness with elastic scaling and resilience to failure. Which leads to adopt the ' reactive manifesto' and consider messaging as a way to communicate between apps. Elastic also may lead to multi-cloud deployments. Address data sharing using a push model to improve decoupling, and performance. Instead of having each service using REST endpoints to pull the data from other services, each service pushes the change to their main business entity state to a event backbone. Each future service in need for those data, pulls from the messaging system. Adopting common patterns like command query responsibility seggregation to help implementing complex queries, joining different business entities owned by different microservices, event sourcing to build logs of what happend, transactional outbox and SAGA for long running transaction. Addressing data eventual consistency to propagate change to other components versus ACID transaction. Support \"always-on\" approach with the deployment to multiple data centers (at least three) being active/active and being able to propagate data in all data centers. Supporting all or part of those requirements will lead to the adoption of event-driven microservices and architecture.","title":"Cloud native applications"},{"location":"concepts/fit-to-purpose/#motivation-for-data-streaming","text":"The central value propositions of data stream are to: lower the cost of integrating via event streams, use event streams to signal state changes in near-real time replay the past to build data projection Applying the concept of data loose value over time, it is important to act on data as early as possible, close to creation time. After a period of time data becomes less valuable. Two time factors are important in this data processing: latency (time to deliver data to consumers) retention (time to keep data). For latency try to reduce the number of network segment between producer and consumers. Considering edge computing as a way to bring event processing close to the source. The event processing adds time to the end to end latency. Considering constraining the processing time frame. Retention is a problem linked to the business requirements, and we need to assess for each topic how long an event is still valuable for the consumers. Not keeping enough events will impact correctness of consumer state, projection views... keeping for too long, increase the cost of storage, but also the time to rebuild data projection.","title":"Motivation for data streaming"},{"location":"concepts/fit-to-purpose/#modern-data-pipeline","text":"As new business applications need to react to events in real time, the adoption of event backbone is really part of the IT toolbox. Modern IT architecture encompasses the adoption of new data hub, where all the data about a 'customer', for example, is accessible in one event backbone. Therefore, it is natural to assess the data movement strategy and assess how to offload some of those ETL jobs running at night, by adopting real time data ingestion. We detailed the new architecture in this modern data lake article, so from a fit for purpose point of view, we need to assess the scope of existing ETL jobs, and refector to streaming logic that can be incorporated into different logs/ topics. With Event Backbone like Kafka, any consumer can join the data log consumption at any point of time, within the retention period. By moving the ETL logic to a streaming application, we do not need to wait for the next morning to get important metrics.","title":"Modern data pipeline"},{"location":"concepts/fit-to-purpose/#mq-versus-kafka","text":"We already addressed the differences between Queueing and Streaming in this chapter . Now in term of technologies we can quickly highlight the followings:","title":"MQ Versus Kafka"},{"location":"concepts/fit-to-purpose/#kafka-characteristics","text":"Keep message for long period of time, messages are not deleted once consumed Suited for high volume and low latency processing Support pub/sub model only Messages are ordered in a topic/partition but not cross partitions Stores and replicates events published to a topic, remove on expired period or on disk space constraint Messages are removed from file system independent of applications Topic can have multiple partitions to make consumer processing parallel. Not supporting two phase commits / XA transaction, but message can be produced with local transaction Multi-region architecture requires data replication across regions with Mirror Maker 2 Applications (producers, consumers, or streaming) are going to a central cluster. Cluster can support multi availability zones But also support extended cluster to go over different regions if those regions have low latency network Scales horizontally, by adding more nodes non-standard API but rich library to support the main programming language. But support also HTTP bridge or proxy to get message sent via HTTP when there is a problem on the broker it takes a lot of time to recover and it impacts all consumers Kafka is easy to setup with kubernetes deployment with real operator, is more difficult to manage with bare metal deployment. Cluster and topics definition can be managed with Gitops and automatically instantiated in new k8s cluster Consumer and producers are build and deployed with simple CI/CD pipeline","title":"Kafka characteristics"},{"location":"concepts/fit-to-purpose/#mq-characteristics","text":"Best suited for point-to-point communication Supports horizontal scaling and high volume processing Supports event mesh , where Brokers can be deployed in different environments and serve the async applications locally, improving communication, performance, placement and scalability. The local transaction supports to write or read a message from a queue is a strength for scalability as if a consumer is not able to process the messagem another one will do. In Kafka, one partition is assigned to a consumer, and may be reassigned to a new consumer after failure, which leads to a very costly work of partition rebalancing. Participates to two-phase commit transaction Exactly one delivery with strong consistency Integrate with Mainframes: transactional applications on mainframe Support JMS for JEE applications Support AMQP for lighter protocol Messages are removed after consumption, they stayed persisted until consumed by all subscribers Strong coupling with subscribers, producer knows its consumers Supports MQ brokers in cluster with leader and followers. Support replication between brokers Support message priority Support dynamic queue creation. Typical case it replay to queue. Support much more queue per broker so it is easier to scale. Easily containerized and managed with Kubernetes operators.","title":"MQ characteristics"},{"location":"concepts/fit-to-purpose/#direct-product-feature-comparison","text":"Kafka IBM MQ Kafka is a pub/sub engine with streams and connectors MQ is a queue,or pub/sub engine All topics are persistent Queues and topics can be persistent or non persistent All subscribers are durable Subscribers can be durable or non durable Adding brokers to requires little work (changing a configuration file) Adding QMGRs requires some work (Add the QMGRs to the cluster, add cluster channels. Queues and Topics need to be added to the cluster.) Topics can be spread across brokers (partitions) with a command Queues and topics can be spread across a cluster by adding them to clustered QMGRs Producers and Consumers are aware of changes made to the cluster All MQ clients require a CCDT file to know of changes if not using a gateway QMGR Can have n number of replication partitions Can have 2 replicas (RDQM) of a QMGR, Multi Instance QMGRs Simple load balancing Load balancing can be simple or more complex using weights and affinity Can reread messages Cannot reread messages that have been already processed All clients connect using a single connection method MQ has Channels which allow different clients to connect, each having the ability to have different security requirements Data Streams processing built in, using Kafka topic for efficiency Stream processing is not built in, but using third party libraries, like MicroProfile Reactive Messaging, ReactiveX, etc. Has connection security, authentication security, and ACLs (read/write to Topic) Has connection security, channel security, authentication security, message security/encryption, ACLs for each Object, third party plugins (Channel Exits) Built on Java, so can run on any platform that support Java 8+ Latest native on AIX, IBM i, Linux systems, Solaris, Windows, z/OS, run as Container Monitoring by using statistics provided by Kafka CLI, open source tools, Prometheus Monitoring using PCF API, MQ Explorer, MQ CLI (runmqsc), Third Party Tools (Tivoli, CA APM, Help Systems, Open Source, etc)","title":"Direct product feature comparison"},{"location":"concepts/fit-to-purpose/#migrating-from-mq-to-kafka","text":"When the real conditions as listed above are met, architects may assess if it makes sense to migrate MQ application to Kafka. Most of the time the investment is not justified. Modern MQ supports the same DevOps and deployment pattern as other cloud native applications. JEE or mainframe applications use MQ in transaction to avoid duplicate messages or loss of messages. Supporting exactly once delivery in Kafka needs some configuration and participation of producer and consumers: far more complex to implement. We recommend adopting the two messaging capabilities for any business applications.","title":"Migrating from MQ to Kafka"},{"location":"concepts/fit-to-purpose/#kafka-streams-vs-apache-flink","text":"Once we have setup data streams, we need technology to support near real-time analytics and complex event processing. Historically, analytics performed on static data was done using batch reporting techniques. However, if insights have to be derived in near real-time, event-driven architectures help to analyse and look for patterns within events. Apache Flink (2016) is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. It is considered to be superior to Apache Spark and Hadoop. It supports batch and graph processing and complex event processing. The major stream processing features offered by Flink are: Support for event time and out of order streams: use event time for consistent results Consistency, fault tolerance, and high availability: guarantees consistent state updates in the presence of failures and consistent data movement between selected sources and sinks Low latency and high throughput: tune the latency-throughput trade off, making the system suitable for both high-throughput data ingestion and transformations, as well as ultra low latency (millisecond range) applications. Expressive and easy-to-use APIs in Scala and Java: map, reduce, join, with window, split,... Easy to implement the business logic using Function. Support for sessions and unaligned windows: Flink completely decouples windowing from fault tolerance, allowing for richer forms of windows, such as sessions. Connectors and integration points: Kafka, Kinesis, Queue, Database, Devices... Developer productivity and operational simplicity: Start in IDE to develop and deploy and deploy to Kubernetes, Yarn , Mesos or containerized Support batch processing Includes Complex Event Processing capabilities Here is simple diagram of Flink architecture from the Flink web site: See this technology summary . See also this article from Confluent about comparing Flink with Kafka Streams.","title":"Kafka Streams vs Apache Flink"},{"location":"concepts/integration/","text":"Updated 06/18/2022 Event driven architecture is a complement of the overall integration reference architecture as presented in IBM Cloud architecture center . In this note, we want to summarize some of the important aspects of agile integration and how some of the technologies delivered as part of IBM Cloud Pak for Integration are used when doing application modernization with event-driven microservice. Some of our labs and reference implementation code, use API management, MQ and APP Connect. First let do a quick review of the major concepts for agile integration. Agile integration concepts \u00b6 The main agile integration concepts as presented in detail in the IBM cloud on agile integration article can be summarized as: Empower extended teams to create integrations, leveraging a complete set of integration styles and capabilities to increase overall team productivity. Agile integration includes container-based, decentralized, microservices-aligned approach for integrating solutions Existing centralized integration architectures, based on ESB pattern, cannot support the demand, in term of team reactivity and scalability at the internet level. ESB pattern provides standardized synchronous connectivity to back-end systems typically over web services (SOAP based). ESB formed a single infrastructure for the whole enterprise, with tens or hundreds of integrations installed on a production server cluster. A single, centralized ESB certainly simplifies consistency and governance of implementation. Interface maintenance is expensive. Any deployment to the shared servers runs the risk of destabilizing existing critical interfaces. SOA encounters the issue of getting the funding at the enterprise wide program to maintain reusable interface. Integration teams are becoming the bottleneck instead of being enabler. SOA is about real-time integration between applications, whereas a microservices architecture is about how the applications are built internally. Microservice enables greater agility by being: small enough to be understood completely by their owning team and changed independently elastic to scale horizontally resilient with changes to one microservice will not affect others at runtime The following diagram illustrates the agile integration modernization transition from a centralized ESB type of architecture, and breaking integration into smaller pieces to make them more agile and more scalable. In this modernization process, development team can introduce API Management to improve decoupling between consumers and providers ultimately moving to a decentralized approach where each team can manage their own integration. Three aspects to agile integration: Decentralized integration ownership : give application teams more control over the creation and exposure of their own integration exposed as APIs, or messages Fine-grained integration deployment to separate integration, scalable independently. Changes to individual integration flows can be automatically rebuilt and deployed independently of other flows to enable safer application of changes and maximize speed to production. Cloud-native integration infrastructure to improve productivity, operational consistency and portability for both applications and integration Cloud Pak for Integration capabilities relevant to this site \u00b6 API management \u00b6 Easier to discover new business assets (APIs and events) in current enterprise systems Existing enterprise assets are made available to new channels and new audiences, with enriched customer experience in integrated omnichannel interactions Support asynchAPI. App connect \u00b6 Connect applications and data sources on premises or in the cloud to coordinate the exchange of business information so that data is available when and where it\u2019s needed. App Connect capabilities: Low-code/no-code integration tooling leverages natural language processing (NLP) and AI to offer custom mapping suggestions when building integration flows. Pre-built smart connectors and a common, shareable asset repository increases speed of delivery and eliminates concerns about mismatched sources, formats, or standards Labs \u00b6 Secure API, App connect and MQ : older version of the UI in screen shots but still relevant use case. Messaging \u00b6 Messaging is to support asynchronous communication between applications. Two technologies are part of the reference architecture, queuing and pub/sub. IBM MQ \u00b6 See a technology summary in this note and this MQ to Kafka lab . Kafka based product \u00b6 This site includes a lot of content around Kafka (see this technology summary) , but the major capabilities of interest in term of agile integration is the pub/sub model, long term persistence via append log and replication to support high availability and resiliency, with data streaming logic, and a lot of connectors to source data or sink data to external systems. Kafka scale and is a reliable messaging system for modern event-driven microservice solution. Bridge your digital ecosystem and core enterprise \u00b6 At a high level, modern integration involves bridging the capabilities between your digital ecosystem and your traditional core enterprise. The bridging takes place in a seamless, frictionless way so that you can uniformly operate your entire business anytime, anywhere, regardless of technological fluctuations. In your digital transformation journey, your digital ecosystem and your core enterprise constantly change. You need a set of integration capabilities to support rapid change to interface, develop new business oriented integration flow to consume cloud services and software as a service, get visibility to the data in motion, integrate with existing transactional systems and system of record. You modern cloud native applications use microservice design, function as a service, and may use agile, no code, integration logic to integrate with existing systems, SOA services, or cloud based services. Modern applications are reusing public cloud services, like CRM application, Database as a service, Chat bot as a services... Those services can be offered by different cloud providers, and architects will select the best services according to their requirements and cost / benefits analysis. A unique cloud provider will not have all the best useful services, and hybrid cloud is a standard approach in the 2020s. What is important is to get a set of tool that makes the integration easy to do with simple configuration to integrate with the needed data and inject those data in the operational messaging system for other to consumer. This agile integration follows the same DevOps pattern as other microservices. The messaging layer can support point to point, request/reply type of communication, or a pub/sub model with long retention time, and data streams processing. This data injection layer can be a buffer to modern data lake . Finally existing applications, system of records, transactional systems have to be integrated, consumed and accessed from modern applications, with new digital channel like mobile and single page web application. API management is an important elements of the integration, to manage and provide API economy but also secure access to internal systems, with controlled traffic. This is the role of the API gateway. Optimize your integration platform \u00b6 A vendor neutral hybrid cloud, that uses open standards and container orchestration technology, presents the optimal platform for modern integration. It addresses multicloud operations, different deployment options, and integration patterns within a modular and scalable environment. As shown in the diagram below, the integration platform must accommodate many integration patterns and have that support ready on demand and as a self-service model. API integration enables synchronous access to fine-grained services, such as create, retrieve, update, and delete operations to business objects across various backends. Thus, the composition of the integration logic on the consumer side. Modern API management also includes the management of AsynchAPI for messaging systems and asynchronous communication between event-driven microservices. Application data integration enables synchronous access to coarse-grained services such as transaction processing across various backends in accordance with enterprise compliance requirements. Thus, the governance of integration logic on the provider side. Enterprise messaging enables asynchronous point-to-point access to services such as those that involve closed heritage systems, transactional integrity systems or heterogeneous partner backends. Event publish/subscribe integration enables asynchronous many-to-many coordination of services across both cloud and on-premises components in an event-driven architecture context. File transfer enables batch integration between SORs that involves the movement of large data files for content across vast physical distances within short time windows. The integration platform must provide a unified framework for security, management operations, and resiliency. The container orchestration platform provides resiliency through the elasticity of container clusters and platform-level security. The unified management component provides ease of operations with a single view across all integration components within the platform. The gateway services provide runtime-level security and enforce access control policies to integration services. API lifecycle management architecture \u00b6 The API lifecycle management reference architecture bridges the gap between cloud and on-premises applications quickly and easily. It allows customers to securely unlock IT assets and to deliver innovative applications with modern architectures. An international bank develops a new business model in a new ecosystem. This model exposes their rewards enterprise application program with a retail partner's online and mobile applications via self-service access to the program APIs. The bank's objective is to leverage the hybrid cloud capability to manage a comprehensive end-to-end integrated experience across the API lifecycle: create, run, manage, secure, socialize, and analyze APIs. An API developer signs on to the API management cloud services account and accesses the API developer User interface or CLI toolkit. The developer creates the synch API and implements business logic. He maps and integrates the API data model to the back-end schema through the transformation and connectivity service. He tests and deploys the API to the runtime and publishes to API management. He can also create Asynch APIs from a messaging system by binding channels to topic or queue and define message payload definition. API owner signs on to the API management cloud services account and accesses the API management component. She includes the synch API endpoint to existing API products, and plans and specifies access control. She publishes the API to the developer portal for external discovery by application developers. An application developer accesses the developer portal, uses search, and discovers the API. The application developer uses the API in an application and deploys that application to the device. The device user opens the application that issues the API request. The request is handled by the API gateway, which performs load balancing and security validation for all API requests. The API gateway validates access policies with API management and invokes the API. The API polyglot runtime executes the API and obtains the data payload from the back end. The API response is sent back to the API gateway. Alternatively, APIs exposed by enterprise applications can be executed on that enterprise application runtime. The API gateway forwards the response to the calling application. The API gateway reports usage metrics and analytics to the API management. API developers and API owners can log on to the API analytics visualization component to view dashboards on API usage metrics and other analytics.","title":"Agile Integration"},{"location":"concepts/integration/#agile-integration-concepts","text":"The main agile integration concepts as presented in detail in the IBM cloud on agile integration article can be summarized as: Empower extended teams to create integrations, leveraging a complete set of integration styles and capabilities to increase overall team productivity. Agile integration includes container-based, decentralized, microservices-aligned approach for integrating solutions Existing centralized integration architectures, based on ESB pattern, cannot support the demand, in term of team reactivity and scalability at the internet level. ESB pattern provides standardized synchronous connectivity to back-end systems typically over web services (SOAP based). ESB formed a single infrastructure for the whole enterprise, with tens or hundreds of integrations installed on a production server cluster. A single, centralized ESB certainly simplifies consistency and governance of implementation. Interface maintenance is expensive. Any deployment to the shared servers runs the risk of destabilizing existing critical interfaces. SOA encounters the issue of getting the funding at the enterprise wide program to maintain reusable interface. Integration teams are becoming the bottleneck instead of being enabler. SOA is about real-time integration between applications, whereas a microservices architecture is about how the applications are built internally. Microservice enables greater agility by being: small enough to be understood completely by their owning team and changed independently elastic to scale horizontally resilient with changes to one microservice will not affect others at runtime The following diagram illustrates the agile integration modernization transition from a centralized ESB type of architecture, and breaking integration into smaller pieces to make them more agile and more scalable. In this modernization process, development team can introduce API Management to improve decoupling between consumers and providers ultimately moving to a decentralized approach where each team can manage their own integration. Three aspects to agile integration: Decentralized integration ownership : give application teams more control over the creation and exposure of their own integration exposed as APIs, or messages Fine-grained integration deployment to separate integration, scalable independently. Changes to individual integration flows can be automatically rebuilt and deployed independently of other flows to enable safer application of changes and maximize speed to production. Cloud-native integration infrastructure to improve productivity, operational consistency and portability for both applications and integration","title":"Agile integration concepts"},{"location":"concepts/integration/#cloud-pak-for-integration-capabilities-relevant-to-this-site","text":"","title":"Cloud Pak for Integration capabilities relevant to this site"},{"location":"concepts/integration/#api-management","text":"Easier to discover new business assets (APIs and events) in current enterprise systems Existing enterprise assets are made available to new channels and new audiences, with enriched customer experience in integrated omnichannel interactions Support asynchAPI.","title":"API management"},{"location":"concepts/integration/#app-connect","text":"Connect applications and data sources on premises or in the cloud to coordinate the exchange of business information so that data is available when and where it\u2019s needed. App Connect capabilities: Low-code/no-code integration tooling leverages natural language processing (NLP) and AI to offer custom mapping suggestions when building integration flows. Pre-built smart connectors and a common, shareable asset repository increases speed of delivery and eliminates concerns about mismatched sources, formats, or standards","title":"App connect"},{"location":"concepts/integration/#labs","text":"Secure API, App connect and MQ : older version of the UI in screen shots but still relevant use case.","title":"Labs"},{"location":"concepts/integration/#messaging","text":"Messaging is to support asynchronous communication between applications. Two technologies are part of the reference architecture, queuing and pub/sub.","title":"Messaging"},{"location":"concepts/integration/#ibm-mq","text":"See a technology summary in this note and this MQ to Kafka lab .","title":"IBM MQ"},{"location":"concepts/integration/#kafka-based-product","text":"This site includes a lot of content around Kafka (see this technology summary) , but the major capabilities of interest in term of agile integration is the pub/sub model, long term persistence via append log and replication to support high availability and resiliency, with data streaming logic, and a lot of connectors to source data or sink data to external systems. Kafka scale and is a reliable messaging system for modern event-driven microservice solution.","title":"Kafka based product"},{"location":"concepts/integration/#bridge-your-digital-ecosystem-and-core-enterprise","text":"At a high level, modern integration involves bridging the capabilities between your digital ecosystem and your traditional core enterprise. The bridging takes place in a seamless, frictionless way so that you can uniformly operate your entire business anytime, anywhere, regardless of technological fluctuations. In your digital transformation journey, your digital ecosystem and your core enterprise constantly change. You need a set of integration capabilities to support rapid change to interface, develop new business oriented integration flow to consume cloud services and software as a service, get visibility to the data in motion, integrate with existing transactional systems and system of record. You modern cloud native applications use microservice design, function as a service, and may use agile, no code, integration logic to integrate with existing systems, SOA services, or cloud based services. Modern applications are reusing public cloud services, like CRM application, Database as a service, Chat bot as a services... Those services can be offered by different cloud providers, and architects will select the best services according to their requirements and cost / benefits analysis. A unique cloud provider will not have all the best useful services, and hybrid cloud is a standard approach in the 2020s. What is important is to get a set of tool that makes the integration easy to do with simple configuration to integrate with the needed data and inject those data in the operational messaging system for other to consumer. This agile integration follows the same DevOps pattern as other microservices. The messaging layer can support point to point, request/reply type of communication, or a pub/sub model with long retention time, and data streams processing. This data injection layer can be a buffer to modern data lake . Finally existing applications, system of records, transactional systems have to be integrated, consumed and accessed from modern applications, with new digital channel like mobile and single page web application. API management is an important elements of the integration, to manage and provide API economy but also secure access to internal systems, with controlled traffic. This is the role of the API gateway.","title":"Bridge your digital ecosystem and core enterprise"},{"location":"concepts/integration/#optimize-your-integration-platform","text":"A vendor neutral hybrid cloud, that uses open standards and container orchestration technology, presents the optimal platform for modern integration. It addresses multicloud operations, different deployment options, and integration patterns within a modular and scalable environment. As shown in the diagram below, the integration platform must accommodate many integration patterns and have that support ready on demand and as a self-service model. API integration enables synchronous access to fine-grained services, such as create, retrieve, update, and delete operations to business objects across various backends. Thus, the composition of the integration logic on the consumer side. Modern API management also includes the management of AsynchAPI for messaging systems and asynchronous communication between event-driven microservices. Application data integration enables synchronous access to coarse-grained services such as transaction processing across various backends in accordance with enterprise compliance requirements. Thus, the governance of integration logic on the provider side. Enterprise messaging enables asynchronous point-to-point access to services such as those that involve closed heritage systems, transactional integrity systems or heterogeneous partner backends. Event publish/subscribe integration enables asynchronous many-to-many coordination of services across both cloud and on-premises components in an event-driven architecture context. File transfer enables batch integration between SORs that involves the movement of large data files for content across vast physical distances within short time windows. The integration platform must provide a unified framework for security, management operations, and resiliency. The container orchestration platform provides resiliency through the elasticity of container clusters and platform-level security. The unified management component provides ease of operations with a single view across all integration components within the platform. The gateway services provide runtime-level security and enforce access control policies to integration services.","title":"Optimize your integration platform"},{"location":"concepts/integration/#api-lifecycle-management-architecture","text":"The API lifecycle management reference architecture bridges the gap between cloud and on-premises applications quickly and easily. It allows customers to securely unlock IT assets and to deliver innovative applications with modern architectures. An international bank develops a new business model in a new ecosystem. This model exposes their rewards enterprise application program with a retail partner's online and mobile applications via self-service access to the program APIs. The bank's objective is to leverage the hybrid cloud capability to manage a comprehensive end-to-end integrated experience across the API lifecycle: create, run, manage, secure, socialize, and analyze APIs. An API developer signs on to the API management cloud services account and accesses the API developer User interface or CLI toolkit. The developer creates the synch API and implements business logic. He maps and integrates the API data model to the back-end schema through the transformation and connectivity service. He tests and deploys the API to the runtime and publishes to API management. He can also create Asynch APIs from a messaging system by binding channels to topic or queue and define message payload definition. API owner signs on to the API management cloud services account and accesses the API management component. She includes the synch API endpoint to existing API products, and plans and specifies access control. She publishes the API to the developer portal for external discovery by application developers. An application developer accesses the developer portal, uses search, and discovers the API. The application developer uses the API in an application and deploys that application to the device. The device user opens the application that issues the API request. The request is handled by the API gateway, which performs load balancing and security validation for all API requests. The API gateway validates access policies with API management and invokes the API. The API polyglot runtime executes the API and obtains the data payload from the back end. The API response is sent back to the API gateway. Alternatively, APIs exposed by enterprise applications can be executed on that enterprise application runtime. The API gateway forwards the response to the calling application. The API gateway reports usage metrics and analytics to the API management. API developers and API owners can log on to the API analytics visualization component to view dashboards on API usage metrics and other analytics.","title":"API lifecycle management architecture"},{"location":"concepts/model/","text":"Updated 06/18/2022 The development of modern business application using cloud native microservices, long running business processes, decision services, different data stores, events, existing software as service like CRM or ERP applications, AI,... are adding complexity on the data view, data transformation and the data modeling. Some older approaches of adopting a unique canonical model, has some advantages and some counter productive ones. With microservice adoption, and applying domain driven design, it is easier to define a bounded context for microservice, but as soon as you start coding you immediately encounter questions about your information model and what shapes it should have. Canonical model is kept at the messaging or integration layer to do a one to one mapping between models. This note presents the reality view of the different data model to consider. The goal is still to apply clear separation of concern, focus on data needed for implementing business logic, and adapt interface for easy consumption and reuse. The following figure illustrates a potential business automation solution using different components, supported on top of different software products. From Top-Left bottom right: Single Page applications are running in Web Browser and use different javascript library to support HTML rendering, business logic implementation and data model as JSON document. The model view is focusing to expose data to user and to support form entries. The app interacts with one Backend For Frontend app's REST API or other services with CORS support. The model and API are designed for this user centric application. Microservices are exposing model via their APIs, in Java EE, it used to be named Data Transfer Object , and it is still used, named this way in microservice. The focus on this model is to enforce reusability, and expose APIs about the main business entity or aggregate (Domain Driven Design - Aggregate). This entity is persisted in different formats, and when using traditional SQL based technology, Object Relational Mapping is needed. In Java, hibernate ORM is used with JPA to support transaction and entity annotation. Quarkus Panache is a nice way to map business entity transparently to DB. But still we need to design those models. Within the microservice, we will also find specific implementation model used to support the business logic the service implements. The adoption of JSON document to get a more flexible, schemaless model, helps at the technology selection to be more agile as the model can evolve easily, but still we need to think and design such models. Modern business applications are using Artificial Intelligence model. To make it more generic, we defined a predictive scoring service which computes a score between 0 to 1, given a feature set represented as tuples. It does not have to be a predictive scoring and can be anything a machine learned model can support, but it is exposed with an API and a flat data model. In the middle we have an integration layer: The modern event backbone to support event streaming. The event model is also defined with schemas, and any application producing such events, needs to have a specific model to represent fact, of what happens in their boundary. Enterprise service buses are still in scope, as service gateway, interface mapping, and heteregeonous integration are needed. The model definition will better fit to the adoption of canonical model to facilitate the one to one mapping between source and destination models. Those integration can do data mapping with application like CRM, ERP running as system. The name ASBO , for application specific business object, was used in the past, and it still applies in modern solutions. We need to consider them as-is or by using mapping to internal models. Some business applications need to get human involved, in the form of workflow, where process application implements the workflow and define process variables to keep data between process tasks. Those data are persisted with process instances within the Business Process Management datasource. The business process can be triggered by user interface (claiming to start a process), by exposing a service, in this case we are back to the DTO definition, or triggered asynchronously via messages (back to a message model). Externalizing business rules, is still a relevant pattern, and in the case of decision modeling, a specific data model to support the rules inference and simplify the rule processing is needed. This is what we called RBO: Rule Business Object in the Agile Business Rule Development methodology . The persisted information model (in system of records or DB) is different than the process variables in BPM, which is different than the rule business object model used to make decisions. A good architecture leverages different information model views to carry the data to the different layers of the business application: the user interface, the database, the service contract, the business process data, the messaging structure, the events, the business rules, the raw document, the AI scoring input data.... There is nothing new in this, but better to keep that in mind anyway when developing new solutions. The information model for such service has to be designed so there is minimum dependency between consumers and providers. The approach of trying to develop a unique rich information model to support all the different consumers, may lead having the consumers dealing with complex data structure sparsely populated, and will increase coupling between components. When defining service specifications it is important to assess if each business entities supported by the service operations is going to have different definitions or if the same definition applies to the whole service portfolio? The scope of reuse of data types is an important architecture decision; there is no one-size-fits-all solution. However, a few different patterns are emerging: One object model per interface : Using an independent data model for every service interface assures the highest level of decoupling between services. As negative, the consumers have to understand different representations of the same business entities across multiple services and have to cope with all the relative data transformations. This strategy can be very effective for coarse-grained service operations that do not exchange large amount of data with consumers. One object model per business domain : the service information models are organized in domains, every domain sharing the same data model. The downside of this approach, is once domains are defined, changing their boundaries can be costly. A single object model for the whole enterprise : the approach is to define a single common set of data structures shared across all the enterprise service interfaces. The amount of model customization is kept to a minimum and its management is centralized. The negatives are having overly complicated model the consumers need to process. As of now it becomes a bad practices to adopt such canonical model at the API, persistence and eventing level.","title":"Devising the data models"},{"location":"concepts/service-mesh/","text":"Updated 06/18/2022 In this note we are grouping the studies around microservice to microservice communication with Kubernetes deployment. We are addressing: how ingress controller helps inside Kubernetes how API gateway helps for API management and service integration how to expose service in hybrid cloud how to discover service All come back to the requirements, skill set and fit to purpose. Definitions \u00b6 Service meshes provide visibility, resiliency, traffic, and security control of distributed application services. They deliver policy-based networking for microservices in the contraints of virtual network and continuous topology updates. Externalizing, via declarations, the logic to support network potential issues, like resiliency, simplifies dramatically developers work. Some misconception to clarify around microservice and APIs: microservices are not fine grained web services APIs are not equivalent to microservices microservices implement APIs in the scope of their domain. Separation between queries and commands may lead to two different services so the APIs will be splitted. microservice implements business logic API is an interface, a way to make a request to get or change data in an application. In modern use API refers to REST web APIs using HTTP protocol, with JSON format (sometime XML is still used). Interface decouples the caller from the implementation. The caller has no idea how API is implemented. A microservice is in fact a component. Micro refers to the granularity of the component not of the exposed interface. The following diagram illustrates all those concepts. We encourage you to go to read integration design and architecture series . Container orchestration like Kubernetes are mainly doing application scheduling, cluster management, resource provisioning, platform and workload monitoring and service discovery. When application solutions are growing in size and complexity, you need to addres the following subjects: visibility on how traffic is flowing between microservices, how routing is done between microservices based on requests content or the origination point or the end point how to support resiliency by handling failure in a graceful manner how to ensure security with identity assertion how to enforce security policy Addressing those subjects help defining the requirements for service mesh. Service mesh architecture defines a data and control planes: Control plane : supports policy and configuration for services in the mesh, and provides aggregation for telemetry. It has API and CLI to centralize control to the services deployed. In Kubernetes control planes are deployed in a system namespace. Data plane : handles the actual inspection, transit, and routing of network traffic. It is responsible for health checking, load balancing, authentication, authorization, inbound (ingress) and outbound (egress) cluster network traffic. Applications / microservices are unaware of data plane. Context \u00b6 Traditional modern architecture involves having different components exposing reusable APIs, addressing different channels (mobile, single page application, traditional server pages or B2B apps), consuming APIs (mobile APIs, back end for front end, shared common apis like authentication, authorization,...) and backend services addressing reusable business services: API management can be added via API gateway. This is a distributed application with cross related communication channels, where any changes to the service interface characteristics impact any of the components. Moving to microservices architecture style adds more communication challenges and DevOps complexity but provides a lot of business values such as: rapid deployment of new business capabilities, co-evolving in parallel of other services. focusing on business domain with clear ownership of the business function and feature roadmap better operation procedure, automated, and with easy rollout and continuous delivery. A/B testing to assess how new feature deployed improve business operations improve resiliency by deploying on multi language cluster As an example we can use the a classical web application with the following capabilities: user authentication user management: add / delete new user user self registration, reset password user permission control user profile asset management risk assessment service Each capability could be grouped by business domain like the user management, asset management, and application access control. So domain separation can be a good microservice boundary. This is the Domain Driven Design Bounded context construct. But if the number of user reach millions then we may need to optimize the runtime processing of reading user credential, and scale the service differently, leading to a service map like the diagram below, where runtime and management are separated services. All of these still does not address the fact that data are distributed and even more with microservices owning their data persistence. As developers and architects, we still have to address the following data integrity problems: two phases commit compensating operation eventual data consistency: some microservice updating data may share those updates with other microservices. data aggregation: adding new views on data, owned by a microservice, to support new aggregates. Examples are preparing data view for machine learning modeling, analytics, or business intelligence... From the previous microservice allocation we can see the needs to propagate data update between services. Adding or unsubscribing a user involves updating the asset the user owns and the authentication runtime service: Adding a new application changes the authorization runtime service. We are now looking at the following questions: how does webapp access APIs for their main service, of back end for front end service. how does deployed microservice access other service: discover and access? How data consistency can be ensured? is there a simpler way to manage cross microservice dependency? The answers depend on the existing infrastructure and environment, and deployment needs. Service routing \u00b6 We have to dissociate intra-cluster communication versus inter clusters or cluster to external services. Without getting into too much detail of IP routing within Kubernetes some important elements of the cluster are important to remember: microservices are packaged as docker image and expose port. When deployed they run in a pod within a node (physical or virtual machine) containers can talk to other containers only if they are on the same machine, or when they have exposed port. Kubernetes is configured with a large flat subnet (e.g. 172.30.0.0/16) which is used for internal application traffic inside of the cluster. Each worker node in the Kubernetes cluster is assigned one or more non-overlapping slices of this network, coordinated by the Kubernetes master node. When a container is created in the cluster, it gets assigned to a worker node and is given an IP address from the slice of the subnet for the worker node. Kube-proxy intercepts and controls where to forward the traffic, either to another worker node running your destination pod, or outside of the cluster Kube-proxy watches the API Server on the Master Node for the addition and removal of Services endpoints. It configures the IPtable rules to capture the traffic for its ClusterIP and forwards it to one of the endpoints. Worker nodes have internal DNS service and load balancer Within Kubernetes, Ingress is a service that balances network traffic workloads in your cluster by forwarding public or private requests to your apps. You use ingress when you need to support HTTP, HTTPS, TLS, load balancing, expose app outside of the cluster, and custom routing rules... One ingress resource is required by namespace. So if microservices are in the same namespace you can define a domain name for those services (e.g. assetmanagement.greencompute.ibmcase.com) and defined path for each service: apiVersion : extensions/v1beta1 kind : Ingress metadata : name : assetmanagement spec : rules : - host : assetmanagement.greencompute.ibmcase.com http : paths : - path : /assetconsumer backend : serviceName : asset-consumer-svc servicePort : 8080 - path : /assetdashboard backend : serviceName : asset-dashboard-bff-svc servicePort : 8080 - path : /assetmgrms backend : serviceName : asset-mgr-ms-svc servicePort : 8080 The backend for front end component, the asset manager microservice and the asset consumer components are exposed in the same domain. The serviceName matches the service exposed for each components. The following diagram presents how an external application accesses deployed microservice within Kubernetes pod. The following diagram shows how Ingress directs communication from the internet to a deployed microservice: A user sends a request to your app by accessing your app's URL. DNS name abstracts the application from the underlying infrastructure. Inter clusters microservice to microservice should use the same approach A DNS system service resolves the hostname in the URL to the portable public IP address of the load balancer Based on the resolved IP address, the client sends the request to the load balancer service that exposes the Application Load Balancer (ALB) The ALB checks if a routing rule for the app path in the cluster exists. If a matching rule is found, the request is forwarded according to the rules that you defined in the Ingress resource to the pod where the app is deployed. If multiple app instances are deployed in the cluster, the ALB load balances the requests between the app pods. To also load balance incoming HTTPS connections, you can configure the ALB to you can use your own TLS certificate to decrypt the network traffic. Microservice to microservice can use this DNS name to communicate between them. Using Ingress, the global load balancer can support parallel, cross region clusters. Service exposition \u00b6 There is an architecture style focusing on APIs which proposes to have different SLA and semantic for external, internet facing API versus internal back end APIs only exposed within intranet. Backend data services are not exposed directly to internet. API Gateway provides a secure end point for external web app to access those business functions. So the decisions on how to expose service are linked to: do you need to do API management do you need to secure APIs do you need to expose to internet do you need to support other protocol then HTTP do you need to have multiple instances of the application When deploying a microservice to Kubernetes it is recommended to use Ingress rule as presented above.. The following yaml file exposes the BFF service using ClusterIP: apiVersion : v1 kind : Service metadata : name : asset-consumer-svc labels : chart : asset-consumer spec : type : ClusterIP ports : - port : 8080 targetPort : 8080 protocol : TCP name : asset-consumer-svc selector : app : asset-consumer Service discovery \u00b6 When deploying on Kubernetes cluster, microservices use the DNS lookup to discover deployed microservice. ISTIO \u00b6 ISTIO provides an easy way to create a network of deployed services with load balancing, service-to-service authentication, traffic flow management, monitoring, etc... By deploying a special sidecar proxy (called Envoy) throughout your environment, all network communication between microservices is intercepted and controlled by ISTIO control panel. The control plane manages the overall network infrastructure and enforces the policy and traffic rules. More reading Istio and Kubernetes 101 Advanced traffic management with ISTIO Istio workshop for IBM Cloud Container service for inspiration but this repository was not touched since 4 years. Asynchronous loosely coupled solution using events \u00b6 If we change of paradigm and use a messaging approach or better an event approach of the data update requirements, we will implement a loosly coupled solution with a pub/sub communication protocol. We need to think about the activities that apply within each service and how they can be of interest to other components. Internal microservice tasks are becoming facts about something happened and those facts may be published as events for others to consume. The first level of refactoring may become: An event is a fact that happens in the past and carry all the data needed, and it becomes a source of record. It becoming consistent if it is played in a messaging backbone via topics. But the persistence of data can be externalized to consumer and then simplify the architecture: Then we can use the history of the persisted events to add features not address before, and outside of the direct scope of a microservice. For example to compute the number of users added last month, just a query on the users topic will get the answer: no or very limited coding needed. We recommend to go deeper in event driven architecture with this site","title":"Service mesh"},{"location":"concepts/service-mesh/#definitions","text":"Service meshes provide visibility, resiliency, traffic, and security control of distributed application services. They deliver policy-based networking for microservices in the contraints of virtual network and continuous topology updates. Externalizing, via declarations, the logic to support network potential issues, like resiliency, simplifies dramatically developers work. Some misconception to clarify around microservice and APIs: microservices are not fine grained web services APIs are not equivalent to microservices microservices implement APIs in the scope of their domain. Separation between queries and commands may lead to two different services so the APIs will be splitted. microservice implements business logic API is an interface, a way to make a request to get or change data in an application. In modern use API refers to REST web APIs using HTTP protocol, with JSON format (sometime XML is still used). Interface decouples the caller from the implementation. The caller has no idea how API is implemented. A microservice is in fact a component. Micro refers to the granularity of the component not of the exposed interface. The following diagram illustrates all those concepts. We encourage you to go to read integration design and architecture series . Container orchestration like Kubernetes are mainly doing application scheduling, cluster management, resource provisioning, platform and workload monitoring and service discovery. When application solutions are growing in size and complexity, you need to addres the following subjects: visibility on how traffic is flowing between microservices, how routing is done between microservices based on requests content or the origination point or the end point how to support resiliency by handling failure in a graceful manner how to ensure security with identity assertion how to enforce security policy Addressing those subjects help defining the requirements for service mesh. Service mesh architecture defines a data and control planes: Control plane : supports policy and configuration for services in the mesh, and provides aggregation for telemetry. It has API and CLI to centralize control to the services deployed. In Kubernetes control planes are deployed in a system namespace. Data plane : handles the actual inspection, transit, and routing of network traffic. It is responsible for health checking, load balancing, authentication, authorization, inbound (ingress) and outbound (egress) cluster network traffic. Applications / microservices are unaware of data plane.","title":"Definitions"},{"location":"concepts/service-mesh/#context","text":"Traditional modern architecture involves having different components exposing reusable APIs, addressing different channels (mobile, single page application, traditional server pages or B2B apps), consuming APIs (mobile APIs, back end for front end, shared common apis like authentication, authorization,...) and backend services addressing reusable business services: API management can be added via API gateway. This is a distributed application with cross related communication channels, where any changes to the service interface characteristics impact any of the components. Moving to microservices architecture style adds more communication challenges and DevOps complexity but provides a lot of business values such as: rapid deployment of new business capabilities, co-evolving in parallel of other services. focusing on business domain with clear ownership of the business function and feature roadmap better operation procedure, automated, and with easy rollout and continuous delivery. A/B testing to assess how new feature deployed improve business operations improve resiliency by deploying on multi language cluster As an example we can use the a classical web application with the following capabilities: user authentication user management: add / delete new user user self registration, reset password user permission control user profile asset management risk assessment service Each capability could be grouped by business domain like the user management, asset management, and application access control. So domain separation can be a good microservice boundary. This is the Domain Driven Design Bounded context construct. But if the number of user reach millions then we may need to optimize the runtime processing of reading user credential, and scale the service differently, leading to a service map like the diagram below, where runtime and management are separated services. All of these still does not address the fact that data are distributed and even more with microservices owning their data persistence. As developers and architects, we still have to address the following data integrity problems: two phases commit compensating operation eventual data consistency: some microservice updating data may share those updates with other microservices. data aggregation: adding new views on data, owned by a microservice, to support new aggregates. Examples are preparing data view for machine learning modeling, analytics, or business intelligence... From the previous microservice allocation we can see the needs to propagate data update between services. Adding or unsubscribing a user involves updating the asset the user owns and the authentication runtime service: Adding a new application changes the authorization runtime service. We are now looking at the following questions: how does webapp access APIs for their main service, of back end for front end service. how does deployed microservice access other service: discover and access? How data consistency can be ensured? is there a simpler way to manage cross microservice dependency? The answers depend on the existing infrastructure and environment, and deployment needs.","title":"Context"},{"location":"concepts/service-mesh/#service-routing","text":"We have to dissociate intra-cluster communication versus inter clusters or cluster to external services. Without getting into too much detail of IP routing within Kubernetes some important elements of the cluster are important to remember: microservices are packaged as docker image and expose port. When deployed they run in a pod within a node (physical or virtual machine) containers can talk to other containers only if they are on the same machine, or when they have exposed port. Kubernetes is configured with a large flat subnet (e.g. 172.30.0.0/16) which is used for internal application traffic inside of the cluster. Each worker node in the Kubernetes cluster is assigned one or more non-overlapping slices of this network, coordinated by the Kubernetes master node. When a container is created in the cluster, it gets assigned to a worker node and is given an IP address from the slice of the subnet for the worker node. Kube-proxy intercepts and controls where to forward the traffic, either to another worker node running your destination pod, or outside of the cluster Kube-proxy watches the API Server on the Master Node for the addition and removal of Services endpoints. It configures the IPtable rules to capture the traffic for its ClusterIP and forwards it to one of the endpoints. Worker nodes have internal DNS service and load balancer Within Kubernetes, Ingress is a service that balances network traffic workloads in your cluster by forwarding public or private requests to your apps. You use ingress when you need to support HTTP, HTTPS, TLS, load balancing, expose app outside of the cluster, and custom routing rules... One ingress resource is required by namespace. So if microservices are in the same namespace you can define a domain name for those services (e.g. assetmanagement.greencompute.ibmcase.com) and defined path for each service: apiVersion : extensions/v1beta1 kind : Ingress metadata : name : assetmanagement spec : rules : - host : assetmanagement.greencompute.ibmcase.com http : paths : - path : /assetconsumer backend : serviceName : asset-consumer-svc servicePort : 8080 - path : /assetdashboard backend : serviceName : asset-dashboard-bff-svc servicePort : 8080 - path : /assetmgrms backend : serviceName : asset-mgr-ms-svc servicePort : 8080 The backend for front end component, the asset manager microservice and the asset consumer components are exposed in the same domain. The serviceName matches the service exposed for each components. The following diagram presents how an external application accesses deployed microservice within Kubernetes pod. The following diagram shows how Ingress directs communication from the internet to a deployed microservice: A user sends a request to your app by accessing your app's URL. DNS name abstracts the application from the underlying infrastructure. Inter clusters microservice to microservice should use the same approach A DNS system service resolves the hostname in the URL to the portable public IP address of the load balancer Based on the resolved IP address, the client sends the request to the load balancer service that exposes the Application Load Balancer (ALB) The ALB checks if a routing rule for the app path in the cluster exists. If a matching rule is found, the request is forwarded according to the rules that you defined in the Ingress resource to the pod where the app is deployed. If multiple app instances are deployed in the cluster, the ALB load balances the requests between the app pods. To also load balance incoming HTTPS connections, you can configure the ALB to you can use your own TLS certificate to decrypt the network traffic. Microservice to microservice can use this DNS name to communicate between them. Using Ingress, the global load balancer can support parallel, cross region clusters.","title":"Service routing"},{"location":"concepts/service-mesh/#service-exposition","text":"There is an architecture style focusing on APIs which proposes to have different SLA and semantic for external, internet facing API versus internal back end APIs only exposed within intranet. Backend data services are not exposed directly to internet. API Gateway provides a secure end point for external web app to access those business functions. So the decisions on how to expose service are linked to: do you need to do API management do you need to secure APIs do you need to expose to internet do you need to support other protocol then HTTP do you need to have multiple instances of the application When deploying a microservice to Kubernetes it is recommended to use Ingress rule as presented above.. The following yaml file exposes the BFF service using ClusterIP: apiVersion : v1 kind : Service metadata : name : asset-consumer-svc labels : chart : asset-consumer spec : type : ClusterIP ports : - port : 8080 targetPort : 8080 protocol : TCP name : asset-consumer-svc selector : app : asset-consumer","title":"Service exposition"},{"location":"concepts/service-mesh/#service-discovery","text":"When deploying on Kubernetes cluster, microservices use the DNS lookup to discover deployed microservice.","title":"Service discovery"},{"location":"concepts/service-mesh/#istio","text":"ISTIO provides an easy way to create a network of deployed services with load balancing, service-to-service authentication, traffic flow management, monitoring, etc... By deploying a special sidecar proxy (called Envoy) throughout your environment, all network communication between microservices is intercepted and controlled by ISTIO control panel. The control plane manages the overall network infrastructure and enforces the policy and traffic rules. More reading Istio and Kubernetes 101 Advanced traffic management with ISTIO Istio workshop for IBM Cloud Container service for inspiration but this repository was not touched since 4 years.","title":"ISTIO"},{"location":"concepts/service-mesh/#asynchronous-loosely-coupled-solution-using-events","text":"If we change of paradigm and use a messaging approach or better an event approach of the data update requirements, we will implement a loosly coupled solution with a pub/sub communication protocol. We need to think about the activities that apply within each service and how they can be of interest to other components. Internal microservice tasks are becoming facts about something happened and those facts may be published as events for others to consume. The first level of refactoring may become: An event is a fact that happens in the past and carry all the data needed, and it becomes a source of record. It becoming consistent if it is played in a messaging backbone via topics. But the persistence of data can be externalized to consumer and then simplify the architecture: Then we can use the history of the persisted events to add features not address before, and outside of the direct scope of a microservice. For example to compute the number of users added last month, just a query on the users topic will get the answer: no or very limited coding needed. We recommend to go deeper in event driven architecture with this site","title":"Asynchronous loosely coupled solution using events"},{"location":"concepts/terms-and-definitions/","text":"Events \u00b6 Events are notifications of change of state. Notifications are issued, or published and interested parties can subscribe and take action on the events. Typically, the issuer of the notification has no knowledge of what action is taken and receives no corresponding feedback that the notification has been processed. Events are notifications of change of state. Typically, events represent the change of state of something of interest to the business. Events are records of something that has happened. Events can't be changed, that is, they are immutable. (We can't change something that has happened in the past). Event streams \u00b6 An event stream is a continuous unbounded series of events. The start of the stream may have occurred before we started to process the stream. The end of the stream is at some unknown point in the future. Events are ordered by the point in time at which each event occurred. When developing event driven solutions, you will typically see two types of event streams: Event streams whose events are defined and published into a stream as part of a solution. Event streams that connect to a near real-time event stream, for example from an IOT device, a voice stream from a telephone system, a video stream, or ship or plane locations from global positioning systems. Event backbone \u00b6 The event backbone is the communication layer in the event driven architecture. It provides the connection between event driven capabilities and in the Cloud Native , it becomes the Pub/Sub communication layer for event driven microservices. At this high level we would consider two types of relevant technologies for the event backbone, Message Brokers and Event Logs . Both technology types could be used to achieve the event communication style, with the \"Publish and subscribe\" model however, it is also important to consider other capabilities which are frequently used within event driven solutions: Keeping an Event Log as a time sequenced as it happened recording of events (Source of the truth). Enabling direct replay of events. Enabling Event Sourcing as a way of recording state changes in distributed systems. Enabling programmatic access to the continuous event stream . When viewed across these wider event driven capabilities, an event log style technology can provide a central component which can support all of these capabilities, whereas a message broker would have to be extended with other components. Selecting the Event Backbone for the reference architecture \u00b6 For the event-driven architecture, we defined the following characteristics to be essential for the event backbone: Publish-subscribe event communication between event producers and consumers Facilitate many consumers with shared central \u201csource of truth\u201d. Capability to store events for a given period of time (event log). This is the shared source of the truth for events. Ability for consumers to subscribe to events. Provide replay of events from history for evolving application instances. Provide programmatic access to continuous stream of events, with minimum time lag. Must be highly scalable and resilient to cloud deployment levels. Event backbone considerations While choosing an event backbone for your event-driven application development, the following functional and non-functional requirements should be taken into consideration: Persistence \u00b6 When source systems generate events, the consumers of those are interested in those events may not be online or available at the same time. So you need a way to store these messages for a configurable period of time until they are consumed and acted upon. Event backbone should be able to provide such event persistence. Observability \u00b6 At times, you need an overall view of how events are ingested by source systems and getting processed by consumers. It could be a management console where events can be observed. Event backbone should provide such observability. Fault tolerance \u00b6 Event backbone could be made of several components. If one of them becomes unavailable, there should not be any impact on the event processors dependent on the backbone. Event backbone needs to provide this resiliency. High availability \u00b6 Event backbone provides persistence of messages/events. If one of the components of the backbone becomes unavailable, there should not be any impact on the availability of these messages/events. Event backbone should be highly available. Performance \u00b6 Event backbone should provide means of accelerating the event processing operations (e.g. parallelising event processing) thereby providing enhanced performance. Delivery guarantees \u00b6 Event backbone should support guaranteed delivery both for producer and consumer. It should support the delivery guarantee options of at least once , at most once , and exactly once . Security \u00b6 The data residing in the event backbone should be secured, at rest as well as in transit. Only authenticated and authorized users should be able to publish and consume messages from the backbone. Topic specific authorizations will also help blocking access by unauthorized consumers. Event backbone should provide these security measures. Stateful operations for events streams \u00b6 Sometimes, source systems generate a continuous flow of 'inter-related' events (e.g. IoT sensors sending data every second). In order to process such messages correctly, the event backbone needs to support for stateful operations like windowing, joins, aggregations. and any type of real time analytics. Event routing options \u00b6 In EDA, event consumers may not be online at all times. So, it should be easier for consumers to subscribe to a topic when it comes online. On-failure hooks \u00b6 Event backbone can support pre-configured actions/behaviors for certain messages. E.g. if a consumer fails to process a message more than a certain number of times, that message can be sent to another topic for re-trying the processing action. Looking across these capabilities, the potential technologies, the amount of adoption and community activity around the technologies, and the considerations listed above, we selected Apache Kafka as our event backbone for the event-driven reference architecture. Event sources \u00b6 When you consider an event-driven architecture, think about event producers and event consumers as the interaction points with events. As you develop event-driven applications following a microservices architecture, the microservices you develop play the role of both event producers and event consumers, with the events being passed as the communication payload between them. However, as you look at the wider opportunities that being event driven offers, you need to widen your view and consider event sources that come from beyond the application code you are writing. These are events that may be produced from outside our immediate system but have business relevance or enable us to gain valuable insights into things that are affecting your business. Here is a list of common event sources: \u00b6 IoT devices or sensors showing device status changes Click Stream data from web or mobile applications Mobile applications (HTTP to Back-end for Front-end service and then to topic) Geospatial data Weather alerts Social media feeds Real-time voice feeds Other messaging backbone Data change event streams from databases (change data capture) IoT devices and sensors \u00b6 With IoT devices and sensors you typically have a gateway providing the connectivity for the device, and a level of event enrichment and filtering. In terms of domain driven design you would see the device and gateway as being the technical domain and the event-driven reference architecture as providing the infrastructure for the applications in a business domain. In practice, the IoT gateway or platform provides the connectivity and is the point of filtering and consolidation of events so that only business-relevant events are passed up to the business domain. The gateway can also be the point where the technical event is enhanced to relate to something recognizable at the business level. One example of this is to relate a device number or identifier in the event to something that the business recognizes. Clickstream data \u00b6 Clickstream data is often used to understand the behavior of users as they navigate their way through web or mobile apps. It provides a recording of the actions they take, such as the clicks, the mouse-movements, and the gestures. Analysis of the clickstream data can lead to a deep understanding of how users actually interact with the application. It enables you to detect where users struggle and to look for ways to improve the experience. Processing the clickstream in real time in an event-driven architecture can also give rise to the opportunity to take direct action in response to what a user is currently doing, or more accurately has just done. There are various \"collectors\" that enable collection of standard clickstream events and allow custom actions to be collected as events typically through tags in Javascript. Within the Apache Open Source communities the Divolte collector is an example of one of these collectors that directly publishes events to Kafka topics. Event standards and schemas \u00b6 Where you have control as the producer of an event we should consider having an event schema and following a standard to provide the best opportunity for portability of the solutions across cloud environments. With a lack of formal standards, a working group under the Cloud Native Computing Foundation (CNCF) has recently been formed to define and propose Cloud Events as the standard. Microservices \u00b6 The event-driven reference architecture provides support for event-driven microservices - microservices which are connected through and communicate via the pub/sub communication protocol within an event backbone. With Kafka as the event backbone and pub/sub messaging provider, microservices can use the Kafka API's to publish and listen for events. On cloud-native platforms, microservices are the application architecture of choice. As businesses become event-driven, event driven pattern needs to extend into our microservices application space. This means that your microservices are still doing REST calls to well-known microservice but they must respond to and send out events, or in event-driven terms they need to be both event producers and consumers to enforce strong decoupling. With the adoption of microservices, the focus on synchronous communication between services has increased. Service mesh packages such as Istio help with the management of communication, service discovery, load balancing, and visibility in this synchronous communication environment. With event-driven microservices, the communication point becomes the Pub/Sub layer of the event backbone. By adopting an event-based approach for intercommunication between microservices, the microservices applications are naturally responsive (event-driven). This approach enhances the loose coupling nature of microservices because it decouples producers and consumers. Further, it enables the sharing of data across microservices through the event log. These event style characteristics are increasingly important considerations when you develop microservices style applications. In practical terms microservices applications are a combination of synchronous API-driven, and asynchronous event-driven communication styles. For the implementation point of view a set of established patterns are used, such as Database per Service, Event Sourcing, Command Query Responsibility Segregation, Saga, ... Event-driven apps with containers \u00b6 While the serverless approach with Cloud Functions provides a simplified event-based programming model, the majority of microservices applications today are developed for and deployed to a container-based cloud-native stack. Within the cloud-native landscape, Kubernetes is the standard platform for container orchestration, and therefore becomes the base for the container platform in the event-driven architecture. As before, the event backbone is the Pub/Sub communication provider and event log for shared data for the microservices. In this context microservices are developed as direct consumers and producers of events on the backbone via topics. The extra work in this environment is in managing consumer instances to respond to the demand of the event stream. You must determine how many consumer instances need to be running to keep pace with, or always be immediately available to execute, the microservice in response to an arriving event. Commands \u00b6 A command, is an instruction to do something. Typically, commands are directed to a particular consumer. The consumer runs the required command or process, and passes back a confirmation to the issuer stating that the command has been processed. The Command concept plays a key role in the Command-Query Responsibility Segregation pattern that is more commonly known as CQRS . Loose coupling \u00b6 Coupling measures the independance between connected applications and systems. Dependances is not one dimension but includes protocol, technology, communication interaction model (synchronous or asynchronous), data model, location, ... Decoupling is complex, has a cost at design level but also at technology choices so at runtime level. Synchronous request-response model is used a lot in RESTful microservice, and it brings advantages for the developers, at it is simple to implement, it has low latency and helps to address quickly receiver failure. The disadvantages come when the receiver failure blocks the requester to serve its own business logic, and may fails too or when the receiver starts to be overloaded by multiple requests. Adding queues as a way to decouple, with asynchronous point to point communication, is a natural solution for previous disadvantages. It decouples at the time level. With high availability queueing middleware the end to end processing is more resilient to receiver failure, and to traffic burst as the receiver is the one controlling the consumption of messages. Still if the queue starts to fill up, producers need to address back preassure. Also lost receiver can be handled by dead letter queues. With point-to-point, only one consumer can get a message. Asynchronous implementations are more complex to do as developers need to address response correlation with some sort of transaction ID, timestamp, consistency, horizontal scalling, and even data model contract evolution. Recalls the CAP theorem which stipulates that distributed applications could not ensure consistency, availability and partition tolerance all at the same time. They can, at most, support only two out of the three. In the internet of scale and for a lot of business application, it is better tohave availability along with eventual consistency, rather than compromising on availability on the whole. Loose coupling is one of the main benefits of event-driven processing. It allows event producers to emit events without any knowledge about who is going to consume those events (temporal decoupling). Likewise, event consumers don't need to be aware of the event emitters. Because of this, event consuming modules and event producer modules can be implemented in different languages or use technologies that are differents and appropriate for specific jobs. Loosely coupled modules are better suited to evolve independently and, when implemented correctly, result in a significant decrease in system complexity. Loose coupling, however, does not mean \u201cno coupling\u201d. An event consumer consumes events that are useful in achieving its goals and in doing so establishes what data it needs and the type and format of that data. The event producer emits events that it hopes are understood and useful to consumers thus establishing an implicit contract with potential consumers. For example, an event notification in XML format must conform to a certain schema that must be known by both the consumer and the producer. One of the most important things that you can do to reduce coupling in an event-driven system is to reduce the number of distinct event types that flow between modules. To do this you must pay attention to the cohesiveness of those modules. Cohesion \u00b6 Cohesion is the degree to which related things are encapsulated together in the same software module. For the purposes of this EDA discussion, a module is defined as an independently deployable software unit that has high cohesion. Cohesion is strongly related to coupling in the sense that a highly cohesive module communicates less with other modules, thus reducing the number of events most importantly, the number of event types in the system. The less frequently modules interact with each other, the less coupled they are. Achieving cohesion in software while optimizing module size for flexibility and adaptability is difficult, but something to strive for. Designing for cohesion starts with a holistic understanding of the problem domain and good analysis work. Sometimes it must also take into account the constraints of the supporting software environment. Monolithic implementations and implementations that are excessively fine-grained must be avoided.","title":"Terms & Definitions"},{"location":"concepts/terms-and-definitions/#events","text":"Events are notifications of change of state. Notifications are issued, or published and interested parties can subscribe and take action on the events. Typically, the issuer of the notification has no knowledge of what action is taken and receives no corresponding feedback that the notification has been processed. Events are notifications of change of state. Typically, events represent the change of state of something of interest to the business. Events are records of something that has happened. Events can't be changed, that is, they are immutable. (We can't change something that has happened in the past).","title":"Events"},{"location":"concepts/terms-and-definitions/#event-streams","text":"An event stream is a continuous unbounded series of events. The start of the stream may have occurred before we started to process the stream. The end of the stream is at some unknown point in the future. Events are ordered by the point in time at which each event occurred. When developing event driven solutions, you will typically see two types of event streams: Event streams whose events are defined and published into a stream as part of a solution. Event streams that connect to a near real-time event stream, for example from an IOT device, a voice stream from a telephone system, a video stream, or ship or plane locations from global positioning systems.","title":"Event streams"},{"location":"concepts/terms-and-definitions/#event-backbone","text":"The event backbone is the communication layer in the event driven architecture. It provides the connection between event driven capabilities and in the Cloud Native , it becomes the Pub/Sub communication layer for event driven microservices. At this high level we would consider two types of relevant technologies for the event backbone, Message Brokers and Event Logs . Both technology types could be used to achieve the event communication style, with the \"Publish and subscribe\" model however, it is also important to consider other capabilities which are frequently used within event driven solutions: Keeping an Event Log as a time sequenced as it happened recording of events (Source of the truth). Enabling direct replay of events. Enabling Event Sourcing as a way of recording state changes in distributed systems. Enabling programmatic access to the continuous event stream . When viewed across these wider event driven capabilities, an event log style technology can provide a central component which can support all of these capabilities, whereas a message broker would have to be extended with other components.","title":"Event backbone"},{"location":"concepts/terms-and-definitions/#selecting-the-event-backbone-for-the-reference-architecture","text":"For the event-driven architecture, we defined the following characteristics to be essential for the event backbone: Publish-subscribe event communication between event producers and consumers Facilitate many consumers with shared central \u201csource of truth\u201d. Capability to store events for a given period of time (event log). This is the shared source of the truth for events. Ability for consumers to subscribe to events. Provide replay of events from history for evolving application instances. Provide programmatic access to continuous stream of events, with minimum time lag. Must be highly scalable and resilient to cloud deployment levels. Event backbone considerations While choosing an event backbone for your event-driven application development, the following functional and non-functional requirements should be taken into consideration:","title":"Selecting the Event Backbone for the reference architecture"},{"location":"concepts/terms-and-definitions/#persistence","text":"When source systems generate events, the consumers of those are interested in those events may not be online or available at the same time. So you need a way to store these messages for a configurable period of time until they are consumed and acted upon. Event backbone should be able to provide such event persistence.","title":"Persistence"},{"location":"concepts/terms-and-definitions/#observability","text":"At times, you need an overall view of how events are ingested by source systems and getting processed by consumers. It could be a management console where events can be observed. Event backbone should provide such observability.","title":"Observability"},{"location":"concepts/terms-and-definitions/#fault-tolerance","text":"Event backbone could be made of several components. If one of them becomes unavailable, there should not be any impact on the event processors dependent on the backbone. Event backbone needs to provide this resiliency.","title":"Fault tolerance"},{"location":"concepts/terms-and-definitions/#high-availability","text":"Event backbone provides persistence of messages/events. If one of the components of the backbone becomes unavailable, there should not be any impact on the availability of these messages/events. Event backbone should be highly available.","title":"High availability"},{"location":"concepts/terms-and-definitions/#performance","text":"Event backbone should provide means of accelerating the event processing operations (e.g. parallelising event processing) thereby providing enhanced performance.","title":"Performance"},{"location":"concepts/terms-and-definitions/#delivery-guarantees","text":"Event backbone should support guaranteed delivery both for producer and consumer. It should support the delivery guarantee options of at least once , at most once , and exactly once .","title":"Delivery guarantees"},{"location":"concepts/terms-and-definitions/#security","text":"The data residing in the event backbone should be secured, at rest as well as in transit. Only authenticated and authorized users should be able to publish and consume messages from the backbone. Topic specific authorizations will also help blocking access by unauthorized consumers. Event backbone should provide these security measures.","title":"Security"},{"location":"concepts/terms-and-definitions/#stateful-operations-for-events-streams","text":"Sometimes, source systems generate a continuous flow of 'inter-related' events (e.g. IoT sensors sending data every second). In order to process such messages correctly, the event backbone needs to support for stateful operations like windowing, joins, aggregations. and any type of real time analytics.","title":"Stateful operations for events streams"},{"location":"concepts/terms-and-definitions/#event-routing-options","text":"In EDA, event consumers may not be online at all times. So, it should be easier for consumers to subscribe to a topic when it comes online.","title":"Event routing options"},{"location":"concepts/terms-and-definitions/#on-failure-hooks","text":"Event backbone can support pre-configured actions/behaviors for certain messages. E.g. if a consumer fails to process a message more than a certain number of times, that message can be sent to another topic for re-trying the processing action. Looking across these capabilities, the potential technologies, the amount of adoption and community activity around the technologies, and the considerations listed above, we selected Apache Kafka as our event backbone for the event-driven reference architecture.","title":"On-failure hooks"},{"location":"concepts/terms-and-definitions/#event-sources","text":"When you consider an event-driven architecture, think about event producers and event consumers as the interaction points with events. As you develop event-driven applications following a microservices architecture, the microservices you develop play the role of both event producers and event consumers, with the events being passed as the communication payload between them. However, as you look at the wider opportunities that being event driven offers, you need to widen your view and consider event sources that come from beyond the application code you are writing. These are events that may be produced from outside our immediate system but have business relevance or enable us to gain valuable insights into things that are affecting your business.","title":"Event sources"},{"location":"concepts/terms-and-definitions/#here-is-a-list-of-common-event-sources","text":"IoT devices or sensors showing device status changes Click Stream data from web or mobile applications Mobile applications (HTTP to Back-end for Front-end service and then to topic) Geospatial data Weather alerts Social media feeds Real-time voice feeds Other messaging backbone Data change event streams from databases (change data capture)","title":"Here is a list of common event sources:"},{"location":"concepts/terms-and-definitions/#iot-devices-and-sensors","text":"With IoT devices and sensors you typically have a gateway providing the connectivity for the device, and a level of event enrichment and filtering. In terms of domain driven design you would see the device and gateway as being the technical domain and the event-driven reference architecture as providing the infrastructure for the applications in a business domain. In practice, the IoT gateway or platform provides the connectivity and is the point of filtering and consolidation of events so that only business-relevant events are passed up to the business domain. The gateway can also be the point where the technical event is enhanced to relate to something recognizable at the business level. One example of this is to relate a device number or identifier in the event to something that the business recognizes.","title":"IoT devices and sensors"},{"location":"concepts/terms-and-definitions/#clickstream-data","text":"Clickstream data is often used to understand the behavior of users as they navigate their way through web or mobile apps. It provides a recording of the actions they take, such as the clicks, the mouse-movements, and the gestures. Analysis of the clickstream data can lead to a deep understanding of how users actually interact with the application. It enables you to detect where users struggle and to look for ways to improve the experience. Processing the clickstream in real time in an event-driven architecture can also give rise to the opportunity to take direct action in response to what a user is currently doing, or more accurately has just done. There are various \"collectors\" that enable collection of standard clickstream events and allow custom actions to be collected as events typically through tags in Javascript. Within the Apache Open Source communities the Divolte collector is an example of one of these collectors that directly publishes events to Kafka topics.","title":"Clickstream data"},{"location":"concepts/terms-and-definitions/#event-standards-and-schemas","text":"Where you have control as the producer of an event we should consider having an event schema and following a standard to provide the best opportunity for portability of the solutions across cloud environments. With a lack of formal standards, a working group under the Cloud Native Computing Foundation (CNCF) has recently been formed to define and propose Cloud Events as the standard.","title":"Event standards and schemas"},{"location":"concepts/terms-and-definitions/#microservices","text":"The event-driven reference architecture provides support for event-driven microservices - microservices which are connected through and communicate via the pub/sub communication protocol within an event backbone. With Kafka as the event backbone and pub/sub messaging provider, microservices can use the Kafka API's to publish and listen for events. On cloud-native platforms, microservices are the application architecture of choice. As businesses become event-driven, event driven pattern needs to extend into our microservices application space. This means that your microservices are still doing REST calls to well-known microservice but they must respond to and send out events, or in event-driven terms they need to be both event producers and consumers to enforce strong decoupling. With the adoption of microservices, the focus on synchronous communication between services has increased. Service mesh packages such as Istio help with the management of communication, service discovery, load balancing, and visibility in this synchronous communication environment. With event-driven microservices, the communication point becomes the Pub/Sub layer of the event backbone. By adopting an event-based approach for intercommunication between microservices, the microservices applications are naturally responsive (event-driven). This approach enhances the loose coupling nature of microservices because it decouples producers and consumers. Further, it enables the sharing of data across microservices through the event log. These event style characteristics are increasingly important considerations when you develop microservices style applications. In practical terms microservices applications are a combination of synchronous API-driven, and asynchronous event-driven communication styles. For the implementation point of view a set of established patterns are used, such as Database per Service, Event Sourcing, Command Query Responsibility Segregation, Saga, ...","title":"Microservices"},{"location":"concepts/terms-and-definitions/#event-driven-apps-with-containers","text":"While the serverless approach with Cloud Functions provides a simplified event-based programming model, the majority of microservices applications today are developed for and deployed to a container-based cloud-native stack. Within the cloud-native landscape, Kubernetes is the standard platform for container orchestration, and therefore becomes the base for the container platform in the event-driven architecture. As before, the event backbone is the Pub/Sub communication provider and event log for shared data for the microservices. In this context microservices are developed as direct consumers and producers of events on the backbone via topics. The extra work in this environment is in managing consumer instances to respond to the demand of the event stream. You must determine how many consumer instances need to be running to keep pace with, or always be immediately available to execute, the microservice in response to an arriving event.","title":"Event-driven apps with containers"},{"location":"concepts/terms-and-definitions/#commands","text":"A command, is an instruction to do something. Typically, commands are directed to a particular consumer. The consumer runs the required command or process, and passes back a confirmation to the issuer stating that the command has been processed. The Command concept plays a key role in the Command-Query Responsibility Segregation pattern that is more commonly known as CQRS .","title":"Commands"},{"location":"concepts/terms-and-definitions/#loose-coupling","text":"Coupling measures the independance between connected applications and systems. Dependances is not one dimension but includes protocol, technology, communication interaction model (synchronous or asynchronous), data model, location, ... Decoupling is complex, has a cost at design level but also at technology choices so at runtime level. Synchronous request-response model is used a lot in RESTful microservice, and it brings advantages for the developers, at it is simple to implement, it has low latency and helps to address quickly receiver failure. The disadvantages come when the receiver failure blocks the requester to serve its own business logic, and may fails too or when the receiver starts to be overloaded by multiple requests. Adding queues as a way to decouple, with asynchronous point to point communication, is a natural solution for previous disadvantages. It decouples at the time level. With high availability queueing middleware the end to end processing is more resilient to receiver failure, and to traffic burst as the receiver is the one controlling the consumption of messages. Still if the queue starts to fill up, producers need to address back preassure. Also lost receiver can be handled by dead letter queues. With point-to-point, only one consumer can get a message. Asynchronous implementations are more complex to do as developers need to address response correlation with some sort of transaction ID, timestamp, consistency, horizontal scalling, and even data model contract evolution. Recalls the CAP theorem which stipulates that distributed applications could not ensure consistency, availability and partition tolerance all at the same time. They can, at most, support only two out of the three. In the internet of scale and for a lot of business application, it is better tohave availability along with eventual consistency, rather than compromising on availability on the whole. Loose coupling is one of the main benefits of event-driven processing. It allows event producers to emit events without any knowledge about who is going to consume those events (temporal decoupling). Likewise, event consumers don't need to be aware of the event emitters. Because of this, event consuming modules and event producer modules can be implemented in different languages or use technologies that are differents and appropriate for specific jobs. Loosely coupled modules are better suited to evolve independently and, when implemented correctly, result in a significant decrease in system complexity. Loose coupling, however, does not mean \u201cno coupling\u201d. An event consumer consumes events that are useful in achieving its goals and in doing so establishes what data it needs and the type and format of that data. The event producer emits events that it hopes are understood and useful to consumers thus establishing an implicit contract with potential consumers. For example, an event notification in XML format must conform to a certain schema that must be known by both the consumer and the producer. One of the most important things that you can do to reduce coupling in an event-driven system is to reduce the number of distinct event types that flow between modules. To do this you must pay attention to the cohesiveness of those modules.","title":"Loose coupling"},{"location":"concepts/terms-and-definitions/#cohesion","text":"Cohesion is the degree to which related things are encapsulated together in the same software module. For the purposes of this EDA discussion, a module is defined as an independently deployable software unit that has high cohesion. Cohesion is strongly related to coupling in the sense that a highly cohesive module communicates less with other modules, thus reducing the number of events most importantly, the number of event types in the system. The less frequently modules interact with each other, the less coupled they are. Achieving cohesion in software while optimizing module size for flexibility and adaptability is difficult, but something to strive for. Designing for cohesion starts with a holistic understanding of the problem domain and good analysis work. Sometimes it must also take into account the constraints of the supporting software environment. Monolithic implementations and implementations that are excessively fine-grained must be avoided.","title":"Cohesion"},{"location":"contribute/","text":"Anyone can contribute to IBM Cloud Architecture reference applications and their associated projects, whether you are an IBMer or not. We welcome your collaboration & contributions happily, as our reference applications are meant to reflect your real world scenarios. There are multiple ways to contribute: report bugs and improvement suggestions, improve documentation, and contribute code. Bug reports, documentation changes, and feature requests \u00b6 If you would like to contribute your experience with an IBM Cloud Architecture project back to the project in the form of encountered bug reports, necessary documentation changes, or new feature requests, this can be done through the use of the repository's Issues list. Before opening a new issue, please reference the existing list to make sure a similar or duplicate item does not already exist. Otherwise, please be as explicit as possible when creating the new item and be sure to include the following: Bug reports Specific Project Version Deployment environment A minimal, but complete, setup of steps to recreate the problem Documentation changes URL to existing incorrect or incomplete documentation (either in the project's GitHub repo or external product documentation) Updates required to correct current inconsistency If possible, a link to a project fork, sample, or workflow to expose the gap in documentation. Feature requests Complete description of project feature request, including but not limited to, components of the existing project that are impacted, as well as additional components that may need to be created. A minimal, but complete, setup of steps to recreate environment necessary to identify the new feature's current gap. The more explicit and thorough you are in opening GitHub Issues, the more efficient your interaction with the maintainers will be. When creating the GitHub Issue for your bug report, documentation change, or feature request, be sure to add as many relevant labels as necessary (that are defined for that specific project). These will vary by project, but will be helpful to the maintainers in quickly triaging your new GitHub issues. Code contributions \u00b6 We really value contributions, and to maximize the impact of code contributions, we request that any contributions follow the guidelines below. If you are new to open source contribution and would like some more pointers or guidance, you may want to check out Your First PR and First Timers Only . These are a few projects that help on-board new contributors to the overall process. Coding and Pull Requests best practices \u00b6 Please ensure you follow the coding standard and code formatting used throughout the existing code base. This may vary project by project, but any specific diversion from normal language standards will be explicitly noted. One feature / bug fix / documentation update per pull request Always pull the latest changes from upstream and rebase before creating any pull request. New pull requests should be created against the integration branch of the repository, if available. This ensures new code is included in full-stack integration tests before being merged into the master branch All new features must be accompanied by associated tests. Make sure all tests pass locally before submitting a pull request. Include tests with every feature enhancement, improve tests with every bug fix Github and git flow \u00b6 The internet is littered with guides and information on how to use and understand git. However, here's a compact guide that follows the suggested workflow Fork the desired repo in github. Clone your repo to your local computer. Add the upstream repository Note: Guide for step 1-3 here: forking a repo Create new development branch off the targeted upstream branch. This will often be master . git checkout -b <my-feature-branch> master Do your work: Write your code Write your tests Pass your tests locally Commit your intermediate changes as you go and as appropriate Repeat until satisfied Fetch latest upstream changes (in case other changes had been delivered upstream while you were developing your new feature). git fetch upstream 7. Rebase to the latest upstream changes, resolving any conflicts. This will 'replay' your local commits, one by one, after the changes delivered upstream while you were locally developing, letting you manually resolve any conflict. git branch --set-upstream-to=upstream/master git rebase Instructions on how to manually resolve a conflict and commit the new change or skip your local replayed commit will be presented on screen by the git CLI. Push the changes to your repository git push origin <my-feature-branch> Create a pull request against the same targeted upstream branch. Creating a pull request Once the pull request has been reviewed, accepted and merged into the main github repository, you should synchronise your remote and local forked github repository master branch with the upstream master branch. To do so: Pull to your local forked repository the latest changes upstream (that is, the pull request). git pull upstream master Push those latest upstream changes pulled locally to your remote forked repository. git push origin master What happens next? \u00b6 All pull requests will be automatically built and unit tested by travis-ci, when implemented by that specific project. You can determine if a given project is enabled for travis-ci unit tests by the existence of a .travis.yml file in the root of the repository or branch. When in use, all travis-ci unit tests must pass completely before any further review or discussion takes place. The repository maintainer will then inspect the commit and, if accepted, will pull the code into the upstream branch. Should a maintainer or reviewer ask for changes to be made to the pull request, these can be made locally and pushed to your forked repository and branch. Commits passing this stage will make it into the next release cycle for the given project.","title":"Contribute to this Site"},{"location":"contribute/#bug-reports-documentation-changes-and-feature-requests","text":"If you would like to contribute your experience with an IBM Cloud Architecture project back to the project in the form of encountered bug reports, necessary documentation changes, or new feature requests, this can be done through the use of the repository's Issues list. Before opening a new issue, please reference the existing list to make sure a similar or duplicate item does not already exist. Otherwise, please be as explicit as possible when creating the new item and be sure to include the following: Bug reports Specific Project Version Deployment environment A minimal, but complete, setup of steps to recreate the problem Documentation changes URL to existing incorrect or incomplete documentation (either in the project's GitHub repo or external product documentation) Updates required to correct current inconsistency If possible, a link to a project fork, sample, or workflow to expose the gap in documentation. Feature requests Complete description of project feature request, including but not limited to, components of the existing project that are impacted, as well as additional components that may need to be created. A minimal, but complete, setup of steps to recreate environment necessary to identify the new feature's current gap. The more explicit and thorough you are in opening GitHub Issues, the more efficient your interaction with the maintainers will be. When creating the GitHub Issue for your bug report, documentation change, or feature request, be sure to add as many relevant labels as necessary (that are defined for that specific project). These will vary by project, but will be helpful to the maintainers in quickly triaging your new GitHub issues.","title":"Bug reports, documentation changes, and feature requests"},{"location":"contribute/#code-contributions","text":"We really value contributions, and to maximize the impact of code contributions, we request that any contributions follow the guidelines below. If you are new to open source contribution and would like some more pointers or guidance, you may want to check out Your First PR and First Timers Only . These are a few projects that help on-board new contributors to the overall process.","title":"Code contributions"},{"location":"contribute/#coding-and-pull-requests-best-practices","text":"Please ensure you follow the coding standard and code formatting used throughout the existing code base. This may vary project by project, but any specific diversion from normal language standards will be explicitly noted. One feature / bug fix / documentation update per pull request Always pull the latest changes from upstream and rebase before creating any pull request. New pull requests should be created against the integration branch of the repository, if available. This ensures new code is included in full-stack integration tests before being merged into the master branch All new features must be accompanied by associated tests. Make sure all tests pass locally before submitting a pull request. Include tests with every feature enhancement, improve tests with every bug fix","title":"Coding and Pull Requests best practices"},{"location":"contribute/#github-and-git-flow","text":"The internet is littered with guides and information on how to use and understand git. However, here's a compact guide that follows the suggested workflow Fork the desired repo in github. Clone your repo to your local computer. Add the upstream repository Note: Guide for step 1-3 here: forking a repo Create new development branch off the targeted upstream branch. This will often be master . git checkout -b <my-feature-branch> master Do your work: Write your code Write your tests Pass your tests locally Commit your intermediate changes as you go and as appropriate Repeat until satisfied Fetch latest upstream changes (in case other changes had been delivered upstream while you were developing your new feature). git fetch upstream 7. Rebase to the latest upstream changes, resolving any conflicts. This will 'replay' your local commits, one by one, after the changes delivered upstream while you were locally developing, letting you manually resolve any conflict. git branch --set-upstream-to=upstream/master git rebase Instructions on how to manually resolve a conflict and commit the new change or skip your local replayed commit will be presented on screen by the git CLI. Push the changes to your repository git push origin <my-feature-branch> Create a pull request against the same targeted upstream branch. Creating a pull request Once the pull request has been reviewed, accepted and merged into the main github repository, you should synchronise your remote and local forked github repository master branch with the upstream master branch. To do so: Pull to your local forked repository the latest changes upstream (that is, the pull request). git pull upstream master Push those latest upstream changes pulled locally to your remote forked repository. git push origin master","title":"Github and git flow"},{"location":"contribute/#what-happens-next","text":"All pull requests will be automatically built and unit tested by travis-ci, when implemented by that specific project. You can determine if a given project is enabled for travis-ci unit tests by the existence of a .travis.yml file in the root of the repository or branch. When in use, all travis-ci unit tests must pass completely before any further review or discussion takes place. The repository maintainer will then inspect the commit and, if accepted, will pull the code into the upstream branch. Should a maintainer or reviewer ask for changes to be made to the pull request, these can be made locally and pushed to your forked repository and branch. Commits passing this stage will make it into the next release cycle for the given project.","title":"What happens next?"},{"location":"introduction/overview/","text":"Introduction to the IBM Automation Event-Driven Reference Architecture \u00b6 The modern digital business works in near real-time; it informs interested parties of things of interest when they happen, makes sense of, and derives insight from an ever-growing number of sources. It learns, predicts, and is intelligent -- it is by nature Event-Driven. Event-driven architecture (EDA) is an architecture pattern that promotes the production, detection, consumption of, and reaction to events. This architectural pattern can be applied to the systems that transmit events among loosely coupled software components and services. Events are a way of capturing a statement of fact. Events occur in a continuous stream as things happen in the real and digital worlds. By taking advantage of this continuous stream, applications can not only react in near real-time, but also reason about the future based upon what has happened in the past. The business value of adopting this architecture is that you can easily extend EDA with new components that are ready to produce or consume events that are already present in the overall system. While events are more visible, new business capabilities are addressed, like near real-time insights. EDA helps also to improve the continuous availability of a microservice architecture. For enterprise IT teams, embracing event-driven development is foundational to the next generation of digital business applications. IT teams will need to be able to design, develop, deploy and operate event-driven solutions, in cloud-native styles. While event-driven architectures and reactive programming models are not new concepts, the move to cloud-native architectures with microservices, container based workloads and \"server-less\" computing allow us to revisit event-driven approaches in this cloud-native context. Indeed, we could think of event-driven as extending the resilience, agility and scale characteristics of \"cloud-native\" to also be reactive and responsive. Two aspects of a cloud-native architecture are essential to developing an event-driven architecture: Microservices - These provide the loosely coupled application architecture which enables deployment in highly distributed patterns for eesilience, agility and scale. Cloud-native platforms with Containers and \"Serverless deployments\" - These provide the application platform and tools which realize the resilience, agility and scale promise of the microservices architectures. An event-driven architecture should provide the following essential event capabilities to the cloud-native platform. Being able to communicate and persist events. Being able to take direct action on events. Processing event streams to derive near real-time insight/intelligence. Providing communication for event driven microservices.","title":"Overview"},{"location":"introduction/overview/#introduction-to-the-ibm-automation-event-driven-reference-architecture","text":"The modern digital business works in near real-time; it informs interested parties of things of interest when they happen, makes sense of, and derives insight from an ever-growing number of sources. It learns, predicts, and is intelligent -- it is by nature Event-Driven. Event-driven architecture (EDA) is an architecture pattern that promotes the production, detection, consumption of, and reaction to events. This architectural pattern can be applied to the systems that transmit events among loosely coupled software components and services. Events are a way of capturing a statement of fact. Events occur in a continuous stream as things happen in the real and digital worlds. By taking advantage of this continuous stream, applications can not only react in near real-time, but also reason about the future based upon what has happened in the past. The business value of adopting this architecture is that you can easily extend EDA with new components that are ready to produce or consume events that are already present in the overall system. While events are more visible, new business capabilities are addressed, like near real-time insights. EDA helps also to improve the continuous availability of a microservice architecture. For enterprise IT teams, embracing event-driven development is foundational to the next generation of digital business applications. IT teams will need to be able to design, develop, deploy and operate event-driven solutions, in cloud-native styles. While event-driven architectures and reactive programming models are not new concepts, the move to cloud-native architectures with microservices, container based workloads and \"server-less\" computing allow us to revisit event-driven approaches in this cloud-native context. Indeed, we could think of event-driven as extending the resilience, agility and scale characteristics of \"cloud-native\" to also be reactive and responsive. Two aspects of a cloud-native architecture are essential to developing an event-driven architecture: Microservices - These provide the loosely coupled application architecture which enables deployment in highly distributed patterns for eesilience, agility and scale. Cloud-native platforms with Containers and \"Serverless deployments\" - These provide the application platform and tools which realize the resilience, agility and scale promise of the microservices architectures. An event-driven architecture should provide the following essential event capabilities to the cloud-native platform. Being able to communicate and persist events. Being able to take direct action on events. Processing event streams to derive near real-time insight/intelligence. Providing communication for event driven microservices.","title":"Introduction to the IBM Automation Event-Driven Reference Architecture"},{"location":"introduction/reference-architecture/","text":"Updated 05/27/2022 We defined the starting point for a cloud-native event-driven architecture to be that it supports at least the following important capabilities: Being able to communicate asynchronously between components to improve elasticity, resilience and responsiveness. Support exactly one delivery of messages in a asynchronous request/response interactions Publish messages as facts or events as immutable records to an append log so subscribers can consume them at any point of time. Processing streams of events to derive real time insight/intelligence. Providing communication mechanism between event-driven microservices and functions. From SOA to EDA and meet in the middle \u00b6 We do not need to present Service Oriented Architecture as a way to organize modern application... Well modern was started in 2003. In 2006, Enterprise Service Bus was at the center of thee SOA adoption with API exposure, mediation flows, and service gateway. ESB is a pattern but also a product. Business services exposes APIs, defined with WSDL and the protocol is XML heavy with SOAP. The rich set of specification in the ws-* helps to address complex business applications, where application can dynamically searching for API producer, bind to the end point, and then call the service. SOA is still a core structure of IT architecture today. Two major characteristics of IT architecture are the scalability and the coupling with others. With the way most of SOA services are done is by pulling data from other services or call operation on API to make something done (delegation, command patterns). The dependencies between service via the service contract, and the heavy use of XML have put some limit to the scalability and coupling dimensions. EDA was already proposed, in 2004, as a way to address scalability, as illustrated by this figure below: EDA helps to decouple communication and contract between services, move to push data as immutable facts amd async communication. New applications started to be implemented, early 2010s, to address scalability need. RESTful and JSON are becoming ubiquituous technology, protocols to use. This is a neat evolution from SOAP and XML, but the applications exposing API with OpenAPI specification are still point to point, and the data pull is still the pattern to get access to data. The reactive manifesto is also promoting sound principles to make modern applications more responsive, resilient and elastics therefore adopting messages. EDA and SOA do not have to compete, but are complementary addressing different kind of problem. Combined together they provide innovative business solutions to problems we never thought could be solved: Event driven business process near real-time analytics real-time flow analysis complex event processing with time window based business rules services act as event source or sink event trigger services service to process events Event Driven Architecture \u00b6 Event-driven architecture is not something new, and it is not just about Kafka. As listed in previous section EDA at the core has to support asynchronous communication between application in two major semantics: request/response with exactly one delivery with no data lost, when a component is asking another component to do something for it. This approach is to address long running transaction or business process and enforce using queuing technology. deliver facts about its own data to an immutable log so other components can derive something about it. This is a pub/sub model with strong time decoupling. To support these EDA has a message as a service capability as cluster of message brokers. The brokers provides the connectivity between the other components: Where: Event sources : generates events from sources such as IoT devices, web apps, mobile apps, mainframe applications, change data capture agents... Mainframe queuing apps are source for messages using the IBM MQ replication feature to expose mainframe messages to the cloud native MQ broker and so to the cloud native microservices. The strong consistency is kept but facts about the business transactions are propagated to the eventual consistency world. Messaging as a service is the core backbone of the architecture to support any type of asynchronous communication: IBM MQ : delivers the exatly one delivery, strong consistency and supports queuing and pub/sub protocol. MQ brokers in cluster support high availability cross data centers to build event-mesh. IBM MQ will be the unique technology to guarantee the end to end message availability. IBM Event Streams : provides an event backbone supporting Pub/Sub protocol only, with immutable append log. Event Streams is based on Apache Kafka and can run on-premise or as managed services Reactive cloud native applications : The top row represents modern applications which adopt the reactive manifesto as a way to be resilient, elastic and message driven. To reduce microservice to microservice dependency and reduce coupling, modern microservices are event-driven and implemented with reactive framework (e.g. vert.x )and reactive messaging (e.g. microprofile 3.0 reactive messaging). Some of those applications can be function and serverless apps, meaning scaling to zero and being able to wake-up on event arrival. Finally it is important to note that business process applications running in BPM can be triggered by event arrival, can generate intermediate events, and can generate events at the process completion. Those business events are important to shared with other applications to being expose to event backbone. The adoption of Kafka as a way to support event backbone capability, also means that records can be saved for a long period of time, but it is relevant to be able to persist those records or an aggregate view of those records to a data lake or a set of s3 buckets. So most of EDA has sink connectors to data lake. The governance of these asynchronous applications is becoming a major requirement when the adoption of such architecture grows. AsyncAPI, combined with schema registry helps defining the intercommunication contracts. While most of the event-driven microservices are exposing Open APIs, enforced by API gateway, it is now possible to do the same enforcement and monitoring with the event endpoint gateway . The bottom row supports new types of application for data streaming : the first set of applications are for getting business insight of the event sequencing by looking at event patterns as supported by the complex event processing engine (Apache Flink), and the second type are to process near-real time analytics to compute analytical processing across multiple event streams. The technologies of choice are Apache Flink and IBM SQL query . Those applications are also cloud native, and run in container deployable inside Kubernetes clusters. Apache Pinot bring Realtime distributed OLAP datastore to support fast indexing, scale horizontally, OLAP queries for user-facing analytics, and application queries. Support low latency < 1s with millions events per s. Pinot can also be used for anomaly detection. In term of physical deployment on OpenShift the following figures illustrates a multi zone deployment, with Event Streams and MQ Operators deployed and managing five Event Streams brokers Cluster, and three MQ brokers This reference architecture is illustrated in the implementation of different solutions: The shipping goods oversea solution The real-time inventory The vaccine at scale solution. Kappa architecture \u00b6 The Kappa Architecture is a software architecture used for processing streaming data. The main premise behind the Kappa Architecture is that we can perform both real-time and batch processing, especially for analytics, with a single technology stack. Data in motion includes append-log based topic, and Apache Kafka acts as the store for the streaming data. Streaming processing is the practice of taking action on a series of data at the time the data is created. It can be done with different technologies like Kafka Streams , Apache Sparks streaming, Apache Flink , Redis streaming, or Hazelcast . The serving layer is where OLAP queries and searches are done, most of the time with indexing and other advanced capabilities are needed to offer excellent response time, high throughput and low latency. It is a simpler alternative to the Lambda Architecture \u2013 as all data is treated as if it were a stream. Both architectures entail the storage of historical data to enable large-scale analytics. Agile integration and EDA \u00b6 Event driven architecture is a complement of the overall integration reference architecture as presented in IBM Cloud architecture center . In this section, we want to summarize some of the important aspects of agile integration and how some of the technologies delivered as part of IBM Cloud Pak for Integration are used when doing application modernization with event-driven microservice. The main agile integration concepts as presented in detail in the IBM cloud on agile integration article can be summarized as: Empower extended teams to create integrations, leveraging a complete set of integration styles and capabilities to increase overall team productivity. Agile integration includes container-based, decentralized, microservices-aligned approach for integrating solutions Existing centralized integration architectures, based on ESB pattern, cannot support the demand, in term of team reactivity and scalability at the internet level. ESB pattern provides standardized synchronous connectivity to back-end systems typically over web services (SOAP based). ESB formed a single infrastructure for the whole enterprise, with tens or hundreds of integrations installed on a production server cluster. A single, centralized ESB certainly simplifies consistency and governance of implementation. Interface maintenance is expensive. Any deployment to the shared servers runs the risk of destabilizing existing critical interfaces. SOA encounters the issue of getting the funding at the enterprise wide program to maintain reusable interface. Integration teams are becoming the bottleneck instead of being enabler. SOA is about real-time integration between applications, whereas a microservices architecture is about how the applications are built internally. Microservice enables greater agility by being: small enough to be understood completely by their owning team and changed independently elastic to scale horizontally resilient with changes to one microservice will not affect others at runtime The following diagram illustrates the agile integration modernization transition from a centralized ESB type of architecture, and breaking integration into smaller pieces to make them more agile and more scalable. In this modernization process, development team can introduce API Management to improve decoupling between consumers and providers ultimately moving to a decentralized approach where each team can manage their own integration. Three aspects to agile integration: Decentralized integration ownership : give application teams more control over the creation and exposure of their own integration exposed as APIs, or messages Fine-grained integration deployment to separate integration, scalable independently. Changes to individual integration flows can be automatically rebuilt and deployed independently of other flows to enable safer application of changes and maximize speed to production. Cloud-native integration infrastructure to improve productivity, operational consistency and portability for both applications and integration Read more on those concepts in this note Integrating with IBM automation products \u00b6 EDA is becoming the new foundation to expose business services, and business applications. Combined with SOA and microservices architecture, it exposes messaging as a services to be used by automation products, like Robot Process Automation bots, Process Mining, Chat Bots, and the more traditional workflow engines and decision engines. Those products are leveraging data in the event backbone or in the queues to do their own processing. But they are also event producer. We can build a new digital automation reference architecture, based on event-driven and messaging communication like in the figure below: On the top row, the capabilities include: Existing business applications, and SOA services: most of them are J2EE applications running on application servers, such as WebSphere Application Server, some are mainframe applications using transactions and strong consistency. Those applications will integrate new microservices or webapps using JSON / RESTful resources exposed via an API Gateway, or will use MQ queue when asynchronous protocol is used. New single page applications are also accessing business services via the API gateway, and use HTTP (RESTful) protocol. Those single page applications can be developed using IBM Business Automation Application . Existing business process applications, running within BPM servers, may have been designed to be triggered by message arrival , and may be able to emit intermediate events within a task execution. IBM BPM, for example, has JMS queue integration to consume and publish messages to JMS queues, which should be defined in IBM MQ Brokers. A business process is the set of combined activities defined as an orchestrated workflow within the business to achieve a business goal which uses system services or refers work to process workers New workflow can be entirely event-driven, and designed to act on event arrival to trigger process instances or task instances. They are also supported by Case management products which are well suited to address unstructured execution path, where next steps are not pre-determined. As stated above, new microservices are becoming message-driven and are using queues for async request / response communication, they ask someone to participate into their business process. And they publish state changes via events to pub/sub topics. Consumers can use Event End point management to access to topics and process the data, do data mapping and data transformation for long persistence into data lake. The key integration between EDA and automation is by adding stateful agents that are aggregating, correlating, and computing the next best actions. Those actions can be to trigger a human workflow, initiate a RPA bot, call one or more decision services. Those stateful agents can use complex event processing engines, rule engines, and integrate with predictive scoring. A lot of new use-cases are covered by these capabilities like fraud detection, know your customer, product recommendations... The predictive scoring models are defined in AI workbench tool, like Watson Studio or Cloud Pak for Data. The data used to develop the model come from data lake, existing datawarehouse but also the messaging service, like Kafka topics. The AI workbench includes tools to do data analysis and visualization, to build training and test sets from any datasources and in particular topics, and tp develop models. Models are integrated to streaming analytics component. With this extended architecture, most of business applications are generating events, directly by code, or by adopting less intruisive techniques like change data capture agents. Those events are representing facts about the busienss entities and the processes implemented cross microservices and applications. This is were Process Mining product are very important to plug into this data stream. Process Mining integration \u00b6 Process mining is a tool to analyze process execution logs or data. The consumption vehicle are cvs files. When adopting EDA, data in the business process may be initiated by a single page application, sent to Kafka as events, cosumed by a human centric, long running, process application, processed by a data streaming application in Flink... Each element of this 'Saga' transaction will event time based fact about the data and so about the process. Then it will make sense to analyze those data. The integration will look like in the figure below, where data processing prepares the expected CVS format, and persists file in scalable, secured long storage like S3 buckets (IBM Cloud Object Storage), and then business analysts loads those files into Process Mining to conduct their analysis: Integration with decision service \u00b6 The figure below illustrates an integration between data produces on Event Streams, consumed by Apache Flink data streaming jobs which detect fraud or business situation which needs to be processed by business rule logic in IBM Operational Decision Management to compute the next best action: Business process integration \u00b6 BPMN has multiple event constructs to support receiving events, generating intermediate events, and sending closing events. Integration with analytics and machine learning \u00b6 The extended architecture extends the basic EDA reference architecture with concepts showing how data science, artificial intelligence and machine learning can be incorporated into an event-driven solution. The following diagram illustrats the event sources on the left injecting events to topics where green components are consuming from. Those components apply filtering, compute aggregates and stateful operation with time window based rules. Some of those components can include training scoring model, to do for example anomaly detection. The model is built with data scientist workbench tool, like Watson Studio. The starting point for data scientists to be able to derive machine learning models or analyze data for trends and behaviors is the existence of the data in a form that they can be consumed. For real-time intelligent solutions, data scientists typically inspect event histories and decision or action records from a system. Then, they reduce this data to some simplified model that scores new event data as it arrives. Getting the data for the data scientist: \u00b6 With near real-time event streams, the challenge is in handling unbounded data or a continuous flow of events. To make this consumable for the data scientist you need to capture the relevant data and store it so that it can be pulled into the analysis and model-building process as required. Following our event-driven reference architecture the event stream would be a Kafka topic on the event backbone. From here there are two possibilities for making that event data available and consumable to the data scientist: The event stream or event log can be accessed directly through Kafka and pulled into the analysis process The event stream can be pre-processed by the streaming analytics system and stored for future use in the analysis process. You have a choice of store type to use. Within public IBM cloud object storage Cloud Object Store can be used as a cost-effective historical store. Both approaches are valid, pre-processing through streaming analytics provides opportunity for greater manipulation of the data, or storing data over time windows for complex event processing. However, the more interesting distinction is where you use a predictive (ML model) to score arriving events or stream data in real time. In this case you may use streaming analytics to extract and save the event data for analysis, model building, and model training and also for scoring (executing) a derived model in line in the real time against arriving event data. The event and decision or action data is made available in cloud object storage for model building through streaming analytics. Models may be developed by tuning and parameter fitting, standard form fitting, classification techniques, and text analytics methods. Increasingly artificial intelligence (AI) and machine learning (ML) frameworks are used to discover and train useful predictive models as an alternative to parameterizing existing model types manually. These techniques lead to process and data flows where the predictive model is trained offline using event histories from the event and the decision or action store possibly augmented with some supervisory outcome labelling, as illustrated by the paths from the Event Backbone and Stream Processing store into Learn/Analyze . A model trained in this way includes some \u201cscoring\u201d API that can be invoked with fresh event data to generate a model-based prediction for future behavior and event properties of that specific context. The scoring function is then easily reincorporated into the streaming analytics processing to generate predictions and insights. These combined techniques can lead to the creation of near real-time intelligent applications: Event-driven architecture Identification of predictive insights using event storming methodology Developing models for these insights using machine learning Near real-time scoring of the insight models using a streaming analytics processing framework These are scalable easily extensible, and adaptable applications responding in near real time to new situations. There are easily extended to build out and evolve from an initial minimal viable product (MVP) because of the loose coupling in the event-driven architecture, , and streams process domains. Data scientist workbench \u00b6 To complete the extended architecture for integration with analytics and machine learning, consider the toolset and frameworks that the data scientist can use to derive the models. Watson Studio provides tools for data scientists, application developers, and subject matter experts to collaboratively and easily work with data to build and train models at scale. For more information see Getting started with Watson Studio. Modern Data Lake \u00b6 One of the standard architecture to build data lake is the lambda architecture with data injection, stream processing, batch processing to data store and then queries as part of the service layer. It is designed to handle massive quantities of data by taking advantage of both batch and stream processing methods. Lambda architecture depends on a data model with an append-only, immutable data source that serves as a system of record. The batch layer precomputes results using a distributed processing system that can handle very large quantities of data. Output from the batch and speed layers are stored in the serving layer, which responds to ad-hoc queries by returning precomputed views or building views from the processed data. The following figure is an enhancement of the lambda architecture with the adoption of Kafka as event backbone for data pipeline and source of truth and streaming processing to support real time analytics and streaming queries. On the left the different data sources, injected using different protocols like MQTT, HTTP, or Kafka Connect... The business applications are supported by different microservices that are exposed by APIs and event-driven. The APIs is managed by API management product. Business events are produced as facts about the business entities, and persisted in the append log of kafka topic. Transactional data can be injected from MQ queues to Kafka topic, via MQ connectors. The data platform offers a set of capabilities to expose data for consumers like Data Science workbench (Watson Studio) via virtualization and data connections. The data are cataloged and governed to ensure integrity and visibility. The storage can be block based, document oriented or table oriented. Batch queries and map reduce can address huge data raw, while streaming queries can support real time aggregates and analytics. Legacy Integration \u00b6 While you create new digital business applications as self-contained systems, you likely need to integrate legacy apps and databases into the event-driven system. Two ways of coming directly into the event-driven architecture are as follows: Where legacy applications are connected with MQ. You can connect directly from MQ to the Kafka in the event backbone. See IBM Event Streams getting started with MQ article . The major idea here is to leverage the transactionality support of MQ, so writing to the databased and to the queue happen in the same transaction: Where databases support the capture of changes to data, you can publish changes as events to Kafka and hence into the event infrastructure. This could leverage the outbox pattern where events are prepared by the application and written, in the same transaction as the other tables, and read by the CDC capture agent. Or use an efficient CDC product to get the change data capture at the transaction level. IBM offers the best CDC product on the market, ( InfoSphere Data Replication 11.4.0 ), with subsecond average latency and support full transactional semantics with exactly once consumption. It includes an efficient Kafka integration . One of the challenges of basic CDC products, is the replication per table pattern, leads to retry to rebuild the transaction integrity using kafka stream to join data from multiple topics. The TCC (Transactionally consistent consumer) technology allows Kafka replication to have semantics similar to a relational database. This dramatically increases the types of business logic that can be implemented. Developer can recreate the order of operations in source transactions across multiple Kafka topics and partitions and consume Kafka records that are free of duplicates by including the Kafka transactionally consistent consumer library in your Java applications. TCC allows: Elimination of any duplicates, even in abnormal termination scenarios Reconstruction of exact transaction order, despite those transactions having been performance optimized and applied out of order to Kafka Reconstruction of exact operation order within a transaction, despite said operations having been applied to different topics and/or partitions. This is not offered by default Kafka's \"exactly once\" functionality Ability for hundreds+ producers to participate in a single transaction. Kafka's implementation has one producer create all messages for a transaction, despite those messages going to different topics. Provides a unique bookmark, so that downstream applications can check-point and resume exactly where they last left off if they fail. We recommend listeing to this presentation from Shawn Roberston - IBM, on A Solution for Leveraging Kafka to Provide End-to-End ACID Transactions The second, very important, feature is on the producer side, with the Kafka custom operation processor (or KCOP) infrastructure. KCOP helps you to control over the Kafka producer records that are written to Kafka topics in response to insert, update, and delete operations that occur on source database tables. It allows a user to programmatically dictate the exact key an byte values of the message written to Kafka. Therefore any individual row transformation message encoding format is achievable. Out of the box it includes Avro, CSV, JSON message encoding formats. It is possible to perform column level RSA encryption on certain values before writing. It also permits enriching of the message with additional annotation if needed. Developers have the complete choice over how data is represented. Eg. Can write data in Kafka Compaction compliant format with deletes being represented as Kafka tombstones or can write the content of the message being deleted. It also supports Kafka Header fields for efficient downstream routing without the need for immediate de-serialization. The KCOP allows a user to determine how many messages are written to Kafka in response to a source operation, the content of the messages, and their destination. Allows for filtering based on column values. Allows for writing the entire row with sensitive data to highly restricted topics and a subset of the columns to wider shared topics. Allows for writing the same message in two different formats to two different topics. Useful in environments where some consuming applications want JSON, others prefer Avro, both can be produced in parallel if desired. Allows for sending special flags to a monitoring topic. Eg. when a transaction exceeds $500, in addition to the regular message, a special topic is written to notifying of the large value transaction The two diagrams above, illustrate a common architecture for data pipeline, using event backbone, where the data is transformed into different data model, that can be consumed by components that act on those data, and move the data document into data lake for big data processing. Finally it is important to note that the deployment of the event streams, CDC can be colocated in the mainframe to reduce operation and runtime cost. It also reduces complexity. In the following diagram, event stream brokers are deployed on OpenShift on Linux on Z and the CDC servers on Linux too. This architecture pattern try to reduce the MIPs utilization on the mainframe to the minimum by still ensuring data pipeline, with transactional integrity. Quality of Service \u2013 autoscaling / balancing between Linux nodes, Resilience. Latency - memory speed (Network -> HiperSocket, with memory speed and bandwidth) Reduce MIPS (avoid Authentication-TLS overhead on z/OS as no network traffic is encrypted) Avoid network spend / management / maintenance between servers Improved QoS for the Kafka service \u2013 inherits Z platform (Event Streams is the only Kafka variant currently supported on Linix on Z) Reduced complexity / management cost Reduced latency / network infrastructure (apply \u2013 Kafka hop is now in memory) \u2013 avoids need for encryption The CDC server uses Transaction Capture Consumer to keep transaction integrity while publishing to kafka topic. CICS Business events are mechanism for declaratively emitting event from CICS routines.","title":"Reference Architecture"},{"location":"introduction/reference-architecture/#from-soa-to-eda-and-meet-in-the-middle","text":"We do not need to present Service Oriented Architecture as a way to organize modern application... Well modern was started in 2003. In 2006, Enterprise Service Bus was at the center of thee SOA adoption with API exposure, mediation flows, and service gateway. ESB is a pattern but also a product. Business services exposes APIs, defined with WSDL and the protocol is XML heavy with SOAP. The rich set of specification in the ws-* helps to address complex business applications, where application can dynamically searching for API producer, bind to the end point, and then call the service. SOA is still a core structure of IT architecture today. Two major characteristics of IT architecture are the scalability and the coupling with others. With the way most of SOA services are done is by pulling data from other services or call operation on API to make something done (delegation, command patterns). The dependencies between service via the service contract, and the heavy use of XML have put some limit to the scalability and coupling dimensions. EDA was already proposed, in 2004, as a way to address scalability, as illustrated by this figure below: EDA helps to decouple communication and contract between services, move to push data as immutable facts amd async communication. New applications started to be implemented, early 2010s, to address scalability need. RESTful and JSON are becoming ubiquituous technology, protocols to use. This is a neat evolution from SOAP and XML, but the applications exposing API with OpenAPI specification are still point to point, and the data pull is still the pattern to get access to data. The reactive manifesto is also promoting sound principles to make modern applications more responsive, resilient and elastics therefore adopting messages. EDA and SOA do not have to compete, but are complementary addressing different kind of problem. Combined together they provide innovative business solutions to problems we never thought could be solved: Event driven business process near real-time analytics real-time flow analysis complex event processing with time window based business rules services act as event source or sink event trigger services service to process events","title":"From SOA to EDA and meet in the middle"},{"location":"introduction/reference-architecture/#event-driven-architecture","text":"Event-driven architecture is not something new, and it is not just about Kafka. As listed in previous section EDA at the core has to support asynchronous communication between application in two major semantics: request/response with exactly one delivery with no data lost, when a component is asking another component to do something for it. This approach is to address long running transaction or business process and enforce using queuing technology. deliver facts about its own data to an immutable log so other components can derive something about it. This is a pub/sub model with strong time decoupling. To support these EDA has a message as a service capability as cluster of message brokers. The brokers provides the connectivity between the other components: Where: Event sources : generates events from sources such as IoT devices, web apps, mobile apps, mainframe applications, change data capture agents... Mainframe queuing apps are source for messages using the IBM MQ replication feature to expose mainframe messages to the cloud native MQ broker and so to the cloud native microservices. The strong consistency is kept but facts about the business transactions are propagated to the eventual consistency world. Messaging as a service is the core backbone of the architecture to support any type of asynchronous communication: IBM MQ : delivers the exatly one delivery, strong consistency and supports queuing and pub/sub protocol. MQ brokers in cluster support high availability cross data centers to build event-mesh. IBM MQ will be the unique technology to guarantee the end to end message availability. IBM Event Streams : provides an event backbone supporting Pub/Sub protocol only, with immutable append log. Event Streams is based on Apache Kafka and can run on-premise or as managed services Reactive cloud native applications : The top row represents modern applications which adopt the reactive manifesto as a way to be resilient, elastic and message driven. To reduce microservice to microservice dependency and reduce coupling, modern microservices are event-driven and implemented with reactive framework (e.g. vert.x )and reactive messaging (e.g. microprofile 3.0 reactive messaging). Some of those applications can be function and serverless apps, meaning scaling to zero and being able to wake-up on event arrival. Finally it is important to note that business process applications running in BPM can be triggered by event arrival, can generate intermediate events, and can generate events at the process completion. Those business events are important to shared with other applications to being expose to event backbone. The adoption of Kafka as a way to support event backbone capability, also means that records can be saved for a long period of time, but it is relevant to be able to persist those records or an aggregate view of those records to a data lake or a set of s3 buckets. So most of EDA has sink connectors to data lake. The governance of these asynchronous applications is becoming a major requirement when the adoption of such architecture grows. AsyncAPI, combined with schema registry helps defining the intercommunication contracts. While most of the event-driven microservices are exposing Open APIs, enforced by API gateway, it is now possible to do the same enforcement and monitoring with the event endpoint gateway . The bottom row supports new types of application for data streaming : the first set of applications are for getting business insight of the event sequencing by looking at event patterns as supported by the complex event processing engine (Apache Flink), and the second type are to process near-real time analytics to compute analytical processing across multiple event streams. The technologies of choice are Apache Flink and IBM SQL query . Those applications are also cloud native, and run in container deployable inside Kubernetes clusters. Apache Pinot bring Realtime distributed OLAP datastore to support fast indexing, scale horizontally, OLAP queries for user-facing analytics, and application queries. Support low latency < 1s with millions events per s. Pinot can also be used for anomaly detection. In term of physical deployment on OpenShift the following figures illustrates a multi zone deployment, with Event Streams and MQ Operators deployed and managing five Event Streams brokers Cluster, and three MQ brokers This reference architecture is illustrated in the implementation of different solutions: The shipping goods oversea solution The real-time inventory The vaccine at scale solution.","title":"Event Driven Architecture"},{"location":"introduction/reference-architecture/#kappa-architecture","text":"The Kappa Architecture is a software architecture used for processing streaming data. The main premise behind the Kappa Architecture is that we can perform both real-time and batch processing, especially for analytics, with a single technology stack. Data in motion includes append-log based topic, and Apache Kafka acts as the store for the streaming data. Streaming processing is the practice of taking action on a series of data at the time the data is created. It can be done with different technologies like Kafka Streams , Apache Sparks streaming, Apache Flink , Redis streaming, or Hazelcast . The serving layer is where OLAP queries and searches are done, most of the time with indexing and other advanced capabilities are needed to offer excellent response time, high throughput and low latency. It is a simpler alternative to the Lambda Architecture \u2013 as all data is treated as if it were a stream. Both architectures entail the storage of historical data to enable large-scale analytics.","title":"Kappa architecture"},{"location":"introduction/reference-architecture/#agile-integration-and-eda","text":"Event driven architecture is a complement of the overall integration reference architecture as presented in IBM Cloud architecture center . In this section, we want to summarize some of the important aspects of agile integration and how some of the technologies delivered as part of IBM Cloud Pak for Integration are used when doing application modernization with event-driven microservice. The main agile integration concepts as presented in detail in the IBM cloud on agile integration article can be summarized as: Empower extended teams to create integrations, leveraging a complete set of integration styles and capabilities to increase overall team productivity. Agile integration includes container-based, decentralized, microservices-aligned approach for integrating solutions Existing centralized integration architectures, based on ESB pattern, cannot support the demand, in term of team reactivity and scalability at the internet level. ESB pattern provides standardized synchronous connectivity to back-end systems typically over web services (SOAP based). ESB formed a single infrastructure for the whole enterprise, with tens or hundreds of integrations installed on a production server cluster. A single, centralized ESB certainly simplifies consistency and governance of implementation. Interface maintenance is expensive. Any deployment to the shared servers runs the risk of destabilizing existing critical interfaces. SOA encounters the issue of getting the funding at the enterprise wide program to maintain reusable interface. Integration teams are becoming the bottleneck instead of being enabler. SOA is about real-time integration between applications, whereas a microservices architecture is about how the applications are built internally. Microservice enables greater agility by being: small enough to be understood completely by their owning team and changed independently elastic to scale horizontally resilient with changes to one microservice will not affect others at runtime The following diagram illustrates the agile integration modernization transition from a centralized ESB type of architecture, and breaking integration into smaller pieces to make them more agile and more scalable. In this modernization process, development team can introduce API Management to improve decoupling between consumers and providers ultimately moving to a decentralized approach where each team can manage their own integration. Three aspects to agile integration: Decentralized integration ownership : give application teams more control over the creation and exposure of their own integration exposed as APIs, or messages Fine-grained integration deployment to separate integration, scalable independently. Changes to individual integration flows can be automatically rebuilt and deployed independently of other flows to enable safer application of changes and maximize speed to production. Cloud-native integration infrastructure to improve productivity, operational consistency and portability for both applications and integration Read more on those concepts in this note","title":"Agile integration and EDA"},{"location":"introduction/reference-architecture/#integrating-with-ibm-automation-products","text":"EDA is becoming the new foundation to expose business services, and business applications. Combined with SOA and microservices architecture, it exposes messaging as a services to be used by automation products, like Robot Process Automation bots, Process Mining, Chat Bots, and the more traditional workflow engines and decision engines. Those products are leveraging data in the event backbone or in the queues to do their own processing. But they are also event producer. We can build a new digital automation reference architecture, based on event-driven and messaging communication like in the figure below: On the top row, the capabilities include: Existing business applications, and SOA services: most of them are J2EE applications running on application servers, such as WebSphere Application Server, some are mainframe applications using transactions and strong consistency. Those applications will integrate new microservices or webapps using JSON / RESTful resources exposed via an API Gateway, or will use MQ queue when asynchronous protocol is used. New single page applications are also accessing business services via the API gateway, and use HTTP (RESTful) protocol. Those single page applications can be developed using IBM Business Automation Application . Existing business process applications, running within BPM servers, may have been designed to be triggered by message arrival , and may be able to emit intermediate events within a task execution. IBM BPM, for example, has JMS queue integration to consume and publish messages to JMS queues, which should be defined in IBM MQ Brokers. A business process is the set of combined activities defined as an orchestrated workflow within the business to achieve a business goal which uses system services or refers work to process workers New workflow can be entirely event-driven, and designed to act on event arrival to trigger process instances or task instances. They are also supported by Case management products which are well suited to address unstructured execution path, where next steps are not pre-determined. As stated above, new microservices are becoming message-driven and are using queues for async request / response communication, they ask someone to participate into their business process. And they publish state changes via events to pub/sub topics. Consumers can use Event End point management to access to topics and process the data, do data mapping and data transformation for long persistence into data lake. The key integration between EDA and automation is by adding stateful agents that are aggregating, correlating, and computing the next best actions. Those actions can be to trigger a human workflow, initiate a RPA bot, call one or more decision services. Those stateful agents can use complex event processing engines, rule engines, and integrate with predictive scoring. A lot of new use-cases are covered by these capabilities like fraud detection, know your customer, product recommendations... The predictive scoring models are defined in AI workbench tool, like Watson Studio or Cloud Pak for Data. The data used to develop the model come from data lake, existing datawarehouse but also the messaging service, like Kafka topics. The AI workbench includes tools to do data analysis and visualization, to build training and test sets from any datasources and in particular topics, and tp develop models. Models are integrated to streaming analytics component. With this extended architecture, most of business applications are generating events, directly by code, or by adopting less intruisive techniques like change data capture agents. Those events are representing facts about the busienss entities and the processes implemented cross microservices and applications. This is were Process Mining product are very important to plug into this data stream.","title":"Integrating with IBM automation products"},{"location":"introduction/reference-architecture/#process-mining-integration","text":"Process mining is a tool to analyze process execution logs or data. The consumption vehicle are cvs files. When adopting EDA, data in the business process may be initiated by a single page application, sent to Kafka as events, cosumed by a human centric, long running, process application, processed by a data streaming application in Flink... Each element of this 'Saga' transaction will event time based fact about the data and so about the process. Then it will make sense to analyze those data. The integration will look like in the figure below, where data processing prepares the expected CVS format, and persists file in scalable, secured long storage like S3 buckets (IBM Cloud Object Storage), and then business analysts loads those files into Process Mining to conduct their analysis:","title":"Process Mining integration"},{"location":"introduction/reference-architecture/#integration-with-decision-service","text":"The figure below illustrates an integration between data produces on Event Streams, consumed by Apache Flink data streaming jobs which detect fraud or business situation which needs to be processed by business rule logic in IBM Operational Decision Management to compute the next best action:","title":"Integration with decision service"},{"location":"introduction/reference-architecture/#business-process-integration","text":"BPMN has multiple event constructs to support receiving events, generating intermediate events, and sending closing events.","title":"Business process integration"},{"location":"introduction/reference-architecture/#integration-with-analytics-and-machine-learning","text":"The extended architecture extends the basic EDA reference architecture with concepts showing how data science, artificial intelligence and machine learning can be incorporated into an event-driven solution. The following diagram illustrats the event sources on the left injecting events to topics where green components are consuming from. Those components apply filtering, compute aggregates and stateful operation with time window based rules. Some of those components can include training scoring model, to do for example anomaly detection. The model is built with data scientist workbench tool, like Watson Studio. The starting point for data scientists to be able to derive machine learning models or analyze data for trends and behaviors is the existence of the data in a form that they can be consumed. For real-time intelligent solutions, data scientists typically inspect event histories and decision or action records from a system. Then, they reduce this data to some simplified model that scores new event data as it arrives.","title":"Integration with analytics and machine learning"},{"location":"introduction/reference-architecture/#getting-the-data-for-the-data-scientist","text":"With near real-time event streams, the challenge is in handling unbounded data or a continuous flow of events. To make this consumable for the data scientist you need to capture the relevant data and store it so that it can be pulled into the analysis and model-building process as required. Following our event-driven reference architecture the event stream would be a Kafka topic on the event backbone. From here there are two possibilities for making that event data available and consumable to the data scientist: The event stream or event log can be accessed directly through Kafka and pulled into the analysis process The event stream can be pre-processed by the streaming analytics system and stored for future use in the analysis process. You have a choice of store type to use. Within public IBM cloud object storage Cloud Object Store can be used as a cost-effective historical store. Both approaches are valid, pre-processing through streaming analytics provides opportunity for greater manipulation of the data, or storing data over time windows for complex event processing. However, the more interesting distinction is where you use a predictive (ML model) to score arriving events or stream data in real time. In this case you may use streaming analytics to extract and save the event data for analysis, model building, and model training and also for scoring (executing) a derived model in line in the real time against arriving event data. The event and decision or action data is made available in cloud object storage for model building through streaming analytics. Models may be developed by tuning and parameter fitting, standard form fitting, classification techniques, and text analytics methods. Increasingly artificial intelligence (AI) and machine learning (ML) frameworks are used to discover and train useful predictive models as an alternative to parameterizing existing model types manually. These techniques lead to process and data flows where the predictive model is trained offline using event histories from the event and the decision or action store possibly augmented with some supervisory outcome labelling, as illustrated by the paths from the Event Backbone and Stream Processing store into Learn/Analyze . A model trained in this way includes some \u201cscoring\u201d API that can be invoked with fresh event data to generate a model-based prediction for future behavior and event properties of that specific context. The scoring function is then easily reincorporated into the streaming analytics processing to generate predictions and insights. These combined techniques can lead to the creation of near real-time intelligent applications: Event-driven architecture Identification of predictive insights using event storming methodology Developing models for these insights using machine learning Near real-time scoring of the insight models using a streaming analytics processing framework These are scalable easily extensible, and adaptable applications responding in near real time to new situations. There are easily extended to build out and evolve from an initial minimal viable product (MVP) because of the loose coupling in the event-driven architecture, , and streams process domains.","title":"Getting the data for the data scientist:"},{"location":"introduction/reference-architecture/#data-scientist-workbench","text":"To complete the extended architecture for integration with analytics and machine learning, consider the toolset and frameworks that the data scientist can use to derive the models. Watson Studio provides tools for data scientists, application developers, and subject matter experts to collaboratively and easily work with data to build and train models at scale. For more information see Getting started with Watson Studio.","title":"Data scientist workbench"},{"location":"introduction/reference-architecture/#modern-data-lake","text":"One of the standard architecture to build data lake is the lambda architecture with data injection, stream processing, batch processing to data store and then queries as part of the service layer. It is designed to handle massive quantities of data by taking advantage of both batch and stream processing methods. Lambda architecture depends on a data model with an append-only, immutable data source that serves as a system of record. The batch layer precomputes results using a distributed processing system that can handle very large quantities of data. Output from the batch and speed layers are stored in the serving layer, which responds to ad-hoc queries by returning precomputed views or building views from the processed data. The following figure is an enhancement of the lambda architecture with the adoption of Kafka as event backbone for data pipeline and source of truth and streaming processing to support real time analytics and streaming queries. On the left the different data sources, injected using different protocols like MQTT, HTTP, or Kafka Connect... The business applications are supported by different microservices that are exposed by APIs and event-driven. The APIs is managed by API management product. Business events are produced as facts about the business entities, and persisted in the append log of kafka topic. Transactional data can be injected from MQ queues to Kafka topic, via MQ connectors. The data platform offers a set of capabilities to expose data for consumers like Data Science workbench (Watson Studio) via virtualization and data connections. The data are cataloged and governed to ensure integrity and visibility. The storage can be block based, document oriented or table oriented. Batch queries and map reduce can address huge data raw, while streaming queries can support real time aggregates and analytics.","title":"Modern Data Lake"},{"location":"introduction/reference-architecture/#legacy-integration","text":"While you create new digital business applications as self-contained systems, you likely need to integrate legacy apps and databases into the event-driven system. Two ways of coming directly into the event-driven architecture are as follows: Where legacy applications are connected with MQ. You can connect directly from MQ to the Kafka in the event backbone. See IBM Event Streams getting started with MQ article . The major idea here is to leverage the transactionality support of MQ, so writing to the databased and to the queue happen in the same transaction: Where databases support the capture of changes to data, you can publish changes as events to Kafka and hence into the event infrastructure. This could leverage the outbox pattern where events are prepared by the application and written, in the same transaction as the other tables, and read by the CDC capture agent. Or use an efficient CDC product to get the change data capture at the transaction level. IBM offers the best CDC product on the market, ( InfoSphere Data Replication 11.4.0 ), with subsecond average latency and support full transactional semantics with exactly once consumption. It includes an efficient Kafka integration . One of the challenges of basic CDC products, is the replication per table pattern, leads to retry to rebuild the transaction integrity using kafka stream to join data from multiple topics. The TCC (Transactionally consistent consumer) technology allows Kafka replication to have semantics similar to a relational database. This dramatically increases the types of business logic that can be implemented. Developer can recreate the order of operations in source transactions across multiple Kafka topics and partitions and consume Kafka records that are free of duplicates by including the Kafka transactionally consistent consumer library in your Java applications. TCC allows: Elimination of any duplicates, even in abnormal termination scenarios Reconstruction of exact transaction order, despite those transactions having been performance optimized and applied out of order to Kafka Reconstruction of exact operation order within a transaction, despite said operations having been applied to different topics and/or partitions. This is not offered by default Kafka's \"exactly once\" functionality Ability for hundreds+ producers to participate in a single transaction. Kafka's implementation has one producer create all messages for a transaction, despite those messages going to different topics. Provides a unique bookmark, so that downstream applications can check-point and resume exactly where they last left off if they fail. We recommend listeing to this presentation from Shawn Roberston - IBM, on A Solution for Leveraging Kafka to Provide End-to-End ACID Transactions The second, very important, feature is on the producer side, with the Kafka custom operation processor (or KCOP) infrastructure. KCOP helps you to control over the Kafka producer records that are written to Kafka topics in response to insert, update, and delete operations that occur on source database tables. It allows a user to programmatically dictate the exact key an byte values of the message written to Kafka. Therefore any individual row transformation message encoding format is achievable. Out of the box it includes Avro, CSV, JSON message encoding formats. It is possible to perform column level RSA encryption on certain values before writing. It also permits enriching of the message with additional annotation if needed. Developers have the complete choice over how data is represented. Eg. Can write data in Kafka Compaction compliant format with deletes being represented as Kafka tombstones or can write the content of the message being deleted. It also supports Kafka Header fields for efficient downstream routing without the need for immediate de-serialization. The KCOP allows a user to determine how many messages are written to Kafka in response to a source operation, the content of the messages, and their destination. Allows for filtering based on column values. Allows for writing the entire row with sensitive data to highly restricted topics and a subset of the columns to wider shared topics. Allows for writing the same message in two different formats to two different topics. Useful in environments where some consuming applications want JSON, others prefer Avro, both can be produced in parallel if desired. Allows for sending special flags to a monitoring topic. Eg. when a transaction exceeds $500, in addition to the regular message, a special topic is written to notifying of the large value transaction The two diagrams above, illustrate a common architecture for data pipeline, using event backbone, where the data is transformed into different data model, that can be consumed by components that act on those data, and move the data document into data lake for big data processing. Finally it is important to note that the deployment of the event streams, CDC can be colocated in the mainframe to reduce operation and runtime cost. It also reduces complexity. In the following diagram, event stream brokers are deployed on OpenShift on Linux on Z and the CDC servers on Linux too. This architecture pattern try to reduce the MIPs utilization on the mainframe to the minimum by still ensuring data pipeline, with transactional integrity. Quality of Service \u2013 autoscaling / balancing between Linux nodes, Resilience. Latency - memory speed (Network -> HiperSocket, with memory speed and bandwidth) Reduce MIPS (avoid Authentication-TLS overhead on z/OS as no network traffic is encrypted) Avoid network spend / management / maintenance between servers Improved QoS for the Kafka service \u2013 inherits Z platform (Event Streams is the only Kafka variant currently supported on Linix on Z) Reduced complexity / management cost Reduced latency / network infrastructure (apply \u2013 Kafka hop is now in memory) \u2013 avoids need for encryption The CDC server uses Transaction Capture Consumer to keep transaction integrity while publishing to kafka topic. CICS Business events are mechanism for declaratively emitting event from CICS routines.","title":"Legacy Integration"},{"location":"introduction/target-audiences/","text":"While the content of this repository is mostly technical in nature and is intended for a technical audience, it also introduces methodology practices, such as Event Storming , which would be used with business leaders to identify key business domain events and actions. You may find it useful to share this information with your business leaders before engaging them in such activities. At a high level, this is what you should expect to learn by working through this repository and the related examples: As an architect, you will understand how the event-driven architecture provides capabilities which support development of event-driven solutions. As a developer, you will understand how to develop event-driven applications and develop analytics based on event streams. As a project manager, you may understand all the artifacts which may be required for an event-driven solution. The related repositories provide sample code and best practices which you may want to reuse during your future implementations. The reference architecture has been designed to be portable and applicable to Public Cloud, Hybrid Cloud and across multiple clouds. Examples given are directly deployable in IBM Public Cloud and Red Hat OpenShift Container Platform.","title":"Target Audiences"},{"location":"introduction/usecases/","text":"Updated 04/19/2022 Business use cases \u00b6 In recent years, due to the business demands for greater responsiveness and awareness of context in business decisions, organizations have taken a more strategic approach to supporting EDA. We have observed the following the main business justifications for adopting real-time processing, event-driven architecture. Response to events in real time : Get visibility of events cross applications, even if they were not designed as event-producer and then act on those events, by deriving synthetic events or trigger business processes. Business are looking at better way to understand user's behavior on their on-line presence, and being able to act quickly for product recommendations, propose ad-hoc support, gather data for fraud detection, cause analysis of potential customer churn, fraud detection... Deliver responsive customer experiences : This is also about scaling web applications, locate processing closer to the end users, be resilient to underlying business services failure, separate read from write models, adoption reactive - manifesto while programming new business services Brings real-time intelligence : is about integrating analytics to real-time data streaming. Moving out of batch processing when it is relevant to compute aggregates on data in motion. This also includes embedding AI model, into the streaming application. Intelligence also means rule based reasoning, and complex event processing helps to recognize event patterns and act on them. The use Event in 2022, a lot of companies are not aware of what's is going on inside the company and with their customer's interactions. Technical use cases \u00b6 Recurring technical needs and use cases are specifics for adopting event-driven architecture. We can list the following important concerns: Communication layer: Adopt messaging and asynchronous communication between applications and microservices for loosely coupled services. Messaging helps exchanging data between services and message brokers are needed to support the persistence and high availability to produce and consume those data. (See this note where we present a way to support a service mesh solution using asynchronous event). The data are pushed as immutables record, or commands are sent with exactly once delivery. Reducing pull data approach. Pub/sub messaging for cloud native applications to improve communication inter microservices. Support Always-on services with asynchronous data replication. Expose data to any application to consume. Time decoupling is important and consumers fetch data when they want. Reactive systems: to support reactive, responsive applications for addressing resiliency, scaling so adopting message buses. Adopt real-time processing moving out of batch processing when possible. Data Agility: The distributed nature of cloud native applications, means we need to address subjects like availability, consistency and resilience to network partitioning. Data consitency could not be guarantee, without strong transaction support, but multi-cloud, multi data centers, application allocations in different container orchestrator, means dealing with eventual consistency. Centralize online data pipeline to decouple applications and microservices. Monitor distributed applications to produce centralized feed of operational data. Logs collector from multiple services. Implement event sourcing pattern out of the box, using configuration to keep message for a long time period. Data are replicated between brokers within the cluster and cross availability zones if needed. As data is visible in highly available manner, persisted in distributed broker cluster, those brokers are becoming data hub, data buffering for larger data pipeline processing. Near real-time analytics insights: Compute real-time aggregates, time window based reasoning, and even complex event-processing which looks after event sequencing patterns. Aggregation of event coming from multiple producers. Look at event sequencing patterns Compute next best action from event streams","title":"Business Use Cases"},{"location":"introduction/usecases/#business-use-cases","text":"In recent years, due to the business demands for greater responsiveness and awareness of context in business decisions, organizations have taken a more strategic approach to supporting EDA. We have observed the following the main business justifications for adopting real-time processing, event-driven architecture. Response to events in real time : Get visibility of events cross applications, even if they were not designed as event-producer and then act on those events, by deriving synthetic events or trigger business processes. Business are looking at better way to understand user's behavior on their on-line presence, and being able to act quickly for product recommendations, propose ad-hoc support, gather data for fraud detection, cause analysis of potential customer churn, fraud detection... Deliver responsive customer experiences : This is also about scaling web applications, locate processing closer to the end users, be resilient to underlying business services failure, separate read from write models, adoption reactive - manifesto while programming new business services Brings real-time intelligence : is about integrating analytics to real-time data streaming. Moving out of batch processing when it is relevant to compute aggregates on data in motion. This also includes embedding AI model, into the streaming application. Intelligence also means rule based reasoning, and complex event processing helps to recognize event patterns and act on them. The use Event in 2022, a lot of companies are not aware of what's is going on inside the company and with their customer's interactions.","title":"Business use cases"},{"location":"introduction/usecases/#technical-use-cases","text":"Recurring technical needs and use cases are specifics for adopting event-driven architecture. We can list the following important concerns: Communication layer: Adopt messaging and asynchronous communication between applications and microservices for loosely coupled services. Messaging helps exchanging data between services and message brokers are needed to support the persistence and high availability to produce and consume those data. (See this note where we present a way to support a service mesh solution using asynchronous event). The data are pushed as immutables record, or commands are sent with exactly once delivery. Reducing pull data approach. Pub/sub messaging for cloud native applications to improve communication inter microservices. Support Always-on services with asynchronous data replication. Expose data to any application to consume. Time decoupling is important and consumers fetch data when they want. Reactive systems: to support reactive, responsive applications for addressing resiliency, scaling so adopting message buses. Adopt real-time processing moving out of batch processing when possible. Data Agility: The distributed nature of cloud native applications, means we need to address subjects like availability, consistency and resilience to network partitioning. Data consitency could not be guarantee, without strong transaction support, but multi-cloud, multi data centers, application allocations in different container orchestrator, means dealing with eventual consistency. Centralize online data pipeline to decouple applications and microservices. Monitor distributed applications to produce centralized feed of operational data. Logs collector from multiple services. Implement event sourcing pattern out of the box, using configuration to keep message for a long time period. Data are replicated between brokers within the cluster and cross availability zones if needed. As data is visible in highly available manner, persisted in distributed broker cluster, those brokers are becoming data hub, data buffering for larger data pipeline processing. Near real-time analytics insights: Compute real-time aggregates, time window based reasoning, and even complex event-processing which looks after event sequencing patterns. Aggregation of event coming from multiple producers. Look at event sequencing patterns Compute next best action from event streams","title":"Technical use cases"},{"location":"journey/101/","text":"Updated 05/28/2022 - Ready for consumption - Open Issue for request for improvement This chapter is to get you understanding what is Event-Driven architecture, what is Kafka, how to consider Messaging as a service as a foundation for event-driven solution, and getting started on IBM Event Streams and IBM MQ. Important concepts around Event-driven solution and Event-driven architecture \u00b6 First to get an agreement on the terms and definitions used in all this body of knowledge content, with a clear definitions for events , event streams , event backbone , event sources .... The main up-to-date reference architecture presentation is in this section with all the component descriptions. This architecture is built after a lot of customer engagements, projects, and review, and it is driven to support the main technical most common use cases we observed for EDA adoption so far. Understand Kafka technology \u00b6 You may want to read from the [Kafka] documentation](https://kafka.apache.org/documentation/#intro_nutshell) to understand the main concepts, but we have also summarized those into this chapter and if you like quick video, here is a seven minutes review of what Kafka is: The major building blocks are summarized in this diagram: To learn more about: Kafka Cluster see this 101 content For Producer introduction see this section For Consumer introduction see this section For Kafka Connector framework introduction So what is IBM Event Streams? \u00b6 Event Streams is the IBM packaging of Kafka for the Enterprise. It uses an award winner integrated user interface, kubernetes operator based on open source Strimzi, schema registry based on OpenSource and connectors. The phylosophie is to bring together Open Source leading products to support event streaming solution developments on kubernetes platform. Combined with event-end point management, no-code integration and IBM MQ, Event Streams brings Kafka to the enterprise. It is available as a Managed Service or as part of Cloud Pak for integration. See this developer.ibm.com article about Event Streams on Cloud for a quick introduction. The Event Streams product documentation as part of IBM Cloud Pak for Integration is here . Runnable demonstrations \u00b6 You can install the product using the Operator Hub, IBM Catalog and the OpenShift console. This lab can help you get an environment up and running quickly. You can then run the Starter Application as explained in this tutorial . You may need to demonstrate how to deploy Event Streams with CLI, and this lab will teach you how to leverage our gitops repository To go deeper in GitOps adoption and discussion, this tutorial is using two public repositories to demonstrate how to do the automatic deployment of event streams and an event-driven solution. Run a simple Kafka Streams and Event Stream demo for real-time inventory using this demonstration scripts and GitOps repository. Event streams resource requirements \u00b6 See the detailed tables in the product documentation. Event Streams environment sizing \u00b6 A lot of information is available for sizing: Cloud Pak for integration system requirements Foundational service specifics Event Streams resource requirements Kafka has a sizing tool (eventsizer) that may be questionable but some people are using it. But in general starts small and then increase the number of nodes over time. Minimum for production is 5 brokers and 3 zookeepers. Use Cases \u00b6 We have different level of use cases in this repository: The generic use cases which present the main drivers for EDA adoption are summarized here . But architects need to search and identify a set of key requirements to be supported by thier own EDA: get visibility of the new data created by existing applications in near real-time to get better business insights and propose new business opportunities to the end-users / customers. This means adopting data streaming, and intelligent query solutions. integrate with existing transactional systems, and continue to do transaction processing while adopting eventual data consistency, characteristics of distributed systems. (Cloud-native serverless or microservices are distributed systems) move to asynchronous communication between serverless and microservices, so they become more responsive, resilient, and scalable (characteristics of \u2018Reactive applications\u2019). get loosely coupled but still understand the API contracts. API being synchronous (RES, SOAP) or async (queue and topic). get clear visibility of data governance, with cataloging applications to be able to continuously assess which apps securely access what, with what intent. We have done some reference implementations to illustrate the major EDA design patterns: Scenario Description Link Shipping fresh food over sea (external) The EDA solution implementation using event driven microservices in different language, and demonstrating different design patterns. EDA reference implementation solution Vaccine delivery at scale (external) An EDA and cross cloud pak solution Vaccine delivery at scale Real time anomaly detection (external) Develop and apply machine learning predictive model on event streams Refrigerator container anomaly detection solution Why microservices are becoming event-driven and why we should care? \u00b6 This article explains why microservices are becoming event-driven and relates to some design patterns and potential microservice code structure. Messaging as a service? \u00b6 Yes EDA is not just Kafka, IBM MQ and Kafka should be part of any serious EDA deployment. The main reasons are explained in this fit for purpose section and in the messaging versus eventing presentation . An introduction to MQ is summarized in this note in the 301 learning journey, developers need to get the MQ developer badge as it cover the basics for developing solution on top of MQ Broker. Fit for purpose \u00b6 We have a general fit for purpose document that can help you reviewing messaging versus eventing, MQ versus Kafka, but also Kafka Streams versus Apache Flink. Getting Started With Event Streams \u00b6 With IBM Cloud \u00b6 The most easy way to start is to create one Event Streams on IBM Cloud service instance, and then connect a starter application. See this service at this URL . Running on you laptop \u00b6 As a developer , you may want to start quickly playing with a local Kafka Broker and MQ Broker on you own laptop. We have developed such compose files in this gitops project under the RunLocally folder. # Kafka based on strimzi image and MQ container docker-compose -f maas-compose.yaml up & # Kafka only docker-compose -f kafka-compose.yaml up & The kafka images are based on Strimzi images, but act as Event Streams one. So any code developed and configured on this image will run the same when deployed on OpenShift using Event Streams. Running on OpenShift \u00b6 Finally if you have access to an OpenShift Cluster, version 4.7 +, you can deploy Event Streams as part of IBM Cloud Pak for Integration very easily using the OpenShift Admin console. See this simple step by step tutorial here which covers how to deployment and configuration an Event Streams cluster using the OpenShift Admin Console or the oc CLI and our manifest from our EDA gitops catalog repository . Show and tell \u00b6 Once you have deployed a cluster, access the administration console and use the very good getting started application . Automate everything with GitOps \u00b6 If you want to adopt a pure GitOps approach we have demonstrated how to use ArgoCD (OpenShift GitOps) to deploy an Event Streams cluster and maintain it states in a simple real time inventory demonstration in this repository . Methodology: getting started on a good path \u00b6 We have adopted the Event Storming workshop to discover the business process from an event point of view, and then apply domain-driven design practices to discover aggregate, business entities, commands, actors and bounded contexts. Bounded context helps to define microservice and business services scope. Below is a figure to illustrate the DDD elements used during those workshops: Bounded context map define the solution. Using a lean startup approach, we focus on defining a minimum viable product, and preselect a set of services to implement. We have detailed how we apply this methodology for a Vaccine delivery solution in this article which can help you understand how to use the method for your own project. Frequently asked questions \u00b6 A separate FAQ document groups the most common questions around Kafka.","title":"Get started (101 content)"},{"location":"journey/101/#important-concepts-around-event-driven-solution-and-event-driven-architecture","text":"First to get an agreement on the terms and definitions used in all this body of knowledge content, with a clear definitions for events , event streams , event backbone , event sources .... The main up-to-date reference architecture presentation is in this section with all the component descriptions. This architecture is built after a lot of customer engagements, projects, and review, and it is driven to support the main technical most common use cases we observed for EDA adoption so far.","title":"Important concepts around Event-driven solution and Event-driven architecture"},{"location":"journey/101/#understand-kafka-technology","text":"You may want to read from the [Kafka] documentation](https://kafka.apache.org/documentation/#intro_nutshell) to understand the main concepts, but we have also summarized those into this chapter and if you like quick video, here is a seven minutes review of what Kafka is: The major building blocks are summarized in this diagram: To learn more about: Kafka Cluster see this 101 content For Producer introduction see this section For Consumer introduction see this section For Kafka Connector framework introduction","title":"Understand Kafka technology"},{"location":"journey/101/#so-what-is-ibm-event-streams","text":"Event Streams is the IBM packaging of Kafka for the Enterprise. It uses an award winner integrated user interface, kubernetes operator based on open source Strimzi, schema registry based on OpenSource and connectors. The phylosophie is to bring together Open Source leading products to support event streaming solution developments on kubernetes platform. Combined with event-end point management, no-code integration and IBM MQ, Event Streams brings Kafka to the enterprise. It is available as a Managed Service or as part of Cloud Pak for integration. See this developer.ibm.com article about Event Streams on Cloud for a quick introduction. The Event Streams product documentation as part of IBM Cloud Pak for Integration is here .","title":"So what is IBM Event Streams?"},{"location":"journey/101/#runnable-demonstrations","text":"You can install the product using the Operator Hub, IBM Catalog and the OpenShift console. This lab can help you get an environment up and running quickly. You can then run the Starter Application as explained in this tutorial . You may need to demonstrate how to deploy Event Streams with CLI, and this lab will teach you how to leverage our gitops repository To go deeper in GitOps adoption and discussion, this tutorial is using two public repositories to demonstrate how to do the automatic deployment of event streams and an event-driven solution. Run a simple Kafka Streams and Event Stream demo for real-time inventory using this demonstration scripts and GitOps repository.","title":"Runnable demonstrations"},{"location":"journey/101/#event-streams-resource-requirements","text":"See the detailed tables in the product documentation.","title":"Event streams resource requirements"},{"location":"journey/101/#event-streams-environment-sizing","text":"A lot of information is available for sizing: Cloud Pak for integration system requirements Foundational service specifics Event Streams resource requirements Kafka has a sizing tool (eventsizer) that may be questionable but some people are using it. But in general starts small and then increase the number of nodes over time. Minimum for production is 5 brokers and 3 zookeepers.","title":"Event Streams environment sizing"},{"location":"journey/101/#use-cases","text":"We have different level of use cases in this repository: The generic use cases which present the main drivers for EDA adoption are summarized here . But architects need to search and identify a set of key requirements to be supported by thier own EDA: get visibility of the new data created by existing applications in near real-time to get better business insights and propose new business opportunities to the end-users / customers. This means adopting data streaming, and intelligent query solutions. integrate with existing transactional systems, and continue to do transaction processing while adopting eventual data consistency, characteristics of distributed systems. (Cloud-native serverless or microservices are distributed systems) move to asynchronous communication between serverless and microservices, so they become more responsive, resilient, and scalable (characteristics of \u2018Reactive applications\u2019). get loosely coupled but still understand the API contracts. API being synchronous (RES, SOAP) or async (queue and topic). get clear visibility of data governance, with cataloging applications to be able to continuously assess which apps securely access what, with what intent. We have done some reference implementations to illustrate the major EDA design patterns: Scenario Description Link Shipping fresh food over sea (external) The EDA solution implementation using event driven microservices in different language, and demonstrating different design patterns. EDA reference implementation solution Vaccine delivery at scale (external) An EDA and cross cloud pak solution Vaccine delivery at scale Real time anomaly detection (external) Develop and apply machine learning predictive model on event streams Refrigerator container anomaly detection solution","title":"Use Cases"},{"location":"journey/101/#why-microservices-are-becoming-event-driven-and-why-we-should-care","text":"This article explains why microservices are becoming event-driven and relates to some design patterns and potential microservice code structure.","title":"Why microservices are becoming event-driven and why we should care?"},{"location":"journey/101/#messaging-as-a-service","text":"Yes EDA is not just Kafka, IBM MQ and Kafka should be part of any serious EDA deployment. The main reasons are explained in this fit for purpose section and in the messaging versus eventing presentation . An introduction to MQ is summarized in this note in the 301 learning journey, developers need to get the MQ developer badge as it cover the basics for developing solution on top of MQ Broker.","title":"Messaging as a service?"},{"location":"journey/101/#fit-for-purpose","text":"We have a general fit for purpose document that can help you reviewing messaging versus eventing, MQ versus Kafka, but also Kafka Streams versus Apache Flink.","title":"Fit for purpose"},{"location":"journey/101/#getting-started-with-event-streams","text":"","title":"Getting Started With Event Streams"},{"location":"journey/101/#with-ibm-cloud","text":"The most easy way to start is to create one Event Streams on IBM Cloud service instance, and then connect a starter application. See this service at this URL .","title":"With IBM Cloud"},{"location":"journey/101/#running-on-you-laptop","text":"As a developer , you may want to start quickly playing with a local Kafka Broker and MQ Broker on you own laptop. We have developed such compose files in this gitops project under the RunLocally folder. # Kafka based on strimzi image and MQ container docker-compose -f maas-compose.yaml up & # Kafka only docker-compose -f kafka-compose.yaml up & The kafka images are based on Strimzi images, but act as Event Streams one. So any code developed and configured on this image will run the same when deployed on OpenShift using Event Streams.","title":"Running on you laptop"},{"location":"journey/101/#running-on-openshift","text":"Finally if you have access to an OpenShift Cluster, version 4.7 +, you can deploy Event Streams as part of IBM Cloud Pak for Integration very easily using the OpenShift Admin console. See this simple step by step tutorial here which covers how to deployment and configuration an Event Streams cluster using the OpenShift Admin Console or the oc CLI and our manifest from our EDA gitops catalog repository .","title":"Running on OpenShift"},{"location":"journey/101/#show-and-tell","text":"Once you have deployed a cluster, access the administration console and use the very good getting started application .","title":"Show and tell"},{"location":"journey/101/#automate-everything-with-gitops","text":"If you want to adopt a pure GitOps approach we have demonstrated how to use ArgoCD (OpenShift GitOps) to deploy an Event Streams cluster and maintain it states in a simple real time inventory demonstration in this repository .","title":"Automate everything with GitOps"},{"location":"journey/101/#methodology-getting-started-on-a-good-path","text":"We have adopted the Event Storming workshop to discover the business process from an event point of view, and then apply domain-driven design practices to discover aggregate, business entities, commands, actors and bounded contexts. Bounded context helps to define microservice and business services scope. Below is a figure to illustrate the DDD elements used during those workshops: Bounded context map define the solution. Using a lean startup approach, we focus on defining a minimum viable product, and preselect a set of services to implement. We have detailed how we apply this methodology for a Vaccine delivery solution in this article which can help you understand how to use the method for your own project.","title":"Methodology: getting started on a good path"},{"location":"journey/101/#frequently-asked-questions","text":"A separate FAQ document groups the most common questions around Kafka.","title":"Frequently asked questions"},{"location":"journey/201/","text":"In this 201 content, you should be able to learn more about Kafka, Event Streams, Messaging, and Event-driven solution. More Kafka \u00b6 We have already covered the Kafka architecture in this section . When we deploy Event Streams on Kubernetes, it uses Operator, and it is in fact a wrapper on top of Strimzi , the open source kafka operator. Developer.ibm learning path: Develop production-ready, Apache Kafka apps Strimzi \u00b6 Strimzi uses the Cluster Operator to deploy and manage Kafka (including Zookeeper) and Kafka Connect clusters. When the Strimzi Cluster Operator is up and runnning, it starts to watch for certain OpenShift or Kubernetes resources containing the desired Kafka and/or Kafka Connect cluster configuration. It supports the following capabilities: Deploy Kafka OOS on any OpenShift or k8s platform Support TLS and SCRAM-SHA authentication, and automated certificate management Define operators for cluster, users and topics All resources are defined in yaml file so easily integrated into GitOps The Cluster Operator is a pod used to deploys and manages Apache Kafka clusters, Kafka Connect, Kafka MirrorMaker (1 and 2), Kafka Bridge, Kafka Exporter, and the Entity Operator. When deployed the following commands goes to the Cluster operator: # Get the current cluster list oc get kafka # get the list of topic oc get kafkatopics Installation on OpenShift \u00b6 The Strimzi operators deployment is done in two phases: Deploy the main operator via Subscription Deploy one to many instances of the Strimzi CRDs: cluster, users, topics... For that we have define subscription and configuration in this eda-gitops-catalog repo . So below are the operations to perform: # clone git clone https://github.com/ibm-cloud-architecture/eda-gitops-catalog.git # Define subscription oc apply -k kafka-strimzi/operator/overlays/stable/ # The subscription creates an operator pod under the openshift-operators project oc get pods -n openshift-operators # Create a project e.g. strimzi oc new-project strimzi # deploy a simple kafka cluster with 3 brokers oc apply -k kafka-strimzi/instance/ # Verify installation oc get pods # should get kafka, zookeeper and the entity operator running. The Strimzi documentation is very good to present a lot of configuration and tuning practices. Application \u00b6 All applications written with Kafka API will work the same way with Strimzi and Event Streams. So developer can use Strimzi images for their local development. Production deployment - High Availability \u00b6 Kafka clustering brings availability for message replication and failover, see details in this high availability section. This chapter presents replicas, in-synch replicas concepts and addresses some broker failure scenarios that are important to understand. When looking how Kafka is deployed on Kubernetes / Openshift it is important to isolate each broker to different worker node as illustrated in this section . In end-to-end deployment, the high availability will become more of a challenge for the producer and consumer. Consumers and producers should better run on separate servers than the brokers nodes. Producer may need to address back preasure on their own. Consumers need to have configuration that permit to do not enforce partition to consumer reassignment too quickly. Consumer process can fail and restart quickly and get the same partition allocated. Event-driven solution GitOps deployment \u00b6 The \"event-driven solution GitOps approach\" article goes over how to use OpenShift GitOps to deploy Event Streams, MQ, and a business solution. You will learn how to bootstrap the GitOps environment and deploy the needed IBM operators then use custom resource to define Event Streams cluster, topics, users... The following solution GitOps repositories are illustrating the proposed approach: refarch-kc-gitops : For the shipping fresh food overseas solution we have defined. It includes the SAGA choreography pattern implemented with Kafka eda-kc-gitops : For the shipping fresh food overseas solution we have defined. It includes the SAGA orchestration pattern implemented with MQ eda-rt-inventory-gitops to deploy the demo of real-time inventory Performance considerations \u00b6 Read this dedicated article on performance, resilience, throughput. EDA Design patterns \u00b6 Event-driven solutions are based on a set of design pattern for application design. In this article , you will find the different pattern which are used a lot in the field like Event sourcing : persists, to an append log, the states of a business entity, such as an Order, as a sequence of immutable state-changing events. Command Query Responsibility Segregation : helps to separate queries from commands and help to address queries with cross-microservice boundary. Saga pattern: Microservices publish events when something happens in the scope of their control like an update in the business entities they are responsible for. A microservice, interested in other business entities, subscribes to those events and it can update its own state and business entities on receipt of these events. Business entity keys need to be unique and immutable. Event reprocessing with dead letter : event driven microservices may have to call external services via a synchronous call. We need to process failure in order to get response from those services using event backbone. Transactional outbox : A service command typically needs to update the database and send messages/events. The approach is to use an outbox table to keep the message to sent and a message relay process to publish events inserted into database to the event backbone. (Source Chris Richardson - Microservices Patterns) Kafka Connect Framework \u00b6 Kafka connect is an open source component for easily integrate external systems with Kafka. It works with any Kafka product such as IBM Event Streams, Red Hat AMQ Streams, or Strimzi. You can learn more about it in this article and with those labs: Connect to S3 source and sink Connect to IBM Cloud Object Storage Connect to a Database with JDBC Sink Connect to MQ queue as source or Sink Connect to RabbitMQ Integrate with MQ \u00b6 Using Kafka Connect framework, IBM has a MQ source connector and MQ Sink connector to integrate easily between Event Streams and IBM MQ The following labs will help you learn more about how to use those connectors and this gitops repository helps you to run a store simulation producing messages to MQ queue, with Kafka Connector injecting those message to Event Streams. Introduction to schema management \u00b6 Schema management and schema registry are mandatory for doing production deployment, of any Kafka based solution. To understand the following components read this note AsyncAPI \u00b6 This article on AsyncAPI management presents the value of using AsyncAPI in API Connect. This blog from development What is Event Endpoint Management? presents the methodology for event endpoint management: event description including the event schema, the topic, and thte communication protocol event discovery, with centralized management of the API self service to easily try the API, but with secure policies enforcement decoupled by API helps to abtract and change implementation if needed. Debezium change data capture \u00b6 Change data capture is the best way to inject data from a database to Kafka. Debezium is the Red Hat led open source project in this area. The IBM InfoSphere Data Replication is a more advanced solution for different sources and to kafka or other middlewares. One lab DB2 debezium not fully operational, looking for contributor to complete it. and an implementation of Postgresql debezium cdc with outpost pattern and quarkus. Mirroring Data \u00b6 To replicate data between Kafka clusters, Mirror Maker 2 is the component to use. It is based on Kafka connector and will support a active-passive type of deployment or active active, which is little bit more complex. See the detail Mirror Maker 2 technology summary","title":"Deeper dive (201 content)"},{"location":"journey/201/#more-kafka","text":"We have already covered the Kafka architecture in this section . When we deploy Event Streams on Kubernetes, it uses Operator, and it is in fact a wrapper on top of Strimzi , the open source kafka operator. Developer.ibm learning path: Develop production-ready, Apache Kafka apps","title":"More Kafka"},{"location":"journey/201/#strimzi","text":"Strimzi uses the Cluster Operator to deploy and manage Kafka (including Zookeeper) and Kafka Connect clusters. When the Strimzi Cluster Operator is up and runnning, it starts to watch for certain OpenShift or Kubernetes resources containing the desired Kafka and/or Kafka Connect cluster configuration. It supports the following capabilities: Deploy Kafka OOS on any OpenShift or k8s platform Support TLS and SCRAM-SHA authentication, and automated certificate management Define operators for cluster, users and topics All resources are defined in yaml file so easily integrated into GitOps The Cluster Operator is a pod used to deploys and manages Apache Kafka clusters, Kafka Connect, Kafka MirrorMaker (1 and 2), Kafka Bridge, Kafka Exporter, and the Entity Operator. When deployed the following commands goes to the Cluster operator: # Get the current cluster list oc get kafka # get the list of topic oc get kafkatopics","title":"Strimzi"},{"location":"journey/201/#installation-on-openshift","text":"The Strimzi operators deployment is done in two phases: Deploy the main operator via Subscription Deploy one to many instances of the Strimzi CRDs: cluster, users, topics... For that we have define subscription and configuration in this eda-gitops-catalog repo . So below are the operations to perform: # clone git clone https://github.com/ibm-cloud-architecture/eda-gitops-catalog.git # Define subscription oc apply -k kafka-strimzi/operator/overlays/stable/ # The subscription creates an operator pod under the openshift-operators project oc get pods -n openshift-operators # Create a project e.g. strimzi oc new-project strimzi # deploy a simple kafka cluster with 3 brokers oc apply -k kafka-strimzi/instance/ # Verify installation oc get pods # should get kafka, zookeeper and the entity operator running. The Strimzi documentation is very good to present a lot of configuration and tuning practices.","title":"Installation on OpenShift"},{"location":"journey/201/#application","text":"All applications written with Kafka API will work the same way with Strimzi and Event Streams. So developer can use Strimzi images for their local development.","title":"Application"},{"location":"journey/201/#production-deployment-high-availability","text":"Kafka clustering brings availability for message replication and failover, see details in this high availability section. This chapter presents replicas, in-synch replicas concepts and addresses some broker failure scenarios that are important to understand. When looking how Kafka is deployed on Kubernetes / Openshift it is important to isolate each broker to different worker node as illustrated in this section . In end-to-end deployment, the high availability will become more of a challenge for the producer and consumer. Consumers and producers should better run on separate servers than the brokers nodes. Producer may need to address back preasure on their own. Consumers need to have configuration that permit to do not enforce partition to consumer reassignment too quickly. Consumer process can fail and restart quickly and get the same partition allocated.","title":"Production deployment - High Availability"},{"location":"journey/201/#event-driven-solution-gitops-deployment","text":"The \"event-driven solution GitOps approach\" article goes over how to use OpenShift GitOps to deploy Event Streams, MQ, and a business solution. You will learn how to bootstrap the GitOps environment and deploy the needed IBM operators then use custom resource to define Event Streams cluster, topics, users... The following solution GitOps repositories are illustrating the proposed approach: refarch-kc-gitops : For the shipping fresh food overseas solution we have defined. It includes the SAGA choreography pattern implemented with Kafka eda-kc-gitops : For the shipping fresh food overseas solution we have defined. It includes the SAGA orchestration pattern implemented with MQ eda-rt-inventory-gitops to deploy the demo of real-time inventory","title":"Event-driven solution GitOps deployment"},{"location":"journey/201/#performance-considerations","text":"Read this dedicated article on performance, resilience, throughput.","title":"Performance considerations"},{"location":"journey/201/#eda-design-patterns","text":"Event-driven solutions are based on a set of design pattern for application design. In this article , you will find the different pattern which are used a lot in the field like Event sourcing : persists, to an append log, the states of a business entity, such as an Order, as a sequence of immutable state-changing events. Command Query Responsibility Segregation : helps to separate queries from commands and help to address queries with cross-microservice boundary. Saga pattern: Microservices publish events when something happens in the scope of their control like an update in the business entities they are responsible for. A microservice, interested in other business entities, subscribes to those events and it can update its own state and business entities on receipt of these events. Business entity keys need to be unique and immutable. Event reprocessing with dead letter : event driven microservices may have to call external services via a synchronous call. We need to process failure in order to get response from those services using event backbone. Transactional outbox : A service command typically needs to update the database and send messages/events. The approach is to use an outbox table to keep the message to sent and a message relay process to publish events inserted into database to the event backbone. (Source Chris Richardson - Microservices Patterns)","title":"EDA Design patterns"},{"location":"journey/201/#kafka-connect-framework","text":"Kafka connect is an open source component for easily integrate external systems with Kafka. It works with any Kafka product such as IBM Event Streams, Red Hat AMQ Streams, or Strimzi. You can learn more about it in this article and with those labs: Connect to S3 source and sink Connect to IBM Cloud Object Storage Connect to a Database with JDBC Sink Connect to MQ queue as source or Sink Connect to RabbitMQ","title":"Kafka Connect Framework"},{"location":"journey/201/#integrate-with-mq","text":"Using Kafka Connect framework, IBM has a MQ source connector and MQ Sink connector to integrate easily between Event Streams and IBM MQ The following labs will help you learn more about how to use those connectors and this gitops repository helps you to run a store simulation producing messages to MQ queue, with Kafka Connector injecting those message to Event Streams.","title":"Integrate with MQ"},{"location":"journey/201/#introduction-to-schema-management","text":"Schema management and schema registry are mandatory for doing production deployment, of any Kafka based solution. To understand the following components read this note","title":"Introduction to schema management"},{"location":"journey/201/#asyncapi","text":"This article on AsyncAPI management presents the value of using AsyncAPI in API Connect. This blog from development What is Event Endpoint Management? presents the methodology for event endpoint management: event description including the event schema, the topic, and thte communication protocol event discovery, with centralized management of the API self service to easily try the API, but with secure policies enforcement decoupled by API helps to abtract and change implementation if needed.","title":"AsyncAPI"},{"location":"journey/201/#debezium-change-data-capture","text":"Change data capture is the best way to inject data from a database to Kafka. Debezium is the Red Hat led open source project in this area. The IBM InfoSphere Data Replication is a more advanced solution for different sources and to kafka or other middlewares. One lab DB2 debezium not fully operational, looking for contributor to complete it. and an implementation of Postgresql debezium cdc with outpost pattern and quarkus.","title":"Debezium change data capture"},{"location":"journey/201/#mirroring-data","text":"To replicate data between Kafka clusters, Mirror Maker 2 is the component to use. It is based on Kafka connector and will support a active-passive type of deployment or active active, which is little bit more complex. See the detail Mirror Maker 2 technology summary","title":"Mirroring Data"},{"location":"methodology/data-intensive/","text":"In this section we are introducing the different elements of the software life cycles, particular to the development of intelligent applications that leverage data, machine learned models, analytics and cloud native microservices. The method supports lightweight development practices, to start the implementation of a MVP (Minimum Viable Product) , to support scaling-up the architecture and the complexity of an end-to-end integrated solution. This method always guarantees to have deployable components at any point in time, a concept known in CI/CD (Continuous Integration, Continuous Delivery) applied to microservices, integration, data and machine learning models. Integrating data - devops and AI-Analytics development practices \u00b6 Most organizations need to manage software lifecycles. The key tenets listed above imply the need for a separate lifecycle for data, because the outcome or deliverable for any of the key tenets should not be considered static and immutable. Like data, you can think of analytics as having its own lifecycle independent from the software lifecycle and the data lifecycle, although they are all complementary. To help achieve digital transformation, your organization should integrate three development lifecycles: Software/Application AI-Analytics Data Each development lifecycle is representative of an iterative workflow that can be used by agile development teams to craft higher value business outcomes. Lifecycles are often timeboxed iterations and can follow a fail-fast paradigm to realize tangible assets. Speed is important in business; most businesses work to meet new needs quickly. The objective of each lifecycle iteration is to address business speed efficiently and proficiently while maximizing business value. Personas \u00b6 We recommend reading this article to assemble the team to support a data-driven project where many roles support the project execution. The following table presents the icons we are using in subsquent figures, with a short description for the role. Differences between analysts and data scientists \u00b6 The MITSloan did a review of the major differences between data scientists and business analysts by considering a set of common dimensions. The table below summarizes the results: Analysts Data Scientists Types of data Structured mostly numeric data All data types, including unstructured data Preferred tools Statistical and modeling tools using data repository Programming language with strong statistical library, machine learning, deep learning. Use ML cluster servers Nature of work Report, predict, prescribe and optimize Explore, discover, investigate and visualize Typical educationl background Operations research, statistics, applied mathematics, predictive analytics Computer science, data science, cognitive science Mindset Entreprenaurial 69%, explore new ideas 58%, gain insights outside of formal projects 54% Entreprenaurial 96%, explore new ideas 85%, gain insights outside of formal projects 89% DevOps lifecycle \u00b6 The software/application development lifecycle (SDLC) is a well-known and supports both traditional and agile development. The SDLC iterates on incorporating business requirements, adopt test driven development, continuous deployment and continuous integration. The diagram below demonstrates the iteration over recurring developer tasks to build the business intelligent application (internal loop), and the release loop (external) to continuously deliver application features to production. Before enterring the development iteration cycles, there are tasks to scope the high level business challenges and opportunities, define the business case for the project, define and build the development and operation strategy, define the target infrastructure, security... The smaller loop represents development iteration, while the outer loop represents software release to production with continous feedback to monitor and assess features acceptance. This task list is not exhaustive, but represents a common ground for our discussion. \"Understanding business objectives\" is a common task in each lifecycle, but in the context of microservice solution, adoption event storming practice and domain driven design will help understanding the business process, the data aggregates, and the bounded contexts. The solution will group a lot of out of the shelves components and a set of microservices supporting the implementation of the business logic and the intelligent application. A lot of things need to be considered while implementing each microservice from a data point of view. We recommend reading data pipeline for data intensive application to assess what needs to be done, and some best practices. Among the tasks described in these release and development iteration loops, we do not need to cover each of them, but may be highlight the Integration service task which has a blue border to demonstrate integration activities between the different lifecycles, like ML model integration which is developed in the MLops iteration . DataOps \u00b6 The data development lifecycle (DataOps) places the data management philosophy into an organic and evolving cycle that is better suited to the constantly changing needs of a business. The DataOps is impacted by the DevOps and the MLOps. It iterates on both the incorporation of business requirements and on the manifestation of data. Data has value and the evaluation of data article introduces to the concepts to recognize the value of data. The discover business objectives activity task, groups a set of different subjects depending of the context: data, integration, machine learning model development. The goal is to highlight the measurable outcomes expected by business stakeholders. The build a business objective article ) presents the concepts and some questions that can be used to assess the general business requirements and current knowledge of the data. And the translating a business problem into an AI and data science solution practice helps the analysis team to assess what data may be needed and what kind of model to develop. As you can see activities are addressing data preparation and understanding, so data architecture need to be in place before doing any data sciences work. As part of the gather data requirements , it is important to review the dimensions of value as introduced in the \"valuation of data\" article , then formalize the value chain of the data in the scope of the project, and finally address if the data contains the correct information to answer the business challenges and support the business process. Transform data for AI and Deploy data integration flow tasks have different border colors to demonstrate integration activities between the different lifecycles. MLOps lifecycle \u00b6 The Machine learning development lifecycle supports the full spectrum of analytical work in the artificial intelligence ladder. This lifecycle incorporates model development and remediation to avoid drift. Because one of the purposes of analytics is to enable an action or a decision, MLOps relies on feedback mechanisms to help enhancing machine models and the overall analytical environment. An example of a feedback mechanism is capturing data points on the positive or negative effects or outcomes from an action or a decision. This process iterates on data. The developed AI or Analytics model is deployed as one to many services that are integrated in the microservice architecture. So synchronization with devops team is important and part of the method. Understanding the data and analytics goals task is explained in this note with the business analytic patterns. Defining the analytic approach task groups sub activities that help to understand the past activity and assess what kind of predictions, actions are expected by the business users. The patterns and goals analysis will help to assess for supervised or unsupervised leaning needs. Integrating the cycles \u00b6 Although the three lifecycles are independents, you can use them together and establish dependencies to help drive business outcomes. Each lifecycle should be agile and should be incorporated into a DevOps process for development and deployment. The intersection of the three lifecycles highlights the need for unified governance. The intersection between software/app and data, highlights integration and access paths to information. The intersection between data and analytics, highlights integration with the underlying data stores. The intersection between analytics and software/app highlights integration and the use of APIs or other data exchange techniques to assist in resolving complex algorithms or access requirements. An other interesting view, is to consider the high level artifacts built in those overlapping areas, as they are very important elements to project managed efficiently to avoid teams waiting for others. Interface definitions and data schema are important elements to address as early as possible in the SDLC. Data access integration includes dedicated microservices managing the full lifecycles and business operations for each major business entities of the solution. The integration can be event-based and adopt an event-driven architecture. The data store integration addresses storage of high volume data, but also access control, any transformation logic, and event data schema to be consumable by AI workbench. The AI model as a service can be mocked-up behind Facade interface so the developed microservice in need to get prescriptive scoring can be developed with less dependencies. Finally the integration of those three lifecycle over time can be presented in a Gantt chart to illustrate the iterations and the different focuses over time. Each development life cycle includes architecture and development tasks. Architecture activities focus on defining infrastructure for runtimes and machine learning environment as well as data topology etc... The different iterations of the data, IA-Analytics and devops life cycle are synchronized via the integration artifacts to build. When components are ready for production, the go-live occurs and the different operate tasks are executed, combined with the different monitoring. From the production execution, the different teams get continuous feedbacks to improve the application. The AI-Analytics tasks are colored in blue and green, on purpose to show the strong dependencies between data and AI. This means the data activities should start as early as possible before doing too much of modeling. Challenges \u00b6 There are a set of standard challenges while developing an IT solution which integrates results from analytics model. We are listing some that we want to address, document and support as requirements. How will the model be made available to developers? Is it a batch process updating/appending static records or real time processing on a data stream or transactional data How to control resource allocation for Machine Learning job. How to manage consistency between model, data and code: version management / data lineage How to assess the features needed for the training sets. The IBM Method for Cloud with DataFirst \u00b6 Every department within your organization has different needs for data and analytics. How can you start your data-driven journey? The IBM Method for Cloud with DataFirst provides strategy and expertise to help you gain the most value from your data. This method starts with your business outcomes that leverage data and analytics, not technology. Defining focus in a collaborative manner is key to deriving early results. Your roadmap and action plan are continuously updated based on lessons learned. This is an iterative and agile approach to help you define, design, and prove a solution for your specific business problem.","title":"Data Intensive App Development"},{"location":"methodology/data-intensive/#integrating-data-devops-and-ai-analytics-development-practices","text":"Most organizations need to manage software lifecycles. The key tenets listed above imply the need for a separate lifecycle for data, because the outcome or deliverable for any of the key tenets should not be considered static and immutable. Like data, you can think of analytics as having its own lifecycle independent from the software lifecycle and the data lifecycle, although they are all complementary. To help achieve digital transformation, your organization should integrate three development lifecycles: Software/Application AI-Analytics Data Each development lifecycle is representative of an iterative workflow that can be used by agile development teams to craft higher value business outcomes. Lifecycles are often timeboxed iterations and can follow a fail-fast paradigm to realize tangible assets. Speed is important in business; most businesses work to meet new needs quickly. The objective of each lifecycle iteration is to address business speed efficiently and proficiently while maximizing business value.","title":"Integrating data - devops and AI-Analytics development practices"},{"location":"methodology/data-intensive/#personas","text":"We recommend reading this article to assemble the team to support a data-driven project where many roles support the project execution. The following table presents the icons we are using in subsquent figures, with a short description for the role.","title":"Personas"},{"location":"methodology/data-intensive/#differences-between-analysts-and-data-scientists","text":"The MITSloan did a review of the major differences between data scientists and business analysts by considering a set of common dimensions. The table below summarizes the results: Analysts Data Scientists Types of data Structured mostly numeric data All data types, including unstructured data Preferred tools Statistical and modeling tools using data repository Programming language with strong statistical library, machine learning, deep learning. Use ML cluster servers Nature of work Report, predict, prescribe and optimize Explore, discover, investigate and visualize Typical educationl background Operations research, statistics, applied mathematics, predictive analytics Computer science, data science, cognitive science Mindset Entreprenaurial 69%, explore new ideas 58%, gain insights outside of formal projects 54% Entreprenaurial 96%, explore new ideas 85%, gain insights outside of formal projects 89%","title":"Differences between analysts and data scientists"},{"location":"methodology/data-intensive/#devops-lifecycle","text":"The software/application development lifecycle (SDLC) is a well-known and supports both traditional and agile development. The SDLC iterates on incorporating business requirements, adopt test driven development, continuous deployment and continuous integration. The diagram below demonstrates the iteration over recurring developer tasks to build the business intelligent application (internal loop), and the release loop (external) to continuously deliver application features to production. Before enterring the development iteration cycles, there are tasks to scope the high level business challenges and opportunities, define the business case for the project, define and build the development and operation strategy, define the target infrastructure, security... The smaller loop represents development iteration, while the outer loop represents software release to production with continous feedback to monitor and assess features acceptance. This task list is not exhaustive, but represents a common ground for our discussion. \"Understanding business objectives\" is a common task in each lifecycle, but in the context of microservice solution, adoption event storming practice and domain driven design will help understanding the business process, the data aggregates, and the bounded contexts. The solution will group a lot of out of the shelves components and a set of microservices supporting the implementation of the business logic and the intelligent application. A lot of things need to be considered while implementing each microservice from a data point of view. We recommend reading data pipeline for data intensive application to assess what needs to be done, and some best practices. Among the tasks described in these release and development iteration loops, we do not need to cover each of them, but may be highlight the Integration service task which has a blue border to demonstrate integration activities between the different lifecycles, like ML model integration which is developed in the MLops iteration .","title":"DevOps lifecycle"},{"location":"methodology/data-intensive/#dataops","text":"The data development lifecycle (DataOps) places the data management philosophy into an organic and evolving cycle that is better suited to the constantly changing needs of a business. The DataOps is impacted by the DevOps and the MLOps. It iterates on both the incorporation of business requirements and on the manifestation of data. Data has value and the evaluation of data article introduces to the concepts to recognize the value of data. The discover business objectives activity task, groups a set of different subjects depending of the context: data, integration, machine learning model development. The goal is to highlight the measurable outcomes expected by business stakeholders. The build a business objective article ) presents the concepts and some questions that can be used to assess the general business requirements and current knowledge of the data. And the translating a business problem into an AI and data science solution practice helps the analysis team to assess what data may be needed and what kind of model to develop. As you can see activities are addressing data preparation and understanding, so data architecture need to be in place before doing any data sciences work. As part of the gather data requirements , it is important to review the dimensions of value as introduced in the \"valuation of data\" article , then formalize the value chain of the data in the scope of the project, and finally address if the data contains the correct information to answer the business challenges and support the business process. Transform data for AI and Deploy data integration flow tasks have different border colors to demonstrate integration activities between the different lifecycles.","title":"DataOps"},{"location":"methodology/data-intensive/#mlops-lifecycle","text":"The Machine learning development lifecycle supports the full spectrum of analytical work in the artificial intelligence ladder. This lifecycle incorporates model development and remediation to avoid drift. Because one of the purposes of analytics is to enable an action or a decision, MLOps relies on feedback mechanisms to help enhancing machine models and the overall analytical environment. An example of a feedback mechanism is capturing data points on the positive or negative effects or outcomes from an action or a decision. This process iterates on data. The developed AI or Analytics model is deployed as one to many services that are integrated in the microservice architecture. So synchronization with devops team is important and part of the method. Understanding the data and analytics goals task is explained in this note with the business analytic patterns. Defining the analytic approach task groups sub activities that help to understand the past activity and assess what kind of predictions, actions are expected by the business users. The patterns and goals analysis will help to assess for supervised or unsupervised leaning needs.","title":"MLOps lifecycle"},{"location":"methodology/data-intensive/#integrating-the-cycles","text":"Although the three lifecycles are independents, you can use them together and establish dependencies to help drive business outcomes. Each lifecycle should be agile and should be incorporated into a DevOps process for development and deployment. The intersection of the three lifecycles highlights the need for unified governance. The intersection between software/app and data, highlights integration and access paths to information. The intersection between data and analytics, highlights integration with the underlying data stores. The intersection between analytics and software/app highlights integration and the use of APIs or other data exchange techniques to assist in resolving complex algorithms or access requirements. An other interesting view, is to consider the high level artifacts built in those overlapping areas, as they are very important elements to project managed efficiently to avoid teams waiting for others. Interface definitions and data schema are important elements to address as early as possible in the SDLC. Data access integration includes dedicated microservices managing the full lifecycles and business operations for each major business entities of the solution. The integration can be event-based and adopt an event-driven architecture. The data store integration addresses storage of high volume data, but also access control, any transformation logic, and event data schema to be consumable by AI workbench. The AI model as a service can be mocked-up behind Facade interface so the developed microservice in need to get prescriptive scoring can be developed with less dependencies. Finally the integration of those three lifecycle over time can be presented in a Gantt chart to illustrate the iterations and the different focuses over time. Each development life cycle includes architecture and development tasks. Architecture activities focus on defining infrastructure for runtimes and machine learning environment as well as data topology etc... The different iterations of the data, IA-Analytics and devops life cycle are synchronized via the integration artifacts to build. When components are ready for production, the go-live occurs and the different operate tasks are executed, combined with the different monitoring. From the production execution, the different teams get continuous feedbacks to improve the application. The AI-Analytics tasks are colored in blue and green, on purpose to show the strong dependencies between data and AI. This means the data activities should start as early as possible before doing too much of modeling.","title":"Integrating the cycles"},{"location":"methodology/data-intensive/#challenges","text":"There are a set of standard challenges while developing an IT solution which integrates results from analytics model. We are listing some that we want to address, document and support as requirements. How will the model be made available to developers? Is it a batch process updating/appending static records or real time processing on a data stream or transactional data How to control resource allocation for Machine Learning job. How to manage consistency between model, data and code: version management / data lineage How to assess the features needed for the training sets.","title":"Challenges"},{"location":"methodology/data-intensive/#the-ibm-method-for-cloud-with-datafirst","text":"Every department within your organization has different needs for data and analytics. How can you start your data-driven journey? The IBM Method for Cloud with DataFirst provides strategy and expertise to help you gain the most value from your data. This method starts with your business outcomes that leverage data and analytics, not technology. Defining focus in a collaborative manner is key to deriving early results. Your roadmap and action plan are continuously updated based on lessons learned. This is an iterative and agile approach to help you define, design, and prove a solution for your specific business problem.","title":"The IBM Method for Cloud with DataFirst"},{"location":"methodology/data-lineage/","text":"From a stream data platform adoption point of view, we have multiple components working on data, with online and offline processing. The following figure illustrates the concepts and components we may need to consider as part of the data lineage discussion: Mobile, web single page application, IoT, microservice app, change data capture (not presented here) and SaaS offering applications are source of events. Event backbone persists those events in append log with long retention time (days). Stream applications are running statefule operations to compute real time analytics or doing complex event processing. Kafka Streams and Apache Flinks are the technology of choices. On the right side, consumer can trigger what will be the next best action to perform on a specific event. Those action can be a business process, a product recommendations, an alert... Often data can be persisted on a longer retention storage, with full cloud availability like S3 buckets (Cloud Object storage on IBM Cloud or on Premise). Big data processing plaform like Sparks can run batch processing, map reduce type, on data at rest, that comes from the data ingestion layer supported but the event backbone. Finally business dashboard can integrate queries on data at rest and in motion via interactive / streaming queries. With those simple component view we can already see data lineage will be complex, so we need practices and tools to support it. As more data is injected into the data platform, the more you need to be able to answer a set of questions like: Where the data is coming from Who create the data Who own it Who can access those data and where How can we ensure data quality Who modify data in motion Data lineage requirements \u00b6 Data lineage describes data origins, movements, characteristics, ownership and quality. As part of larger data governance initiative, it may encompass data security, access control, encryption and confidentiality. Contracts \u00b6 In the REST APIs, or even SOA, worlds request/response are defined via standards like OpenAPI or WSDL. In the event and streaming processing the AsynchAPI is the specification to define schema and middleware binding. OpenAPI \u00b6 We do not need to present OpenAPI but just the fact that those APIs represent request/response communication and may be managed and integrated with the development life cycle. Modern API management platform should support their life cycle end to end but also support new specifications like GraphQL and AsynchAPI. AsynchAPI \u00b6 Without duplicating the specification is the specification to define schema and middleware binding. we want to highlight here what are the important parts to consider in the data goverance: The Asynch API documentation which includes: The server definition to address the broker binding, with URL and protocol to be used. (http, kafka, mqtt, amqp, ws) The channel definition which represents how to reach the brokers. It looks like a Path definition in OpenAPI The message definition which could be of any value. Apache Avro is one way to present message but it does not have to be. Security to access the server via user / password, TLS certificates, API keys, OAuth2... The message schema The format of the file describing the API can be Yaml or Json. Schema \u00b6 Schema is an important way to ensure data quality by defining a strong, but flexible, contract between producers and consumers and to understand the exposed payload in each topics. Schema definitions improve applications robustness as downstream data consumers are protected from malformed data, as only valid data will be permitted in the topic. Schemas need to be compatible from version to version and Apache Avro supports defining default values for non existing attribute of previous versioned schema. Schema Registry enforces full compatibility when creating a new version of a schema. Full compatibility means that old data can be read with the new data schema, and new data can also be read with a previous data schema. Here is an example of such definition: \"fields\" : [ { \"name\" : \"newAttribute\" , \"type\" : \"string\" , \"default\" : \"defaultValue\" } ] Remarks: if you want backward compatibility, being able to read old messages, you need to add new fields with default value.To support forward compatibility you need to add default value to deleted field, and if you want full compatibility you need to add default value on all fields. Metadata about the event can be added as part of the field definition, and should include source application reference, may be a unique identifier created at the source application to ensure traceability end to end, and when intermediate streaming application are doing transformation on the same topic, the transformer app reference. Here is an example of such data: \"fields\" : [ { \"name\" : \"metadata\" , \"type\" : { \"name\" : \"Metadata\" , \"type\" : \"record\" , \"fields\" : [ { \"name\" : \"sourceApp\" , \"type\" : \"string\" }, { \"name\" : \"eventUUID\" , \"type\" : \"string\" }, { \"name\" : \"transformApps\" , \"type\" : { \"name\" : \"TransformerReference\" , \"type\" : \"array\" , \"items\" : \"string\" } } ] } } ] The classical integration with schema registry is presented in the figure below: Schema registry can be deployed in different data centers and serves multi Kafka clusters. For DR, you need to use a 'primary' server and one secondary in different data center. Both will receive schema update via DevOps pipeline. One of the main open source Schema Registry is Apicurio , which is integrated with Event Streams and in most of our implementation. Apicurio can persist schema definition inside Kafka Topic and so schema replication can also being synchronize via Mirror Maker 2 replication. If Postgresql is used for persistence then postgresql can be used for replicating schemas. We recommend reading our schema registry summary and our OpenShift lab or Event Streams on Cloud schema registry lab . Integrating schema management with code generation and devOps pipeline is addressed in this repository.","title":"Data lineage"},{"location":"methodology/data-lineage/#data-lineage-requirements","text":"Data lineage describes data origins, movements, characteristics, ownership and quality. As part of larger data governance initiative, it may encompass data security, access control, encryption and confidentiality.","title":"Data lineage requirements"},{"location":"methodology/data-lineage/#contracts","text":"In the REST APIs, or even SOA, worlds request/response are defined via standards like OpenAPI or WSDL. In the event and streaming processing the AsynchAPI is the specification to define schema and middleware binding.","title":"Contracts"},{"location":"methodology/data-lineage/#openapi","text":"We do not need to present OpenAPI but just the fact that those APIs represent request/response communication and may be managed and integrated with the development life cycle. Modern API management platform should support their life cycle end to end but also support new specifications like GraphQL and AsynchAPI.","title":"OpenAPI"},{"location":"methodology/data-lineage/#asynchapi","text":"Without duplicating the specification is the specification to define schema and middleware binding. we want to highlight here what are the important parts to consider in the data goverance: The Asynch API documentation which includes: The server definition to address the broker binding, with URL and protocol to be used. (http, kafka, mqtt, amqp, ws) The channel definition which represents how to reach the brokers. It looks like a Path definition in OpenAPI The message definition which could be of any value. Apache Avro is one way to present message but it does not have to be. Security to access the server via user / password, TLS certificates, API keys, OAuth2... The message schema The format of the file describing the API can be Yaml or Json.","title":"AsynchAPI"},{"location":"methodology/data-lineage/#schema","text":"Schema is an important way to ensure data quality by defining a strong, but flexible, contract between producers and consumers and to understand the exposed payload in each topics. Schema definitions improve applications robustness as downstream data consumers are protected from malformed data, as only valid data will be permitted in the topic. Schemas need to be compatible from version to version and Apache Avro supports defining default values for non existing attribute of previous versioned schema. Schema Registry enforces full compatibility when creating a new version of a schema. Full compatibility means that old data can be read with the new data schema, and new data can also be read with a previous data schema. Here is an example of such definition: \"fields\" : [ { \"name\" : \"newAttribute\" , \"type\" : \"string\" , \"default\" : \"defaultValue\" } ] Remarks: if you want backward compatibility, being able to read old messages, you need to add new fields with default value.To support forward compatibility you need to add default value to deleted field, and if you want full compatibility you need to add default value on all fields. Metadata about the event can be added as part of the field definition, and should include source application reference, may be a unique identifier created at the source application to ensure traceability end to end, and when intermediate streaming application are doing transformation on the same topic, the transformer app reference. Here is an example of such data: \"fields\" : [ { \"name\" : \"metadata\" , \"type\" : { \"name\" : \"Metadata\" , \"type\" : \"record\" , \"fields\" : [ { \"name\" : \"sourceApp\" , \"type\" : \"string\" }, { \"name\" : \"eventUUID\" , \"type\" : \"string\" }, { \"name\" : \"transformApps\" , \"type\" : { \"name\" : \"TransformerReference\" , \"type\" : \"array\" , \"items\" : \"string\" } } ] } } ] The classical integration with schema registry is presented in the figure below: Schema registry can be deployed in different data centers and serves multi Kafka clusters. For DR, you need to use a 'primary' server and one secondary in different data center. Both will receive schema update via DevOps pipeline. One of the main open source Schema Registry is Apicurio , which is integrated with Event Streams and in most of our implementation. Apicurio can persist schema definition inside Kafka Topic and so schema replication can also being synchronize via Mirror Maker 2 replication. If Postgresql is used for persistence then postgresql can be used for replicating schemas. We recommend reading our schema registry summary and our OpenShift lab or Event Streams on Cloud schema registry lab . Integrating schema management with code generation and devOps pipeline is addressed in this repository.","title":"Schema"},{"location":"methodology/domain-driven-design/","text":"This section describes how to apply domain driven design with event based application and describe the high level steps which uses output from the event storming session and derives a set of micro services design specifications. Overview \u00b6 Event storming is part of the domain-driven design methodology. And domain-driven design was deeply described in Eric Evans's \"Domain Driven Design: Tackling Complexity in the Heart of Software\" book from 2004. From this body of knowledge we can derive the following important elements: Domain Driven Design is about understanding the business domain in which the solution has to be developed Context is important to define the business term definitions within it For microservice we need to find the boundary of responsability to be able to apply the clear separation of concern pattern. The goals for the design step are: Improve communication between subject matter experts and developer, make the code readable by human and long term maintanable To support highly modular cloud native microservices. To adopt event coupled microservices - facilitating independent modification and evolution of each microservice separately. To allow applying event-driven patterns such as event sourcing, CQRS and SAGA to address some of the challenges of distributed system implementation: data consitency, transaction cross domains, and complex queries between aggregates managed by different services. DDD suits better long term project, where the domain of knowledge is important and complex, and may be less appropriate for small projects. Starting materials \u00b6 At IBM client engineering, we start with a design thinking workshop enhanced with event storming to model the to-be scenario with an end-to-end business process discovery as needed. Discovering events using event storming simplifies the process discovery and sequencing of tasks. BPMN modeling may also being used to model a process but it has the tendency to lock the thinking in a flow, while event storming focuses on what happened as facts, and so it easier to get events sequencing. Also events coming from no-where could still be discovered and have their value. Event storming workshop starts some domain driven design analysis by getting some of the following materials: Domains - sub domains Event Sequence flow. Events \u2013 business term definition: the start of ubiquitous language. Critical events. Business entities, aggregates, value objects Commands Actors - Users \u2013 based on Empathy maps and stakeholders list. Event linkages. Business Policies. Event prediction and probability flows. The derivation of those materials was described in the event storming introduction . Here is an example of starting material illustrating the beginning of the process for the Reefer shipping business process, with order placement and shipping contract elaboration: From there we complement this analysis by extending it with the domain driven design elements. Domain driven design steps \u00b6 Step 1: Assess domains and sub-domains \u00b6 Domain is what an organization does, and it includes the \"how to\", it performs its operations. Domain may be composed of sub-domains. A \"Core\" domain is a part of the business domain that is of primary importance to the success of the organization, the organization needs to excel at it to make business impact and difference. The application to be built is within a domain. Domain model is a conceptual object model representing part of the domain to be used in the solution. It includes behavior and data. During the event storming analysis, you define the domains and groups a set of sub-domains together. In the following figure, the container shipping domain is what the application we have to develop belongs to, and is composed of sub-domains like orders, shipping, inventory, .... Other domains like weather, CRM, invoice, are supporting the shipping domain but are not the focus of the design. Here is an example of such domain and subdomains: We have three core sub-domains and the rest are supports. Shipping over seas company needs to excel at managing container inventory, managing the shipping, the itineraries, and vessels. Step 2: Define the application context \u00b6 At the high level, when doing the analysis, you should have some insight decisions of the top level application to develop, with some ideas of the other systems to interact with. A classical \"system context diagram\" is a efficient tool to represent the application high level context. The external systems to integrate with, are strongly correlated to the domains discovered from previous step. For the EDA reference implementation solution the system context diagram is visible here . Each interface to those system needs to be documented using the interface characteristics approach presented by IBM Kim Clark. The full set of interface characteristics to consider for each system to integrate with is summarized below: FUNCTIONAL DEFINITION Principal data objects Operation/function Read or change Request/response objects TECHNICAL INTERFACE Transport Protocol Data format INTERACTION TYPE Request-response or fire-forget Thread-blocking or asynchronous Batch or individual Message size PERFORMANCE Response times Throughput Volumes Concurrency INTEGRITY Validation Transactionality Statefulness Event sequence Idempotence SECURITY Identity/authentication Authorization Data ownership Privacy RELIABILITY Availability Delivery assurance ERROR HANDLING Error management capabilities Known exception conditions Unexpected error presentation So with a system context diagram and interface characteristics to external system we have a good undestanding of the application context and interaction with external domains. Step 3: Define the ubiquitous language \u00b6 Eric Evans: \"To communicate effectively, the code must be based on the same language used to write the requirements, the same language the developers speak with each other and with domain experts\" This is where the work from the event storming and the relation with the business experts should help. Domain experts use their jargon while technical team members have their own language tuned for discussing the domain in terms of design. The terminology of day-to-day discussions is disconnected from the terminology embedded in the code, so the ubiquitous language helps to allign knowledge with design elements, code and tests. (Think about EJB, JPA entity, all the JEE design jargon versus ShippingOrder, order provisioning, fullfillment... ) The vocabulary of that ubiquitous language includes the class names and prominent operation names. The language includes terms to discuss rules that have been made explicit in the model. Be sure to commit the team to exercising that language relentlessly in all communication within the business and in the code. Use the same language in diagrams, writing, and especially speech. Play with the model as you talk about the system. Describe scenarios out loud using the elements and interactions of the model, combining concepts in ways allowed by the model. Find easier ways to say what you need to say, and then take those new ideas back down to the diagrams and code. Entities and Value Objects \u00b6 Entities are part of the ubiquitous language, and represent business concepts that can be uniquely identified by some attributes. They have a life cycle that is important to model, specially in the context of an event-driven solution. Value Objects represent things in the domain but without identity, and they are frequently transient, created for an operation and then discarded. Some time, in a certain context, a value object could become an entity. As an example, an Order will be a value object in the context of a fullfilment domain, while a core entity in order management domain. Below is an example of entities (Customer and Shipping Order) and value objects (delivery history and delivery specification): Aggregate boundaries \u00b6 An aggregate is a cluster of associated objects that we treat as a unit for the purpose of data changes. It is a collection of values and entities which are bound together by a root entity (the aggregate root ). An entity is most likely an aggregate and every things related to it define its boundaries. External client consider the aggregate as a all. It guaranteees the consistency of changes made within the aggregate. Bounded Contexts \u00b6 Bounded Context explicitly defines the boundaries of your model. A language in one bounded context can model the business domain for the solving of a particular problem. This concept is critical in large software projects. A Bounded Context sets the limits around what a specific team works on and helps them to define their own vocabulary within that particular context. When you define a bounded context, you define who uses it, how they use it, where it applies within a larger application context, and what it consists of, in terms of things like OpenAPI documentation and code repositories. Within a business context every use of a given domain terms, phrases, or sentences, the Ubiquitous Language , inside the boundary has a specific contextual meaning. So order context is a bounded context and groups order, ordered product type, pickup and shipping addresses, delivery specifications, delivery history.... Context maps \u00b6 Bounded contexts are not independent, a solution uses multiple bounded contexts, which are interacting with each others. The touchpoint between business contexts is a contract . To define relationships and integrations between bounded contexts we can consider the nature of the collaboration between teams, which can be grouped as: Cooperation : Uses well established communication and control. Change to the APIs are immediately communicated, the integration is both ways and teams solve together integration issues. If more formal cooperation is needed, the shared kernel pattern can be used and technic like contract testing is used. Sometime the shared kernel can be deployed a mediation flow, canonical model in enterprise service bus. But the scope has to be small anyway. Customer-supplier : the supplier context provides a service to its customers. Upstream or the downstream team can dictate the integration contract. With conformist supplier defines the contract based on its model and domain, and customer conforms to it. When the customer needs to translate the upstream bounded context model into its own model, it uses anticorruption layer . Anticorruption layer helps to isolate from messy model, model with high velocity of changes, or when the consumer model is core to its operations. When power is on the consumer side, then the supplier uses open-host service by decoupling its interface from the implementation, and design it for the consumer. This is the published language . Separate ways : no collaboration at all. Which leads to duplicate functionality in multiple bounded contexts. Avoid this solution when integrating sub-domains. The context map illustrates the integration between bounded contexts. It is the first high level design of the system components and the models they implements. The diagram below represents a simple view of e-commerce domain with the sub domains and bounded contexts Business operation API \u00b6 As part of the commands discovered during the event storming, some of them are related to business operations that could be managed and have business objective in term of cost control or new revenu stream. If needed, some more specific requirement about the APIs can be formalized using the API adoption canvas , which helps to capture all key aspects of API adoption with a lean approach. The characteristics to consider for a development point of view and DDD are: API product description SLA and expected performance Exposure to developer communities Expected short, medium and long term API management and versioning strategy For an implementation point of view, when mapping API to RESTful resource the data and verbs need to be defined, When APIs are asynchronous, the description of the delivery channel becomes important. Repositories \u00b6 Repository represents the infrastructure service to persist the root aggregate during its full life cycle. Client applications request objects from the repository using query methods that select objects based on criteria specified by the client, typically the value of certain attributes. Application logic never accesses storage implementation directly, only via the repository. Event linked microservices design - structure \u00b6 A complete event driven microservice specification (the target of this design step) includes specifications of the following elements: Event Topics Used to configure the Event Backbone Mapped to the life cycle of the root entity Topics can be chained to address different consumer semantic Single partition for keeping order and support exactly once delivery Event types within each event topic Microservices: They may be finer grained than aggregates or mapped to aggregate boundaries. They may separate query and command; possibly multiple queries. They could define demonstration control and serve main User Interface. Reference the related Entities and value objects within each microservice. Define APIs ( Synchronous or asynchronous) using standards like openAPI. Could be done bottom up from the code, as most of TDD implementation will lead to. Topics and events Subscribed to. Events published / emitted. List of end to end interactions: List of logic segments per microservice Recovery processing, scaling: We expect this to be highly patterned and template driven not requiring example-specific design. Step 4: Define modules \u00b6 Each aggregate will be implemented as some composition of: a command microservice managing state changes to the entities in this aggregate possibly one or more separate (CQRS) query services providing internal or external API query capabilities additional simulation, predictive analytics or User Interface microservices The command microservice will be built around a collection of active entites for the aggregate, keyed by some primary key. The separation of each aggregate into specific component microservices as outlined above, will be a complete list of microservices for the build / sprint. Identify the data collections, and collection organization (keying structure) in each command and query microservice for this build. Step 5: Limit the context and scope for this particular build / sprint \u00b6 We assume that we are developing a particular build for a sprint within some agile development approach, deferring additional functions and complexity to later sprints: Working from the initial list of aggregates, select which aggregates will be included in this build For each aggregate the possible choices are: to completely skip and workaround the aggregate in this build. to include a full lifecycle implementation of the aggregate to provide a simplified lifecycle implementation - typically a table of entities is initialized at start up, and state changes to existing entities are tracked Determine whether there are simulation services or predictive analytics service to be included in the build Identify the external query APIs and command APIs which this build should support Create entity lifecycle diagrams for entites having a full lifecycle implementation in this build / sprint. Step 6: Generate microservice interaction diagrams for the build \u00b6 The diagram will show API calls initiating state change. They should map the commands discovered during the event storming sessions. It shows for each interaction whether this is a synchronous API calls or an asynchronous event interaction via the event backbone. The diagram labels each specific event interaction between microservices trigerring a state change. Typically queries are synchronous API calls since the caller cannot usefully proceeed until a result is returned. From these, we can extract: a complete list of event types on each event backbone topic, with information passed on each event type. the complete list of \u201clogic segments\u201d for each microservice processing action in response to an API call or initiating event. When, at the next level of detail, the individual fields in each event are specified and typed, the CloudEvents standard may be used as a starting point. Step 7: TDD meets DDD \u00b6 Test Driven Development is a well established practice to develop software efficiently. Since 2003 and extreme programming practice, it helps when doing refactoring, improve code quality and focus on simple design. As Dan North stated in 2018, it could have been bettwe name 'Example Guided Design' as starting by the test improve the design, and writing tests that map example and end user stories will make it more clear and mapped to the requirements. With DDD, TDD needs to support the ubiquituous language and have tests intent clearly stated. We can easily adopt the following practices: Isolate tests per bounded context Adopt collective ownership of code and tests Pair programming between tester and developer Enforce contunuous feedbacks Keep tests in source control with the code and for integration tests in a dedicated folder or even repository. Have their executions as part of CI/CD pipeline. Write test with te Gherkin syntax: Feature: title of the scenario or test method Given [initial context] When [event or trigger] then [expected output] Here is an example of such structure in Junit 5 test: public void given_order_created_should_emit_event () throws OrderCreationException { assume_there_is_no_event_emitted (); OrderEventPayload new_order = ShippingOrderTestDataFactory . given_a_new_order (); // when service . createOrder ( new_order ); // then assertTrue ( order_created_event_generated ()); OrderCommandEvent createdOrderEvent = ( OrderCommandEvent ) commandEventProducer . getEventEmitted (); assertTrue ( new_order . getOrderID (). equals ((( OrderEventPayload ) createdOrderEvent . getPayload ()). getOrderID ())); } Step 8: Address failover and other NFRs \u00b6 NFRs impact architecture decision, technology choices and coding. So address those NFRs as early as possible during requirements and analysis and then design phases. If a microservice fails it will need to recover its internal state by reloading data from one or more topics, from the latest committed read. In general, command and query microservices will have a standard pattern for doing this. Any custom event filtering and service specific logic should be specified. Concepts and rationale underlying the design approach \u00b6 What is the difference between event information stored in the event backbone and state data stored in the microservices? The event information stored persistently in the event backbone is organized by topic and, within each topic, entirely by event time-of-occurrence. While the state information in a microservice is a list (collection) of all currently active entities of the owning aggregate (e.g. all orders, all voyages etc) and the current state of each such entity. The entity records are keyed by primary key, like an OrderID. While implementing microservice using event sourcing, CQRS, the persisted entity records are complementary to the historically organized information in the event backbone. When is it acceptable to be using synchronous interactions between services instead of asynchronous event interacts through the event backbone? For non-state-changing queries, for which the response is always instantaneously available a synchronous query call may be acceptable and will provide a simpler more understandable interface. Any processing which can be though of as being triggered by some state change in another aggregate should be modelled with an asynchronous event, because as the solution evolves other new microservices may also need to be aware of such event. We do not want to have to go back and change logic existing service where this event originated to have that microservice actively report the event to all potential consumers. How do we save microservices from having to maintain data collections with complex secondary indexing for which eventual consistency will be hard to implement? Each command microservice should do all its state changing updates using the primary key lookup only for its entities. Each asynchronous event interaction between microservices should carry primary entityIds ( orderID, VoyageID, shipID) for any entities associated with the interaction. Each query which might require speciaoized secondary indexing to respond to queries can be implemented in a separate CQRS query service which subscribes to events to do all internal updating and receives events from the event backbone in a ( Consistent) eventually correct order. This allows for recovery of any failed service by rebuilding it in \"eventually correct\" order. Applied DDD for the reference implementation \u00b6 See this article to see how we applied DDD for the shipping good over seas implementation . More reading and sources \u00b6 Eric Evans' book Scott Millet and Nick Tune's book: \"Patterns, Principles, and Practices of Domain-Driven Design\" Kyle Brown's article: Apply Domain-Driven Design to microservices architecture","title":"Domain-Driven Design"},{"location":"methodology/domain-driven-design/#overview","text":"Event storming is part of the domain-driven design methodology. And domain-driven design was deeply described in Eric Evans's \"Domain Driven Design: Tackling Complexity in the Heart of Software\" book from 2004. From this body of knowledge we can derive the following important elements: Domain Driven Design is about understanding the business domain in which the solution has to be developed Context is important to define the business term definitions within it For microservice we need to find the boundary of responsability to be able to apply the clear separation of concern pattern. The goals for the design step are: Improve communication between subject matter experts and developer, make the code readable by human and long term maintanable To support highly modular cloud native microservices. To adopt event coupled microservices - facilitating independent modification and evolution of each microservice separately. To allow applying event-driven patterns such as event sourcing, CQRS and SAGA to address some of the challenges of distributed system implementation: data consitency, transaction cross domains, and complex queries between aggregates managed by different services. DDD suits better long term project, where the domain of knowledge is important and complex, and may be less appropriate for small projects.","title":"Overview"},{"location":"methodology/domain-driven-design/#starting-materials","text":"At IBM client engineering, we start with a design thinking workshop enhanced with event storming to model the to-be scenario with an end-to-end business process discovery as needed. Discovering events using event storming simplifies the process discovery and sequencing of tasks. BPMN modeling may also being used to model a process but it has the tendency to lock the thinking in a flow, while event storming focuses on what happened as facts, and so it easier to get events sequencing. Also events coming from no-where could still be discovered and have their value. Event storming workshop starts some domain driven design analysis by getting some of the following materials: Domains - sub domains Event Sequence flow. Events \u2013 business term definition: the start of ubiquitous language. Critical events. Business entities, aggregates, value objects Commands Actors - Users \u2013 based on Empathy maps and stakeholders list. Event linkages. Business Policies. Event prediction and probability flows. The derivation of those materials was described in the event storming introduction . Here is an example of starting material illustrating the beginning of the process for the Reefer shipping business process, with order placement and shipping contract elaboration: From there we complement this analysis by extending it with the domain driven design elements.","title":"Starting materials"},{"location":"methodology/domain-driven-design/#domain-driven-design-steps","text":"","title":"Domain driven design steps"},{"location":"methodology/domain-driven-design/#step-1-assess-domains-and-sub-domains","text":"Domain is what an organization does, and it includes the \"how to\", it performs its operations. Domain may be composed of sub-domains. A \"Core\" domain is a part of the business domain that is of primary importance to the success of the organization, the organization needs to excel at it to make business impact and difference. The application to be built is within a domain. Domain model is a conceptual object model representing part of the domain to be used in the solution. It includes behavior and data. During the event storming analysis, you define the domains and groups a set of sub-domains together. In the following figure, the container shipping domain is what the application we have to develop belongs to, and is composed of sub-domains like orders, shipping, inventory, .... Other domains like weather, CRM, invoice, are supporting the shipping domain but are not the focus of the design. Here is an example of such domain and subdomains: We have three core sub-domains and the rest are supports. Shipping over seas company needs to excel at managing container inventory, managing the shipping, the itineraries, and vessels.","title":"Step 1: Assess domains and sub-domains"},{"location":"methodology/domain-driven-design/#step-2-define-the-application-context","text":"At the high level, when doing the analysis, you should have some insight decisions of the top level application to develop, with some ideas of the other systems to interact with. A classical \"system context diagram\" is a efficient tool to represent the application high level context. The external systems to integrate with, are strongly correlated to the domains discovered from previous step. For the EDA reference implementation solution the system context diagram is visible here . Each interface to those system needs to be documented using the interface characteristics approach presented by IBM Kim Clark. The full set of interface characteristics to consider for each system to integrate with is summarized below: FUNCTIONAL DEFINITION Principal data objects Operation/function Read or change Request/response objects TECHNICAL INTERFACE Transport Protocol Data format INTERACTION TYPE Request-response or fire-forget Thread-blocking or asynchronous Batch or individual Message size PERFORMANCE Response times Throughput Volumes Concurrency INTEGRITY Validation Transactionality Statefulness Event sequence Idempotence SECURITY Identity/authentication Authorization Data ownership Privacy RELIABILITY Availability Delivery assurance ERROR HANDLING Error management capabilities Known exception conditions Unexpected error presentation So with a system context diagram and interface characteristics to external system we have a good undestanding of the application context and interaction with external domains.","title":"Step 2: Define the application context"},{"location":"methodology/domain-driven-design/#step-3-define-the-ubiquitous-language","text":"Eric Evans: \"To communicate effectively, the code must be based on the same language used to write the requirements, the same language the developers speak with each other and with domain experts\" This is where the work from the event storming and the relation with the business experts should help. Domain experts use their jargon while technical team members have their own language tuned for discussing the domain in terms of design. The terminology of day-to-day discussions is disconnected from the terminology embedded in the code, so the ubiquitous language helps to allign knowledge with design elements, code and tests. (Think about EJB, JPA entity, all the JEE design jargon versus ShippingOrder, order provisioning, fullfillment... ) The vocabulary of that ubiquitous language includes the class names and prominent operation names. The language includes terms to discuss rules that have been made explicit in the model. Be sure to commit the team to exercising that language relentlessly in all communication within the business and in the code. Use the same language in diagrams, writing, and especially speech. Play with the model as you talk about the system. Describe scenarios out loud using the elements and interactions of the model, combining concepts in ways allowed by the model. Find easier ways to say what you need to say, and then take those new ideas back down to the diagrams and code.","title":"Step 3: Define the ubiquitous language"},{"location":"methodology/domain-driven-design/#entities-and-value-objects","text":"Entities are part of the ubiquitous language, and represent business concepts that can be uniquely identified by some attributes. They have a life cycle that is important to model, specially in the context of an event-driven solution. Value Objects represent things in the domain but without identity, and they are frequently transient, created for an operation and then discarded. Some time, in a certain context, a value object could become an entity. As an example, an Order will be a value object in the context of a fullfilment domain, while a core entity in order management domain. Below is an example of entities (Customer and Shipping Order) and value objects (delivery history and delivery specification):","title":"Entities and Value Objects"},{"location":"methodology/domain-driven-design/#aggregate-boundaries","text":"An aggregate is a cluster of associated objects that we treat as a unit for the purpose of data changes. It is a collection of values and entities which are bound together by a root entity (the aggregate root ). An entity is most likely an aggregate and every things related to it define its boundaries. External client consider the aggregate as a all. It guaranteees the consistency of changes made within the aggregate.","title":"Aggregate boundaries"},{"location":"methodology/domain-driven-design/#bounded-contexts","text":"Bounded Context explicitly defines the boundaries of your model. A language in one bounded context can model the business domain for the solving of a particular problem. This concept is critical in large software projects. A Bounded Context sets the limits around what a specific team works on and helps them to define their own vocabulary within that particular context. When you define a bounded context, you define who uses it, how they use it, where it applies within a larger application context, and what it consists of, in terms of things like OpenAPI documentation and code repositories. Within a business context every use of a given domain terms, phrases, or sentences, the Ubiquitous Language , inside the boundary has a specific contextual meaning. So order context is a bounded context and groups order, ordered product type, pickup and shipping addresses, delivery specifications, delivery history....","title":"Bounded Contexts"},{"location":"methodology/domain-driven-design/#context-maps","text":"Bounded contexts are not independent, a solution uses multiple bounded contexts, which are interacting with each others. The touchpoint between business contexts is a contract . To define relationships and integrations between bounded contexts we can consider the nature of the collaboration between teams, which can be grouped as: Cooperation : Uses well established communication and control. Change to the APIs are immediately communicated, the integration is both ways and teams solve together integration issues. If more formal cooperation is needed, the shared kernel pattern can be used and technic like contract testing is used. Sometime the shared kernel can be deployed a mediation flow, canonical model in enterprise service bus. But the scope has to be small anyway. Customer-supplier : the supplier context provides a service to its customers. Upstream or the downstream team can dictate the integration contract. With conformist supplier defines the contract based on its model and domain, and customer conforms to it. When the customer needs to translate the upstream bounded context model into its own model, it uses anticorruption layer . Anticorruption layer helps to isolate from messy model, model with high velocity of changes, or when the consumer model is core to its operations. When power is on the consumer side, then the supplier uses open-host service by decoupling its interface from the implementation, and design it for the consumer. This is the published language . Separate ways : no collaboration at all. Which leads to duplicate functionality in multiple bounded contexts. Avoid this solution when integrating sub-domains. The context map illustrates the integration between bounded contexts. It is the first high level design of the system components and the models they implements. The diagram below represents a simple view of e-commerce domain with the sub domains and bounded contexts","title":"Context maps"},{"location":"methodology/domain-driven-design/#business-operation-api","text":"As part of the commands discovered during the event storming, some of them are related to business operations that could be managed and have business objective in term of cost control or new revenu stream. If needed, some more specific requirement about the APIs can be formalized using the API adoption canvas , which helps to capture all key aspects of API adoption with a lean approach. The characteristics to consider for a development point of view and DDD are: API product description SLA and expected performance Exposure to developer communities Expected short, medium and long term API management and versioning strategy For an implementation point of view, when mapping API to RESTful resource the data and verbs need to be defined, When APIs are asynchronous, the description of the delivery channel becomes important.","title":"Business operation API"},{"location":"methodology/domain-driven-design/#repositories","text":"Repository represents the infrastructure service to persist the root aggregate during its full life cycle. Client applications request objects from the repository using query methods that select objects based on criteria specified by the client, typically the value of certain attributes. Application logic never accesses storage implementation directly, only via the repository.","title":"Repositories"},{"location":"methodology/domain-driven-design/#event-linked-microservices-design-structure","text":"A complete event driven microservice specification (the target of this design step) includes specifications of the following elements: Event Topics Used to configure the Event Backbone Mapped to the life cycle of the root entity Topics can be chained to address different consumer semantic Single partition for keeping order and support exactly once delivery Event types within each event topic Microservices: They may be finer grained than aggregates or mapped to aggregate boundaries. They may separate query and command; possibly multiple queries. They could define demonstration control and serve main User Interface. Reference the related Entities and value objects within each microservice. Define APIs ( Synchronous or asynchronous) using standards like openAPI. Could be done bottom up from the code, as most of TDD implementation will lead to. Topics and events Subscribed to. Events published / emitted. List of end to end interactions: List of logic segments per microservice Recovery processing, scaling: We expect this to be highly patterned and template driven not requiring example-specific design.","title":"Event linked microservices design - structure"},{"location":"methodology/domain-driven-design/#step-4-define-modules","text":"Each aggregate will be implemented as some composition of: a command microservice managing state changes to the entities in this aggregate possibly one or more separate (CQRS) query services providing internal or external API query capabilities additional simulation, predictive analytics or User Interface microservices The command microservice will be built around a collection of active entites for the aggregate, keyed by some primary key. The separation of each aggregate into specific component microservices as outlined above, will be a complete list of microservices for the build / sprint. Identify the data collections, and collection organization (keying structure) in each command and query microservice for this build.","title":"Step 4: Define modules"},{"location":"methodology/domain-driven-design/#step-5-limit-the-context-and-scope-for-this-particular-build-sprint","text":"We assume that we are developing a particular build for a sprint within some agile development approach, deferring additional functions and complexity to later sprints: Working from the initial list of aggregates, select which aggregates will be included in this build For each aggregate the possible choices are: to completely skip and workaround the aggregate in this build. to include a full lifecycle implementation of the aggregate to provide a simplified lifecycle implementation - typically a table of entities is initialized at start up, and state changes to existing entities are tracked Determine whether there are simulation services or predictive analytics service to be included in the build Identify the external query APIs and command APIs which this build should support Create entity lifecycle diagrams for entites having a full lifecycle implementation in this build / sprint.","title":"Step 5: Limit the context and scope for this particular build / sprint"},{"location":"methodology/domain-driven-design/#step-6-generate-microservice-interaction-diagrams-for-the-build","text":"The diagram will show API calls initiating state change. They should map the commands discovered during the event storming sessions. It shows for each interaction whether this is a synchronous API calls or an asynchronous event interaction via the event backbone. The diagram labels each specific event interaction between microservices trigerring a state change. Typically queries are synchronous API calls since the caller cannot usefully proceeed until a result is returned. From these, we can extract: a complete list of event types on each event backbone topic, with information passed on each event type. the complete list of \u201clogic segments\u201d for each microservice processing action in response to an API call or initiating event. When, at the next level of detail, the individual fields in each event are specified and typed, the CloudEvents standard may be used as a starting point.","title":"Step 6: Generate microservice interaction diagrams for the build"},{"location":"methodology/domain-driven-design/#step-7-tdd-meets-ddd","text":"Test Driven Development is a well established practice to develop software efficiently. Since 2003 and extreme programming practice, it helps when doing refactoring, improve code quality and focus on simple design. As Dan North stated in 2018, it could have been bettwe name 'Example Guided Design' as starting by the test improve the design, and writing tests that map example and end user stories will make it more clear and mapped to the requirements. With DDD, TDD needs to support the ubiquituous language and have tests intent clearly stated. We can easily adopt the following practices: Isolate tests per bounded context Adopt collective ownership of code and tests Pair programming between tester and developer Enforce contunuous feedbacks Keep tests in source control with the code and for integration tests in a dedicated folder or even repository. Have their executions as part of CI/CD pipeline. Write test with te Gherkin syntax: Feature: title of the scenario or test method Given [initial context] When [event or trigger] then [expected output] Here is an example of such structure in Junit 5 test: public void given_order_created_should_emit_event () throws OrderCreationException { assume_there_is_no_event_emitted (); OrderEventPayload new_order = ShippingOrderTestDataFactory . given_a_new_order (); // when service . createOrder ( new_order ); // then assertTrue ( order_created_event_generated ()); OrderCommandEvent createdOrderEvent = ( OrderCommandEvent ) commandEventProducer . getEventEmitted (); assertTrue ( new_order . getOrderID (). equals ((( OrderEventPayload ) createdOrderEvent . getPayload ()). getOrderID ())); }","title":"Step 7: TDD meets DDD"},{"location":"methodology/domain-driven-design/#step-8-address-failover-and-other-nfrs","text":"NFRs impact architecture decision, technology choices and coding. So address those NFRs as early as possible during requirements and analysis and then design phases. If a microservice fails it will need to recover its internal state by reloading data from one or more topics, from the latest committed read. In general, command and query microservices will have a standard pattern for doing this. Any custom event filtering and service specific logic should be specified.","title":"Step 8: Address failover and other NFRs"},{"location":"methodology/domain-driven-design/#concepts-and-rationale-underlying-the-design-approach","text":"What is the difference between event information stored in the event backbone and state data stored in the microservices? The event information stored persistently in the event backbone is organized by topic and, within each topic, entirely by event time-of-occurrence. While the state information in a microservice is a list (collection) of all currently active entities of the owning aggregate (e.g. all orders, all voyages etc) and the current state of each such entity. The entity records are keyed by primary key, like an OrderID. While implementing microservice using event sourcing, CQRS, the persisted entity records are complementary to the historically organized information in the event backbone. When is it acceptable to be using synchronous interactions between services instead of asynchronous event interacts through the event backbone? For non-state-changing queries, for which the response is always instantaneously available a synchronous query call may be acceptable and will provide a simpler more understandable interface. Any processing which can be though of as being triggered by some state change in another aggregate should be modelled with an asynchronous event, because as the solution evolves other new microservices may also need to be aware of such event. We do not want to have to go back and change logic existing service where this event originated to have that microservice actively report the event to all potential consumers. How do we save microservices from having to maintain data collections with complex secondary indexing for which eventual consistency will be hard to implement? Each command microservice should do all its state changing updates using the primary key lookup only for its entities. Each asynchronous event interaction between microservices should carry primary entityIds ( orderID, VoyageID, shipID) for any entities associated with the interaction. Each query which might require speciaoized secondary indexing to respond to queries can be implemented in a separate CQRS query service which subscribes to events to do all internal updating and receives events from the event backbone in a ( Consistent) eventually correct order. This allows for recovery of any failed service by rebuilding it in \"eventually correct\" order.","title":"Concepts and rationale underlying the design approach"},{"location":"methodology/domain-driven-design/#applied-ddd-for-the-reference-implementation","text":"See this article to see how we applied DDD for the shipping good over seas implementation .","title":"Applied DDD for the reference implementation"},{"location":"methodology/domain-driven-design/#more-reading-and-sources","text":"Eric Evans' book Scott Millet and Nick Tune's book: \"Patterns, Principles, and Practices of Domain-Driven Design\" Kyle Brown's article: Apply Domain-Driven Design to microservices architecture","title":"More reading and sources"},{"location":"methodology/event-storming/","text":"In this article we are presenting an end to end set of activities to run a successful Minimum Viable Product for an event-driven solution using cloud native microservices and event backbone as the core technology approach. The discovery and analysis of the MVP scope starts with an event storming workshop where designer, architect work hand to hand with business users and domain subject matter experts. From the different outcomes of the workshop, the development team starts to outline components, microservices, business entity life cycle, etc... in a short design iteration . The scope is well defined Epics, Hill and user stories defined, at least for the first iterations, and the MVP can start. Event Storming workshop introduction \u00b6 Event storming is a workshop format for quickly exploring complex business domains by focusing on domain events generated in the context of a business process or a business application. A domain event is something meaningful to the experts that happened in the domain. The workshop focuses on communication between product owner, domain experts and developers. The event storming method was introduced and publicized by Alberto Brandolini in \"Introducing event storming book\" . This approach is recognized in the Domain Driven Design (DDD) community as a technique for rapid capture of a solution design and improved team understanding of the domain. Domain represents some area of the business that has the analysis focus. This article outlines the method and describes refinements and extensions that are useful in designs for an event-driven architecture. This extended approach adds an insight storming step to identify and capture value adding predictive insights about possible future events. The predictive insights are generated by using data science analysis, data models, artificial intelligence (AI), or machine learning (ML). This article describes in general terms all the steps to run an event storming workshop. The output of an actual workshop done on a sample problem - a world wide container shipment, is further detailed in the container shipment analysis example . Conducting the event and insight storming workshop \u00b6 Before conducting an event storming workshop, complete a Design Thinking Workshop in which Personas and Empathy Maps are developed and business pains and goals are defined. The event storming workshop adds more specific design on the events occuring at each step of the process, natural contexts for microservices and predictive insights to guide operation of the system. With this approach, a team that includes business owners and stakeholders can define a Minimal Viable Prototype (MVP) design for the solution. The resulting design is organized as a collection of loosely coupled microservices linked through an event-driven architecture and one or more event backbones. This style of design can be deployed into multicloud execution environments and allows for scaling and agile deployment. Preparations for the event storming workshop include the following steps: Get a room big enough to hold at least 6 to 8 persons and with enough wall space on which to stick big paper sheets: you will need a lot of wall space to define the models. Obtain green, orange, blue, and red square sticky notes, black sharpie pens and blue painter's tape. Discourage the use of open laptops during the meeting. Limit the number of chairs so that the team stays focused and connected and conversation flows easily. Concepts \u00b6 Many of the concepts addressed during the event storming workshop are defined in the Domain Driven Design approach. The following diagrams present the elements used during the analysis. The first diagram shows the initial set of concepts that are used in the process. Domain events are also named business events . An event is some action or happening which occurred in the system at a specific time in the past. The first step in the event storming process consists of these actions: Identifying all relevant events in the domain and specific process being analyzed, Writing a very short description of each event on a \"sticky\" note and placing all the event \"sticky\" notes in sequence on a timeline. The act of writing event descriptions often results in questions to be resolved later, or discussions about definitions that need to be recorded to ensure that everyone agrees on basic domain concepts. A timeline of domain events is the critical output of the first step in the event storming process. The timeline gives everyone a common understanding of when events take place in relation to each other. You still need to be able to take this initial level of understanding and move it towards an implementation. In making that step, you must expand your thinking to encompass the idea of a command, which is the action that kicks off the processing that triggers an event. As part of understanding the role of the command, you will also want to know who invokes a command (actors) and what information is needed to allow the command to be executed. This diagram show how those analysis elements are linked together: One-View Figure. Actors consume data by using a user interface and use the UI to interact with the system via commands. Actors could also be replace by articial intelligent agents. Commands are the result of some user decision or policy, and act on relevant data which are part of a Read model in the CQRS pattern. Policies (represented by lilac stickies) are reactive logics that take place after an event occurs, and trigger other commands. Policies always start with the phrase \"whenever...\". They can be a manual step a human follows, such as a documented procedure or guidance, or they may be automated. When applying the Agile Business Rule Development methodology it will be mapped to a Decision within the Decision Model Notation . External systems produce events. Data can be presented to users in a user interface or modified by the system. Events can be created by commands or by external systems including IOT devices. They can be triggerred by the processing of other events or by some period of elapsed time. When an event is repeated or occurs regularly on a schedule, draw a clock or calendar icon in the corner of the sticky note for that event. As the events are identified and sequenced into a time line, you might find multiple independent subsequences that are not directly coupled to each other and that represent different perspectives of the system, but occur in overlapped periods of time. These parallel event streams can be addressed by putting them into separate swimlanes delineated by using horizontal blue painter's tape. As the events are organized into a timeline, possibly with swim lanes, you can identify pivotal events. Pivotal events indicate major changes in the domain and often form the boundary between one phase of the system and another. Pivotal events will typically separate (a bounded context in DDD terms). Pivotal events are identified with vertical blue painters tape (crossing all the swimlanes). An example of a section of a completed event time line with pivotal events and swimlanes is shown below. Conducting the workshop \u00b6 The goal of the workshop is to better understand the business problem to address with a future application. But the approach can also apply to finding solutions to bottlenecks or other issues in existing applications. The workshop helps the team to understand the big picture of the solution by building a timeline of domain events as they occur during the business process life span. During the workshop, avoid documenting processing steps. The event storming method is not trying to specify a particular implementation. Instead, the focus in initial stages of the workshop is on identifying and sequencing the events that occur in the solution. The event timeline is a useful representation of the overall steps, communicating what must happen while remaining open to many possible implementation approaches. Step 1: Domain events discovery \u00b6 Begin by writing each domain event on an orange sticky note with a few words and a verb in a past tense. Describe What's happened . At first just \"storm\" the events by having each domain expert generate an individual lists of domain events. You might not need to initially place the events on the ordered timeline as they write them. The events must be worded in a way that is meaningful to the domain experts and business stakeholder. You are explaining what happens in business terms, not what happens inside the implementation of the system. You don't need to describe all the events in your domain, but you must cover the process that you are interested in exploring from end to end. Therefore, make sure that you identify the start and end events and place them on the timeline at the beginning and end of the wall covered with paper. Place the other events that you identified between these two endpoints in the closest approximation that the team can agree to a sequential order. Don\u2019t worry about overlaps at this point; overlaps are addressed later. Step 2: Tell the story \u00b6 In this step, you retell the story by talking about how to relate events to particular personas. A member of the team (often the facilitator, but others can do this as well) acts this out by taking on the perspective of a persona in the domain, such as a \"manufacturer\" who wants to ship a widget to a customer, and asking which events follow which other events. Start at the beginning of that persona's interaction and ask \"what happens next?\". Pick up and rearrange the events that the team storms. If you discover events that are duplicates, take those off the board. If events are in the wrong order, move them into the right order. When some parts are unclear, add questions or comments by using the red stickies.. Red stickies indicate that the team needs to follow up and clarify issues later. Likewise you want to use this time to document assumptions on the definition stickies. This is also a good time to rephrase events as you proceed through the story. Sometimes you need to rephrase an event description by putting the verbing in past tense, or adjusting the terms that are used to relate clearly to other identified events. In this step you focus on the mainline \"happy\" end-to-end path to avoid getting bogged down in details of exceptions and error handling. Exceptions can be added later Step 3: Find the Boundaries \u00b6 The next step of this part of the process is to find the boundaries of your system by looking at the events. Two types of boundaries can emerge; the first type of boundary is a time boundary. Often specific key \"pivotal events\" indicate a change from one aspect of a system to another. This can happen at a hand-off from one persona to another, but it can also happen at a change of geographical, legal, or other type of boundary. If you notice that the terms that are used on the event stickies change at these boundaries, you are seeing a \"bounded context\" in Domain Driven Design terms. Highlight pivotal events by putting up blue painter\u2019s tape vertically behind the event. The second type of boundary is a subject boundary. You can detect a subject boundary by looking for the following conditions: You have multiple simultaneous series of events that only come together at a later time. You see the same terms being used in the event descriptions for a particular series of events. You can \u201cread\u201d a series of events from the point of view of a different persona when you are replaying them. You can delineate these different sets of simultaneous event streams by applying blue painter\u2019s tape horizontally, dividing the board into different swim lanes. Below is an example of a set of ordered domain events with pivotal events and subject swim lanes indicated. This example comes from applying event storming to the domain of container shipping process and is discussed in more detail in the container shipment analysis example . When the reefer container is plugged to the Vessel, it starts to emit telemetries, we change context. Step 4: Locate the Commands \u00b6 In this step you shift from analysis of the domain to the first stages of system design. Up until this point, you are simply trying to understand how the events in the domain relate to one another - this is why the participation of domain experts is so critical. However, to build a system that implements the business process that you are interested in, you have to move on to the question of how these events come into being. Commands are the most common mechanism by which events are created. The key to finding commands is to ask the question: \"Why did this event occur?\". In this step, the focus of the process moves to the sequence of actions that lead to events. Your goal is to find the causes for which the events record the effects. Expected event trigger types are: A human operator makes a decision and issues a command Some external system or sensor provides a stimulus An event results from some policy - typically automated processing of a precursor event The completion of some determined period of elapsed time. The triggering command is identified in a blue (sticky) note. Command may become a microservice operation exposed via API. The human persona issuing the command is identified and shown in a yellow note. Some events may be created by applying business policies. The diagram below illustrates the manufacturer actor using the place a shipment order command to create a shipment order placed event, as well as . It is possible to chain events and commands as presented in the \"one view\" figure above in the concepts section. Step 5: Describe the Data \u00b6 You can't truly define a command without understanding the data that is needed for the command to execute in order to produce the event. You can identify several types of data during this step. First, users (personas) need data from the user interface in order to make decisions before executing a command. That data forms part of the read model in a CQRS implementation. For each command and event pair, you add a data description of the expected attributes and data elements needed to take such a decision. Here is a simple example for a shipment order placed event created from a place a shipment order action . Another important part of the process that becomes more fully fleshed out at this step is the description of policies that can trigger the generation of an event from a previous event (or set of events). Assess if the data element is a main business entity, uniquely identified by a key, supported by multiple commands. It has a life span over the business process. This will lead to develop an entity life cycle analysis. This first level of data definition helps to assess the microservice scope and responsibility as you start to see commonalities emerge from the data used among several related events. Those concepts become more obvious in the next step. Step 6: Identify the Aggregates \u00b6 In DDD, entities and value objects can exist independently, but often, the relations are such that an entity or a value object has no value without its context. Aggregates provide that context by being those \"roots\" that comprise one or more entities and value objects that are linked together through a lifecycle. The following diagram illustrates a detailed example of aggregates for shipment of a temperature sensitive product overseas. In event storming, we may not be able to get this level of detail during the first workshop, but aggregates emerge through the process by grouping events and commands that are related together. This grouping not only consists of related data (entities and value objects) but also related actions (commands) that are connected by the lifecycle of that aggregate. Aggregates ultimately suggest microservice boundaries. In the container shipment example, you can see that you can group several commands and event pairs (with their associated data) together that are related through the lifecycle of an order for shipping. Step 7: Define Bounded Context \u00b6 In this step, you define terms and concepts with a clear meaning valid in a clear boundary and you define the context within which a model applies. (The term definition can change outside of the business unit for which an application is developed). The following items may be addressed: Which team owns the model? Which part of the model transit between team organization? What are the different code bases foreseen we need to implement? What are the different data schema ? (database or json or xml schemas) Here is an example of bounded context that will, most likely, lead to a microservice: Keep the model strictly consistent within these bounds. Step 8: Looking forward with insight storming \u00b6 In event atorming for Event Driven Architecture (EDA) solutions it is helpful to include an additional method step at this point identifying useful predictive analytics insights. Insights storming extends the basic methodology by looking forward and considering what if you could know in advance that an event is going to occur. How would this change your actions, and what would you do in advance of that event actually happening? You can think of insight storming as extending the analysis to Derived Events . Rather than being the factual recording of a past event, a derived event is a forward-looking or predictive event, that is, \"this event is probably going to happen at some time in the next n hours\u201d. By using this forward-looking insight combined with the known business data from earlier events, human actors and event triggering policies can make better decisions about how to react to new events as they occur. Insight storming amounts to asking workshop participants the question: \"What data would be helpful at each event trigger to assist the human user or automated event triggering policy make the best possible decision of how and when to act?\" An important motivation that drives the use of an event-driven architecture is that it simplifies design and realization of highly responsive systems that react immediately and intelligently, that is, in a personalized and context-aware way, and optimally to new events as they occur. This immediately suggests that predictive analytics and models to generate predictive insights have an important role to play. Predictive analytic insights are effectively probabilistic statements about which future events are likely to occur and what are the likely properties of those events. These probabilistic statements are typicaly generated by using models created by data scientists or using AI or ML. Correlating or joining independently gathered sources of information can also generate important predictive insights or be input to predictive analytic models. Business owners and stakeholders in the event storming workshop can offer good intuitions in several areas: Which probabilistic insights are likely to lead to improved or optimal decision making and action? The action could take the form of an immediate response to an event when it occurs. The action could be proactive behavior to avoid an undesirable event. What combined sources of information are likely to help create a model to predict this insight? With basic event storming, you look backwards at each event because an event is something that has already happened. When you identify data needed for an actor or policy to decide when and how to issue a command, there is a tendency to restrict consideration to properties of earlier known and captured business events. In insight storming you extend the approach to explicitly look forward and consider what is the probability that a particular event will occur at some future time and what would be its expected property values? How would this change the best action to take when and if this event occurs? Is there action we can take now proactively in advance of an expected undesirable event to prevent it happening or mitigate the consequences? The insight method step amounts to getting workshop participants to identify derived events and the data sources needed for the models that generate them. Adding an insight storming step using the questions above into the workshop will improve decision making and proactive behavior in the resulting design. Insights can be published into a bus and subscribed to by any decision step guidance. By identifying derived events, you can integrate analytic models and machine learning into the designed solution. Event and derived event feeds can be processed, filtered, joined, aggregated, modeled and scored to create valuable predictive insights. Use the following new notations for the insight step: Pale blue stickies for derived events. Parallelogram shape to show when events and derived events are combined to enable deeper insight models and predictions. Identify predictive insights as early as possible in the development life cycle. The best opportunity to do this is to add this step to the event storming workshop. The two diagrams below show the results of the insight storming step for the use case of container shipment analysis. The first diagram captures insights and associated linkages for each refrigerated container, identifying when automated changes to the thermostat settings can be made, when unit maintenance should be scheduled and when the container contents must be considered spoiled. The second diagram captures insights that could trigger recommendations to adjust ship course or speed in response to expected severe weather forcasts for the route ahead or predicted congestion and expected docking and unloading delays at the next port of call. Design iteration \u00b6 Attention we are not proposing to apply a waterfall approach, but before starting the deeper implementation with iterations, we want to spend sometime to define in more details what we want to build, how to organize the CI/CD projects and pipeline, select the development, test and product plaform, and define epics, user stories, components, microservices... This iteration can take from few hours to a week, depending on the expected MVP goals. For an event-driven solution a MVP for a single application should not take more than 3 to 4 iterations. Event storming to user stories and epics \u00b6 In agile methodology, creating user stories or epics is one of the most important elements in project management. The commands and policies related to events can be easily described as user stories, because commands and decisions are done by actors. The actor could be a system as well. For the data you must model the \"Create, Read, Update, Delete\" operations as user stories, mostly supported by a system actor. An event is the result or outcome of a user story. Events can be added as part of the acceptance criteria of the user stories to verify that the event really occurs. Applying to the container shipment use case \u00b6 The K Container Shipment use case demonstrates an implementation solution to validate the event-driven architecture. The container shipment analysis example , shows event storming and design thinking main artifacts, including artifacts for the monitoring of refrigerated containers. Some practice notes \u00b6 you can apply event storming at different level: for example at the beginning of a project to understand the high level process at stake. with a big group of people, you will stay at the high level. But it can be used to model a lower level microservice, to assess event consumed and produced. Further Readings \u00b6 Introduction to event storming from Alberto Brandolini Event Storming Guide Wikipedia Domain Driven Design Eric Evans: \"Domain Driven Design - Tacking complexity in the heart of software\" Domain drive design with event storming introduction video Patterns related to Domain Driven Design by Martin Fowler Applying DDD and event storming for event-driven microservice implementation from our own work","title":"Event Storming"},{"location":"methodology/event-storming/#event-storming-workshop-introduction","text":"Event storming is a workshop format for quickly exploring complex business domains by focusing on domain events generated in the context of a business process or a business application. A domain event is something meaningful to the experts that happened in the domain. The workshop focuses on communication between product owner, domain experts and developers. The event storming method was introduced and publicized by Alberto Brandolini in \"Introducing event storming book\" . This approach is recognized in the Domain Driven Design (DDD) community as a technique for rapid capture of a solution design and improved team understanding of the domain. Domain represents some area of the business that has the analysis focus. This article outlines the method and describes refinements and extensions that are useful in designs for an event-driven architecture. This extended approach adds an insight storming step to identify and capture value adding predictive insights about possible future events. The predictive insights are generated by using data science analysis, data models, artificial intelligence (AI), or machine learning (ML). This article describes in general terms all the steps to run an event storming workshop. The output of an actual workshop done on a sample problem - a world wide container shipment, is further detailed in the container shipment analysis example .","title":"Event Storming workshop introduction"},{"location":"methodology/event-storming/#conducting-the-event-and-insight-storming-workshop","text":"Before conducting an event storming workshop, complete a Design Thinking Workshop in which Personas and Empathy Maps are developed and business pains and goals are defined. The event storming workshop adds more specific design on the events occuring at each step of the process, natural contexts for microservices and predictive insights to guide operation of the system. With this approach, a team that includes business owners and stakeholders can define a Minimal Viable Prototype (MVP) design for the solution. The resulting design is organized as a collection of loosely coupled microservices linked through an event-driven architecture and one or more event backbones. This style of design can be deployed into multicloud execution environments and allows for scaling and agile deployment. Preparations for the event storming workshop include the following steps: Get a room big enough to hold at least 6 to 8 persons and with enough wall space on which to stick big paper sheets: you will need a lot of wall space to define the models. Obtain green, orange, blue, and red square sticky notes, black sharpie pens and blue painter's tape. Discourage the use of open laptops during the meeting. Limit the number of chairs so that the team stays focused and connected and conversation flows easily.","title":"Conducting the event and insight storming workshop"},{"location":"methodology/event-storming/#concepts","text":"Many of the concepts addressed during the event storming workshop are defined in the Domain Driven Design approach. The following diagrams present the elements used during the analysis. The first diagram shows the initial set of concepts that are used in the process. Domain events are also named business events . An event is some action or happening which occurred in the system at a specific time in the past. The first step in the event storming process consists of these actions: Identifying all relevant events in the domain and specific process being analyzed, Writing a very short description of each event on a \"sticky\" note and placing all the event \"sticky\" notes in sequence on a timeline. The act of writing event descriptions often results in questions to be resolved later, or discussions about definitions that need to be recorded to ensure that everyone agrees on basic domain concepts. A timeline of domain events is the critical output of the first step in the event storming process. The timeline gives everyone a common understanding of when events take place in relation to each other. You still need to be able to take this initial level of understanding and move it towards an implementation. In making that step, you must expand your thinking to encompass the idea of a command, which is the action that kicks off the processing that triggers an event. As part of understanding the role of the command, you will also want to know who invokes a command (actors) and what information is needed to allow the command to be executed. This diagram show how those analysis elements are linked together: One-View Figure. Actors consume data by using a user interface and use the UI to interact with the system via commands. Actors could also be replace by articial intelligent agents. Commands are the result of some user decision or policy, and act on relevant data which are part of a Read model in the CQRS pattern. Policies (represented by lilac stickies) are reactive logics that take place after an event occurs, and trigger other commands. Policies always start with the phrase \"whenever...\". They can be a manual step a human follows, such as a documented procedure or guidance, or they may be automated. When applying the Agile Business Rule Development methodology it will be mapped to a Decision within the Decision Model Notation . External systems produce events. Data can be presented to users in a user interface or modified by the system. Events can be created by commands or by external systems including IOT devices. They can be triggerred by the processing of other events or by some period of elapsed time. When an event is repeated or occurs regularly on a schedule, draw a clock or calendar icon in the corner of the sticky note for that event. As the events are identified and sequenced into a time line, you might find multiple independent subsequences that are not directly coupled to each other and that represent different perspectives of the system, but occur in overlapped periods of time. These parallel event streams can be addressed by putting them into separate swimlanes delineated by using horizontal blue painter's tape. As the events are organized into a timeline, possibly with swim lanes, you can identify pivotal events. Pivotal events indicate major changes in the domain and often form the boundary between one phase of the system and another. Pivotal events will typically separate (a bounded context in DDD terms). Pivotal events are identified with vertical blue painters tape (crossing all the swimlanes). An example of a section of a completed event time line with pivotal events and swimlanes is shown below.","title":"Concepts"},{"location":"methodology/event-storming/#conducting-the-workshop","text":"The goal of the workshop is to better understand the business problem to address with a future application. But the approach can also apply to finding solutions to bottlenecks or other issues in existing applications. The workshop helps the team to understand the big picture of the solution by building a timeline of domain events as they occur during the business process life span. During the workshop, avoid documenting processing steps. The event storming method is not trying to specify a particular implementation. Instead, the focus in initial stages of the workshop is on identifying and sequencing the events that occur in the solution. The event timeline is a useful representation of the overall steps, communicating what must happen while remaining open to many possible implementation approaches.","title":"Conducting the workshop"},{"location":"methodology/event-storming/#step-1-domain-events-discovery","text":"Begin by writing each domain event on an orange sticky note with a few words and a verb in a past tense. Describe What's happened . At first just \"storm\" the events by having each domain expert generate an individual lists of domain events. You might not need to initially place the events on the ordered timeline as they write them. The events must be worded in a way that is meaningful to the domain experts and business stakeholder. You are explaining what happens in business terms, not what happens inside the implementation of the system. You don't need to describe all the events in your domain, but you must cover the process that you are interested in exploring from end to end. Therefore, make sure that you identify the start and end events and place them on the timeline at the beginning and end of the wall covered with paper. Place the other events that you identified between these two endpoints in the closest approximation that the team can agree to a sequential order. Don\u2019t worry about overlaps at this point; overlaps are addressed later.","title":"Step 1: Domain events discovery"},{"location":"methodology/event-storming/#step-2-tell-the-story","text":"In this step, you retell the story by talking about how to relate events to particular personas. A member of the team (often the facilitator, but others can do this as well) acts this out by taking on the perspective of a persona in the domain, such as a \"manufacturer\" who wants to ship a widget to a customer, and asking which events follow which other events. Start at the beginning of that persona's interaction and ask \"what happens next?\". Pick up and rearrange the events that the team storms. If you discover events that are duplicates, take those off the board. If events are in the wrong order, move them into the right order. When some parts are unclear, add questions or comments by using the red stickies.. Red stickies indicate that the team needs to follow up and clarify issues later. Likewise you want to use this time to document assumptions on the definition stickies. This is also a good time to rephrase events as you proceed through the story. Sometimes you need to rephrase an event description by putting the verbing in past tense, or adjusting the terms that are used to relate clearly to other identified events. In this step you focus on the mainline \"happy\" end-to-end path to avoid getting bogged down in details of exceptions and error handling. Exceptions can be added later","title":"Step 2: Tell the story"},{"location":"methodology/event-storming/#step-3-find-the-boundaries","text":"The next step of this part of the process is to find the boundaries of your system by looking at the events. Two types of boundaries can emerge; the first type of boundary is a time boundary. Often specific key \"pivotal events\" indicate a change from one aspect of a system to another. This can happen at a hand-off from one persona to another, but it can also happen at a change of geographical, legal, or other type of boundary. If you notice that the terms that are used on the event stickies change at these boundaries, you are seeing a \"bounded context\" in Domain Driven Design terms. Highlight pivotal events by putting up blue painter\u2019s tape vertically behind the event. The second type of boundary is a subject boundary. You can detect a subject boundary by looking for the following conditions: You have multiple simultaneous series of events that only come together at a later time. You see the same terms being used in the event descriptions for a particular series of events. You can \u201cread\u201d a series of events from the point of view of a different persona when you are replaying them. You can delineate these different sets of simultaneous event streams by applying blue painter\u2019s tape horizontally, dividing the board into different swim lanes. Below is an example of a set of ordered domain events with pivotal events and subject swim lanes indicated. This example comes from applying event storming to the domain of container shipping process and is discussed in more detail in the container shipment analysis example . When the reefer container is plugged to the Vessel, it starts to emit telemetries, we change context.","title":"Step 3: Find the Boundaries"},{"location":"methodology/event-storming/#step-4-locate-the-commands","text":"In this step you shift from analysis of the domain to the first stages of system design. Up until this point, you are simply trying to understand how the events in the domain relate to one another - this is why the participation of domain experts is so critical. However, to build a system that implements the business process that you are interested in, you have to move on to the question of how these events come into being. Commands are the most common mechanism by which events are created. The key to finding commands is to ask the question: \"Why did this event occur?\". In this step, the focus of the process moves to the sequence of actions that lead to events. Your goal is to find the causes for which the events record the effects. Expected event trigger types are: A human operator makes a decision and issues a command Some external system or sensor provides a stimulus An event results from some policy - typically automated processing of a precursor event The completion of some determined period of elapsed time. The triggering command is identified in a blue (sticky) note. Command may become a microservice operation exposed via API. The human persona issuing the command is identified and shown in a yellow note. Some events may be created by applying business policies. The diagram below illustrates the manufacturer actor using the place a shipment order command to create a shipment order placed event, as well as . It is possible to chain events and commands as presented in the \"one view\" figure above in the concepts section.","title":"Step 4: Locate the Commands"},{"location":"methodology/event-storming/#step-5-describe-the-data","text":"You can't truly define a command without understanding the data that is needed for the command to execute in order to produce the event. You can identify several types of data during this step. First, users (personas) need data from the user interface in order to make decisions before executing a command. That data forms part of the read model in a CQRS implementation. For each command and event pair, you add a data description of the expected attributes and data elements needed to take such a decision. Here is a simple example for a shipment order placed event created from a place a shipment order action . Another important part of the process that becomes more fully fleshed out at this step is the description of policies that can trigger the generation of an event from a previous event (or set of events). Assess if the data element is a main business entity, uniquely identified by a key, supported by multiple commands. It has a life span over the business process. This will lead to develop an entity life cycle analysis. This first level of data definition helps to assess the microservice scope and responsibility as you start to see commonalities emerge from the data used among several related events. Those concepts become more obvious in the next step.","title":"Step 5: Describe the Data"},{"location":"methodology/event-storming/#step-6-identify-the-aggregates","text":"In DDD, entities and value objects can exist independently, but often, the relations are such that an entity or a value object has no value without its context. Aggregates provide that context by being those \"roots\" that comprise one or more entities and value objects that are linked together through a lifecycle. The following diagram illustrates a detailed example of aggregates for shipment of a temperature sensitive product overseas. In event storming, we may not be able to get this level of detail during the first workshop, but aggregates emerge through the process by grouping events and commands that are related together. This grouping not only consists of related data (entities and value objects) but also related actions (commands) that are connected by the lifecycle of that aggregate. Aggregates ultimately suggest microservice boundaries. In the container shipment example, you can see that you can group several commands and event pairs (with their associated data) together that are related through the lifecycle of an order for shipping.","title":"Step 6: Identify the Aggregates"},{"location":"methodology/event-storming/#step-7-define-bounded-context","text":"In this step, you define terms and concepts with a clear meaning valid in a clear boundary and you define the context within which a model applies. (The term definition can change outside of the business unit for which an application is developed). The following items may be addressed: Which team owns the model? Which part of the model transit between team organization? What are the different code bases foreseen we need to implement? What are the different data schema ? (database or json or xml schemas) Here is an example of bounded context that will, most likely, lead to a microservice: Keep the model strictly consistent within these bounds.","title":"Step 7: Define Bounded Context"},{"location":"methodology/event-storming/#step-8-looking-forward-with-insight-storming","text":"In event atorming for Event Driven Architecture (EDA) solutions it is helpful to include an additional method step at this point identifying useful predictive analytics insights. Insights storming extends the basic methodology by looking forward and considering what if you could know in advance that an event is going to occur. How would this change your actions, and what would you do in advance of that event actually happening? You can think of insight storming as extending the analysis to Derived Events . Rather than being the factual recording of a past event, a derived event is a forward-looking or predictive event, that is, \"this event is probably going to happen at some time in the next n hours\u201d. By using this forward-looking insight combined with the known business data from earlier events, human actors and event triggering policies can make better decisions about how to react to new events as they occur. Insight storming amounts to asking workshop participants the question: \"What data would be helpful at each event trigger to assist the human user or automated event triggering policy make the best possible decision of how and when to act?\" An important motivation that drives the use of an event-driven architecture is that it simplifies design and realization of highly responsive systems that react immediately and intelligently, that is, in a personalized and context-aware way, and optimally to new events as they occur. This immediately suggests that predictive analytics and models to generate predictive insights have an important role to play. Predictive analytic insights are effectively probabilistic statements about which future events are likely to occur and what are the likely properties of those events. These probabilistic statements are typicaly generated by using models created by data scientists or using AI or ML. Correlating or joining independently gathered sources of information can also generate important predictive insights or be input to predictive analytic models. Business owners and stakeholders in the event storming workshop can offer good intuitions in several areas: Which probabilistic insights are likely to lead to improved or optimal decision making and action? The action could take the form of an immediate response to an event when it occurs. The action could be proactive behavior to avoid an undesirable event. What combined sources of information are likely to help create a model to predict this insight? With basic event storming, you look backwards at each event because an event is something that has already happened. When you identify data needed for an actor or policy to decide when and how to issue a command, there is a tendency to restrict consideration to properties of earlier known and captured business events. In insight storming you extend the approach to explicitly look forward and consider what is the probability that a particular event will occur at some future time and what would be its expected property values? How would this change the best action to take when and if this event occurs? Is there action we can take now proactively in advance of an expected undesirable event to prevent it happening or mitigate the consequences? The insight method step amounts to getting workshop participants to identify derived events and the data sources needed for the models that generate them. Adding an insight storming step using the questions above into the workshop will improve decision making and proactive behavior in the resulting design. Insights can be published into a bus and subscribed to by any decision step guidance. By identifying derived events, you can integrate analytic models and machine learning into the designed solution. Event and derived event feeds can be processed, filtered, joined, aggregated, modeled and scored to create valuable predictive insights. Use the following new notations for the insight step: Pale blue stickies for derived events. Parallelogram shape to show when events and derived events are combined to enable deeper insight models and predictions. Identify predictive insights as early as possible in the development life cycle. The best opportunity to do this is to add this step to the event storming workshop. The two diagrams below show the results of the insight storming step for the use case of container shipment analysis. The first diagram captures insights and associated linkages for each refrigerated container, identifying when automated changes to the thermostat settings can be made, when unit maintenance should be scheduled and when the container contents must be considered spoiled. The second diagram captures insights that could trigger recommendations to adjust ship course or speed in response to expected severe weather forcasts for the route ahead or predicted congestion and expected docking and unloading delays at the next port of call.","title":"Step 8: Looking forward with insight storming"},{"location":"methodology/event-storming/#design-iteration","text":"Attention we are not proposing to apply a waterfall approach, but before starting the deeper implementation with iterations, we want to spend sometime to define in more details what we want to build, how to organize the CI/CD projects and pipeline, select the development, test and product plaform, and define epics, user stories, components, microservices... This iteration can take from few hours to a week, depending on the expected MVP goals. For an event-driven solution a MVP for a single application should not take more than 3 to 4 iterations.","title":"Design iteration"},{"location":"methodology/event-storming/#event-storming-to-user-stories-and-epics","text":"In agile methodology, creating user stories or epics is one of the most important elements in project management. The commands and policies related to events can be easily described as user stories, because commands and decisions are done by actors. The actor could be a system as well. For the data you must model the \"Create, Read, Update, Delete\" operations as user stories, mostly supported by a system actor. An event is the result or outcome of a user story. Events can be added as part of the acceptance criteria of the user stories to verify that the event really occurs.","title":"Event storming to user stories and epics"},{"location":"methodology/event-storming/#applying-to-the-container-shipment-use-case","text":"The K Container Shipment use case demonstrates an implementation solution to validate the event-driven architecture. The container shipment analysis example , shows event storming and design thinking main artifacts, including artifacts for the monitoring of refrigerated containers.","title":"Applying to the container shipment use case"},{"location":"methodology/event-storming/#some-practice-notes","text":"you can apply event storming at different level: for example at the beginning of a project to understand the high level process at stake. with a big group of people, you will stay at the high level. But it can be used to model a lower level microservice, to assess event consumed and produced.","title":"Some practice notes"},{"location":"methodology/event-storming/#further-readings","text":"Introduction to event storming from Alberto Brandolini Event Storming Guide Wikipedia Domain Driven Design Eric Evans: \"Domain Driven Design - Tacking complexity in the heart of software\" Domain drive design with event storming introduction video Patterns related to Domain Driven Design by Martin Fowler Applying DDD and event storming for event-driven microservice implementation from our own work","title":"Further Readings"},{"location":"methodology/governance/","text":"Digital businesses are real-time, responsive, scalable and flexible while focussing on delivering outstanding customer experience. API and Microservices focus has achieved a lot in enabling this transformation, supporting real-time interactions and increasing levels of agility through decoupling applications. But digital business requires more, it needs to become more timesensitive, contextual and event-driven in nature. Events is how the modern digital business operates, and eventdriven architecture allows IT to align to this. Governance involves a lot of different view points, so in this section we will cover what we think are important to govern the development, deployment and maintenance of event-driven solution. The first important question to address is do I need to use event-driven solution? This is what the fit for purpose is about. Most EDA adoption starts from the adoption of some new programming model based on the reactive manifesto, or by selecting a modern middleware to support loosely coupled microservice, like Kafka, all this in the context of one business application. From the first successful project, it is important to consider adopting a more strategic approach: to select technologies to apply common architecture and design patterns to adopt data models governance and control data lineage to apply common methodology to quickly onboard new business initiative, and be able to deploy new capability on top of the EDA in question of days or weeks. Fit for Purpose \u00b6 In EDA context the fit for purpose needs to help responding to the high level question on when to use event-driven solution for a new application, but also to address when to use modern pub/sub event backbone versus queuing products, what data stream processing to use, what are the different use cases and benefits. We have developed the content in a separate note as the subject can become long to describe. Architecture patterns \u00b6 We have already describe in this section as set of event-driven architecture patterns that can be leveraged while establishing EDA practices which includes how to integrate with data in topic for doing feature engineering. An example of Jupiter notebook can also be reused for this as described in this data collection article . Legacy integration and coexistence between legacy applications or mainframe transactional application and microservices is presented in this section . The modern data pipeline is also an important architecture pattern where data ingestion layer is the same as the microservice integration middleware and provide buffering capability as well as stateful data stream processing. From an implementation point of view the following design patterns are often part of the EDA adoption: Event sourcing Command Query Responsibility Segregation Saga pattern Transactional outbox Event reprocessing with dead letter You can also review the other microservice patterns in this note . Getting developers on board \u00b6 Data lineage \u00b6 Data governance is a well established practices in most companies. Solutions need to address the following needs: Which sources/producers are feeding data? What is the data schema? How to classify data Which process reads the data and how is it transformed? When was the data last updated? Which apps/ users access private data? In the context of event-driven architecture, one focus will be to ensure re-use of event topics, control the schema definition, have a consistent and governed ways to manage topic as service, domain and event data model definition, access control, encryption and traceability. As this subject is very important, we have started to address it in a separate 'data lineage' note . Deployment Strategy \u00b6 Hybrid cloud, Containerized security and access control Strimzi - cruise control active - active topic as self service Ongoing Maintenance \u00b6 Procedures performed regularly to keep a system healthy Cluster rebalancing Add new broker - partition reassignment Product migration Operational Monitoring \u00b6 assess partition in heavy load assess broker in heavy load assess partition leadership assignment Problem identification System health confirmation Error Handling \u00b6 Procedures, tools and tactics to resolve problems when they arise","title":"Governance"},{"location":"methodology/governance/#fit-for-purpose","text":"In EDA context the fit for purpose needs to help responding to the high level question on when to use event-driven solution for a new application, but also to address when to use modern pub/sub event backbone versus queuing products, what data stream processing to use, what are the different use cases and benefits. We have developed the content in a separate note as the subject can become long to describe.","title":"Fit for Purpose"},{"location":"methodology/governance/#architecture-patterns","text":"We have already describe in this section as set of event-driven architecture patterns that can be leveraged while establishing EDA practices which includes how to integrate with data in topic for doing feature engineering. An example of Jupiter notebook can also be reused for this as described in this data collection article . Legacy integration and coexistence between legacy applications or mainframe transactional application and microservices is presented in this section . The modern data pipeline is also an important architecture pattern where data ingestion layer is the same as the microservice integration middleware and provide buffering capability as well as stateful data stream processing. From an implementation point of view the following design patterns are often part of the EDA adoption: Event sourcing Command Query Responsibility Segregation Saga pattern Transactional outbox Event reprocessing with dead letter You can also review the other microservice patterns in this note .","title":"Architecture patterns"},{"location":"methodology/governance/#getting-developers-on-board","text":"","title":"Getting developers on board"},{"location":"methodology/governance/#data-lineage","text":"Data governance is a well established practices in most companies. Solutions need to address the following needs: Which sources/producers are feeding data? What is the data schema? How to classify data Which process reads the data and how is it transformed? When was the data last updated? Which apps/ users access private data? In the context of event-driven architecture, one focus will be to ensure re-use of event topics, control the schema definition, have a consistent and governed ways to manage topic as service, domain and event data model definition, access control, encryption and traceability. As this subject is very important, we have started to address it in a separate 'data lineage' note .","title":"Data lineage"},{"location":"methodology/governance/#deployment-strategy","text":"Hybrid cloud, Containerized security and access control Strimzi - cruise control active - active topic as self service","title":"Deployment Strategy"},{"location":"methodology/governance/#ongoing-maintenance","text":"Procedures performed regularly to keep a system healthy Cluster rebalancing Add new broker - partition reassignment Product migration","title":"Ongoing Maintenance"},{"location":"methodology/governance/#operational-monitoring","text":"assess partition in heavy load assess broker in heavy load assess partition leadership assignment Problem identification System health confirmation","title":"Operational Monitoring"},{"location":"methodology/governance/#error-handling","text":"Procedures, tools and tactics to resolve problems when they arise","title":"Error Handling"},{"location":"news/","text":"09/06/2022 \u00b6 update to mkdocs, update to journeys, reference architecture. 06/01/2022 \u00b6 Added Tech Academy site for enablement: EDA Tech Academy 04/04/2022 \u00b6 Update to fit for purpose, adding business and technical use cases. 01/21/2022 \u00b6 Add Flow Architecture summary Update to reactive messaging section Update to Avro and schema registry technical note . 12/01/2021 \u00b6 Update to the reference architecture diagram to add App Connect Enterprise as a source to messaging services Update to Flink note on exactly once delivery 11/04/2021 \u00b6 Add kubernetes deployment diagram for messaging as a service Update MQ sink connector tutorial for lab 1 and lab 2. Update to Kafka Connect framework content 10/20/2021 \u00b6 Integrate SOA and EDA co-existence section 10/12/2021 \u00b6 Learning Journey 101 content to get you started on EDA. Step by step tutorial to deploy Event Streams 2021.0.3 release on OpenShift Generic business use cases for EDA adoption 10/02/2021 \u00b6 Update to the reference architecture diagram to illustrate messaging as a service as the backbone for EDA. How Automation fits on top of EDA? some explanations are here . Split producer into basic and advanced concepts Split consumer content into basics and advanced concepts 09/20/2021 \u00b6 Apache Flink introduction and need to know article 07/30/2021 \u00b6 Devising data models : when you need to think about data views and data representation depending of the needs and semantic.","title":"What's new"},{"location":"news/#09062022","text":"update to mkdocs, update to journeys, reference architecture.","title":"09/06/2022"},{"location":"news/#06012022","text":"Added Tech Academy site for enablement: EDA Tech Academy","title":"06/01/2022"},{"location":"news/#04042022","text":"Update to fit for purpose, adding business and technical use cases.","title":"04/04/2022"},{"location":"news/#01212022","text":"Add Flow Architecture summary Update to reactive messaging section Update to Avro and schema registry technical note .","title":"01/21/2022"},{"location":"news/#12012021","text":"Update to the reference architecture diagram to add App Connect Enterprise as a source to messaging services Update to Flink note on exactly once delivery","title":"12/01/2021"},{"location":"news/#11042021","text":"Add kubernetes deployment diagram for messaging as a service Update MQ sink connector tutorial for lab 1 and lab 2. Update to Kafka Connect framework content","title":"11/04/2021"},{"location":"news/#10202021","text":"Integrate SOA and EDA co-existence section","title":"10/20/2021"},{"location":"news/#10122021","text":"Learning Journey 101 content to get you started on EDA. Step by step tutorial to deploy Event Streams 2021.0.3 release on OpenShift Generic business use cases for EDA adoption","title":"10/12/2021"},{"location":"news/#10022021","text":"Update to the reference architecture diagram to illustrate messaging as a service as the backbone for EDA. How Automation fits on top of EDA? some explanations are here . Split producer into basic and advanced concepts Split consumer content into basics and advanced concepts","title":"10/02/2021"},{"location":"news/#09202021","text":"Apache Flink introduction and need to know article","title":"09/20/2021"},{"location":"news/#07302021","text":"Devising data models : when you need to think about data views and data representation depending of the needs and semantic.","title":"07/30/2021"},{"location":"patterns/api-mgt/","text":"This page summarizes some of the best practices for introducing API Management in development practices and architecture patterns within an enterprise setting. Moving from a Pure API Gateway to an API Management system \u00b6 An API Gateway helps provide security, control, integration, and optimized access to a full range of mobile, web, application programming interface (API), service-oriented architecture (SOA), B2B and cloud workloads. A gateway is used in the following patterns: As a Security Gateway , placed between the consumer facing firewall and the system of records facing firewall (DMZ). It is used for both policy enforcement and consistent security policies across business channels. As an API Gateway , both as an internal and external gateway, with centralized service governance and policy enforcement, and with traffic monitoring. To provide connectivity (HTTP) and mediation (XML, JSON) services in the internal network, close to the system of record. API Management gives enterprises greater flexibility when reusing the functionality of API integrations and helps save time and money without trading off security. An API Management system supports a broader scope of features for API lifecycle management, including: API lifecycle management to activate, retire, or stage an API product. API governance with security, access, and versioning. Analytics, dashboards, and third party data offload for usage analysis. API socialization based on a portal for the developer community that allows self-service and discovery. An API developer toolkit to facilitate the creation and testing of APIs. Classical Pain Points \u00b6 Some of the familiar pain points that indicate the need for a broader API Management product include: Current API details like endpoints, request/response message format, error conditions, test messages, and SLAs are not easily available or not well documented. Difficult to tell which subscribers are really using the API and how often, without building a custom solution. Difficult to differentiate between business-critical subscribers versus low value subscribers. Managing different lines of business and organizations is complex. No dynamic scaling built into the solution, which often means making hardware investments for maximum load or worst availability scenarios. Difficult to evolve the APIs, such as moving from SOAP based web services to RESTful services to GraphQL No support for AsyncAPI to automate and formalize the documentation or code generation of any event-driven APIs. The need to ensure consistent security rules Integrating CI/CD pipelines with the API lifecycle Enterprise APIs across boundaries \u00b6 If you consider a typical API Management product, it includes a set of components as presented in the figure below that could be deployed on an on-premise Kubernetes-based platform or in several Cloud provider regions. APIs served by different applications or microservices can be deployed in multiple regions but still be managed by one central API Management server. Here is how it works: An API developer signs on to the API Management cloud services account and accesses the API developer User interface or CLI toolkit. The developer creates the sync API and implements business logic. He maps and integrates the API data model to the back-end schema through the transformation and connectivity service. He tests and deploys the API to the runtime and publishes to the API Management system. He can also create Async APIs from a messaging system by binding channels to topics or queues and define the message payload. The API Product Manager or API owner signs on to the API Management cloud services account and accesses the API Management component. She includes the sync API endpoint to existing API products, and plans and specifies access controls. She publishes the API to the developer portal for external discovery by application developers. An application developer accesses the developer portal and uses search to discover any available APIs. The application developer uses the API in an application and deploys that application to the device The device user opens the application that issues the API request. The request is handled by the API gateway, which performs load balancing and security validation for all API requests. The API gateway also validates access policies with API Management and invokes the API. The API polyglot runtime executes the API and obtains the data payload from the backend. The API response is sent back to the API gateway. Alternatively, APIs exposed by enterprise applications can be executed on that enterprise application runtime. The API gateway forwards the response to the calling application. The API gateway reports usage metrics and analytics to the API Management system. API developers and API owners can log on to the API analytics visualization component to view dashboards on API usage metrics and other analytics. Deployment across cloud providers could look like the diagram below, using API Connect in Cloud Pak for Integration: On the left side (green boxes), the consumers of the API register to a Developer portal to get the metadata about the API they want to consume. The Developer portal also allows them to test the APIs to better understand how they work. The consumers register their applications as API subscribers. These applications can run on the cloud or on-premise. The API Gateway services colocated with the target application services to reduce latency. These would be deployed as StatefulSet on an OpenShift cluster, which means as a set of pods with consistent identities . Identities are defined as: Network : A single stable DNS and hostname. Storage : As many VolumeClaims as requested. The StatefulSet guarantees that a given network identity will always map to the same storage identity. An API Gateway acts as a reverse proxy, and, in this case, exposes the Inventory APIs , enforcing user authentication and security policies, and handling traffic monitoring, rate limiting, and statistics. The API Gateway can also perform transformations and aggregate various services to fulfill a request. The Developer Portals can be separated, or centralized depending on API characteristics exposed from different clouds (e.g. different Developer Portals for internal and external APIs). In the example above, the portal is deployed onto the Cloud Provider as a container inside an OpenShift cluster. The Analytic service is also a StatefulSet and gets metrics from the gateway. The application microservices are accessing remote services and this traffic can also go to the API gateway. Those services will also integrate with existing backend services running on-premise, whether they are deployed or not on OpenShift. In the diagram above, the management service for the API Management product is depicted as running on-premise to illustrate that it is a central deployment to manage multiple gateways. Gateway services, Developer Portal services, and Analytics services are scoped to a single region, unlike the Management System, which can communicate across availability zones. The different API Management services run on OpenShift which can help ensure high availability of each of the components. Open API \u00b6 Within the API Management system, the OpenAPI document can be created top-down using a Swagger-based UI or bottom up using Annotation in the Java JAXRS resource classes. Either way, the API can be uploaded to the API Management product. The important parts are to define the operations exposed and the request / response structure of the data model. Support for Async API \u00b6 Cloud Pak for Integration (CP4I) 2021.1, which includes APIConnect V10, also provides some support for the AsyncAPI specification. AsyncAPI is an open source initiative that focuses on making Event-Driven Architectures (EDAs) as easy to work with as REST APIs. The AsyncAPI specification (currently at 2.0.0) establishes standards for events and EDAs, covering everything \"from documentation to code generation, and from discovery to event management\" asyncapi.com/docs . The goal is to enable the creation of better tooling in the message-driven space, better governance of asynchronous APIs, and standardization in documentation for asynchronous APIs. In short, Async API is to Message-driven architectures what OpenAPI is to REST APIs. While OpenAPI is the recommended practice for RESTful APIs, adopting AsyncAPI is the recommended practice for event-driven APIs. AsyncAPI Documents \u00b6 An AsyncAPI document is a file in either YAML or JSON format that defines and annotates the different components of an event-driven API. For example, AsyncAPI can formally describe how to connect to a Kafka cluster, the details of the Kafka topics (channels in AsyncAPI), and the type of data in messages. AsyncAPI includes both formal schema definitions and space for free-text descriptions See this blog . Here is what it looks like: You may have noticed the similarities with OpenAPI. AsyncAPI was initially an adaptation of OpenAPI, which does not include support for the Message Queuing Telemetry Transport (MQTT) and the Advanced Message Queuing Protocol (AMQP). The creator of the AsyncAPI specification, Fran M\u00e9ndez, describes what he did at first with just OpenAPI to make up for the lack of MQTT and AMQP support: \"paths were AMQP topics, GET was SUBSCRIBE, and POST was PUBLISH--and ugly hack at best...\". This forced him to write additonal code to support the necessary EDA-based documentation and code generation. Many companies use OpenAPI, but in real-world situations, systems need formalized documentation and code generation support for both REST APIs and events. Here are the structural differences (and similarities) between OpenAPI and AsyncAPI: Source: https://www.asyncapi.com/docs/getting-started/coming-from-openapi Note a few things: AsyncAPI is compatible with OpenAPI schemas, which is quite useful since the information flowing in the events is very similar to the one the REST APIs have to handle in requests and responses. The message payload in AsyncAPI does not have to be an AsyncAPI/OpenAPI schema; it can be any value such as an Apache Avro schema, which is considered to be one of the better choices for stream data, where data is modeled as streams (see the section titled Why Use Avro for Kafka below). The AsyncAPI server object is almost identical to its OpenAPI counterpart with the exception that scheme has been renamed to protocol and AsyncAPI introduces a new property called protocolVersion . AsyncAPI channel parameters are the equivalent of OpenAPI path parameters, except that AsyncAPI does not have the notion of query and cookie , and header parameters can be defined in the message object. Describing Kafka with AsyncAPI \u00b6 It is also important to understand how to use AsyncAPI from the perspective of a Kafka user. The following section summarizes what is described in more detail in this article written by Dale Lane. First, there are some minor differences in terminology between Kafka and AsyncAPI that you should note: Comparing Kafka and AsyncAPI Terminology \u00b6 The AsyncAPI Document \u00b6 Considering the structure in the diagram above, let's look at some of the parts of an AsyncAPI document: Info \u00b6 The Info section has three parts which represent the minimum required information about the application: title , version , and description (optional), used as follows: asyncapi : 2.0.0 ... info : title : Account Service version : 1.0.0 description : This service is in charge of processing user signups In the description field you can use markdown language for rich-text formatting. Id \u00b6 The Id is the application identifier. It can be a URN (recommended) or URL. The important thing is that it must be a unique ID for your document. Here is an example: asyncapi : '2.0.0' ... id : 'urn:uk:co:usera:apimgmtdemo:inventoryevents' ... Servers \u00b6 The servers section allows you to add and define which servers client applications can connect to, for sending and receiving messages (for example, this could be a list of server objects, each uniquely identifying a Kafka broker). Here is an example, where you have three Kafka brokers in the same cluster: servers : broker1 : url : andy-broker-0:9092 protocol : kafka protocolVersion : '1.0.0' description : This is the first broker broker2 : url : andy-broker-1:9092 protocol : kafka protocolVersion : '1.0.0' description : This is the second broker broker3 : url : andy-broker-2:9092 protocol : kafka protocolVersion : '1.0.0' description : This is the third broker The example above uses the kafka protocol for the different brokers, which is a popular protocol for streaming solutions, but the protocol can be any. For example, the most common protocols include: mqtt which is widely adopted by the Internet of Things and mobile apps, amqp , which is popular for its reliable queueing, ws for WebSockets, frequently used in browsers, and http , which is used in HTTP streaming APIs. Difference Between the AMQP and MQTT Protocols \u00b6 AMQP was mainly popularized by RabbitMQ. It provides reliable queuing, topic-based publish-and-subscribe messaging, flexible routing, transactions, and security. The main reasons to use AMQP are reliability and interoperability. AMQP exchanges route messages directly in fanout form, by topic, and also based on headers. MQTT The design principles and aims of MQTT are much more simple and focused than those of AMQP. it provides publish-and-subscribe messaging (no queues, in spite of the name) and was specifically designed for resource-constrained devices and low bandwidth, high latency networks such as dial up lines and satellite links. Basically, it can be used effectively in embedded systems. When using a broker-centric architecture such as Kafka or MQ, you normally specify the URL of the broker. For more classic client-server models, such as REST APIs, your server should be the URL of the server. One limitation in AsyncAPI documents is that you cannot include multiple Kafka clusters (such as a production cluster and clusters for dev, test, and staging environments) in a single document. One workaround is to list all brokers for all clusters in the same document, and then rely on the description or extension fields to explain which ones are in which cluster. This is, however, not recommended because it could interfere with code generators or other parts of the AsyncAPI ecosystem which may consider them as all being members of one large cluster. So, as a best practice, avoid this workaround and stick to one cluster per AsyncAPI document. NOTE : As with OpenAPI, you can add additional attributes to the spec using the x- prefix, which identifies an entry as your own extension to the AsyncAPI specs. Security \u00b6 If the Kafka cluster doesn\u2019t have auth enabled, the protocol used should be kafka . Otherwise, if client applications are required to provide credentials, the protocol should be kafka-secure . To identify the type of credentials, add a security section to the server object. The value you put there is the name of a securityScheme object you define in the components section. The types of security schemes that you can specify aren\u2019t Kafka-specific. Choose the value that describes your type of approach to security. For example, if you\u2019re using SASL/SCRAM, which is a username/password-based approach to auth, you could describe this as userPassword . Here is an example: asyncapi : 2.0.0 ... servers : broker1 : url : localhost:9092 description : Production server protocol : kafka-secure protocolVersion : '1.0.0' security : - saslScramCreds : [] ... components : securitySchemes : saslScramCreds : type : userPassword description : Info about how/where to get credentials x-mykafka-sasl-mechanism : 'SCRAM-SHA-256' The description field allows you to explain the security options that Kafka clients need to use. You could also use an extension (with the -x prefix). Channels \u00b6 All brokers support communication through multiple channels (known as topics , event types , routing keys , event names or other terms depending on the system). Channels are assigned a name or identifier. The channels section of the specification stores all of the mediums where messages flow through. Here is a simple example: channels : hello : publish : message : payload : type : string pattern : '^hello .+$' In this example, you only have one channel called hello. An app would subscribe to this channel to receive hello {name} messages. Notice that the payload object defines how the message must be structured. In this example, the message must be of type string and match the regular expression '^hello .+$' in the format hello {name} string. Each topic (or channel) identifies the operations that you want to describe in the spec. Here is another example: asyncapi : '2.0.0' ... channels : my.topic.name : description : This is my Kafka topic subscribe : operationId : someUniqueId summary : Interesting messages description : You can get really interesting messages from this topic tags : - name : awesome - name : interesting ... For each operation, you can provide a unique id, a short one-line text summary, and a more detailed description in plain text or markdown formatting. Bindings \u00b6 AsyncAPI puts protocol-specific values in sections called bindings . The bindings sections allows you to specify the values that Kafka clients should use to perform the operation. The values you can describe here include the consumer group id and the client id. If there are expectations about the format of these values, then you can describe those by using regular expressions: asyncapi : '2.0.0' ... channels : my.topic.name : description : This is my Kafka topic subscribe : ... bindings : kafka : groupId : type : string pattern : '^[A-Z]{10}[0-5]$' clientId : type : string pattern : '^[a-z]{22}$' bindingVersion : '0.1.0' You can instead specify a discrete set of values, in the form of enumerations: asyncapi : '2.0.0' ... channels : my.topic.name : description : This is my Kafka topic subscribe : ... bindings : kafka : groupId : type : string enum : - validone - validtwo clientId : type : string enum : - validoption bindingVersion : '0.1.0' Messages \u00b6 A message is how information is exchanged via a channel between servers and applications. According to the AsyncAPI specifications, a message must contain a payload and may also contain headers. The headers may be subdivided into protocol-defined headers and header properties defined by the application which can act as supporting metadata. The payload contains the data, defined by the application, which must be serialized into a supported format (JSON, XML, Avro, binary, etc.). Because a message is a generic mechanism, it can support multiple interaction patterns such as event, command, request, or response. As with all the other levels of the spec, you can provide background and narrative in a description field for the message: asyncapi : '2.0.0' ... channels : my.topic.name : ... subscribe : ... message : description : Description of a single message Summary \u00b6 In short, the following diagram summarizes the sections described above: For more information, the official AsyncAPI specifications can be found here . Why Use Avro for Kafka? \u00b6 Apache Avro is \"an open source data serialization system that helps with data exchange between systems, programming languages, and processing frameworks\" (https://www.confluent.io/blog/avro-kafka-data/). Avro is a great fit for stream data because it has the following features: Direct mapping to and from JSON, but typically much faster than JSON, with much smaller encodings Compact format, making it more efficient for high-volume usage Bindings for a wide variety of programming languages A rich, extensible schema language defined in pure JSON A simple API Management Demo \u00b6 Besides a new, event-driven approach to its API model, ACME Inc needs a way to securely provide self-service access to different versions of its APIs, to enable their developers to discover and easily use these APIs, and to be able to redirect API calls based on several criteria. IBM API Connect (APIC) is a complete and scalable API Management platform that allows them to do these things, in addition to exposing, managing, and monetizing APIs across clouds. API Connect is also available with other capabilities as an IBM Cloud Pak\u00ae solution. The API Management demo demonstrates three main areas of interest: Version Control API Documentation & Discovery Redirecting to different APIs based on certain criteria Types of APIs \u00b6 An API is a set of functions that provide some business or technical capability and can be called by applications by using a defined protocol. Applications are typically mobile or web applications, and they use the HTTP protocol. An API definition is composed of paths, and can be one of the following types: REST API \u00b6 A REST API is a defined set of interactions that uses the HTTP protocol, typically by using JSON or XML as the data format that is exchanged. For example, a data request might use an HTTP GET method, and a data record might use an HTTP POST method. The choice of data format depends on the type of application that is calling the API. JSON is commonly used for web pages or mobile applications that present a user interface (by using JavaScript or HTML), whereas XML has been traditionally used for machine-to-machine scenarios, although that is changing. In IBM API Connect, you can create REST APIs by using user interface functions to create models and data sources (top-down). These are then used by your REST API definition and exposed to your users. API Connect also supports a bottom-up approach where you can import existing APIs into the system and benefit from the API Management capabilities of the product. Alternatively, you can expose and secure your existing APIs by using a Proxy or Invoke policy. In either case, you can configure your API definition either by using the API Manager, or by writing an OpenAPI definition file and publishing it using either API Manager or the command line interface. SOAP API \u00b6 API Connect also allows you to create SOAP API definitions that are based on an existing Web Services Description Language (WSDL) file. You can use this facility to benefit from the capabilities that are provided by API Connect, which include analytics and mapping between variables. You can also expose the API by using the Developer Portal for any existing SOAP services in your organization, including any SOAP services that are part of a service-oriented architecture (SOA) or Enterprise Service Bus (ESB) infrastructure. You can create SOAP API definitions through either the command line interface, or through the API Manager UI. GraphQL \u00b6 GraphQL is a query language for APIs that gives an application client greater control over what data it retrieves in an API request when compared with a REST API request. IBM\u00ae API Connect enables you to create a GraphQL API proxy definition that proxies a backend GraphQL server, and to define rate limiting controls that reflect the amount of data that is returned from the server by a request to the GraphQL API. The Developer Portal also supports testing GraphQL APIs. See this dedicated section . AsyncAPI \u00b6 We already address in detail in previous sections . API Manager \u00b6 You can manage your APIs by using API Connect's API Manager UI: The API Manager UI allows you to manage private internal APIs as well as public external APIs. API Manager is an on-premises offering that provides the capabilities required to externalize and manage your services as REST or SOAP APIs. Developer Portal \u00b6 The Developer Portal is a convenient place to share APIs with application developers. After a Developer Portal has been enabled through the API Manager, and one or more API Products have been published, application developers can browse and use the APIs from the Developer Portal dashboard, as shown below: The Developer Portal can be used as is when it is first enabled, or it can be customized to fit the corporate branding and design requirements of a particular organization. You can configure the Developer Portal for test and development purposes, or for internal use only. Create Capability \u00b6 Developer can leverage API Connect's Create capability, to build APIs (top-down) with a built-in Swagger (OpenAPI) editor or using a simple guided model-driven approach. APIC allows developers to create a new API from scratch or an API based on the schema of an existing data source, such as a database. Creating a REST proxy API from an existing target service \u00b6 As mentioned earlier, it is also possible to create a REST proxy API from an existing target service (bottom-up) that you import into the API Connect system. Essentially, if you have an existing REST service that you want to expose through an IBM API Connect API definition, you can create a proxy API and specify the target endpoint by using the API Manager. Explore Capability \u00b6 Developers can use API Connect's Explore capability to quickly examine their new APIs and try their operations. Developers can add their APIs to the API Connect server and have the choice of running them on the cloud or on-premise, as mentioned above. This can be done through API Connect's UI or with the provided CLI. Developers and Administrators can then connect to the API Connect Server and see their running APIs, including those that were created as well as those that were added based on existing data sources. Subscribing to an API in the Developer Portal \u00b6 To subscribe to an API, from the Developer Portal, the develper clicks API Products to find and subscribe to any available APIs: In the example above, an API called FindBranch Version 2.0, which is contained in the FindBranch Auto Products product is available. Clicking Subscribe enables the developer to use the API: Under the Application heading, the developer can click Select App for the new application: From the next screen below, the developer can click Next: Doing this shows the screen below, which confirms to the developer that his or her applicatin is now subscribed to the selected API under the selected plan. Pressing Done in the next screen completes the subscription. Testing An API in the Developer Portal \u00b6 To test an API, the developer clicks API Products in the Developer Portal dashboard to show all available products: Clicking a product, such as FindBranch auto product, for example, and then clicking the FindBranch API from the provided list shows the available API operations. The developer can click GET/details , for instance, to see the details of the GET operation: Clicking the Try it tab and pressing Send allows the developer to test the API to better understand how it works: As you can see above, this shows the request/response from invoking the API. API Connect shows a returned response of 200 OK and the message body, indicating that the REST API operation call was successful. API Product Managers \u00b6 Administrators or API Product Managers can make changes such as adding corporate security polices or transformations before publishing the APIs. They can also control the visibility and define the rate limit for the APIs. Once the APIs are ready, API Product Managers can expose them for developer consumption and self-service in the Developer Portal. Developers who have signed up can discover and use any APIs which were exposed. Because the APIs were documented while they were being created using OpenAPI notation, developers can not only view the APIs but also try them out, as demonstrated in the demo. API Connect provides source code examples in several different languages that show how to use the API operations. Languages supported include Curl, Ruby, Python, Java, Go, and Node.js. Testing \u00b6 Besides the small individual tests that you can run from the Developer Portal to understand how an API works, as explained above, it is possible to use API Connect Test and Monitor to effortlessly generate and schedule API test assertions. This browser based tool also provides dashboards for monitoring API test assertions and for receiving alerts on the health of your APIs. You can use an HTTP Client within API Connect Test and Monitor to generate a request to an API and get the expected results: You enter any required parameters and authorization token and press Send to send the request. API Connect Test and Monitor then shows you the response payload as a formatted JSON object: To generate a test, you then click Generate Test , and in the dialog that appears name the test and add it to a project. When you click the checkmark in the upper right corner of the dialog, API Connect Test and Monitor automatically generates test assertions based on the response payload. You can add, delete, or modify these assertions, or even add new ones from a broad selection: You can also reorder the assertions by dragging and dropping them: Although coding is not required, if you wish, you change the underlying code by clicking CODE to enter the code view: Once you have your test arranged the way you want it, you can run it to generate a test report, which highlights successes and failures: Test Scheduling \u00b6 IBM API Connect Test and Monitor also lets you automate test scheduling at regular intervals. To do this, you first save any test you may have open and exit out of the composer. Then publish your test to prepare it for scheduling: Once your test is published, you can click Schedule to schedule regular test intervals. Test and Monitor allows you to tweak the test schedule as necessary. You can create a new run, name it, and choose the times you wish to run your tests: Clicking Save Run schedules the tests. Finally, you can access the dashboard to monitor the health of your API: GraphQL APIs \u00b6 IBM\u00ae API Connect enables you to create a GraphQL API proxy definition that proxies a backend GraphQL server, and to define rate limiting controls that reflect the amount of data that is returned from the server by a request to the GraphQL API. The Developer Portal also supports testing GraphQL APIs. Advantages of GraphQL over REST APIs \u00b6 GraphQL provides the following particular advantages over REST APIs: The application client can request only the data that it needs. For example, when retrieving bank account records, request only the account number and current balance for each account, but not the customer name and address details. With a REST API request, either the backend REST service must provide separate endpoints for different data subsets, or the application client must retrieve the complete records and then discard the unwanted data. The application client can retrieve multiple related resources in a single request. For example, a customer's bank account record might include an array that references other finance products that the customer holds. If an application client wants to retrieve the bank account details for a specific customer, and details of the other finance products for that customer, then with a REST API the client would first retrieve the bank account details, then make separate requests for each of the other products. A GraphQL API can be designed to allow the client to retrieve all this information in a single request. However, this flexibility presents rate limiting challenges, because two seemingly very similar requests might return vastly different amounts of data, and what might have required multiple REST API requests, each counting towards the rate limit, might require only a single GraphQL API request. It is important therefore that rate limiting controls are imposed that reflect the amount of data that is retrieved. API Connect extends the GraphQL standard by providing, in a GraphQL API definition, the ability to configure a range of settings that are used to calculate the complexity of a GraphQL request and an associated cost that counts towards the rate limit. Creating a GraphQL proxy API \u00b6 The demo also showed how API Connect supports GraphQL. To create a GraphQL proxy API from an existing GraphQL server, click Add , then API (from REST, GraphQL or SOAP) : Then select, From existing GraphQL service (GraphQL proxy) : Give it a name and complete the rest of the parameters, especially the GraphQL Server URL field which, of course, cannot be left blank. Notice that, once you enter the URL, API Connect will find the service and automatically detect the schema. You can click Next and API Connect will attempt to connect to the GraphQL server, introspect it, and pull in any objects (in this case, queries and operations) that it can find on that server: The gateway will sometimes provide warnings (17 in the example above), which are really just recommendations on how you can optimize the query. Clicking Next allows you to then activate the API for publishing: Finally, clicking Next shows a screen such as the one below with checkmarks: This shows that the gateway was able to connect to the GraphQL server and detect its schema, paths, and operations, validate them, and automatically populate the canvas with different policies, including security policies: Further Readings \u00b6 IBM Redbook on Agile Integration A Demo of Event Endpoint Management - Cloud Pak for Integration","title":"API management"},{"location":"patterns/api-mgt/#moving-from-a-pure-api-gateway-to-an-api-management-system","text":"An API Gateway helps provide security, control, integration, and optimized access to a full range of mobile, web, application programming interface (API), service-oriented architecture (SOA), B2B and cloud workloads. A gateway is used in the following patterns: As a Security Gateway , placed between the consumer facing firewall and the system of records facing firewall (DMZ). It is used for both policy enforcement and consistent security policies across business channels. As an API Gateway , both as an internal and external gateway, with centralized service governance and policy enforcement, and with traffic monitoring. To provide connectivity (HTTP) and mediation (XML, JSON) services in the internal network, close to the system of record. API Management gives enterprises greater flexibility when reusing the functionality of API integrations and helps save time and money without trading off security. An API Management system supports a broader scope of features for API lifecycle management, including: API lifecycle management to activate, retire, or stage an API product. API governance with security, access, and versioning. Analytics, dashboards, and third party data offload for usage analysis. API socialization based on a portal for the developer community that allows self-service and discovery. An API developer toolkit to facilitate the creation and testing of APIs.","title":"Moving from a Pure API Gateway to an API Management system"},{"location":"patterns/api-mgt/#classical-pain-points","text":"Some of the familiar pain points that indicate the need for a broader API Management product include: Current API details like endpoints, request/response message format, error conditions, test messages, and SLAs are not easily available or not well documented. Difficult to tell which subscribers are really using the API and how often, without building a custom solution. Difficult to differentiate between business-critical subscribers versus low value subscribers. Managing different lines of business and organizations is complex. No dynamic scaling built into the solution, which often means making hardware investments for maximum load or worst availability scenarios. Difficult to evolve the APIs, such as moving from SOAP based web services to RESTful services to GraphQL No support for AsyncAPI to automate and formalize the documentation or code generation of any event-driven APIs. The need to ensure consistent security rules Integrating CI/CD pipelines with the API lifecycle","title":"Classical Pain Points"},{"location":"patterns/api-mgt/#enterprise-apis-across-boundaries","text":"If you consider a typical API Management product, it includes a set of components as presented in the figure below that could be deployed on an on-premise Kubernetes-based platform or in several Cloud provider regions. APIs served by different applications or microservices can be deployed in multiple regions but still be managed by one central API Management server. Here is how it works: An API developer signs on to the API Management cloud services account and accesses the API developer User interface or CLI toolkit. The developer creates the sync API and implements business logic. He maps and integrates the API data model to the back-end schema through the transformation and connectivity service. He tests and deploys the API to the runtime and publishes to the API Management system. He can also create Async APIs from a messaging system by binding channels to topics or queues and define the message payload. The API Product Manager or API owner signs on to the API Management cloud services account and accesses the API Management component. She includes the sync API endpoint to existing API products, and plans and specifies access controls. She publishes the API to the developer portal for external discovery by application developers. An application developer accesses the developer portal and uses search to discover any available APIs. The application developer uses the API in an application and deploys that application to the device The device user opens the application that issues the API request. The request is handled by the API gateway, which performs load balancing and security validation for all API requests. The API gateway also validates access policies with API Management and invokes the API. The API polyglot runtime executes the API and obtains the data payload from the backend. The API response is sent back to the API gateway. Alternatively, APIs exposed by enterprise applications can be executed on that enterprise application runtime. The API gateway forwards the response to the calling application. The API gateway reports usage metrics and analytics to the API Management system. API developers and API owners can log on to the API analytics visualization component to view dashboards on API usage metrics and other analytics. Deployment across cloud providers could look like the diagram below, using API Connect in Cloud Pak for Integration: On the left side (green boxes), the consumers of the API register to a Developer portal to get the metadata about the API they want to consume. The Developer portal also allows them to test the APIs to better understand how they work. The consumers register their applications as API subscribers. These applications can run on the cloud or on-premise. The API Gateway services colocated with the target application services to reduce latency. These would be deployed as StatefulSet on an OpenShift cluster, which means as a set of pods with consistent identities . Identities are defined as: Network : A single stable DNS and hostname. Storage : As many VolumeClaims as requested. The StatefulSet guarantees that a given network identity will always map to the same storage identity. An API Gateway acts as a reverse proxy, and, in this case, exposes the Inventory APIs , enforcing user authentication and security policies, and handling traffic monitoring, rate limiting, and statistics. The API Gateway can also perform transformations and aggregate various services to fulfill a request. The Developer Portals can be separated, or centralized depending on API characteristics exposed from different clouds (e.g. different Developer Portals for internal and external APIs). In the example above, the portal is deployed onto the Cloud Provider as a container inside an OpenShift cluster. The Analytic service is also a StatefulSet and gets metrics from the gateway. The application microservices are accessing remote services and this traffic can also go to the API gateway. Those services will also integrate with existing backend services running on-premise, whether they are deployed or not on OpenShift. In the diagram above, the management service for the API Management product is depicted as running on-premise to illustrate that it is a central deployment to manage multiple gateways. Gateway services, Developer Portal services, and Analytics services are scoped to a single region, unlike the Management System, which can communicate across availability zones. The different API Management services run on OpenShift which can help ensure high availability of each of the components.","title":"Enterprise APIs across boundaries"},{"location":"patterns/api-mgt/#open-api","text":"Within the API Management system, the OpenAPI document can be created top-down using a Swagger-based UI or bottom up using Annotation in the Java JAXRS resource classes. Either way, the API can be uploaded to the API Management product. The important parts are to define the operations exposed and the request / response structure of the data model.","title":"Open API"},{"location":"patterns/api-mgt/#support-for-async-api","text":"Cloud Pak for Integration (CP4I) 2021.1, which includes APIConnect V10, also provides some support for the AsyncAPI specification. AsyncAPI is an open source initiative that focuses on making Event-Driven Architectures (EDAs) as easy to work with as REST APIs. The AsyncAPI specification (currently at 2.0.0) establishes standards for events and EDAs, covering everything \"from documentation to code generation, and from discovery to event management\" asyncapi.com/docs . The goal is to enable the creation of better tooling in the message-driven space, better governance of asynchronous APIs, and standardization in documentation for asynchronous APIs. In short, Async API is to Message-driven architectures what OpenAPI is to REST APIs. While OpenAPI is the recommended practice for RESTful APIs, adopting AsyncAPI is the recommended practice for event-driven APIs.","title":"Support for Async API"},{"location":"patterns/api-mgt/#asyncapi-documents","text":"An AsyncAPI document is a file in either YAML or JSON format that defines and annotates the different components of an event-driven API. For example, AsyncAPI can formally describe how to connect to a Kafka cluster, the details of the Kafka topics (channels in AsyncAPI), and the type of data in messages. AsyncAPI includes both formal schema definitions and space for free-text descriptions See this blog . Here is what it looks like: You may have noticed the similarities with OpenAPI. AsyncAPI was initially an adaptation of OpenAPI, which does not include support for the Message Queuing Telemetry Transport (MQTT) and the Advanced Message Queuing Protocol (AMQP). The creator of the AsyncAPI specification, Fran M\u00e9ndez, describes what he did at first with just OpenAPI to make up for the lack of MQTT and AMQP support: \"paths were AMQP topics, GET was SUBSCRIBE, and POST was PUBLISH--and ugly hack at best...\". This forced him to write additonal code to support the necessary EDA-based documentation and code generation. Many companies use OpenAPI, but in real-world situations, systems need formalized documentation and code generation support for both REST APIs and events. Here are the structural differences (and similarities) between OpenAPI and AsyncAPI: Source: https://www.asyncapi.com/docs/getting-started/coming-from-openapi Note a few things: AsyncAPI is compatible with OpenAPI schemas, which is quite useful since the information flowing in the events is very similar to the one the REST APIs have to handle in requests and responses. The message payload in AsyncAPI does not have to be an AsyncAPI/OpenAPI schema; it can be any value such as an Apache Avro schema, which is considered to be one of the better choices for stream data, where data is modeled as streams (see the section titled Why Use Avro for Kafka below). The AsyncAPI server object is almost identical to its OpenAPI counterpart with the exception that scheme has been renamed to protocol and AsyncAPI introduces a new property called protocolVersion . AsyncAPI channel parameters are the equivalent of OpenAPI path parameters, except that AsyncAPI does not have the notion of query and cookie , and header parameters can be defined in the message object.","title":"AsyncAPI Documents"},{"location":"patterns/api-mgt/#describing-kafka-with-asyncapi","text":"It is also important to understand how to use AsyncAPI from the perspective of a Kafka user. The following section summarizes what is described in more detail in this article written by Dale Lane. First, there are some minor differences in terminology between Kafka and AsyncAPI that you should note:","title":"Describing Kafka with AsyncAPI"},{"location":"patterns/api-mgt/#comparing-kafka-and-asyncapi-terminology","text":"","title":"Comparing Kafka and AsyncAPI Terminology"},{"location":"patterns/api-mgt/#the-asyncapi-document","text":"Considering the structure in the diagram above, let's look at some of the parts of an AsyncAPI document:","title":"The AsyncAPI Document"},{"location":"patterns/api-mgt/#info","text":"The Info section has three parts which represent the minimum required information about the application: title , version , and description (optional), used as follows: asyncapi : 2.0.0 ... info : title : Account Service version : 1.0.0 description : This service is in charge of processing user signups In the description field you can use markdown language for rich-text formatting.","title":"Info"},{"location":"patterns/api-mgt/#id","text":"The Id is the application identifier. It can be a URN (recommended) or URL. The important thing is that it must be a unique ID for your document. Here is an example: asyncapi : '2.0.0' ... id : 'urn:uk:co:usera:apimgmtdemo:inventoryevents' ...","title":"Id"},{"location":"patterns/api-mgt/#servers","text":"The servers section allows you to add and define which servers client applications can connect to, for sending and receiving messages (for example, this could be a list of server objects, each uniquely identifying a Kafka broker). Here is an example, where you have three Kafka brokers in the same cluster: servers : broker1 : url : andy-broker-0:9092 protocol : kafka protocolVersion : '1.0.0' description : This is the first broker broker2 : url : andy-broker-1:9092 protocol : kafka protocolVersion : '1.0.0' description : This is the second broker broker3 : url : andy-broker-2:9092 protocol : kafka protocolVersion : '1.0.0' description : This is the third broker The example above uses the kafka protocol for the different brokers, which is a popular protocol for streaming solutions, but the protocol can be any. For example, the most common protocols include: mqtt which is widely adopted by the Internet of Things and mobile apps, amqp , which is popular for its reliable queueing, ws for WebSockets, frequently used in browsers, and http , which is used in HTTP streaming APIs.","title":"Servers"},{"location":"patterns/api-mgt/#difference-between-the-amqp-and-mqtt-protocols","text":"AMQP was mainly popularized by RabbitMQ. It provides reliable queuing, topic-based publish-and-subscribe messaging, flexible routing, transactions, and security. The main reasons to use AMQP are reliability and interoperability. AMQP exchanges route messages directly in fanout form, by topic, and also based on headers. MQTT The design principles and aims of MQTT are much more simple and focused than those of AMQP. it provides publish-and-subscribe messaging (no queues, in spite of the name) and was specifically designed for resource-constrained devices and low bandwidth, high latency networks such as dial up lines and satellite links. Basically, it can be used effectively in embedded systems. When using a broker-centric architecture such as Kafka or MQ, you normally specify the URL of the broker. For more classic client-server models, such as REST APIs, your server should be the URL of the server. One limitation in AsyncAPI documents is that you cannot include multiple Kafka clusters (such as a production cluster and clusters for dev, test, and staging environments) in a single document. One workaround is to list all brokers for all clusters in the same document, and then rely on the description or extension fields to explain which ones are in which cluster. This is, however, not recommended because it could interfere with code generators or other parts of the AsyncAPI ecosystem which may consider them as all being members of one large cluster. So, as a best practice, avoid this workaround and stick to one cluster per AsyncAPI document. NOTE : As with OpenAPI, you can add additional attributes to the spec using the x- prefix, which identifies an entry as your own extension to the AsyncAPI specs.","title":"Difference Between the AMQP and MQTT Protocols"},{"location":"patterns/api-mgt/#security","text":"If the Kafka cluster doesn\u2019t have auth enabled, the protocol used should be kafka . Otherwise, if client applications are required to provide credentials, the protocol should be kafka-secure . To identify the type of credentials, add a security section to the server object. The value you put there is the name of a securityScheme object you define in the components section. The types of security schemes that you can specify aren\u2019t Kafka-specific. Choose the value that describes your type of approach to security. For example, if you\u2019re using SASL/SCRAM, which is a username/password-based approach to auth, you could describe this as userPassword . Here is an example: asyncapi : 2.0.0 ... servers : broker1 : url : localhost:9092 description : Production server protocol : kafka-secure protocolVersion : '1.0.0' security : - saslScramCreds : [] ... components : securitySchemes : saslScramCreds : type : userPassword description : Info about how/where to get credentials x-mykafka-sasl-mechanism : 'SCRAM-SHA-256' The description field allows you to explain the security options that Kafka clients need to use. You could also use an extension (with the -x prefix).","title":"Security"},{"location":"patterns/api-mgt/#channels","text":"All brokers support communication through multiple channels (known as topics , event types , routing keys , event names or other terms depending on the system). Channels are assigned a name or identifier. The channels section of the specification stores all of the mediums where messages flow through. Here is a simple example: channels : hello : publish : message : payload : type : string pattern : '^hello .+$' In this example, you only have one channel called hello. An app would subscribe to this channel to receive hello {name} messages. Notice that the payload object defines how the message must be structured. In this example, the message must be of type string and match the regular expression '^hello .+$' in the format hello {name} string. Each topic (or channel) identifies the operations that you want to describe in the spec. Here is another example: asyncapi : '2.0.0' ... channels : my.topic.name : description : This is my Kafka topic subscribe : operationId : someUniqueId summary : Interesting messages description : You can get really interesting messages from this topic tags : - name : awesome - name : interesting ... For each operation, you can provide a unique id, a short one-line text summary, and a more detailed description in plain text or markdown formatting.","title":"Channels"},{"location":"patterns/api-mgt/#bindings","text":"AsyncAPI puts protocol-specific values in sections called bindings . The bindings sections allows you to specify the values that Kafka clients should use to perform the operation. The values you can describe here include the consumer group id and the client id. If there are expectations about the format of these values, then you can describe those by using regular expressions: asyncapi : '2.0.0' ... channels : my.topic.name : description : This is my Kafka topic subscribe : ... bindings : kafka : groupId : type : string pattern : '^[A-Z]{10}[0-5]$' clientId : type : string pattern : '^[a-z]{22}$' bindingVersion : '0.1.0' You can instead specify a discrete set of values, in the form of enumerations: asyncapi : '2.0.0' ... channels : my.topic.name : description : This is my Kafka topic subscribe : ... bindings : kafka : groupId : type : string enum : - validone - validtwo clientId : type : string enum : - validoption bindingVersion : '0.1.0'","title":"Bindings"},{"location":"patterns/api-mgt/#messages","text":"A message is how information is exchanged via a channel between servers and applications. According to the AsyncAPI specifications, a message must contain a payload and may also contain headers. The headers may be subdivided into protocol-defined headers and header properties defined by the application which can act as supporting metadata. The payload contains the data, defined by the application, which must be serialized into a supported format (JSON, XML, Avro, binary, etc.). Because a message is a generic mechanism, it can support multiple interaction patterns such as event, command, request, or response. As with all the other levels of the spec, you can provide background and narrative in a description field for the message: asyncapi : '2.0.0' ... channels : my.topic.name : ... subscribe : ... message : description : Description of a single message","title":"Messages"},{"location":"patterns/api-mgt/#summary","text":"In short, the following diagram summarizes the sections described above: For more information, the official AsyncAPI specifications can be found here .","title":"Summary"},{"location":"patterns/api-mgt/#why-use-avro-for-kafka","text":"Apache Avro is \"an open source data serialization system that helps with data exchange between systems, programming languages, and processing frameworks\" (https://www.confluent.io/blog/avro-kafka-data/). Avro is a great fit for stream data because it has the following features: Direct mapping to and from JSON, but typically much faster than JSON, with much smaller encodings Compact format, making it more efficient for high-volume usage Bindings for a wide variety of programming languages A rich, extensible schema language defined in pure JSON","title":"Why Use Avro for Kafka?"},{"location":"patterns/api-mgt/#a-simple-api-management-demo","text":"Besides a new, event-driven approach to its API model, ACME Inc needs a way to securely provide self-service access to different versions of its APIs, to enable their developers to discover and easily use these APIs, and to be able to redirect API calls based on several criteria. IBM API Connect (APIC) is a complete and scalable API Management platform that allows them to do these things, in addition to exposing, managing, and monetizing APIs across clouds. API Connect is also available with other capabilities as an IBM Cloud Pak\u00ae solution. The API Management demo demonstrates three main areas of interest: Version Control API Documentation & Discovery Redirecting to different APIs based on certain criteria","title":"A simple API Management Demo"},{"location":"patterns/api-mgt/#types-of-apis","text":"An API is a set of functions that provide some business or technical capability and can be called by applications by using a defined protocol. Applications are typically mobile or web applications, and they use the HTTP protocol. An API definition is composed of paths, and can be one of the following types:","title":"Types of APIs"},{"location":"patterns/api-mgt/#rest-api","text":"A REST API is a defined set of interactions that uses the HTTP protocol, typically by using JSON or XML as the data format that is exchanged. For example, a data request might use an HTTP GET method, and a data record might use an HTTP POST method. The choice of data format depends on the type of application that is calling the API. JSON is commonly used for web pages or mobile applications that present a user interface (by using JavaScript or HTML), whereas XML has been traditionally used for machine-to-machine scenarios, although that is changing. In IBM API Connect, you can create REST APIs by using user interface functions to create models and data sources (top-down). These are then used by your REST API definition and exposed to your users. API Connect also supports a bottom-up approach where you can import existing APIs into the system and benefit from the API Management capabilities of the product. Alternatively, you can expose and secure your existing APIs by using a Proxy or Invoke policy. In either case, you can configure your API definition either by using the API Manager, or by writing an OpenAPI definition file and publishing it using either API Manager or the command line interface.","title":"REST API"},{"location":"patterns/api-mgt/#soap-api","text":"API Connect also allows you to create SOAP API definitions that are based on an existing Web Services Description Language (WSDL) file. You can use this facility to benefit from the capabilities that are provided by API Connect, which include analytics and mapping between variables. You can also expose the API by using the Developer Portal for any existing SOAP services in your organization, including any SOAP services that are part of a service-oriented architecture (SOA) or Enterprise Service Bus (ESB) infrastructure. You can create SOAP API definitions through either the command line interface, or through the API Manager UI.","title":"SOAP API"},{"location":"patterns/api-mgt/#graphql","text":"GraphQL is a query language for APIs that gives an application client greater control over what data it retrieves in an API request when compared with a REST API request. IBM\u00ae API Connect enables you to create a GraphQL API proxy definition that proxies a backend GraphQL server, and to define rate limiting controls that reflect the amount of data that is returned from the server by a request to the GraphQL API. The Developer Portal also supports testing GraphQL APIs. See this dedicated section .","title":"GraphQL"},{"location":"patterns/api-mgt/#asyncapi","text":"We already address in detail in previous sections .","title":"AsyncAPI"},{"location":"patterns/api-mgt/#api-manager","text":"You can manage your APIs by using API Connect's API Manager UI: The API Manager UI allows you to manage private internal APIs as well as public external APIs. API Manager is an on-premises offering that provides the capabilities required to externalize and manage your services as REST or SOAP APIs.","title":"API Manager"},{"location":"patterns/api-mgt/#developer-portal","text":"The Developer Portal is a convenient place to share APIs with application developers. After a Developer Portal has been enabled through the API Manager, and one or more API Products have been published, application developers can browse and use the APIs from the Developer Portal dashboard, as shown below: The Developer Portal can be used as is when it is first enabled, or it can be customized to fit the corporate branding and design requirements of a particular organization. You can configure the Developer Portal for test and development purposes, or for internal use only.","title":"Developer Portal"},{"location":"patterns/api-mgt/#create-capability","text":"Developer can leverage API Connect's Create capability, to build APIs (top-down) with a built-in Swagger (OpenAPI) editor or using a simple guided model-driven approach. APIC allows developers to create a new API from scratch or an API based on the schema of an existing data source, such as a database.","title":"Create Capability"},{"location":"patterns/api-mgt/#creating-a-rest-proxy-api-from-an-existing-target-service","text":"As mentioned earlier, it is also possible to create a REST proxy API from an existing target service (bottom-up) that you import into the API Connect system. Essentially, if you have an existing REST service that you want to expose through an IBM API Connect API definition, you can create a proxy API and specify the target endpoint by using the API Manager.","title":"Creating a REST proxy API from an existing target service"},{"location":"patterns/api-mgt/#explore-capability","text":"Developers can use API Connect's Explore capability to quickly examine their new APIs and try their operations. Developers can add their APIs to the API Connect server and have the choice of running them on the cloud or on-premise, as mentioned above. This can be done through API Connect's UI or with the provided CLI. Developers and Administrators can then connect to the API Connect Server and see their running APIs, including those that were created as well as those that were added based on existing data sources.","title":"Explore Capability"},{"location":"patterns/api-mgt/#subscribing-to-an-api-in-the-developer-portal","text":"To subscribe to an API, from the Developer Portal, the develper clicks API Products to find and subscribe to any available APIs: In the example above, an API called FindBranch Version 2.0, which is contained in the FindBranch Auto Products product is available. Clicking Subscribe enables the developer to use the API: Under the Application heading, the developer can click Select App for the new application: From the next screen below, the developer can click Next: Doing this shows the screen below, which confirms to the developer that his or her applicatin is now subscribed to the selected API under the selected plan. Pressing Done in the next screen completes the subscription.","title":"Subscribing to an API in the Developer Portal"},{"location":"patterns/api-mgt/#testing-an-api-in-the-developer-portal","text":"To test an API, the developer clicks API Products in the Developer Portal dashboard to show all available products: Clicking a product, such as FindBranch auto product, for example, and then clicking the FindBranch API from the provided list shows the available API operations. The developer can click GET/details , for instance, to see the details of the GET operation: Clicking the Try it tab and pressing Send allows the developer to test the API to better understand how it works: As you can see above, this shows the request/response from invoking the API. API Connect shows a returned response of 200 OK and the message body, indicating that the REST API operation call was successful.","title":"Testing An API in the Developer Portal"},{"location":"patterns/api-mgt/#api-product-managers","text":"Administrators or API Product Managers can make changes such as adding corporate security polices or transformations before publishing the APIs. They can also control the visibility and define the rate limit for the APIs. Once the APIs are ready, API Product Managers can expose them for developer consumption and self-service in the Developer Portal. Developers who have signed up can discover and use any APIs which were exposed. Because the APIs were documented while they were being created using OpenAPI notation, developers can not only view the APIs but also try them out, as demonstrated in the demo. API Connect provides source code examples in several different languages that show how to use the API operations. Languages supported include Curl, Ruby, Python, Java, Go, and Node.js.","title":"API Product Managers"},{"location":"patterns/api-mgt/#testing","text":"Besides the small individual tests that you can run from the Developer Portal to understand how an API works, as explained above, it is possible to use API Connect Test and Monitor to effortlessly generate and schedule API test assertions. This browser based tool also provides dashboards for monitoring API test assertions and for receiving alerts on the health of your APIs. You can use an HTTP Client within API Connect Test and Monitor to generate a request to an API and get the expected results: You enter any required parameters and authorization token and press Send to send the request. API Connect Test and Monitor then shows you the response payload as a formatted JSON object: To generate a test, you then click Generate Test , and in the dialog that appears name the test and add it to a project. When you click the checkmark in the upper right corner of the dialog, API Connect Test and Monitor automatically generates test assertions based on the response payload. You can add, delete, or modify these assertions, or even add new ones from a broad selection: You can also reorder the assertions by dragging and dropping them: Although coding is not required, if you wish, you change the underlying code by clicking CODE to enter the code view: Once you have your test arranged the way you want it, you can run it to generate a test report, which highlights successes and failures:","title":"Testing"},{"location":"patterns/api-mgt/#test-scheduling","text":"IBM API Connect Test and Monitor also lets you automate test scheduling at regular intervals. To do this, you first save any test you may have open and exit out of the composer. Then publish your test to prepare it for scheduling: Once your test is published, you can click Schedule to schedule regular test intervals. Test and Monitor allows you to tweak the test schedule as necessary. You can create a new run, name it, and choose the times you wish to run your tests: Clicking Save Run schedules the tests. Finally, you can access the dashboard to monitor the health of your API:","title":"Test Scheduling"},{"location":"patterns/api-mgt/#graphql-apis","text":"IBM\u00ae API Connect enables you to create a GraphQL API proxy definition that proxies a backend GraphQL server, and to define rate limiting controls that reflect the amount of data that is returned from the server by a request to the GraphQL API. The Developer Portal also supports testing GraphQL APIs.","title":"GraphQL APIs"},{"location":"patterns/api-mgt/#advantages-of-graphql-over-rest-apis","text":"GraphQL provides the following particular advantages over REST APIs: The application client can request only the data that it needs. For example, when retrieving bank account records, request only the account number and current balance for each account, but not the customer name and address details. With a REST API request, either the backend REST service must provide separate endpoints for different data subsets, or the application client must retrieve the complete records and then discard the unwanted data. The application client can retrieve multiple related resources in a single request. For example, a customer's bank account record might include an array that references other finance products that the customer holds. If an application client wants to retrieve the bank account details for a specific customer, and details of the other finance products for that customer, then with a REST API the client would first retrieve the bank account details, then make separate requests for each of the other products. A GraphQL API can be designed to allow the client to retrieve all this information in a single request. However, this flexibility presents rate limiting challenges, because two seemingly very similar requests might return vastly different amounts of data, and what might have required multiple REST API requests, each counting towards the rate limit, might require only a single GraphQL API request. It is important therefore that rate limiting controls are imposed that reflect the amount of data that is retrieved. API Connect extends the GraphQL standard by providing, in a GraphQL API definition, the ability to configure a range of settings that are used to calculate the complexity of a GraphQL request and an associated cost that counts towards the rate limit.","title":"Advantages of GraphQL over REST APIs"},{"location":"patterns/api-mgt/#creating-a-graphql-proxy-api","text":"The demo also showed how API Connect supports GraphQL. To create a GraphQL proxy API from an existing GraphQL server, click Add , then API (from REST, GraphQL or SOAP) : Then select, From existing GraphQL service (GraphQL proxy) : Give it a name and complete the rest of the parameters, especially the GraphQL Server URL field which, of course, cannot be left blank. Notice that, once you enter the URL, API Connect will find the service and automatically detect the schema. You can click Next and API Connect will attempt to connect to the GraphQL server, introspect it, and pull in any objects (in this case, queries and operations) that it can find on that server: The gateway will sometimes provide warnings (17 in the example above), which are really just recommendations on how you can optimize the query. Clicking Next allows you to then activate the API for publishing: Finally, clicking Next shows a screen such as the one below with checkmarks: This shows that the gateway was able to connect to the GraphQL server and detect its schema, paths, and operations, validate them, and automatically populate the canvas with different policies, including security policies:","title":"Creating a GraphQL proxy API"},{"location":"patterns/api-mgt/#further-readings","text":"IBM Redbook on Agile Integration A Demo of Event Endpoint Management - Cloud Pak for Integration","title":"Further Readings"},{"location":"patterns/cep/","text":"Enterprises need to identify and act on event streams to address specific business situations. The processing logic groups complex combinations of business events that have or have not occurred in a time frame and at runtime, recognize those situations and react to them with prescribed business actions. Situation detection is an asynchronous, near real-time, and uses temporal and geospatial characteristics of the events to take action. As an example, in account hijacking use case, the processing will involve consuming events from user_login, password_changed, address_changed, new beneficiary added, money_transfert_initiated, in this order and with time windows contraints. The action will freeze the money transfert, alert the account owner, identify the IP address of the logged person, add more data for investigation, or alert operation team... React to multiple complex events with prescribed business actions \u00b6 Situational decision automation refers to detecting a situation, or an event, and then deciding how to act. The interresting part of this procesing is the integration of multiple event streams and the need to keep state in a context of a business entity. Context is important. The context can be a business entity on which a set of events are important to address. In previous example, the Account entity may be the context on which we want to address the events to better understand the event flow with time constraint. How to keep those business entity state related to real time processing is an implementation challenge. To take business decision, you need more information than just looking at the event payload. From a modeling point of view, you need to create a model of business entities and their associated business events, define the target situation, and define the rules to be applied for the situation. Event storming helps to identify events and business entities and their relationships. But we need to enhance the method with rule harvesting, situation modeling and action definition. The model and its associated logic are deployed to a situational processing service. The decisions that are made by the situational processing service can be refined and enhanced by using predictive analytics models. By using situational decision automation as part of an event-driven solution, you can process continuous event streams to derive insights and intelligence. You can analyze an event stream and then extract event data from it so that data scientists can understand and derive machine learning models. You can run analytical processes and machine learning models against the event stream and match complex event patterns across streams and time windows to help you decide and act. The Situational decision automation architecture can help you to build and implement such a solution. Benefits of situational decision automation \u00b6 By adopting decision automation for events, you can optimize your business processes and realize these benefits: Spot risks and opportunities by detecting and predicting event patterns, and react to events as they happen Make more informed decisions by applying business rules and business logic on data streams Improve business responsiveness and minimize compliance risks The reference architecture \u00b6 To make it simple, situational decision acts on event streams, so needs to be connected to event backbones. The reference architecture may look like in the following figure where complex event processing component like Apache Flink will continuously process data streams and generate synthetic events a separate service can act on:","title":"Situational decision"},{"location":"patterns/cep/#react-to-multiple-complex-events-with-prescribed-business-actions","text":"Situational decision automation refers to detecting a situation, or an event, and then deciding how to act. The interresting part of this procesing is the integration of multiple event streams and the need to keep state in a context of a business entity. Context is important. The context can be a business entity on which a set of events are important to address. In previous example, the Account entity may be the context on which we want to address the events to better understand the event flow with time constraint. How to keep those business entity state related to real time processing is an implementation challenge. To take business decision, you need more information than just looking at the event payload. From a modeling point of view, you need to create a model of business entities and their associated business events, define the target situation, and define the rules to be applied for the situation. Event storming helps to identify events and business entities and their relationships. But we need to enhance the method with rule harvesting, situation modeling and action definition. The model and its associated logic are deployed to a situational processing service. The decisions that are made by the situational processing service can be refined and enhanced by using predictive analytics models. By using situational decision automation as part of an event-driven solution, you can process continuous event streams to derive insights and intelligence. You can analyze an event stream and then extract event data from it so that data scientists can understand and derive machine learning models. You can run analytical processes and machine learning models against the event stream and match complex event patterns across streams and time windows to help you decide and act. The Situational decision automation architecture can help you to build and implement such a solution.","title":"React to multiple complex events with prescribed business actions"},{"location":"patterns/cep/#benefits-of-situational-decision-automation","text":"By adopting decision automation for events, you can optimize your business processes and realize these benefits: Spot risks and opportunities by detecting and predicting event patterns, and react to events as they happen Make more informed decisions by applying business rules and business logic on data streams Improve business responsiveness and minimize compliance risks","title":"Benefits of situational decision automation"},{"location":"patterns/cep/#the-reference-architecture","text":"To make it simple, situational decision acts on event streams, so needs to be connected to event backbones. The reference architecture may look like in the following figure where complex event processing component like Apache Flink will continuously process data streams and generate synthetic events a separate service can act on:","title":"The reference architecture"},{"location":"patterns/cqrs/","text":"Command-Query Responsibility Segregation (CQRS) \u00b6 Problems and Constraints \u00b6 A domain model encapsulates domain data with the behavior for maintaining the correctness of that data as it is modified, structuring the data based on how it is stored in the database and to facilitate managing the data. Multiple clients can independently update the data concurrently. Different clients may not use the data the way the domain model structures it, and may not agree with each other on how it should be structured. When a domain model becomes overburdened managing complex aggregate objects, concurrent updates, and numerous cross-cutting views, how can it be refactored to separate different aspects of how the data is used? An application accesses data both to read and to modify it. The primitive data tasks are often expressed as create, read, update, and delete (CRUD); using them is known as CRUDing the data. Application code often does not make much distinction between the tasks; individual operations may mix reading the data with changing the data as needed. This simple approach works well when all clients of the data can use the same structure and contention is low. A single domain model can manage the data, make it accessible as domain objects, and ensure updates maintain its consistency. However, this approach becomes inadaquate when different clients want different views across multiple sets of data, when the data is too widely used, and/or when multiple clients updating the data my unknowlingly conflict with each other. For example, in a microservices architecture, each microservice should store and manage its own data, but a user interface may need to display data from several microservices. A query that gathers bits of data from multiple sources can be inefficient (time and bandwidth consumed accessing multiple data sources, CPU consumed transforming data, memory consumed by intermediate objects) and must be repeated each time the data is accessed. Another example is an enterprise database of record managing data required by multiple applications. It can become overloaded with too many clients needing too many connections to run too many threads performing too many transactions--such that the database becomes a performance bottleneck and can even crash. Another example is maintaining consistency of the data while clients concurrently make independent updates to the data. While each update may be consistent, they may conflict with each other. Database locking ensures that the updates don't change the same data concurrently, but doesn't ensure multiple independent changes result in a consistent data model. When data usage is more complex than a single domain model can facilitate, a more sophisticated approach is needed. Solution and Pattern \u00b6 Refactor a domain model to separate operations for querying data and operations for updating data so that they may be handled independently. The CQRS pattern strictly segregates operations that read data from operations that update data. An operation can read data (the R in CRUD) or can write data (the CUD in CRUD), but not both. This separation can make using data much more manageable in several respects. The read operations and the write operations are simpler to implement because their functionality is more finely focused. The operations can be developed independently, potentially by separate teams. The operations can be optimized independently and can evolve independently, following changing user requirements more easily. These optimized operations can scale better, perform better, and security can be applied more precisely. The full CQRS pattern uses separate read and write databases. In doing so, the pattern segregates not just the APIs for accessing data or the models for managing data, but even segregates the database itself into two, a read/write database that is effectively write-only and one or more read-only databases. The adoption of the pattern can be applied in phases, incrementally from existing code. To illustrate this, we will use four stages that could be used incrementally or a developer can go directly from stage 0 to 3 without considering the others: Stage 0: Typical application data access Stage 1: Separate read and write APIs Stage 2: Separate read and write models Stage 3: Separate read and write databases Typical application data access \u00b6 Before even beginning to apply the pattern, let\u2019s consider the typical app design for accessing data. This diagram shows an app with a domain model for accessing data persisted in a database of record, i.e. a single source of truth for that data. The domain model has an API that at a minimum enables clients to perform CRUD tasks on domain objects within the model. The domain model is an object representation of the database documents or records. It is comprised of domain objects that represent individual documents or records and the business logic for managing and using them. Domain-Driven Design (DDD) models these domain objects as entities \u2014\u201cobjects that have a distinct identity that runs through time and different representations\u201d\u2014and aggregates \u2014\u201ca cluster of domain objects that can be treated as a single unit\u201d; the aggregate root maintains the integrity of the aggregate as a whole. Ideally, the domain model\u2019s API should be more domain-specific than simply CRUDing of data. Instead, it should expose higher-level operations that represent business functionality like findCustomer() , placeOrder() , transferFunds() , and so on. These operations read and update data as needed, sometimes doing both in a single operation. They are correct as long as they fit the way the business works. Separate read and write APIs \u00b6 The first and most visible step in applying the CQRS pattern is splitting the CRUD API into separate read and write APIs. This diagram shows the same domain model as before, but its single CRUD API is split into retrieve and modify APIs. The two APIs share the existing domain model but split the behavior: Read : The retrieve API is used to read the existing state of the objects in the domain model without changing that state. The API treats the domain state as read only. Write : The modify API is used to change the objects in the domain model. Changes are made using CUD tasks: create new domain objects, update the state in existing ones, and delete ones that are no longer needed. The operations in this API do not return result values, they return success (ack or void) or failure (nak or throw an exception). The create operation might return the primary of key of the entity, which can be generated either by the domain model or in the data source. This separation of APIs is an application of the Command Query Separation (CQS) pattern, which says to clearly separate methods that change state from those that don\u2019t. To do so, each of an object\u2019s methods can be in one of two categories (but not both): Query : Returns a result. Does not change the system\u2019s state nor cause any side effects that change the state. Command (a.k.a. modifiers or mutators): Changes the state of a system. Does not return a value, just an indication of success or failure. With this approach, the domain model works the same and provides access to the data the same as before. What has changed is the API for using the domain model. Whereas a higher-level operation might previously have both changed the state of the application and returned a part of that state, now each such operation is redesigned to only do one or the other. When the domain model splits its API into read and write operations, clients using the API must likewise split their functionality into querying and updating functionality. Most new web based applications are based in the single page application, with components and services that use and encapsulate remote API. So this separation of backend API fits well with modern web applications. This stage depends on the domain model being able to implement both the retrieve and modify APIs. A single domain model requires the retrieve and modify behavior to have similar, corresponding implementations. For them to evolve independently, the two APIs will need to be implemented with separate read and write models. Separate read and write models \u00b6 The second step in applying the CQRS pattern is to split the domain model into separate read and write models. This doesn\u2019t just change the API for accessing domain functionality, it also changes the design of how that functionality is structured and implemented. This diagram shows that the domain model becomes the basis for a write model that handles changes to the domain objects, along with a separate read model used to access the state of the app. Naturally, the read model implements the retrieve API and the write model implements the modify API. Now, the application consists not only of separate APIs for querying and updating the domain objects, there\u2019s also separate business functionality for doing so. Both the read business functionality and the write business functionality share the same database. The write model is implemented by specializing the domain model to focus solely on maintaining the valid structure of domain objects when changing their state and by applying any business rules. Meanwhile, responsibility for returning domain objects is shifted to a separate read model. The read model defines data transfer objects (DTOs) designed specifically for the model to return just the data the client wants in a structure the client finds convenient. The read model knows how to gather the data used to populate the DTOs. DTOs encapsulate little if any domain functionality, they just bundle data into a convenient package that can easily be transmitted using a single method call, especially between processes. The read model should be able to implement the retrieve API by implementing the necessary queries and executing them. If the retrieve API is already built to return domain objects as results, the read model can continue to do so, or better yet, implements DTO types that are compatible with the domain objects and returns those. Likewise, the modify API was already implemented using the domain model, so the write model should preserve that. The write model may enhance the implementation to more explicitly implement a command interface or use command objects. This phase assumes that the read and write models can both be implemented using the same database of record the domain model has been using. To the extent this is true, the implementations of reading and writing can evolve independently and be optimized independently. This independence may become increasingly limited since they are both bound to the same database with a single schema or data model. To enable the read and write models to evolve independently, they may each need their own database. Separate read and write databases \u00b6 The third step in applying the CQRS pattern\u2014-which implements the complete CQRS pattern solution\u2014-is splitting the database of record into separate read and write databases. This diagram shows the write model and read model, each supported by its own database. The overall solution consists of two main parts: the write solution that supports updating the data and the read solution that supports querying the data. The two parts are connected by the event bus. The write model has its own read/write database and the read model has its own read-only database. The read/write database still serves as the database of record (the single source of truth for the data) but is mostly used write-only: mostly written to and rarely read. Reading is offloaded onto a separate read database that contains the same data but is used read-only. The query database is effectively a cache of the database of record, with all of the inherit benefits and complexity of the Caching pattern. The query database contains a copy of the data in the database of record, with the copy structured and staged for easier access by the clients using the retrieve API. As a copy, overhead is needed to keep the copy synchronized with changes in the original. Latency in this synchronization process creates eventual consistency, during which the data copy is stale. The separate databases enable the separate read and write models and their respective retrieve and modify APIs to truly evolve independently. Not only can the read model or write model\u2019s implementation change without changing the other, but how each stores its data can be changed independently. This solution offers the following advantages: Scaling : The query load is moved from the write database to the read database. If the database of record is a scalability bottleneck and a lot of the load on it is caused by queries, unloading those query responsibilities can significantly improve the scalability of the combined data access. Performance : The schemas of the two databases can be different, enabling them to be designed and optimized independently for better performance. The write database can be optimized for data consistency and correctness, with capabilities such as stored procedures that fit the write model and assist with data updates. The read database can store the data in units that better fit the read model and are better optimized for querying, with larger rows requiring fewer joins. Notice that the design for this stage is significantly more complex than the design for the previous stage. Separate databases with copies of the same data may make data modeling and using data easier, but they require significant overhead to synchronize the data and keep the copies consistent. CQRS employs a couple of design features that support keeping the databases synchronized: Command Bus for queuing commands (optional): A more subtle and optional design decision is to queue the commands produced by the modify API, shown in the diagram as the command bus. This can significantly increase the throughput of multiple apps updating the database, as well as serialize updates to help avoid--or at least detect--merge conflicts. With the bus, a client making an update does not block synchronously while the change is written to the database. Rather, the request to change the database is captured as a command ( Design Patterns ) and put on a message queue, after which the client can proceed with other work. Asynchronously in the background, the write model processes the commands at the maximum sustainable rate that the database can handle, without the database ever becoming overloaded. If the database becomes temporarily unavailable, the commands queue and will be processed when the database becomes available once more. Event Bus for publishing update events (required): Whenever the write database is updated, a change notification is published as an event on the event bus. Interested parties can subscribe to the event bus to be notified when the database is updated. One such party is an event processor for the query database, which receives update events and processes them by updating the query database accordingly. In this way, every time the write database is updated, a corresponding update is made to the read database to keep it in sync. The connection between the command bus and the event bus is facilitated by an application of the Event Sourcing pattern , which keeps a change log that is suitable for publishing. Event sourcing maintains not only the current state of the data but also the history of how that current state was reached. For each command on the command bus, the write model performs these tasks to process the command: Logs the change Updates the database with the change Creates an update event describing the change and publishes it to the event bus The changes that are logged can be the commands from the command bus or the update events published to the event bus Considerations \u00b6 Keep these decisions in mind while applying this pattern: Client impact : Applying CQRS not only changes how data is stored and accessed, but also changes the APIs that clients use to access data. This means that each client must be redesigned to use the new APIs. Riskiness : A lot of the complexity of the pattern solution involves duplicating the data in two databases and keeping them synchronized. Risk comes from querying data that is stale or downright wrong because of problems with the synchronization. Eventual consistency : Clients querying data must expect that updates will have latency. In a microservices architecture, eventual data consistency is a given and acceptable in many of cases. Command queuing : Using a command bus as part of the write solution to queue the commands is optional but powerful. In addition to the benefits of queuing, the command objects can easily be stored in the change log and easily be converted into notification events. (In the next section, we illustrate a way to use event bus to queue commands as well.) Change log : The log of changes to the database of record can be either the list of commands from the command bus or the list of event notifications published on the event bus. The Event Sourcing pattern assumes it\u2019s a log of events, but that pattern doesn\u2019t include the command bus. An event list may be easier to scan as a history, whereas a command list is easier to replay. Create keys : Strick interpretation of the Command Query Separation (CQS) pattern says that command operations do not have return types. A possible exception is commands that create data: An operation that creates a new record or document typically returns the key for accessing the new data, which is convenient for the client. However, if the create operation is invoked asynchronously by a command on a command bus, the write model will need to perform a callback on the client to return the key. Messaging queues and topics : While messaging is used to implement both the command bus and event bus, the two busses use messaging differently. The command bus guarantees exactly once delivery. The event bus broadcasts each event to all interested event processors. Query database persistence : The database of record is always persistent. The query database is a cache that can be a persistent cache or an in-memory cache. If the cache is in-memory and is lost, it must be rebuilt completely from the database of record. Security : Controls on reading data and updating data can be applied separately using the two parts of the solution. Combining event sourcing and CQRS \u00b6 The CQRS application pattern is frequently associated with event sourcing: when doing event sourcing and domain driven design, we event source the aggregates or root entities. Aggregate creates events that are persisted. On top of the simple create, update and read by ID operations, the business requirements want to perform complex queries that can't be answered by a single aggregate. By just using event sourcing to be able to respond to a query like \"what are the orders of a customer\", then we have to rebuild the history of all orders and filter per customer. It is a lot of computation. This is linked to the problem of having conflicting domain models between query and persistence. As introduced in previous section, creations and updates are done as state notification events (change of state), and are persisted in the event log/store. The following figure, presents two separate microservices, one supporting the write model, and multiple other supporting the queries: The query part is separate processes that consume change log events and build a projection for future queries. The \"write\" part may persist in SQL while the read may use document oriented database with strong indexing and query capabilities. Or use in-memory database, or distributed cache... They do not need to be in the same language. With CQRS and ES the projections are retroactives. New query equals implementing new projection and read the events from the beginning of time or the recent committed state and snapshot. Read and write models are strongly decoupled and can evolve independently. It is important to note that the 'Command' part can still handle simple queries, primary-key based, like get order by id, or queries that do not involve joins. The event backbone, use a pub/sub model, and Kafka is a good candidate as an implementation technology. With this structure, the Read model microservice will most likely consume events from multiple topics to build the data projection based on joining those data streams. A query, to assess if the cold-chain was respected on the fresh food order shipment, will go to the voyage, container metrics, and order to be able to answer this question. This is where CQRS shines. We can note that, we can separate the API definition and management in a API gateway. The shipment order microservice is implementing this pattern. Some implementation items to consider: Consistency (ensure the data constraints are respected for each data transaction): CQRS without event sourcing has the same consistency guarantees as the database used to persist data and events. With Event Sourcing the consistency could be different, one for the write model and one for the read model. On write model, strong consistency is important to ensure the current state of the system is correct, so it leverages transaction, lock and sharding. On read side, we need less consistency, as they mostly work on stale data. Locking data on the read operation is not reasonable. Scalability : Separating read and write as two different microservices allows for high availability. Caching at the read level can be used to increase performance response time, and can be deployed as multiple standalone instances (Pods in Kubernetes). It is also possible to separate the query implementations between different services. Functions as service / serverless are good technology choices to implement complex queries. Availability : The write model sacrafices consistency for availability. This is a fact. The read model is eventually consistent so high availability is possible. In case of failure the system disables the writing of data but still is able to read them as they are served by different databases and services. With CQRS, the write model can evolve over time without impacting the read model, as long as the event model doesn't change. The read model requires additional tables, but they are often simpler and easier to understand. CQRS results in an increased number of objects, with commands, operations, events,... and packaging in deployable components or containers. It adds potentially different type of data sources. It is more complex. Some challenges to always consider: How to support event structure version management? How much data to keep in the event store (history)? How to adopt data duplication which results to eventual data consistency?. The CQRS pattern was introduced by Greg Young , and described in Martin Fowler's work on microservices. As you can see in previous figure, as soon as we see two arrows from the same component, we have to ask ourselves how does it work: the write model has to persist Order in its own database and then sends OrderCreated event to the topic... Should those operations be atomic and controlled with transaction? We detail this in next section. Keeping the write model on Mainframe \u00b6 It is very important to note that the system of records and transaction processing is still easier to run on mainframe to support strong consistency. But with the move to cloud native development, it does not mean we have to move all the system of records to the cloud. Data pipelines can be put in place, but CQRS should help by keeping the write model on the current system of records and without impacting the current MIPS utilization move data in the eventual consistency workd of the cloud native, distributed computing world. In the figure above, the write model follows the current transaction processing on the mainframce, change data capture push data to Event backbone for getting real time visibility into the distributed world. The read is the costly operation, dues to the joins to be done to build the projection views needed to expose data depending of the business use cases. This is more true with distributed microservices. All the write operations for the business entities kept in the mainframe's system of records are still done via the transaction. Reference data can be injected in one load job to topic and persisted in the event store so streaming applications can leverage them by joining with transactional data. The consistency challenges \u00b6 As introduced in the previous section, there is a potential problem of data inconsistency: once a command saves changes into the database, the consumers do not see the new or updated data until event notification completes processing. With traditional Java service, using JPA and JMS, the save and send operations can be part of the same XA transaction and both succeed or fail. With event sourcing pattern, the source of trust is the event source, which acts as a version control system, as shown in the diagram below. The steps for syncronizing changes to the data are: The write model creates the event and publishes it The consumer receives the event and extracts its payload The consumer updates its local datasource with the payload data If the consumer fails to process the update, it can persist the event to an error log Each error in the log can be replayed A command line interface replays an event via an admin API, which searches in the topic using this order id to replay the save operation This implementation causes a problem for the createOrder(order): string operation: The Order Service is supposed to return the new order complete with the order id that is a unique key, a key most likely created by the database. If updating the database fails, there is no new order yet and so no database key to use as the order ID. To avoid this problem, if the underlying technology supports assigning the new order's key, the service can generate the order ID and use that as the order's key in the database. It is important to clearly study the Kafka consumer API and the different parameters on how to support the read offset. We are addressing those implementation best practices in our consumer note. CQRS and Change Data Capture \u00b6 There are other ways to support this dual operations level: When using Kafka, Kafka Connect has the capability to subscribe to databases via JDBC, allowing to poll tables for updates and then produce events to Kafka. There is an open-source change data capture solution based on extracting change events from database transaction logs, Debezium that helps to respond to insert, update and delete operations on databases and generate events accordingly. It supports databases like MySQL, Postgres, MongoDB and others. Write the order to the database and in the same transaction write to an event table ( \"outbox pattern\" ). Then use a polling to get the events to send to Kafka from this event table and delete the row in the table once the event is sent. Use the Change Data Capture from the database transaction log and generate events from this log. The IBM Infosphere CDC product helps to implement this pattern. For more detail about this solution see this product tour . The CQRS implementation using CDC will look like in the following diagram: What is important to note is that the event needs to be flexible on the data payload. We are presenting a event model in the reference implementation. On the view side, updates to the view part need to be idempotent. Delay in the view \u00b6 There is a delay between the data persistence and the availability of the data in the Read model. For most business applications, it is perfectly acceptable. In web based data access most of the data are at stale. When there is a need for the client, calling the query operation, to know if the data is up-to-date, the service can define a versioning strategy. When the order data was entered in a form within a single page application like our kc- user interface , the \"create order\" operation should return the order with its unique key freshly created and the Single Page Application will have the last data. Here is an example of such operation: @POST public Response create ( OrderCreate dto ) { Order order = new Order ( UUID . randomUUID (). toString (), dto . getProductID (),...); // ... return Response . ok (). entity ( order ). build () } Schema change \u00b6 What to do when we need to add attribute to event?. So we need to create a versioninig schema for event structure. You need to use flexible schema like json schema, Apache Avro or protocol buffer and may be, add an event adapter (as a function?) to translate between the different event structures. Code reference \u00b6 The following project includes two sub modules, each deployable as a microservice to illustrate the command and query part: https://github.com/ibm-cloud-architecture/refarch-kc-order-ms Further readings https://www.codeproject.com/Articles/555855/Introduction-to-CQRS http://udidahan.com/2009/12/09/clarified-cqrs https://martinfowler.com/bliki/CQRS.html https://microservices.io/patterns/data/cqrs.html https://community.risingstack.com/when-to-use-cqrs https://dzone.com/articles/concepts-of-cqrs https://martinfowler.com/bliki/CommandQuerySeparation.html https://www.martinfowler.com/eaaCatalog/domainModel.html https://dddcommunity.org/learning-ddd/what_is_ddd/ https://martinfowler.com/bliki/EvansClassification.html https://martinfowler.com/bliki/DDD_Aggregate.html https://martinfowler.com/eaaCatalog/dataTransferObject.html https://en.wikipedia.org/wiki/Command_pattern https://www.pearson.com/us/higher-education/program/Gamma-Design-Patterns-Elements-of-Reusable-Object-Oriented-Software/PGM14333.html","title":"CQRS"},{"location":"patterns/cqrs/#command-query-responsibility-segregation-cqrs","text":"","title":"Command-Query Responsibility Segregation (CQRS)"},{"location":"patterns/cqrs/#problems-and-constraints","text":"A domain model encapsulates domain data with the behavior for maintaining the correctness of that data as it is modified, structuring the data based on how it is stored in the database and to facilitate managing the data. Multiple clients can independently update the data concurrently. Different clients may not use the data the way the domain model structures it, and may not agree with each other on how it should be structured. When a domain model becomes overburdened managing complex aggregate objects, concurrent updates, and numerous cross-cutting views, how can it be refactored to separate different aspects of how the data is used? An application accesses data both to read and to modify it. The primitive data tasks are often expressed as create, read, update, and delete (CRUD); using them is known as CRUDing the data. Application code often does not make much distinction between the tasks; individual operations may mix reading the data with changing the data as needed. This simple approach works well when all clients of the data can use the same structure and contention is low. A single domain model can manage the data, make it accessible as domain objects, and ensure updates maintain its consistency. However, this approach becomes inadaquate when different clients want different views across multiple sets of data, when the data is too widely used, and/or when multiple clients updating the data my unknowlingly conflict with each other. For example, in a microservices architecture, each microservice should store and manage its own data, but a user interface may need to display data from several microservices. A query that gathers bits of data from multiple sources can be inefficient (time and bandwidth consumed accessing multiple data sources, CPU consumed transforming data, memory consumed by intermediate objects) and must be repeated each time the data is accessed. Another example is an enterprise database of record managing data required by multiple applications. It can become overloaded with too many clients needing too many connections to run too many threads performing too many transactions--such that the database becomes a performance bottleneck and can even crash. Another example is maintaining consistency of the data while clients concurrently make independent updates to the data. While each update may be consistent, they may conflict with each other. Database locking ensures that the updates don't change the same data concurrently, but doesn't ensure multiple independent changes result in a consistent data model. When data usage is more complex than a single domain model can facilitate, a more sophisticated approach is needed.","title":"Problems and Constraints"},{"location":"patterns/cqrs/#solution-and-pattern","text":"Refactor a domain model to separate operations for querying data and operations for updating data so that they may be handled independently. The CQRS pattern strictly segregates operations that read data from operations that update data. An operation can read data (the R in CRUD) or can write data (the CUD in CRUD), but not both. This separation can make using data much more manageable in several respects. The read operations and the write operations are simpler to implement because their functionality is more finely focused. The operations can be developed independently, potentially by separate teams. The operations can be optimized independently and can evolve independently, following changing user requirements more easily. These optimized operations can scale better, perform better, and security can be applied more precisely. The full CQRS pattern uses separate read and write databases. In doing so, the pattern segregates not just the APIs for accessing data or the models for managing data, but even segregates the database itself into two, a read/write database that is effectively write-only and one or more read-only databases. The adoption of the pattern can be applied in phases, incrementally from existing code. To illustrate this, we will use four stages that could be used incrementally or a developer can go directly from stage 0 to 3 without considering the others: Stage 0: Typical application data access Stage 1: Separate read and write APIs Stage 2: Separate read and write models Stage 3: Separate read and write databases","title":"Solution and Pattern"},{"location":"patterns/cqrs/#typical-application-data-access","text":"Before even beginning to apply the pattern, let\u2019s consider the typical app design for accessing data. This diagram shows an app with a domain model for accessing data persisted in a database of record, i.e. a single source of truth for that data. The domain model has an API that at a minimum enables clients to perform CRUD tasks on domain objects within the model. The domain model is an object representation of the database documents or records. It is comprised of domain objects that represent individual documents or records and the business logic for managing and using them. Domain-Driven Design (DDD) models these domain objects as entities \u2014\u201cobjects that have a distinct identity that runs through time and different representations\u201d\u2014and aggregates \u2014\u201ca cluster of domain objects that can be treated as a single unit\u201d; the aggregate root maintains the integrity of the aggregate as a whole. Ideally, the domain model\u2019s API should be more domain-specific than simply CRUDing of data. Instead, it should expose higher-level operations that represent business functionality like findCustomer() , placeOrder() , transferFunds() , and so on. These operations read and update data as needed, sometimes doing both in a single operation. They are correct as long as they fit the way the business works.","title":"Typical application data access"},{"location":"patterns/cqrs/#separate-read-and-write-apis","text":"The first and most visible step in applying the CQRS pattern is splitting the CRUD API into separate read and write APIs. This diagram shows the same domain model as before, but its single CRUD API is split into retrieve and modify APIs. The two APIs share the existing domain model but split the behavior: Read : The retrieve API is used to read the existing state of the objects in the domain model without changing that state. The API treats the domain state as read only. Write : The modify API is used to change the objects in the domain model. Changes are made using CUD tasks: create new domain objects, update the state in existing ones, and delete ones that are no longer needed. The operations in this API do not return result values, they return success (ack or void) or failure (nak or throw an exception). The create operation might return the primary of key of the entity, which can be generated either by the domain model or in the data source. This separation of APIs is an application of the Command Query Separation (CQS) pattern, which says to clearly separate methods that change state from those that don\u2019t. To do so, each of an object\u2019s methods can be in one of two categories (but not both): Query : Returns a result. Does not change the system\u2019s state nor cause any side effects that change the state. Command (a.k.a. modifiers or mutators): Changes the state of a system. Does not return a value, just an indication of success or failure. With this approach, the domain model works the same and provides access to the data the same as before. What has changed is the API for using the domain model. Whereas a higher-level operation might previously have both changed the state of the application and returned a part of that state, now each such operation is redesigned to only do one or the other. When the domain model splits its API into read and write operations, clients using the API must likewise split their functionality into querying and updating functionality. Most new web based applications are based in the single page application, with components and services that use and encapsulate remote API. So this separation of backend API fits well with modern web applications. This stage depends on the domain model being able to implement both the retrieve and modify APIs. A single domain model requires the retrieve and modify behavior to have similar, corresponding implementations. For them to evolve independently, the two APIs will need to be implemented with separate read and write models.","title":"Separate read and write APIs"},{"location":"patterns/cqrs/#separate-read-and-write-models","text":"The second step in applying the CQRS pattern is to split the domain model into separate read and write models. This doesn\u2019t just change the API for accessing domain functionality, it also changes the design of how that functionality is structured and implemented. This diagram shows that the domain model becomes the basis for a write model that handles changes to the domain objects, along with a separate read model used to access the state of the app. Naturally, the read model implements the retrieve API and the write model implements the modify API. Now, the application consists not only of separate APIs for querying and updating the domain objects, there\u2019s also separate business functionality for doing so. Both the read business functionality and the write business functionality share the same database. The write model is implemented by specializing the domain model to focus solely on maintaining the valid structure of domain objects when changing their state and by applying any business rules. Meanwhile, responsibility for returning domain objects is shifted to a separate read model. The read model defines data transfer objects (DTOs) designed specifically for the model to return just the data the client wants in a structure the client finds convenient. The read model knows how to gather the data used to populate the DTOs. DTOs encapsulate little if any domain functionality, they just bundle data into a convenient package that can easily be transmitted using a single method call, especially between processes. The read model should be able to implement the retrieve API by implementing the necessary queries and executing them. If the retrieve API is already built to return domain objects as results, the read model can continue to do so, or better yet, implements DTO types that are compatible with the domain objects and returns those. Likewise, the modify API was already implemented using the domain model, so the write model should preserve that. The write model may enhance the implementation to more explicitly implement a command interface or use command objects. This phase assumes that the read and write models can both be implemented using the same database of record the domain model has been using. To the extent this is true, the implementations of reading and writing can evolve independently and be optimized independently. This independence may become increasingly limited since they are both bound to the same database with a single schema or data model. To enable the read and write models to evolve independently, they may each need their own database.","title":"Separate read and write models"},{"location":"patterns/cqrs/#separate-read-and-write-databases","text":"The third step in applying the CQRS pattern\u2014-which implements the complete CQRS pattern solution\u2014-is splitting the database of record into separate read and write databases. This diagram shows the write model and read model, each supported by its own database. The overall solution consists of two main parts: the write solution that supports updating the data and the read solution that supports querying the data. The two parts are connected by the event bus. The write model has its own read/write database and the read model has its own read-only database. The read/write database still serves as the database of record (the single source of truth for the data) but is mostly used write-only: mostly written to and rarely read. Reading is offloaded onto a separate read database that contains the same data but is used read-only. The query database is effectively a cache of the database of record, with all of the inherit benefits and complexity of the Caching pattern. The query database contains a copy of the data in the database of record, with the copy structured and staged for easier access by the clients using the retrieve API. As a copy, overhead is needed to keep the copy synchronized with changes in the original. Latency in this synchronization process creates eventual consistency, during which the data copy is stale. The separate databases enable the separate read and write models and their respective retrieve and modify APIs to truly evolve independently. Not only can the read model or write model\u2019s implementation change without changing the other, but how each stores its data can be changed independently. This solution offers the following advantages: Scaling : The query load is moved from the write database to the read database. If the database of record is a scalability bottleneck and a lot of the load on it is caused by queries, unloading those query responsibilities can significantly improve the scalability of the combined data access. Performance : The schemas of the two databases can be different, enabling them to be designed and optimized independently for better performance. The write database can be optimized for data consistency and correctness, with capabilities such as stored procedures that fit the write model and assist with data updates. The read database can store the data in units that better fit the read model and are better optimized for querying, with larger rows requiring fewer joins. Notice that the design for this stage is significantly more complex than the design for the previous stage. Separate databases with copies of the same data may make data modeling and using data easier, but they require significant overhead to synchronize the data and keep the copies consistent. CQRS employs a couple of design features that support keeping the databases synchronized: Command Bus for queuing commands (optional): A more subtle and optional design decision is to queue the commands produced by the modify API, shown in the diagram as the command bus. This can significantly increase the throughput of multiple apps updating the database, as well as serialize updates to help avoid--or at least detect--merge conflicts. With the bus, a client making an update does not block synchronously while the change is written to the database. Rather, the request to change the database is captured as a command ( Design Patterns ) and put on a message queue, after which the client can proceed with other work. Asynchronously in the background, the write model processes the commands at the maximum sustainable rate that the database can handle, without the database ever becoming overloaded. If the database becomes temporarily unavailable, the commands queue and will be processed when the database becomes available once more. Event Bus for publishing update events (required): Whenever the write database is updated, a change notification is published as an event on the event bus. Interested parties can subscribe to the event bus to be notified when the database is updated. One such party is an event processor for the query database, which receives update events and processes them by updating the query database accordingly. In this way, every time the write database is updated, a corresponding update is made to the read database to keep it in sync. The connection between the command bus and the event bus is facilitated by an application of the Event Sourcing pattern , which keeps a change log that is suitable for publishing. Event sourcing maintains not only the current state of the data but also the history of how that current state was reached. For each command on the command bus, the write model performs these tasks to process the command: Logs the change Updates the database with the change Creates an update event describing the change and publishes it to the event bus The changes that are logged can be the commands from the command bus or the update events published to the event bus","title":"Separate read and write databases"},{"location":"patterns/cqrs/#considerations","text":"Keep these decisions in mind while applying this pattern: Client impact : Applying CQRS not only changes how data is stored and accessed, but also changes the APIs that clients use to access data. This means that each client must be redesigned to use the new APIs. Riskiness : A lot of the complexity of the pattern solution involves duplicating the data in two databases and keeping them synchronized. Risk comes from querying data that is stale or downright wrong because of problems with the synchronization. Eventual consistency : Clients querying data must expect that updates will have latency. In a microservices architecture, eventual data consistency is a given and acceptable in many of cases. Command queuing : Using a command bus as part of the write solution to queue the commands is optional but powerful. In addition to the benefits of queuing, the command objects can easily be stored in the change log and easily be converted into notification events. (In the next section, we illustrate a way to use event bus to queue commands as well.) Change log : The log of changes to the database of record can be either the list of commands from the command bus or the list of event notifications published on the event bus. The Event Sourcing pattern assumes it\u2019s a log of events, but that pattern doesn\u2019t include the command bus. An event list may be easier to scan as a history, whereas a command list is easier to replay. Create keys : Strick interpretation of the Command Query Separation (CQS) pattern says that command operations do not have return types. A possible exception is commands that create data: An operation that creates a new record or document typically returns the key for accessing the new data, which is convenient for the client. However, if the create operation is invoked asynchronously by a command on a command bus, the write model will need to perform a callback on the client to return the key. Messaging queues and topics : While messaging is used to implement both the command bus and event bus, the two busses use messaging differently. The command bus guarantees exactly once delivery. The event bus broadcasts each event to all interested event processors. Query database persistence : The database of record is always persistent. The query database is a cache that can be a persistent cache or an in-memory cache. If the cache is in-memory and is lost, it must be rebuilt completely from the database of record. Security : Controls on reading data and updating data can be applied separately using the two parts of the solution.","title":"Considerations"},{"location":"patterns/cqrs/#combining-event-sourcing-and-cqrs","text":"The CQRS application pattern is frequently associated with event sourcing: when doing event sourcing and domain driven design, we event source the aggregates or root entities. Aggregate creates events that are persisted. On top of the simple create, update and read by ID operations, the business requirements want to perform complex queries that can't be answered by a single aggregate. By just using event sourcing to be able to respond to a query like \"what are the orders of a customer\", then we have to rebuild the history of all orders and filter per customer. It is a lot of computation. This is linked to the problem of having conflicting domain models between query and persistence. As introduced in previous section, creations and updates are done as state notification events (change of state), and are persisted in the event log/store. The following figure, presents two separate microservices, one supporting the write model, and multiple other supporting the queries: The query part is separate processes that consume change log events and build a projection for future queries. The \"write\" part may persist in SQL while the read may use document oriented database with strong indexing and query capabilities. Or use in-memory database, or distributed cache... They do not need to be in the same language. With CQRS and ES the projections are retroactives. New query equals implementing new projection and read the events from the beginning of time or the recent committed state and snapshot. Read and write models are strongly decoupled and can evolve independently. It is important to note that the 'Command' part can still handle simple queries, primary-key based, like get order by id, or queries that do not involve joins. The event backbone, use a pub/sub model, and Kafka is a good candidate as an implementation technology. With this structure, the Read model microservice will most likely consume events from multiple topics to build the data projection based on joining those data streams. A query, to assess if the cold-chain was respected on the fresh food order shipment, will go to the voyage, container metrics, and order to be able to answer this question. This is where CQRS shines. We can note that, we can separate the API definition and management in a API gateway. The shipment order microservice is implementing this pattern. Some implementation items to consider: Consistency (ensure the data constraints are respected for each data transaction): CQRS without event sourcing has the same consistency guarantees as the database used to persist data and events. With Event Sourcing the consistency could be different, one for the write model and one for the read model. On write model, strong consistency is important to ensure the current state of the system is correct, so it leverages transaction, lock and sharding. On read side, we need less consistency, as they mostly work on stale data. Locking data on the read operation is not reasonable. Scalability : Separating read and write as two different microservices allows for high availability. Caching at the read level can be used to increase performance response time, and can be deployed as multiple standalone instances (Pods in Kubernetes). It is also possible to separate the query implementations between different services. Functions as service / serverless are good technology choices to implement complex queries. Availability : The write model sacrafices consistency for availability. This is a fact. The read model is eventually consistent so high availability is possible. In case of failure the system disables the writing of data but still is able to read them as they are served by different databases and services. With CQRS, the write model can evolve over time without impacting the read model, as long as the event model doesn't change. The read model requires additional tables, but they are often simpler and easier to understand. CQRS results in an increased number of objects, with commands, operations, events,... and packaging in deployable components or containers. It adds potentially different type of data sources. It is more complex. Some challenges to always consider: How to support event structure version management? How much data to keep in the event store (history)? How to adopt data duplication which results to eventual data consistency?. The CQRS pattern was introduced by Greg Young , and described in Martin Fowler's work on microservices. As you can see in previous figure, as soon as we see two arrows from the same component, we have to ask ourselves how does it work: the write model has to persist Order in its own database and then sends OrderCreated event to the topic... Should those operations be atomic and controlled with transaction? We detail this in next section.","title":"Combining event sourcing and CQRS"},{"location":"patterns/cqrs/#keeping-the-write-model-on-mainframe","text":"It is very important to note that the system of records and transaction processing is still easier to run on mainframe to support strong consistency. But with the move to cloud native development, it does not mean we have to move all the system of records to the cloud. Data pipelines can be put in place, but CQRS should help by keeping the write model on the current system of records and without impacting the current MIPS utilization move data in the eventual consistency workd of the cloud native, distributed computing world. In the figure above, the write model follows the current transaction processing on the mainframce, change data capture push data to Event backbone for getting real time visibility into the distributed world. The read is the costly operation, dues to the joins to be done to build the projection views needed to expose data depending of the business use cases. This is more true with distributed microservices. All the write operations for the business entities kept in the mainframe's system of records are still done via the transaction. Reference data can be injected in one load job to topic and persisted in the event store so streaming applications can leverage them by joining with transactional data.","title":"Keeping the write model on Mainframe"},{"location":"patterns/cqrs/#the-consistency-challenges","text":"As introduced in the previous section, there is a potential problem of data inconsistency: once a command saves changes into the database, the consumers do not see the new or updated data until event notification completes processing. With traditional Java service, using JPA and JMS, the save and send operations can be part of the same XA transaction and both succeed or fail. With event sourcing pattern, the source of trust is the event source, which acts as a version control system, as shown in the diagram below. The steps for syncronizing changes to the data are: The write model creates the event and publishes it The consumer receives the event and extracts its payload The consumer updates its local datasource with the payload data If the consumer fails to process the update, it can persist the event to an error log Each error in the log can be replayed A command line interface replays an event via an admin API, which searches in the topic using this order id to replay the save operation This implementation causes a problem for the createOrder(order): string operation: The Order Service is supposed to return the new order complete with the order id that is a unique key, a key most likely created by the database. If updating the database fails, there is no new order yet and so no database key to use as the order ID. To avoid this problem, if the underlying technology supports assigning the new order's key, the service can generate the order ID and use that as the order's key in the database. It is important to clearly study the Kafka consumer API and the different parameters on how to support the read offset. We are addressing those implementation best practices in our consumer note.","title":"The consistency challenges"},{"location":"patterns/cqrs/#cqrs-and-change-data-capture","text":"There are other ways to support this dual operations level: When using Kafka, Kafka Connect has the capability to subscribe to databases via JDBC, allowing to poll tables for updates and then produce events to Kafka. There is an open-source change data capture solution based on extracting change events from database transaction logs, Debezium that helps to respond to insert, update and delete operations on databases and generate events accordingly. It supports databases like MySQL, Postgres, MongoDB and others. Write the order to the database and in the same transaction write to an event table ( \"outbox pattern\" ). Then use a polling to get the events to send to Kafka from this event table and delete the row in the table once the event is sent. Use the Change Data Capture from the database transaction log and generate events from this log. The IBM Infosphere CDC product helps to implement this pattern. For more detail about this solution see this product tour . The CQRS implementation using CDC will look like in the following diagram: What is important to note is that the event needs to be flexible on the data payload. We are presenting a event model in the reference implementation. On the view side, updates to the view part need to be idempotent.","title":"CQRS and Change Data Capture"},{"location":"patterns/cqrs/#delay-in-the-view","text":"There is a delay between the data persistence and the availability of the data in the Read model. For most business applications, it is perfectly acceptable. In web based data access most of the data are at stale. When there is a need for the client, calling the query operation, to know if the data is up-to-date, the service can define a versioning strategy. When the order data was entered in a form within a single page application like our kc- user interface , the \"create order\" operation should return the order with its unique key freshly created and the Single Page Application will have the last data. Here is an example of such operation: @POST public Response create ( OrderCreate dto ) { Order order = new Order ( UUID . randomUUID (). toString (), dto . getProductID (),...); // ... return Response . ok (). entity ( order ). build () }","title":"Delay in the view"},{"location":"patterns/cqrs/#schema-change","text":"What to do when we need to add attribute to event?. So we need to create a versioninig schema for event structure. You need to use flexible schema like json schema, Apache Avro or protocol buffer and may be, add an event adapter (as a function?) to translate between the different event structures.","title":"Schema change"},{"location":"patterns/cqrs/#code-reference","text":"The following project includes two sub modules, each deployable as a microservice to illustrate the command and query part: https://github.com/ibm-cloud-architecture/refarch-kc-order-ms Further readings https://www.codeproject.com/Articles/555855/Introduction-to-CQRS http://udidahan.com/2009/12/09/clarified-cqrs https://martinfowler.com/bliki/CQRS.html https://microservices.io/patterns/data/cqrs.html https://community.risingstack.com/when-to-use-cqrs https://dzone.com/articles/concepts-of-cqrs https://martinfowler.com/bliki/CommandQuerySeparation.html https://www.martinfowler.com/eaaCatalog/domainModel.html https://dddcommunity.org/learning-ddd/what_is_ddd/ https://martinfowler.com/bliki/EvansClassification.html https://martinfowler.com/bliki/DDD_Aggregate.html https://martinfowler.com/eaaCatalog/dataTransferObject.html https://en.wikipedia.org/wiki/Command_pattern https://www.pearson.com/us/higher-education/program/Gamma-Design-Patterns-Elements-of-Reusable-Object-Oriented-Software/PGM14333.html","title":"Code reference"},{"location":"patterns/data-pipeline/","text":"In this article we are highlighting some practices to design and develop data intensive application in the context of microservice solution. This is strongly linked to the adoption of event-driven microservices, but addresses the data consistency and eventual data consistency discussions, as well as the establishment of a data fabric services. Context \u00b6 A typical modern business solution will include a set of microservices working together in choreography to exchange data. The adoption of event-driven microservices, with all the related design patterns, is described in separate articles that you can read here . When zooming to a particular data intensive microservice we will find a set of important data centric features that may look like in the diagram below, which presents one component of a bigger distributed system. The services involved include: API to define the service contract: OpenAPI or AsynchAPI Databases to store data for long term - document oriented or SQL based Caches to speed up retrieving data for expensive queries Search indexes to support search on a corpus Stream processing to pub/sub messages, which are now also considered as long duration datastore (Kafka). Interactive queries on top of data streams, and aggregates. Message contract and schemas Unified data service which includes big data storage, but also data catalog and queries on data at rest. Subjects of concern \u00b6 When designing such application we need to address a set of important subjects: What is the type of workload on the database? Read heavy, global access query, write heavy or balanced. Expected throughtput? Is there any fluctuation during the day? does it need to scale over time? How much data to store? what will be the pattern for this size (Always grow)? How content is access via which expected security and access control? What will be the expected durability? Week or forever? Expected latency and number of concurrent users? How the data model is modeled, does it need to support relationship integrity? Join queries? Structured or semi-structured content? Do we need strong schema, or more flexible one? Do we need search on unstructured data? NoSql? How to ensure data correctness and completeness? How to address good performance when exposing data, even when app is running slowly? How to scale and address increase in transaction volume and data size? What data to expose to other services via messaging ? Which formats? What data to expose to other services via APIs ? How to support application reliability when some components are not performing within their SLA? How to be fault-tolerant? How to test fault-tolerance? How does adding horizontal compute power impact the data access? How to support disaster recovery? In modern big data applications, hardware redundancy is not suffisant, the design needs to support unexpected faults, to avoid cascading failures or to support new version deployment with rolling-upgrade capability. When addressing scalability and load growth, we need to define the load parameters: number of transactions per second, the number of read and write operations, the number of active sessions, ... on average and at peak. Each microservice in a solution will have its own load characteristics. From there, we need to address the following issues: How does load growth impact performance while keeping existing compute resources? What is the increase of compute resource needed to support same performance while load growth? The solution problem is a combination of different characteristics to address: read volume, write volume, data store volume, data complexity and size, response time, access logic... For batch processing the measurement is the throughput: number of records per second or time to process n records. For real-time processing the response time measures the time to get a response from a client's point of view after sending a request. When defining service level agreement, it is important to use the median response time and a percentile of outliers. An example the median could be at 300ms at P99 (99/100) under 1s. Tail latencies, or high percentiles of response time, impact directly user experience and cost money. Distributed data \u00b6 Adopting microservice architecture, means distributed systems and distributed data. The main motivations for that are scalability (load data operations could not be supported by one server), high availability (by sharing the same processing between multiple machines), and reducing latency to distribute data close to the end users. Vertical scaling is still bounded by hardware resources, so at higher load we need to support horizontal scaling by adding more machines to the cluster or cross multiple clusters. When adding machines, we may want to adopt different techniques for data sharing: shared memory shared storage shared nothing: cpu, memory and disk are per node. Cluster manages node orchestration over network. This architecture brings new challenges. Compendium Designing data intensive application - Martin Kleppmann Assemble the team to support a data-driven project - author: Stacey Ronaghan The valuation of data - author: Neal Fishman Define business objectives - author: Neal Fishman Recognize the value of data - author: Neal Fishman Translate a business problem into an AI and Data Science solution - authors: Tommy Eunice, Edd Biddle, Paul Christensen Prepare your data for AI and data science - authors: Edd Biddle, Paul Christensen Define your data strategy -authors: Beth Ackerman, Paul Christensen Normalize data to its atomic level - author: Neal Fishman Understand data needs to support AI and Data Science solutions - authors: Tommy Eunice, Edd Biddle, Paul Christensen Run thought experiments by using hypothesis-driven analysis - author: Edd Biddle, Paul Christensen Deliver a singular data function - author: Neal Fishman Construct your data topology - authors: Neal Fishman, Paul Christensen Build your data lake design - author: Paul Christensen Put AI and data science to work in your organization - authors: Edd Biddle, Paul Christensen Look behind the curtain of AI - author: Edd Biddle Select and develop an AI and data science model - author: Edd Biddle Enhance and optimize your AI and data science models - author: Edd Biddle Establish data governance - author: Neal Fishman Deploy an AI model - author: Sujatha Perepa","title":"Data Intensive App"},{"location":"patterns/data-pipeline/#context","text":"A typical modern business solution will include a set of microservices working together in choreography to exchange data. The adoption of event-driven microservices, with all the related design patterns, is described in separate articles that you can read here . When zooming to a particular data intensive microservice we will find a set of important data centric features that may look like in the diagram below, which presents one component of a bigger distributed system. The services involved include: API to define the service contract: OpenAPI or AsynchAPI Databases to store data for long term - document oriented or SQL based Caches to speed up retrieving data for expensive queries Search indexes to support search on a corpus Stream processing to pub/sub messages, which are now also considered as long duration datastore (Kafka). Interactive queries on top of data streams, and aggregates. Message contract and schemas Unified data service which includes big data storage, but also data catalog and queries on data at rest.","title":"Context"},{"location":"patterns/data-pipeline/#subjects-of-concern","text":"When designing such application we need to address a set of important subjects: What is the type of workload on the database? Read heavy, global access query, write heavy or balanced. Expected throughtput? Is there any fluctuation during the day? does it need to scale over time? How much data to store? what will be the pattern for this size (Always grow)? How content is access via which expected security and access control? What will be the expected durability? Week or forever? Expected latency and number of concurrent users? How the data model is modeled, does it need to support relationship integrity? Join queries? Structured or semi-structured content? Do we need strong schema, or more flexible one? Do we need search on unstructured data? NoSql? How to ensure data correctness and completeness? How to address good performance when exposing data, even when app is running slowly? How to scale and address increase in transaction volume and data size? What data to expose to other services via messaging ? Which formats? What data to expose to other services via APIs ? How to support application reliability when some components are not performing within their SLA? How to be fault-tolerant? How to test fault-tolerance? How does adding horizontal compute power impact the data access? How to support disaster recovery? In modern big data applications, hardware redundancy is not suffisant, the design needs to support unexpected faults, to avoid cascading failures or to support new version deployment with rolling-upgrade capability. When addressing scalability and load growth, we need to define the load parameters: number of transactions per second, the number of read and write operations, the number of active sessions, ... on average and at peak. Each microservice in a solution will have its own load characteristics. From there, we need to address the following issues: How does load growth impact performance while keeping existing compute resources? What is the increase of compute resource needed to support same performance while load growth? The solution problem is a combination of different characteristics to address: read volume, write volume, data store volume, data complexity and size, response time, access logic... For batch processing the measurement is the throughput: number of records per second or time to process n records. For real-time processing the response time measures the time to get a response from a client's point of view after sending a request. When defining service level agreement, it is important to use the median response time and a percentile of outliers. An example the median could be at 300ms at P99 (99/100) under 1s. Tail latencies, or high percentiles of response time, impact directly user experience and cost money.","title":"Subjects of concern"},{"location":"patterns/data-pipeline/#distributed-data","text":"Adopting microservice architecture, means distributed systems and distributed data. The main motivations for that are scalability (load data operations could not be supported by one server), high availability (by sharing the same processing between multiple machines), and reducing latency to distribute data close to the end users. Vertical scaling is still bounded by hardware resources, so at higher load we need to support horizontal scaling by adding more machines to the cluster or cross multiple clusters. When adding machines, we may want to adopt different techniques for data sharing: shared memory shared storage shared nothing: cpu, memory and disk are per node. Cluster manages node orchestration over network. This architecture brings new challenges. Compendium Designing data intensive application - Martin Kleppmann Assemble the team to support a data-driven project - author: Stacey Ronaghan The valuation of data - author: Neal Fishman Define business objectives - author: Neal Fishman Recognize the value of data - author: Neal Fishman Translate a business problem into an AI and Data Science solution - authors: Tommy Eunice, Edd Biddle, Paul Christensen Prepare your data for AI and data science - authors: Edd Biddle, Paul Christensen Define your data strategy -authors: Beth Ackerman, Paul Christensen Normalize data to its atomic level - author: Neal Fishman Understand data needs to support AI and Data Science solutions - authors: Tommy Eunice, Edd Biddle, Paul Christensen Run thought experiments by using hypothesis-driven analysis - author: Edd Biddle, Paul Christensen Deliver a singular data function - author: Neal Fishman Construct your data topology - authors: Neal Fishman, Paul Christensen Build your data lake design - author: Paul Christensen Put AI and data science to work in your organization - authors: Edd Biddle, Paul Christensen Look behind the curtain of AI - author: Edd Biddle Select and develop an AI and data science model - author: Edd Biddle Enhance and optimize your AI and data science models - author: Edd Biddle Establish data governance - author: Neal Fishman Deploy an AI model - author: Sujatha Perepa","title":"Distributed data"},{"location":"patterns/dlq/","text":"Dead Letter Queue \u00b6 Event reprocessing with dead letter pattern \u00b6 With event driven microservice, it is not just about pub/sub: there are use cases where the microservice needs to call existing service via an HTTP or RPC call. The call may fail. So what should be the processing to be done to retry and gracefully fail by leveraging the power of topics and the concept of dead letter. This pattern is influenced by the adoption of Kafka as event backbone and the offset management offered by Kafka. Once a message is read from a Kafka topic by a consumer the offset can be automatically committed so the consumer can poll the next batch of events, or in most the case manually committed, to support business logic to process those events. The figure below demonstrates the problem to address: the reefer management microservice get order event to process by allocating a reefer container to the order. The call to update the container inventory fails. At first glance, the approach is to commit the offset only when the three internal operations are succesful: write to reefer database (2), update the container inventory using legacy application service (3), and produce new event to orders (4) and containers (5) topics. Step (4) and (5) will not be done as no response from (3) happened. In fact a better approach is to commit the read offset on the orders topic, and then starts the processing: the goal is to do not impact the input throughput. In case of step (2) or (3) fails the order data is published to an order-retries topic (4) with some added metadata like number of retries and timestamp. A new order retry service or function consumes the order retry events (5) and do a new call to the remote service using a delay according to the number of retries already done: this is to pace the calls to a service that has issue for longer time. If the call (6) fails this function creates a new event in the order-retries topic with a retry counter increased by one. If the number of retry reaches a certain threshold then the order is sent to order-dead-letter topic (7) for human to work on. A CLI could read from this dead letter topic to deal with the data, or retry automatically once we know the backend service works. Using this approach we delay the call to the remote service without putting too much preassure on it. We have implemented the dead letter pattern when integrating the container manager microservice with an external BPM end point. The integration test detail is in this note and the integration test here . For more detail we recommend this article from Uber engineering: Building Reliable Reprocessing and Dead Letter Queues with Apache Kafka .","title":"Dead Letter Queue"},{"location":"patterns/dlq/#dead-letter-queue","text":"","title":"Dead Letter Queue"},{"location":"patterns/dlq/#event-reprocessing-with-dead-letter-pattern","text":"With event driven microservice, it is not just about pub/sub: there are use cases where the microservice needs to call existing service via an HTTP or RPC call. The call may fail. So what should be the processing to be done to retry and gracefully fail by leveraging the power of topics and the concept of dead letter. This pattern is influenced by the adoption of Kafka as event backbone and the offset management offered by Kafka. Once a message is read from a Kafka topic by a consumer the offset can be automatically committed so the consumer can poll the next batch of events, or in most the case manually committed, to support business logic to process those events. The figure below demonstrates the problem to address: the reefer management microservice get order event to process by allocating a reefer container to the order. The call to update the container inventory fails. At first glance, the approach is to commit the offset only when the three internal operations are succesful: write to reefer database (2), update the container inventory using legacy application service (3), and produce new event to orders (4) and containers (5) topics. Step (4) and (5) will not be done as no response from (3) happened. In fact a better approach is to commit the read offset on the orders topic, and then starts the processing: the goal is to do not impact the input throughput. In case of step (2) or (3) fails the order data is published to an order-retries topic (4) with some added metadata like number of retries and timestamp. A new order retry service or function consumes the order retry events (5) and do a new call to the remote service using a delay according to the number of retries already done: this is to pace the calls to a service that has issue for longer time. If the call (6) fails this function creates a new event in the order-retries topic with a retry counter increased by one. If the number of retry reaches a certain threshold then the order is sent to order-dead-letter topic (7) for human to work on. A CLI could read from this dead letter topic to deal with the data, or retry automatically once we know the backend service works. Using this approach we delay the call to the remote service without putting too much preassure on it. We have implemented the dead letter pattern when integrating the container manager microservice with an external BPM end point. The integration test detail is in this note and the integration test here . For more detail we recommend this article from Uber engineering: Building Reliable Reprocessing and Dead Letter Queues with Apache Kafka .","title":"Event reprocessing with dead letter pattern"},{"location":"patterns/event-sourcing/","text":"Event Sourcing \u00b6 Problems and Constraints \u00b6 Most business applications are state based persistent where any update changes the previous state of business entities. The database keeps the last committed update. But some business application needs to explain how it reaches its current state . For that the application needs to keep history of business facts. Traditional domain oriented implementation builds a domain data model and map it to a RDBMS. As an example, in the simple Order model below, the database record will keep the last state of the Order entity, with the different addresses and the last ordered items in separate tables. If you need to implement a query that looks at what happened to the order over a time period, you need to change the model and add historical records, basically building a log table. Designing a service to manage the life cycle of this order will, most of the time, add a \"delete operation\" to remove data. For legal reason, most businesses do not remove data. As an example, a business ledger has to include new record(s) to compensate a previous transaction. There is no erasing of previously logged transactions. It is always possible to understand what was done in the past. Most business application needs to keep this capability. Solution and Pattern \u00b6 Event sourcing persists the state of a business entity, such an Order, as a sequence of state-changing events or immuttable \"facts\" ordered over time. Event sourcing has its roots in the domain-driven design community. When the state of a system changes, an application issues a notification event of the state change. Any interested parties can become consumers of the event and take required actions. The state-change event is immutable stored in an event log or event store in time order. The event log becomes the principal source of truth. The system state can be recreated from a point in time by reprocessing the events. The history of state changes becomes an audit record for the business and is often a useful source of data for business analysts to gain insights into the business. You can see the \"removing an item\" event in the log is a new event. With this capability, we can count how often a specific product is removed for the shopping cart. In some cases, the event sourcing pattern is implemented completely within the event backbone. Kafka topic and partitions are the building blocks for event sourcing. However, you can also consider implementing the pattern with an external event store, which provides optimizations for how the data may be accessed and used. For example IBM Db2 Event store can provide the handlers and event store connected to the backbone and can provide optimization for down stream analytical processing of the data. An event store needs to store only three pieces of information: The type of event or aggregate. The sequence number of the event. The data as a serialized entity. More data can be added to help with diagnosis and audit, but the core functionality only requires a narrow set of fields. This gives rise to a very simple data design that can be heavily optimized for appending and retrieving sequences of records. With a central event logs, as provides by Kafka, producers append events to the log, and consumers read them from an offset (a sequence number). To get the final state of an entity, the consumer needs to replay all the events, which means replaying the changes to the state from the last committed offset or from the last snapshot or the origin of \"time\". Advantages \u00b6 The main goal is to be able to understand what happens to a business entity over time. But there are a set of interesting things that can be done: We can rebuild the data view within a microservice after it crashes, by reloading the event log. As events are ordered with time, we can apply complex event processing with temporal queries, time window operations, and looking at non-event. Be able to reverse the state and correct data with new events. Considerations \u00b6 When replaying the events, it may be important to avoid generating side effects. A common side effect is to send a notification on state change to other consumers. So the consumer of events need to be adapted to the query and business requirements. For example, if the code needs to answer to the question: \"what happened to the order ID 75 over time?\" then there is no side effect, only a report can be created each time the consumer runs. Sometime it may be too long to replay hundreds of events. In that case we can use snapshot, to capture the current state of an entity, and then replay events from the most recent snapshot. This is an optimization technique not needed for all event sourcing implementations. When state change events are in low volume there is no need for snapshots. Kafka is supporting the event sourcing pattern with the topic and partition . In our reference implementation we are validating event sourcing with Kafka in the Order microservices and demonstrated in this set of test cases. The event sourcing pattern is well described in this article on microservices.io . It is a very important pattern to support eventual data consistency between microservices and for data synchronization between system as the event store becomes the source of truth. See also this event sourcing article from Martin Fowler, where he is also using ship movement examples. Our implementation differs as we are using Kafka topic as event store and use different entities to support the container shipping process: the Order, the Container and Voyage entities... Another common use case, where event sourcing helps, is when developers push a new code version that corrupts the data: being able to see what was done on the data, and being able to reload from a previous state helps fixing problems. Command sourcing \u00b6 Command sourcing is a similar pattern as the event sourcing one, but the commands that modify the states are persisted instead of the events. This allows commands to be processed asynchronously, which can be relevant when the command execution takes a lot of time. One derived challenge is that the command may be executed multiple times, especially in case of failure. Therefore, it has to be idempotent ( making multiple identical requests has the same effect as making a single request). Finally, there is a need also to perform validation of the command to avoid keeping wrong commands in queue. For example, AddItem command is becoming AddItemValidated , then once persisted to a database it becomes an event as ItemAdded . So mixing command and event sourcing is a common practice. Business transactions are not ACID and span multiple services, they are more a serie of steps, each step is supported by a microservice responsible to update its own entity. We talk about \"eventual data consistency\". The event backbone needs to guarantee that events are delivered at least once and the microservices are responsible to manage their offset from the stream source and deal with inconsistency, by detecting duplicate events. At the microservice level, updating data and emitting event needs to be an atomic operation, to avoid inconsistency if the service crashes after the update to the datasource and before emitting the event. This can be done with an eventTable added to the microservice datasource and an event publisher that reads this table on a regular basis and change the state of the event once published. Another solution is to have a database transaction log reader or miner responsible to publish event on new row added to the log. One other approach to avoid the two-phase commit and inconsistency is to use an Event Store or Event Sourcing pattern to keep track of what is done on the business entity with enough information to rebuild the data state. Events are becoming facts describing state changes done on the business entity. Code repository \u00b6 All the microservices implementing the Reefer management solution is using event sourcing, as we use kafka with long persistence. The order management service is using CQRS combined with event sourcing: https://github.com/ibm-cloud-architecture/refarch-kc-order-ms and an integration test validate the pattern here . Compendium \u00b6 Martin Fowler - event sourcing pattern Microservice design pattern from Chris Richardson Greg Young video on event sourcing at the goto; conference","title":"Event Sourcing"},{"location":"patterns/event-sourcing/#event-sourcing","text":"","title":"Event Sourcing"},{"location":"patterns/event-sourcing/#problems-and-constraints","text":"Most business applications are state based persistent where any update changes the previous state of business entities. The database keeps the last committed update. But some business application needs to explain how it reaches its current state . For that the application needs to keep history of business facts. Traditional domain oriented implementation builds a domain data model and map it to a RDBMS. As an example, in the simple Order model below, the database record will keep the last state of the Order entity, with the different addresses and the last ordered items in separate tables. If you need to implement a query that looks at what happened to the order over a time period, you need to change the model and add historical records, basically building a log table. Designing a service to manage the life cycle of this order will, most of the time, add a \"delete operation\" to remove data. For legal reason, most businesses do not remove data. As an example, a business ledger has to include new record(s) to compensate a previous transaction. There is no erasing of previously logged transactions. It is always possible to understand what was done in the past. Most business application needs to keep this capability.","title":"Problems and Constraints"},{"location":"patterns/event-sourcing/#solution-and-pattern","text":"Event sourcing persists the state of a business entity, such an Order, as a sequence of state-changing events or immuttable \"facts\" ordered over time. Event sourcing has its roots in the domain-driven design community. When the state of a system changes, an application issues a notification event of the state change. Any interested parties can become consumers of the event and take required actions. The state-change event is immutable stored in an event log or event store in time order. The event log becomes the principal source of truth. The system state can be recreated from a point in time by reprocessing the events. The history of state changes becomes an audit record for the business and is often a useful source of data for business analysts to gain insights into the business. You can see the \"removing an item\" event in the log is a new event. With this capability, we can count how often a specific product is removed for the shopping cart. In some cases, the event sourcing pattern is implemented completely within the event backbone. Kafka topic and partitions are the building blocks for event sourcing. However, you can also consider implementing the pattern with an external event store, which provides optimizations for how the data may be accessed and used. For example IBM Db2 Event store can provide the handlers and event store connected to the backbone and can provide optimization for down stream analytical processing of the data. An event store needs to store only three pieces of information: The type of event or aggregate. The sequence number of the event. The data as a serialized entity. More data can be added to help with diagnosis and audit, but the core functionality only requires a narrow set of fields. This gives rise to a very simple data design that can be heavily optimized for appending and retrieving sequences of records. With a central event logs, as provides by Kafka, producers append events to the log, and consumers read them from an offset (a sequence number). To get the final state of an entity, the consumer needs to replay all the events, which means replaying the changes to the state from the last committed offset or from the last snapshot or the origin of \"time\".","title":"Solution and Pattern"},{"location":"patterns/event-sourcing/#advantages","text":"The main goal is to be able to understand what happens to a business entity over time. But there are a set of interesting things that can be done: We can rebuild the data view within a microservice after it crashes, by reloading the event log. As events are ordered with time, we can apply complex event processing with temporal queries, time window operations, and looking at non-event. Be able to reverse the state and correct data with new events.","title":"Advantages"},{"location":"patterns/event-sourcing/#considerations","text":"When replaying the events, it may be important to avoid generating side effects. A common side effect is to send a notification on state change to other consumers. So the consumer of events need to be adapted to the query and business requirements. For example, if the code needs to answer to the question: \"what happened to the order ID 75 over time?\" then there is no side effect, only a report can be created each time the consumer runs. Sometime it may be too long to replay hundreds of events. In that case we can use snapshot, to capture the current state of an entity, and then replay events from the most recent snapshot. This is an optimization technique not needed for all event sourcing implementations. When state change events are in low volume there is no need for snapshots. Kafka is supporting the event sourcing pattern with the topic and partition . In our reference implementation we are validating event sourcing with Kafka in the Order microservices and demonstrated in this set of test cases. The event sourcing pattern is well described in this article on microservices.io . It is a very important pattern to support eventual data consistency between microservices and for data synchronization between system as the event store becomes the source of truth. See also this event sourcing article from Martin Fowler, where he is also using ship movement examples. Our implementation differs as we are using Kafka topic as event store and use different entities to support the container shipping process: the Order, the Container and Voyage entities... Another common use case, where event sourcing helps, is when developers push a new code version that corrupts the data: being able to see what was done on the data, and being able to reload from a previous state helps fixing problems.","title":"Considerations"},{"location":"patterns/event-sourcing/#command-sourcing","text":"Command sourcing is a similar pattern as the event sourcing one, but the commands that modify the states are persisted instead of the events. This allows commands to be processed asynchronously, which can be relevant when the command execution takes a lot of time. One derived challenge is that the command may be executed multiple times, especially in case of failure. Therefore, it has to be idempotent ( making multiple identical requests has the same effect as making a single request). Finally, there is a need also to perform validation of the command to avoid keeping wrong commands in queue. For example, AddItem command is becoming AddItemValidated , then once persisted to a database it becomes an event as ItemAdded . So mixing command and event sourcing is a common practice. Business transactions are not ACID and span multiple services, they are more a serie of steps, each step is supported by a microservice responsible to update its own entity. We talk about \"eventual data consistency\". The event backbone needs to guarantee that events are delivered at least once and the microservices are responsible to manage their offset from the stream source and deal with inconsistency, by detecting duplicate events. At the microservice level, updating data and emitting event needs to be an atomic operation, to avoid inconsistency if the service crashes after the update to the datasource and before emitting the event. This can be done with an eventTable added to the microservice datasource and an event publisher that reads this table on a regular basis and change the state of the event once published. Another solution is to have a database transaction log reader or miner responsible to publish event on new row added to the log. One other approach to avoid the two-phase commit and inconsistency is to use an Event Store or Event Sourcing pattern to keep track of what is done on the business entity with enough information to rebuild the data state. Events are becoming facts describing state changes done on the business entity.","title":"Command sourcing"},{"location":"patterns/event-sourcing/#code-repository","text":"All the microservices implementing the Reefer management solution is using event sourcing, as we use kafka with long persistence. The order management service is using CQRS combined with event sourcing: https://github.com/ibm-cloud-architecture/refarch-kc-order-ms and an integration test validate the pattern here .","title":"Code repository"},{"location":"patterns/event-sourcing/#compendium","text":"Martin Fowler - event sourcing pattern Microservice design pattern from Chris Richardson Greg Young video on event sourcing at the goto; conference","title":"Compendium"},{"location":"patterns/intro/","text":"Introduction \u00b6 In this set of articles, we will detail some of the most import event-driven patterns that can be utilised during your event-driven microservice implementation. Adopting messaging (Pub/Sub) as a microservice communication approach involves using, at least, the following patterns: Decompose by subdomain : The domain-driven design approach is useful to identify and classify business functions and the corresponding microservices that would be associated with them. With the event storming method, aggregates help to find those subdomains of responsibility. (Source Chris Richardson - Microservices Patterns) Database per service : Each service persists data privately and is accessible only via its API. Services are loosely coupled limiting impact to other services when schema changes occur in the database. The chosen database technology is driven by business requirements. (Source Chris Richardson - Microservices Patterns) The implementation of transactions that span multiple services is complex and enforces using the Saga pattern. Queries that span multiple entities are a challenge and CQRS represents an interesting solution. Strangler pattern : Used to incrementally migrate an existing, monolithic, application by replacing a set of features to a microservice but keep both running in parallel. Applying a domain driven design approach, you may strangle the application using bounded context. But then as soon as this pattern is applied, you need to assess the co-existence between existing bounded contexts and the new microservices. One of the challenges will be to define where the write and read operations occurs, and how data should be replicated between the contexts. This is where event driven architecture helps. Event sourcing : persists, to an append log, the states of a business entity, such as an Order, as a sequence of immutable state-changing events. Command Query Responsibility Segregation : helps to separate queries from commands and help to address queries with cross-microservice boundary. Saga pattern: Microservices publish events when something happens in the scope of their control like an update in the business entities they are responsible for. A microservice, interested in other business entities, subscribes to those events and it can update its own state and business entities on receipt of these events. Business entity keys need to be unique and immutable. Event reprocessing with dead letter : event driven microservices may have to call external services via a synchronous call. We need to process failure in order to get response from those services using event backbone. Transactional outbox : A service command typically needs to update the database and send messages/events. The approach is to use an outbox table to keep the message to sent and a message relay process to publish events inserted into database to the event backbone. (Source Chris Richardson - Microservices Patterns) Strangler pattern \u00b6 Problem \u00b6 How to migrate a monolithic application to a microservice based architecture without doing the huge effort of redeveloping the application from a blank slate. Replacing and rewriting an existing application can be a huge investment. Rewriting a subset of business functions while running current application in parallel may be relevant and reduce risk and velocity of changes. The figure below illustrates a typical mainframe application, with external Java based user interface connected to the mainframe via iop/corba and with three different applications to manage product, order and customer. Solution \u00b6 The approach is to use a \"strangler\" interface to dispatch a request to new or old features. Existing features to migrate are selected by trying to isolate sub components. One of main challenges is to isolate the data store and disover how the new microservices and the legacy application are accessing the shared data. Continuous data replication can be a solution to propagate write model to read model. Write model will most likely stays on the monolitic application, change data capture can be used, with event backbone to propagate change to read model. The facade needs to be scalable and not a single point of failure. It needs to support new APIs (RESTful) and old API (most likely SOAP). The following figure illustrates an implementation using an event driven solution with data replication to synchronize the write model to the read model on the mainframe. Transactional outbox \u00b6 When distributed transaction is not supported by the messaging middleware (like current Kafka version), it is important to ensure consistency between the records in the database and the events published. In the reference implementation we used the approach to publish to the topic as soon as an order is received via the API and then the same code, is consuming this events to persist to the database. With this approach if write to the topic operation fails, the application can return an error to the user, if the write operation to the database fails, the code can reload from the non-committed record. But there is another solution presented by the transactional outbox. For detailed information about this pattern see the documentation of the pattern in Chris Richardson's site: Transactional outbox . To summarize this pattern, the approach is to use an outbox table to keep the messages to sent and a message relay process to publish events inserted into database to the event backbone. In modern solution this relay is a change data capture agent. The following schema illustrates the approach: To get a concrete example of this pattern we have developed a deep dive lab using Quarkus Debezium outbox pattern and a DB2 database.","title":"Introduction"},{"location":"patterns/intro/#introduction","text":"In this set of articles, we will detail some of the most import event-driven patterns that can be utilised during your event-driven microservice implementation. Adopting messaging (Pub/Sub) as a microservice communication approach involves using, at least, the following patterns: Decompose by subdomain : The domain-driven design approach is useful to identify and classify business functions and the corresponding microservices that would be associated with them. With the event storming method, aggregates help to find those subdomains of responsibility. (Source Chris Richardson - Microservices Patterns) Database per service : Each service persists data privately and is accessible only via its API. Services are loosely coupled limiting impact to other services when schema changes occur in the database. The chosen database technology is driven by business requirements. (Source Chris Richardson - Microservices Patterns) The implementation of transactions that span multiple services is complex and enforces using the Saga pattern. Queries that span multiple entities are a challenge and CQRS represents an interesting solution. Strangler pattern : Used to incrementally migrate an existing, monolithic, application by replacing a set of features to a microservice but keep both running in parallel. Applying a domain driven design approach, you may strangle the application using bounded context. But then as soon as this pattern is applied, you need to assess the co-existence between existing bounded contexts and the new microservices. One of the challenges will be to define where the write and read operations occurs, and how data should be replicated between the contexts. This is where event driven architecture helps. Event sourcing : persists, to an append log, the states of a business entity, such as an Order, as a sequence of immutable state-changing events. Command Query Responsibility Segregation : helps to separate queries from commands and help to address queries with cross-microservice boundary. Saga pattern: Microservices publish events when something happens in the scope of their control like an update in the business entities they are responsible for. A microservice, interested in other business entities, subscribes to those events and it can update its own state and business entities on receipt of these events. Business entity keys need to be unique and immutable. Event reprocessing with dead letter : event driven microservices may have to call external services via a synchronous call. We need to process failure in order to get response from those services using event backbone. Transactional outbox : A service command typically needs to update the database and send messages/events. The approach is to use an outbox table to keep the message to sent and a message relay process to publish events inserted into database to the event backbone. (Source Chris Richardson - Microservices Patterns)","title":"Introduction"},{"location":"patterns/intro/#strangler-pattern","text":"","title":"Strangler pattern"},{"location":"patterns/intro/#problem","text":"How to migrate a monolithic application to a microservice based architecture without doing the huge effort of redeveloping the application from a blank slate. Replacing and rewriting an existing application can be a huge investment. Rewriting a subset of business functions while running current application in parallel may be relevant and reduce risk and velocity of changes. The figure below illustrates a typical mainframe application, with external Java based user interface connected to the mainframe via iop/corba and with three different applications to manage product, order and customer.","title":"Problem"},{"location":"patterns/intro/#solution","text":"The approach is to use a \"strangler\" interface to dispatch a request to new or old features. Existing features to migrate are selected by trying to isolate sub components. One of main challenges is to isolate the data store and disover how the new microservices and the legacy application are accessing the shared data. Continuous data replication can be a solution to propagate write model to read model. Write model will most likely stays on the monolitic application, change data capture can be used, with event backbone to propagate change to read model. The facade needs to be scalable and not a single point of failure. It needs to support new APIs (RESTful) and old API (most likely SOAP). The following figure illustrates an implementation using an event driven solution with data replication to synchronize the write model to the read model on the mainframe.","title":"Solution"},{"location":"patterns/intro/#transactional-outbox","text":"When distributed transaction is not supported by the messaging middleware (like current Kafka version), it is important to ensure consistency between the records in the database and the events published. In the reference implementation we used the approach to publish to the topic as soon as an order is received via the API and then the same code, is consuming this events to persist to the database. With this approach if write to the topic operation fails, the application can return an error to the user, if the write operation to the database fails, the code can reload from the non-committed record. But there is another solution presented by the transactional outbox. For detailed information about this pattern see the documentation of the pattern in Chris Richardson's site: Transactional outbox . To summarize this pattern, the approach is to use an outbox table to keep the messages to sent and a message relay process to publish events inserted into database to the event backbone. In modern solution this relay is a change data capture agent. The following schema illustrates the approach: To get a concrete example of this pattern we have developed a deep dive lab using Quarkus Debezium outbox pattern and a DB2 database.","title":"Transactional outbox"},{"location":"patterns/realtime-analytics/","text":"Realtime Analytics \u00b6 One of the essential elements of modern event-driven solutions is the ability to process continuous event streams to derive real time insights and intelligence. In this section we will take more detailed look at what this means in terms of required capabilities and the technology choices that are available to provide these as part of the Event Driven Architecture. Streaming analytics (near real-time analytics) \u00b6 Streaming analytics provides the capabilities to look into and understand the events flowing through unbounded real-time event streams. Streaming applications process the event flow and allow data and analytical functions to be applied to information in the stream. Streaming applications are written as multistep flows across the following capabilities: Ingest many sources of events. Prepare data by transforming, filtering, correlating, aggregating on some metrics and leveraging other data sources for data enrichment. Detect and predict event patterns using scoring and classification. Decide by applying business rules and business logic. Act by directly executing an action, or in event-driven systems publishing an event notification or command. Basic streaming analytics capabilities \u00b6 To support the near real-time analytical processing of the unbounded event streams, the following capabilities are essential to the event stream processing component: Continuous event ingestion and analytical processing. Processing across multiple event streams. Low latency processing, where data do not have to be stored. Processing of high-volume and high-velocity streams of data. Continuous query and analysis of the feed. Correlation across events and streams. Windowing and stateful processing. Query and analysis of stored data. Development and execution of data pipelines. Development and execution of analytics pipelines. Scoring of machine learning models in line in the near real-time event stream processing. Support for near real-time analytics and decision-making \u00b6 Beyond the basic capabilities, consider supporting other frequently-seen event stream types and processing capabilities in your event stream processing component. By creating functions for these stream types and processes in the streaming application code, you can simplify the problem and reduce the development time. These capabilities include the following: Geospatial Location-based analytics Geofencing & map matching Spatio-temporal hangout detection Time series analysis Timestamped data analysis Anomaly detection & forecasting Text analytics Natural Language Processing & Natural Language Understanding Sentiment analysis & entity extraction Video and audio Speech-to-text conversion Image recognition Rules Decisions described as business logic Complex Event Processing (CEP) Temporal pattern detection Entity Analytics Relationships between entities Probabilistic matching Application programming languages and standards \u00b6 Few standards exist for event stream applications and languages. Typically, streaming engines have provided language-specific programming models tied to a specific platform. The commonly used languages include the following: Python supports working with data and is popular with data scientists and data engineers. Java is the pervasive application development language. Kafka Streams offers a DSL to support most of the event streaming processing implementation. Scala adds functional programming and immutable objects to Java. Other platform specific languages have emerged when near real-time processing demands stringent performance requirements real time processing performance is required. More recently Google initiated the Apache Beam project https://beam.apache.org/ to provide a unified programming model for streaming analytics applications. Beam is a higher-level unified programming model that provides a standard way of writing streaming analytics applications in many supported languages, including Java, Python, Go and SQL. Streaming analytics engines typically support this unified programming model through a Beam runner that takes the code and converts it to platform-native executable code for the specific engine. See https://beam.apache.org/documentation/runners/capability-matrix/ for details of supporting engines and the capabilities. Leading engines include Google Cloud DataFlow, Apache Flink , Apache Spark, Apache Apex, and IBM Streams. Run time characteristics \u00b6 In operational terms streaming analytics engines must receive and analyze arriving data continuously: The \"Feed Never Ends\" The collection is unbounded. Not a request response set based model. The \"Firehose Doesn\u2019t Stop\" Keep drinking and keep up. The processing rate is greater than or equal to the feed rate. The analytics engine must be resilient and self-healing. These specialized demands and concerns, which are not found in many other information processing environments, have led to highly-optimized runtimes and engines for stateful, parallel processing of analytical workloads across multiple event streams. Products \u00b6 Streaming Analytics \u00b6 The market for streaming analytics products is quite confused with lots of different offering and very few standards to bring them together. The potential product selection list for the streaming analytics component in the event driven architecture would need to consider: Top Open Source projects: Flink - real time streaming engine, both real time and batch analytics in one tool. Spark Streaming - micro batch processing through spark engine. Storm - Has not shown enough adoption. Kafka Streams - new/emerging API access for processing event streams in Kafka using a graph of operators Major Cloud Platform Providers support: Google Cloud DataFlow \u2013 proprietary engine open source streams application language ( Beam ) Azure Stream Analytics \u2013 proprietary engine , SQL interface Amazon Kinesis - proprietary AWS IBM offerings IBM Streams/streaming Analytics (High performing parallel processing engine for real time analytics work loads) IBM Event streams (Kafka based event log/streaming platform) Evaluation of the various options, highlights The proprietary engines from the major providers, Google, MicroSoft, Amazon and IBM Streams continue to provide significant benefits in terms of performance and functionality for real time analysis of high volume realtime event streams. Kafka streams provides a convenient programming model for microservices to interact with the event stream data, but doesnt provide the optimized stream processing engine required for high volume real time analytics. Our decision for the Event Driven Architecture is to include: IBM streams as the performant, functionally rich real time event stream processing engine Event Streams ( Kafka Streams ), for manipulation of event streams within microservices IBM streams also supports Apache Beam as the open source Streams Application language, which would allow portability of streams applications across, Flink, Spark, Google DataFlow.. An anomaly detection in event-driven solutions \u00b6 We have implemented a separate solution based on the fresh food delivery to do anomaly detection for Refrigerated container in this repository with the following high level component view:","title":"Near real-time analytics"},{"location":"patterns/realtime-analytics/#realtime-analytics","text":"One of the essential elements of modern event-driven solutions is the ability to process continuous event streams to derive real time insights and intelligence. In this section we will take more detailed look at what this means in terms of required capabilities and the technology choices that are available to provide these as part of the Event Driven Architecture.","title":"Realtime Analytics"},{"location":"patterns/realtime-analytics/#streaming-analytics-near-real-time-analytics","text":"Streaming analytics provides the capabilities to look into and understand the events flowing through unbounded real-time event streams. Streaming applications process the event flow and allow data and analytical functions to be applied to information in the stream. Streaming applications are written as multistep flows across the following capabilities: Ingest many sources of events. Prepare data by transforming, filtering, correlating, aggregating on some metrics and leveraging other data sources for data enrichment. Detect and predict event patterns using scoring and classification. Decide by applying business rules and business logic. Act by directly executing an action, or in event-driven systems publishing an event notification or command.","title":"Streaming analytics (near real-time analytics)"},{"location":"patterns/realtime-analytics/#basic-streaming-analytics-capabilities","text":"To support the near real-time analytical processing of the unbounded event streams, the following capabilities are essential to the event stream processing component: Continuous event ingestion and analytical processing. Processing across multiple event streams. Low latency processing, where data do not have to be stored. Processing of high-volume and high-velocity streams of data. Continuous query and analysis of the feed. Correlation across events and streams. Windowing and stateful processing. Query and analysis of stored data. Development and execution of data pipelines. Development and execution of analytics pipelines. Scoring of machine learning models in line in the near real-time event stream processing.","title":"Basic streaming analytics capabilities"},{"location":"patterns/realtime-analytics/#support-for-near-real-time-analytics-and-decision-making","text":"Beyond the basic capabilities, consider supporting other frequently-seen event stream types and processing capabilities in your event stream processing component. By creating functions for these stream types and processes in the streaming application code, you can simplify the problem and reduce the development time. These capabilities include the following: Geospatial Location-based analytics Geofencing & map matching Spatio-temporal hangout detection Time series analysis Timestamped data analysis Anomaly detection & forecasting Text analytics Natural Language Processing & Natural Language Understanding Sentiment analysis & entity extraction Video and audio Speech-to-text conversion Image recognition Rules Decisions described as business logic Complex Event Processing (CEP) Temporal pattern detection Entity Analytics Relationships between entities Probabilistic matching","title":"Support for near real-time analytics and decision-making"},{"location":"patterns/realtime-analytics/#application-programming-languages-and-standards","text":"Few standards exist for event stream applications and languages. Typically, streaming engines have provided language-specific programming models tied to a specific platform. The commonly used languages include the following: Python supports working with data and is popular with data scientists and data engineers. Java is the pervasive application development language. Kafka Streams offers a DSL to support most of the event streaming processing implementation. Scala adds functional programming and immutable objects to Java. Other platform specific languages have emerged when near real-time processing demands stringent performance requirements real time processing performance is required. More recently Google initiated the Apache Beam project https://beam.apache.org/ to provide a unified programming model for streaming analytics applications. Beam is a higher-level unified programming model that provides a standard way of writing streaming analytics applications in many supported languages, including Java, Python, Go and SQL. Streaming analytics engines typically support this unified programming model through a Beam runner that takes the code and converts it to platform-native executable code for the specific engine. See https://beam.apache.org/documentation/runners/capability-matrix/ for details of supporting engines and the capabilities. Leading engines include Google Cloud DataFlow, Apache Flink , Apache Spark, Apache Apex, and IBM Streams.","title":"Application programming languages and standards"},{"location":"patterns/realtime-analytics/#run-time-characteristics","text":"In operational terms streaming analytics engines must receive and analyze arriving data continuously: The \"Feed Never Ends\" The collection is unbounded. Not a request response set based model. The \"Firehose Doesn\u2019t Stop\" Keep drinking and keep up. The processing rate is greater than or equal to the feed rate. The analytics engine must be resilient and self-healing. These specialized demands and concerns, which are not found in many other information processing environments, have led to highly-optimized runtimes and engines for stateful, parallel processing of analytical workloads across multiple event streams.","title":"Run time characteristics"},{"location":"patterns/realtime-analytics/#products","text":"","title":"Products"},{"location":"patterns/realtime-analytics/#streaming-analytics","text":"The market for streaming analytics products is quite confused with lots of different offering and very few standards to bring them together. The potential product selection list for the streaming analytics component in the event driven architecture would need to consider: Top Open Source projects: Flink - real time streaming engine, both real time and batch analytics in one tool. Spark Streaming - micro batch processing through spark engine. Storm - Has not shown enough adoption. Kafka Streams - new/emerging API access for processing event streams in Kafka using a graph of operators Major Cloud Platform Providers support: Google Cloud DataFlow \u2013 proprietary engine open source streams application language ( Beam ) Azure Stream Analytics \u2013 proprietary engine , SQL interface Amazon Kinesis - proprietary AWS IBM offerings IBM Streams/streaming Analytics (High performing parallel processing engine for real time analytics work loads) IBM Event streams (Kafka based event log/streaming platform) Evaluation of the various options, highlights The proprietary engines from the major providers, Google, MicroSoft, Amazon and IBM Streams continue to provide significant benefits in terms of performance and functionality for real time analysis of high volume realtime event streams. Kafka streams provides a convenient programming model for microservices to interact with the event stream data, but doesnt provide the optimized stream processing engine required for high volume real time analytics. Our decision for the Event Driven Architecture is to include: IBM streams as the performant, functionally rich real time event stream processing engine Event Streams ( Kafka Streams ), for manipulation of event streams within microservices IBM streams also supports Apache Beam as the open source Streams Application language, which would allow portability of streams applications across, Flink, Spark, Google DataFlow..","title":"Streaming Analytics"},{"location":"patterns/realtime-analytics/#an-anomaly-detection-in-event-driven-solutions","text":"We have implemented a separate solution based on the fresh food delivery to do anomaly detection for Refrigerated container in this repository with the following high level component view:","title":"An anomaly detection in event-driven solutions"},{"location":"patterns/saga/","text":"Saga \u00b6 Updated 06/18/2022 Problems and Constraints \u00b6 With the adoption of one data source per microservice, there is an interesting challenge on how to support long running transaction cross microservices. With event backbone two phase commit is not an option. Solution and Pattern \u00b6 Introduced in 1987 by Hector Garcaa-Molrna Kenneth Salem paper the Saga pattern helps to support a long running transaction that can be broken up to a collection of sub transactions that can be interleaved any way with other transactions. With microservice each transaction updates data within a single service, each subsequent steps may be triggered by previous completion. The following figure, based on our solution implementation , illustrates those concepts for an order transaction: When the order is created, the business process says, we need to allocate a \"voyage\" (a Vessel container carrier), assign refrigerator containers and update the list of containers to load on the ship. Those actions / commands are chained. The final state (in this schema, not in the reality, as the process has more steps) is the Order assigned state in the order microservice. With a unique application implementation, the integrity between order, voyage and container tables will be done via transactions. With distributed system we could not easily apply two phase commit transaction so the Saga pattern will help. SAGA pattern supports two types of implementation: Choreography and Orchestration. Services choreography \u00b6 With Choreography each service produces and listens to other service\u2019s events and decides if an action should be taken or not. The first service executes a transaction to its own data store and then publishes an event ( OrderCreated event (1)) as fact about its business entity update. It maintains the business entity status, (order.status) to the Pending state until the saga is completed. This event is listened by one or more services which execute local transactions and publish new events (VoyageAllocated (3), ReeferAssigned (4), PaymentProcessed (5)). The distributed transaction ends when the last service executes its local transaction or when a service does not publish any events or the event published is not polled by any of the saga\u2019s participants. For example, the Order microservice gets all the events from the other services and changed the Order state to be Accepted . In case of failure, the source microservice is keeping state and timer to monitor for the expected completion events. When a message from any service is missing, the source service, needs to trigger a compensation process: Rolling back a distributed transaction does not come for free. Normally you have to implement another operation/transaction to compensate for what has been done before. This will be a new event sent by the service responsible of the transaction integrity. In the order example, in the rare case where one of the service is not able to provide a positive response, no voyage found, or no Reefer container found, then the order needs to change to 'Uncompleted' status, and an event to the orders topic will claim the orderID is now uncompleted (OrderUncompleted event Step 1 above) . Any service that has something allocated for this orderId will 'unroll' their changes in their own data source (Steps 2,3,4 below). Also it is important to note, that if one of the service is taking time to answer this may not be a problem as the order is in pending state. If the business requirement stipulates to address an order within a small time period then the compensation process may start. Uncompleted orders can be reviewed by a business user for manual handling. Email can be automatically sent to the customer about issue related to his order. There are a lot of different ways to handle order issue at the business level. See the eda-saga-choreography repository for our last implementation based on Quarkus, and IBM Event Streams. Services orchestration \u00b6 With orchestration, one service is responsible to drive each participant on what to do and when. As we do not want to loose any message as part of this orchestration the technology of choice to support strong consistency and exactly once delivery, is to use IBM MQ, as illustrated by the following figure: An example of Saga implementation using MQ is described in this repository and the orchestration implemenation with MQ is in the eda-kc-order-cmd-mq repo . An alternate approach is to use Kafka with producer using full acknowledge, idempotence, and a batch size of 1, and different topics mostly configured as queue: one consumer in each consumer group, manual commit, poll one message at a time. It uses the different topics to control the saga by issuing event commands to the different service. It uses the event backbone as a queue processing to support the asynchronous invocations. In this case the event should be exactly once delivered and idempotent. Each participant produces response in their context and to the order topic. The orchestration layer needs to keep a state machine and acts once all the expected responses are received. If anything fails, the orchestrator is also responsible for coordinating the compensation process by sending rollback events with orderID and their respective impacted entity key (voyageID, reeferID, transactionID). Each participant will undo its previous operations. Orchestrator is a State Machine where each transformation corresponds to a command or message. See also this article from Chris Richardson on the Saga pattern. We have implemented the choreography saga pattern in the order management, voyage and refeer management microservices within the EDA reference implementation solution with a detailed explanation of the integration tests to validate the happy path , and the exception path with compensation. Repositories to demonstrate the Saga patterns \u00b6 eda-saga-choreography repository Order service Reefer service Voyage service eda-saga-orchestration repository includes subfolders with the 3 services.","title":"Saga"},{"location":"patterns/saga/#saga","text":"Updated 06/18/2022","title":"Saga"},{"location":"patterns/saga/#problems-and-constraints","text":"With the adoption of one data source per microservice, there is an interesting challenge on how to support long running transaction cross microservices. With event backbone two phase commit is not an option.","title":"Problems and Constraints"},{"location":"patterns/saga/#solution-and-pattern","text":"Introduced in 1987 by Hector Garcaa-Molrna Kenneth Salem paper the Saga pattern helps to support a long running transaction that can be broken up to a collection of sub transactions that can be interleaved any way with other transactions. With microservice each transaction updates data within a single service, each subsequent steps may be triggered by previous completion. The following figure, based on our solution implementation , illustrates those concepts for an order transaction: When the order is created, the business process says, we need to allocate a \"voyage\" (a Vessel container carrier), assign refrigerator containers and update the list of containers to load on the ship. Those actions / commands are chained. The final state (in this schema, not in the reality, as the process has more steps) is the Order assigned state in the order microservice. With a unique application implementation, the integrity between order, voyage and container tables will be done via transactions. With distributed system we could not easily apply two phase commit transaction so the Saga pattern will help. SAGA pattern supports two types of implementation: Choreography and Orchestration.","title":"Solution and Pattern"},{"location":"patterns/saga/#services-choreography","text":"With Choreography each service produces and listens to other service\u2019s events and decides if an action should be taken or not. The first service executes a transaction to its own data store and then publishes an event ( OrderCreated event (1)) as fact about its business entity update. It maintains the business entity status, (order.status) to the Pending state until the saga is completed. This event is listened by one or more services which execute local transactions and publish new events (VoyageAllocated (3), ReeferAssigned (4), PaymentProcessed (5)). The distributed transaction ends when the last service executes its local transaction or when a service does not publish any events or the event published is not polled by any of the saga\u2019s participants. For example, the Order microservice gets all the events from the other services and changed the Order state to be Accepted . In case of failure, the source microservice is keeping state and timer to monitor for the expected completion events. When a message from any service is missing, the source service, needs to trigger a compensation process: Rolling back a distributed transaction does not come for free. Normally you have to implement another operation/transaction to compensate for what has been done before. This will be a new event sent by the service responsible of the transaction integrity. In the order example, in the rare case where one of the service is not able to provide a positive response, no voyage found, or no Reefer container found, then the order needs to change to 'Uncompleted' status, and an event to the orders topic will claim the orderID is now uncompleted (OrderUncompleted event Step 1 above) . Any service that has something allocated for this orderId will 'unroll' their changes in their own data source (Steps 2,3,4 below). Also it is important to note, that if one of the service is taking time to answer this may not be a problem as the order is in pending state. If the business requirement stipulates to address an order within a small time period then the compensation process may start. Uncompleted orders can be reviewed by a business user for manual handling. Email can be automatically sent to the customer about issue related to his order. There are a lot of different ways to handle order issue at the business level. See the eda-saga-choreography repository for our last implementation based on Quarkus, and IBM Event Streams.","title":"Services choreography"},{"location":"patterns/saga/#services-orchestration","text":"With orchestration, one service is responsible to drive each participant on what to do and when. As we do not want to loose any message as part of this orchestration the technology of choice to support strong consistency and exactly once delivery, is to use IBM MQ, as illustrated by the following figure: An example of Saga implementation using MQ is described in this repository and the orchestration implemenation with MQ is in the eda-kc-order-cmd-mq repo . An alternate approach is to use Kafka with producer using full acknowledge, idempotence, and a batch size of 1, and different topics mostly configured as queue: one consumer in each consumer group, manual commit, poll one message at a time. It uses the different topics to control the saga by issuing event commands to the different service. It uses the event backbone as a queue processing to support the asynchronous invocations. In this case the event should be exactly once delivered and idempotent. Each participant produces response in their context and to the order topic. The orchestration layer needs to keep a state machine and acts once all the expected responses are received. If anything fails, the orchestrator is also responsible for coordinating the compensation process by sending rollback events with orderID and their respective impacted entity key (voyageID, reeferID, transactionID). Each participant will undo its previous operations. Orchestrator is a State Machine where each transformation corresponds to a command or message. See also this article from Chris Richardson on the Saga pattern. We have implemented the choreography saga pattern in the order management, voyage and refeer management microservices within the EDA reference implementation solution with a detailed explanation of the integration tests to validate the happy path , and the exception path with compensation.","title":"Services orchestration"},{"location":"patterns/saga/#repositories-to-demonstrate-the-saga-patterns","text":"eda-saga-choreography repository Order service Reefer service Voyage service eda-saga-orchestration repository includes subfolders with the 3 services.","title":"Repositories to demonstrate the Saga patterns"},{"location":"patterns/topic-replication/","text":"Topic Replication \u00b6 As an introduction to the scope of the data replication in the context of distributed system, we encourage to read our summary in this article . In this section we are going to address the data replication in the context of Kafka clusters, topic mirroring, using Kafka Mirror Maker 2. Problem statement \u00b6 We suppose we need to replicate data in Kafka topics between different Kafka clusters running in different availability zones or data centers. There are multiple motivations for such replication. We can list at least the followings: Support disaster recovery, using different data centers to replicate microservice generated data in an active - passive mode. Move data closer to end user to improve performance and latency, this is more an active - active model, where the same microservices are deployed on the different data centers. The need to build on-premise data aggregation or data lake layer to perform batch analytics jobs from data gathered from different remote environments. Isolate secure data from on-premise cluster, with data encryption and data transformation to remove personal identifiable information, but still exchange such data between environments. Produce to two data centers \u00b6 The simplest way to ensure data replication is to have producers writing to two data centers. When using Kafka the producer code needs to connect to two set brokers and write to the two target topics. The diagram below illustrates this approach: The problem of this approach is the code needs to write in two clusters and address connectivity issue, and dual writes that may not happen in cohesive way. An alternate is to have the layer on top of the producers doing the replication itself so producers on both data centers are getting the same source of data and can inject on their related topic: In this approach the producer code is classical and less intruisive to the type of deployment. Data mirroring \u00b6 Data mirroring is the technology to consume from one topic and publish to a remote topic, transparently to the core business application. This is an asynchronous mechanism, leveraging the append log mechanism from Kafka. MirrorMaker 2.0 is a new feature as part of Kafka 2.4 to support data replication between clusters. It is based on Kafka Connect framework, and it supports data and metadata replication, like the topic configuration, the offset and the consumer checkpoints are synchronously replicated to the target cluster. We have done extensive testing with MirrorMaker 2 with two different environments: One running a local, on-premise cluster using Kafka 2.5 open source packaging, like Strimzi vanilla Kafka, and IBM Event Streams on Cloud. One Cloud Pak for Integration Event Streams deployment to IBM Event Streams on Cloud - managed service. In the figure above, Mirror Maker 2.0 is used to do bi-directional replication between topics defined in both clusters. The grey topic is replicated from right to left. The light blue topic is replicated from left to right. Microservices on both data centers can consume messages from both topics. Then we want to address two main scenarios: Active - passive , which addresses a disaster recovery approach where consumers reconnect to a new cluster after the first one fails. Active - active deployments where participant clusters have producers and consumers and some topics are replicated so the messages are processed by different consumers on the two data centers. We address the active active scenarios for the two test environments in a separate note Active - Passive \u00b6 To support active - passive, one site has producers and consumers on local topics, and topic data are replicated to the passive cluster without online consumers. The tricky parts for this approach is the retention period in each topic, and then the backend systems impacted by the recovery. So the data replication from active - passive, embraces a larger scope than just topic mirroring. Still knowing how MirrorMaker 2 is doing offset replication is important. MirroMaker 2 tracks offset per consumer group. There are two topics created on the target cluster to manage the offset mapping between the source and target clusters and the checkpoints of the last committed offset in the source topic/partitions/consumer group. When a producer sends its record it gets the offset in the partition the record was saved. In the diagram below we have a source topic A - partition 1 with the last write offset done by a producer to be 5, and the last read committed offset by the consumer assigned to partition 1 being 3. The last replicated offset 3 is mapped as 12 in the target partition on the remote topic. offset # do not match between mirrored partitions. So if the consumer needs to reconnect to the target cluster, it will read from the last committed offset which is 12 in this environment. This information is saved in the checkpoint topic. Offset synch are emitted at the beginning of the replication and when there is a situation which leads that the numbering sequencing diverges. For example the normal behavior increases the offset by one 2,3,4,5,6,7 which is mapped to the offsets 12,13,14,15,16,... If the write operation for offset 20 at the source is a 17 on the target then MM 2 emits a new offset synch records to the offset-synch topic, so the offset replication is not done all the time, only when diverging to improve performance. The checkpoint and offset_synch topics enable replication to be fully restored from the correct offset position on failover. Active - Active \u00b6 For active/ active producers are writing to their topics locally to the local cluster on both data centers. The code is the same, and deployed in both environments. The main consideration is what is their own datasource. If for example the datasource is a click stream data from webapp running locally with different end users, then the local consumer may do something specific for this data center, that the consumer on the other side do not need to know and process. So the consumers that really want to consume from local topic and replicated one, have very special requirements, for example, computing aggregates and joins. In this case the processing is active - active with different semantic. The other example is when we want to have the same data eventually consistent on the backend, the consumer are the same, writing to the same backend system, on both side, but consuming from two different topics. The following diagram illustrates this approach. The data in both backend will be eventual consistent overtime.","title":"Topic Replication"},{"location":"patterns/topic-replication/#topic-replication","text":"As an introduction to the scope of the data replication in the context of distributed system, we encourage to read our summary in this article . In this section we are going to address the data replication in the context of Kafka clusters, topic mirroring, using Kafka Mirror Maker 2.","title":"Topic Replication"},{"location":"patterns/topic-replication/#problem-statement","text":"We suppose we need to replicate data in Kafka topics between different Kafka clusters running in different availability zones or data centers. There are multiple motivations for such replication. We can list at least the followings: Support disaster recovery, using different data centers to replicate microservice generated data in an active - passive mode. Move data closer to end user to improve performance and latency, this is more an active - active model, where the same microservices are deployed on the different data centers. The need to build on-premise data aggregation or data lake layer to perform batch analytics jobs from data gathered from different remote environments. Isolate secure data from on-premise cluster, with data encryption and data transformation to remove personal identifiable information, but still exchange such data between environments.","title":"Problem statement"},{"location":"patterns/topic-replication/#produce-to-two-data-centers","text":"The simplest way to ensure data replication is to have producers writing to two data centers. When using Kafka the producer code needs to connect to two set brokers and write to the two target topics. The diagram below illustrates this approach: The problem of this approach is the code needs to write in two clusters and address connectivity issue, and dual writes that may not happen in cohesive way. An alternate is to have the layer on top of the producers doing the replication itself so producers on both data centers are getting the same source of data and can inject on their related topic: In this approach the producer code is classical and less intruisive to the type of deployment.","title":"Produce to two data centers"},{"location":"patterns/topic-replication/#data-mirroring","text":"Data mirroring is the technology to consume from one topic and publish to a remote topic, transparently to the core business application. This is an asynchronous mechanism, leveraging the append log mechanism from Kafka. MirrorMaker 2.0 is a new feature as part of Kafka 2.4 to support data replication between clusters. It is based on Kafka Connect framework, and it supports data and metadata replication, like the topic configuration, the offset and the consumer checkpoints are synchronously replicated to the target cluster. We have done extensive testing with MirrorMaker 2 with two different environments: One running a local, on-premise cluster using Kafka 2.5 open source packaging, like Strimzi vanilla Kafka, and IBM Event Streams on Cloud. One Cloud Pak for Integration Event Streams deployment to IBM Event Streams on Cloud - managed service. In the figure above, Mirror Maker 2.0 is used to do bi-directional replication between topics defined in both clusters. The grey topic is replicated from right to left. The light blue topic is replicated from left to right. Microservices on both data centers can consume messages from both topics. Then we want to address two main scenarios: Active - passive , which addresses a disaster recovery approach where consumers reconnect to a new cluster after the first one fails. Active - active deployments where participant clusters have producers and consumers and some topics are replicated so the messages are processed by different consumers on the two data centers. We address the active active scenarios for the two test environments in a separate note","title":"Data mirroring"},{"location":"patterns/topic-replication/#active-passive","text":"To support active - passive, one site has producers and consumers on local topics, and topic data are replicated to the passive cluster without online consumers. The tricky parts for this approach is the retention period in each topic, and then the backend systems impacted by the recovery. So the data replication from active - passive, embraces a larger scope than just topic mirroring. Still knowing how MirrorMaker 2 is doing offset replication is important. MirroMaker 2 tracks offset per consumer group. There are two topics created on the target cluster to manage the offset mapping between the source and target clusters and the checkpoints of the last committed offset in the source topic/partitions/consumer group. When a producer sends its record it gets the offset in the partition the record was saved. In the diagram below we have a source topic A - partition 1 with the last write offset done by a producer to be 5, and the last read committed offset by the consumer assigned to partition 1 being 3. The last replicated offset 3 is mapped as 12 in the target partition on the remote topic. offset # do not match between mirrored partitions. So if the consumer needs to reconnect to the target cluster, it will read from the last committed offset which is 12 in this environment. This information is saved in the checkpoint topic. Offset synch are emitted at the beginning of the replication and when there is a situation which leads that the numbering sequencing diverges. For example the normal behavior increases the offset by one 2,3,4,5,6,7 which is mapped to the offsets 12,13,14,15,16,... If the write operation for offset 20 at the source is a 17 on the target then MM 2 emits a new offset synch records to the offset-synch topic, so the offset replication is not done all the time, only when diverging to improve performance. The checkpoint and offset_synch topics enable replication to be fully restored from the correct offset position on failover.","title":"Active - Passive"},{"location":"patterns/topic-replication/#active-active","text":"For active/ active producers are writing to their topics locally to the local cluster on both data centers. The code is the same, and deployed in both environments. The main consideration is what is their own datasource. If for example the datasource is a click stream data from webapp running locally with different end users, then the local consumer may do something specific for this data center, that the consumer on the other side do not need to know and process. So the consumers that really want to consume from local topic and replicated one, have very special requirements, for example, computing aggregates and joins. In this case the processing is active - active with different semantic. The other example is when we want to have the same data eventually consistent on the backend, the consumer are the same, writing to the same backend system, on both side, but consuming from two different topics. The following diagram illustrates this approach. The data in both backend will be eventual consistent overtime.","title":"Active - Active"},{"location":"scenarios/overview/","text":"Kafka Connect Scenarios \u00b6 Scenario Description Link Realtime Inventory An end-to-end data pipeline lab scenario, connecting multiple components of a realtime inventory system via Kafka Connect. Scenarios - Realtime Inventory Reference Implementations \u00b6 Scenario Description Link Shipping fresh food over sea (external) The EDA solution implementation using event driven microservices in different language, and demonstrating different design patterns. EDA reference implementation solution Vaccine delivery at scale (external) An EDA and cross cloud pak solution Vaccine delivery at scale Real time anomaly detection (external) Develop and apply machine learning predictive model on event streams Refrigerator container anomaly detection solution Real time inventory (external) Develop a real time inventory from sale events with Kafka streams. (GitOps) RT inventory","title":"Overview"},{"location":"scenarios/overview/#kafka-connect-scenarios","text":"Scenario Description Link Realtime Inventory An end-to-end data pipeline lab scenario, connecting multiple components of a realtime inventory system via Kafka Connect. Scenarios - Realtime Inventory","title":"Kafka Connect Scenarios"},{"location":"scenarios/overview/#reference-implementations","text":"Scenario Description Link Shipping fresh food over sea (external) The EDA solution implementation using event driven microservices in different language, and demonstrating different design patterns. EDA reference implementation solution Vaccine delivery at scale (external) An EDA and cross cloud pak solution Vaccine delivery at scale Real time anomaly detection (external) Develop and apply machine learning predictive model on event streams Refrigerator container anomaly detection solution Real time inventory (external) Develop a real time inventory from sale events with Kafka streams. (GitOps) RT inventory","title":"Reference Implementations"},{"location":"scenarios/saga-orchestration/","text":"An IBM Cloud Pak for Integration - Event Streams use case Updated 02/22/2022 - Work in progress Introduction \u00b6 The Saga pattern helps to support a long running transaction that can be broken up to a collection of sub transactions that can be interleaved any way with other transactions. The SAGA orchestration is done by the order service that sends commands to drive each SAGA participant on what to do and when. To support strong consistency and exactly once delivery we are using Queues managed by IBM MQ. The Saga will be started by adding a new order, or updating major characteristics of an existing order. The demonstration illustrates the happy path, where each participants respond positively, and one incomplete path, where the order will not be satisfied because of lack of refrigerator containers. So the compensation logic will roll back the Vessel assignment. The following figure illustrates the component of this demonstration: The order microservice implements the SAGA orchestration, and Create Read operations for the Order Entity. The voyage microservice implements one of the participant of the SAGA and manages Vessel itineraries. This project illustrates how you can use the AMQP JMS client from Apache Qpid to interact with IBM MQ AMQP 1.0 servers in a Quarkus application using the Quarkus Qpid JMS extension. The reefer microservice Use Case Guided Tour \u00b6 Full Demo Narration \u00b6 Developer Corner \u00b6","title":"SAGA with MQ Orchestration"},{"location":"scenarios/saga-orchestration/#introduction","text":"The Saga pattern helps to support a long running transaction that can be broken up to a collection of sub transactions that can be interleaved any way with other transactions. The SAGA orchestration is done by the order service that sends commands to drive each SAGA participant on what to do and when. To support strong consistency and exactly once delivery we are using Queues managed by IBM MQ. The Saga will be started by adding a new order, or updating major characteristics of an existing order. The demonstration illustrates the happy path, where each participants respond positively, and one incomplete path, where the order will not be satisfied because of lack of refrigerator containers. So the compensation logic will roll back the Vessel assignment. The following figure illustrates the component of this demonstration: The order microservice implements the SAGA orchestration, and Create Read operations for the Order Entity. The voyage microservice implements one of the participant of the SAGA and manages Vessel itineraries. This project illustrates how you can use the AMQP JMS client from Apache Qpid to interact with IBM MQ AMQP 1.0 servers in a Quarkus application using the Quarkus Qpid JMS extension. The reefer microservice","title":"Introduction"},{"location":"scenarios/saga-orchestration/#use-case-guided-tour","text":"","title":"Use Case Guided Tour"},{"location":"scenarios/saga-orchestration/#full-demo-narration","text":"","title":"Full Demo Narration"},{"location":"scenarios/saga-orchestration/#developer-corner","text":"","title":"Developer Corner"},{"location":"snippets/","text":"Snippets \u00b6 This folder is for commonly used bits of information that need a single source of truth. Some examples include: Code blocks that may be used in several locations Topics that may need to be included in multiple places (e.g. \"create a service\") Use your best judgement, wherever content needs to be re-used easily across the site For information on how to use Snippets, see the pymdown-extension documentation As of 09/08/2022 we are not using this capability yet.","title":"Snippets"},{"location":"snippets/#snippets","text":"This folder is for commonly used bits of information that need a single source of truth. Some examples include: Code blocks that may be used in several locations Topics that may need to be included in multiple places (e.g. \"create a service\") Use your best judgement, wherever content needs to be re-used easily across the site For information on how to use Snippets, see the pymdown-extension documentation As of 09/08/2022 we are not using this capability yet.","title":"Snippets"},{"location":"technology/advanced-kafka/","text":"Kafka Advanced Concepts \u00b6 High Availability \u00b6 As a distributed cluster, Kafka brokers ensure high availability to process records. Topic has replication factor to support not loosing data in case of broker failure. You need at least 3 brokers to ensure availability and a replication factor set to 3 for each topic, so no data should be lost. But for production deployment, it is strongly recommended to use 5 brokers cluster to ensure the quorum is always set, even when doing product upgrade. Replica factor can still be set to 3. The brokers need to run on separate physical machines, and when cluster extends over multiple availability zones, a rack awareness configuration can be defined. The rack concept can represent an availability zone, data center, or an actual rack in your data center. Enabling rack awareness is done in a cluster definition (see Strimzi): spec : kafka : rack : topologyKey : topology.kubernetes.io/zone For Zookeeper 3,5,7 nodes are needed depending of the number of objects to manage. You can start with 3 and add nodes overtime. Replication and partition leadership \u00b6 Partition enables data locality, elasticity, scalability, high performance, parallelism, and fault tolerance. Each partition is replicated at least 3 times and allocated in different brokers. One replicas is the leader . In the case of broker failure (broker 1 in figure below), one of the existing replica in the remaining running brokers will take the leader role as soon as the broker is identified as not responding: The record key in each record sent by producer, determines the partition allocation in Kafka : the records with the same key (hashcode of the key to be exact) will be in the same partition (It is possible to use more complex behavior via configuration and even code). As Kafka is keeping its cluster states in Apache Zookeeper , you also need to have at least a three node cluster for zookeeper. From version 2.8, it is possible to do not use Zookeeper, but not yet for production deployment: the brokers are using a Raft quorum algorithm, and an internal topic to keep state and metadata (See the process.roles property from the broker configuration). Writes to Zookeeper are only be performed on changes to the membership of consumer groups or on changes to the Kafka cluster topology itself. Assuming you are using the most recent Kafka version, it is possible to have a unique zookeeper cluster for multiple kafka clusters. But the latency between Kafka and Zookeeper needs to be under few milliseconds (< 15ms) anyway. Zookeepers and Brokers should have high availability communication via dual network, and each broker and node allocated on different racks and blades. To contact the cluster, consumers and producers are using a list of bootstrap server names (also named advertiser.listeners ). The list is used for cluster discovery, it does not need to keep the full set of server names or IP addresses. A Kafka cluster has exactly one broker that acts as the controller. Per design Kafka aims to run within a single data center. But it is still recommended to use multiple racks connected with low latency dual networks. With multiple racks you will have better fault tolerance, as one rack failure will impact only one broker. There is a configuration property to assign kafka broker using rack awareness. (See the broker configuration from the product documentation). As introduced on the topic introduction section , data are replicated between brokers. The following diagram illustrates the best case scenario where followers fetch data from the partition leader, and acknowledge the replications: Usually replicas is done in-sync, and the configuration settings specify the number of in-sync replicas needed: for example, a replica 3 can have a minimum in-sync of 2, to tolerate 1 (= 3-2) out of sync replica (1 broker outage). The leader maintains a set of in-sync-replicas (ISR) broker list: all the nodes which are up-to-date with the leader\u2019s log, and actively acknowledging new writes. Every write goes through the leader and is propagated to every node in the In Sync Replica set. Followers consume messages from the leader just as a normal Kafka consumer would and apply them to their own log. Having the followers pull from the leader has the nice property of allowing the follower to naturally batch together log entries they are applying to their log. Once all nodes in the ISR have acknowledged the request, the partition leader broker considers it committed, and can acknowledge to the producer. A message is considered committed when all in-sync replicas for that partition have applied this message to their log. If a leader fails, followers elect a new one. The partition leadership is dynamic and changes as servers come and go. Applications do not need to take specific actions to handle the change in the leadership of a partition. The Kafka client library automatically reconnects to the new leader, although you will see increased latency while the cluster settles: later we will see this problem with partition rebalancing. Any replica in the ISR is eligible to be elected leader. When a leader waits to get acknowledge before committing a message there will be more potential leaders. With (#failure + 1) replicas there is no data lost. But there is a risk of having the single broker separated from the Zookeeper cluster when network partition occurs. To tolerate f failures, both the majority vote and the ISR approach will wait for the same number of replicas to acknowledge before committing a message. Having higher replicas number like 5, will duplicate 5 times the data (more disk used) and will impact throughput as data is sent 1+4 times over the network. Get acknowledgement will take a little bit more time too. Another important design distinction is that Kafka does not require that crashed nodes recover with all their data intact. Kafka protocol by allowing a replica to rejoin the ISR, ensures that before rejoining, it must fully re-sync again even if it lost unflushed data in its crash. When a producer sends message, it can control how to get the response from the committed message: wait for all replicas to succeed, wait for one acknowledge, fire and forget. Consumers receive only committed messages. Always assess the latency requirements and consumers needs. Throughput is linked to the number of partitions within a topic and having more consumers running in parallel. Consumers and producers should better run on separate servers than the brokers nodes. Running in parallel, also means the order of event arrivals will be lost. Most of the time, consumers are processing events from a unique partition and Kafka records to partition assignment will guarantee that records with the same key hashcode will be in the same partition. So orders are preserved within a partition. But if consumer needs to read from multiple partitions and if ordered records is needed, the consumer needs to rebuild the order by implementing adhoc logic, based on time stamp. For high availability, assess any potential single point of failure, such as server, rack, network, power supply... We recommend reading this event stream article for planning your kafka on Kubernetes installation. For the consumers code maintenance, the re-creation of the consumer instance within the consumer group will trigger a partition rebalancing. This includes all the state of the aggregated data calculations that were persisted on disk. Until this process is finished real-time events are not processed. It is possible to limit this impact by setting the group.initial.rebalance.delay.ms to delay the rebalancing process once one instance of the consumer dies. Nevertheless the rebalancing will still occur when the updated consumer will rejoin the consumer group. When consumers are stream processing using Kafka streams, it is important to note that during the rollover the downstream processing will see a lag in event arrival: the time for the consumer to reread from the last committed offset. So if the end-to-end timing is becoming important, we need to setup a standby consumer group (group B). This consumer group has different name, but does the same processing logic, and is consuming the same events from the same topic as the active consumer group (group A). The difference is that they do not send events to the downstream topic until they are set up active. So the process is to set group B active while cluster A is set inactive. The downstream processing will not be impacted. Finally to be exhaustive, the control of the segment size for the change log topic, may be considered to avoid having the stream processing doing a lot of computation to reload its state when it restarts. To add new broker, you can deploy the runtime to a new server / rack / blade, and give a unique broker ID. Broker will process new topic, but it is possible to use thr kafka-reassign-partitions tool to migrate some existing topic/ partitions to the new server. The tool is used to reassign partitions across brokers. An ideal partition distribution would ensure even data load and partition sizes across all brokers. High Availability in the context of Kubernetes deployment \u00b6 The combination of Kafka with Kubernetes seems to be a sound approach, but it is not that easy to achieve. Kubernetes workloads prefer to be stateless, Kafka is a stateful platform and manages its own brokers, and replications across known servers. It knows the underlying infrastructure. In Kubernetes, nodes and pods may change dynamically. Clients need to be able to access each of the broker directly once they get the connection metadata. Having a kubernetes service object which will round robin across all brokers in the cluster will not work with Kafka. The figure below illustrates a Kubernetes deployment, where Zookeeper and kafka brokers are allocated to 3 worker nodes, with some event driven microservices deployed in separate worker nodes. Those microservices are consumers and producers of events from one to many topics. The advantages of deploying Kafka on Kubernetes cluster is to facilitate the management of stateful sets, by scheduling both the persistence volume and broker pods in a clean rolling rehydration. Services add a logical name to access brokers for any deployed workload within the cluster. The virtual network also enables transparent TLS communication between components. For any Kubernetes deployment real high availability is constrained by the application / workload deployed on it. The Kubernetes platform supports high availability by having at least the following configuration: At least three master nodes (always an odd number of nodes). One is active at master, the others are in standby. The election of the master is using the quorum algorithm. At least three worker nodes, but with Zookeeper and Kafka clusters, we may need to have at least three more nodes as we do not want to have Zookeeper and Kafka brokers sharing the same host with other pods if the Kakfa traffic is supposed to grow. Externalize the management stack to three manager nodes Shared storage outside of the cluster to support private image registry, audit logs, and statefulset data persistence (like the Kakfa broker file systems). Use etcd cluster: See recommendations from this article . The virtual IP manager assigns virtual IP addresses to master and proxy nodes and monitors the health of the cluster. It leverages etcd for storing information, so it is important that etcd is high available too and connected to low latency network below 10ms. Traditionally disaster recovery and high availability were always consider separated subjects. Now active/active deployment where workloads are deployed in different data centers, is becoming a common IT's request. For sure, you need multiple Kafka Brokers, which will connect to the same ZooKeeper Ensemble running at least five nodes (you can tolerate the loss of one server during the planned maintenance of another server). One Zookeeper server acts as a lead and the two others as stand-by. The diagram above illustrates a simple deployment where Zookeeper servers and Kafka brokers are running in pods, in different worker nodes. It is a viable solution to start deploying solution on top of Kafka. When you have bigger cluster, it may be interesting to separate Zookeeper from Kafka nodes to limit the risk of failover, as Zookeeper keeps state of the Kafka cluster topology and metadata. You will limit to have both the Zookeeper leader and one Kafka broker dying at the same time. We use Kubernetes anti-affinity to ensure they are scheduled onto separate worker nodes that the ones used by Zookeeper. It uses the labels on pods with a rule like: **Kafka** pod should not run on same node as zookeeper pods . Here is an example of such spec: apiVersion : v1 kind : Pod metadata : name : with-pod-affinity spec : affinity : podAntiAffinity : requiredDuringSchedulingIgnoredDuringExecution : labelSelector : matchExpressions : - key : name operator : In values : - gc-zookeeper topologyKey : kubernetes.io/hostname We recommend reading the \"running zookeeper in k8s tutorial\" for understanding such configuration. For optimum performance, provision a fast storage class for persistence volume. Kafka uses the log.dirs property to configure the driver to persist logs. So you need to define multiple volumes/ drives to support log.dirs . Zookeeper should not be used by other applications deployed in k8s cluster, it has to be dedicated for one Kafka cluster only. In a multi-cluster configuration being used for disaster recovery purposes, messages sent between clusters will have different offsets in the two clusters. It is usual to use timestamps for position information when restarting applications for recovery after a disaster. For Kafka streaming with stateful processing like joins, event aggregation and correlation coming from multiple partitions, it is not easy to achieve high availability cross clusters: in the strictest case every event must be processed by the streaming service exactly once. Which means: producer emits data to different sites and be able to re-emit in case of failure. Brokers are known by producer via a list of hostnames and port numbers. communications between Zookeepers and cluster nodes are redundant and safe for data losses consumers ensure idempotence... They have to tolerate data duplication and manage data integrity in their persistence layer. Within Kafka's boundary, data will not be lost, when doing proper configuration, also to support high availability the complexity moves to the producer and the consumer implementation. Kafka configuration is an art and you need to tune the parameters by use case: Partition replication for at least 3 replicas. Recall that in case of node failure, coordination of partition re-assignments is provided with ZooKeeper. End to end latency needs to be measured from producer (when a message is sent) to consumer (when it is read). A consumer is able to get a message when the brokers finish replicating to all in-synch replicas. Use the producer buffering capability to pace the message to the broker. Can use memory or time based threshold via producer properties. Define the number of partitions to drive consumer parallelism. More consumers running in parallel the higher is the throughput. When using multiple partitions the global ordering of message is lost. Assess the retention hours to control when old messages in topic can be deleted. It is possible to keep messages forever, and for some application it makes fully sense. Control the maximum message size the server can receive. Zookeeper is not CPU intensive and each server should have a least 2 GB of heap space and 4GB reserved. Two CPUs per server should be sufficient. Servers keep their entire state machine in memory, and write every mutation to a durable WAL (Write Ahead Log) on persistent storage. To prevent the WAL from growing without bound, ZooKeeper servers periodically snapshot their in memory state to storage. Use fast and dynamically provisioned persistence storage for both WAL and snapshot. Performance Considerations \u00b6 Performance will vary depending of the current Kafka broker nodes load: in Kubernetes deployment, with small production topology, nodes may be shared with other pods. It is recommended to control the environment with dedicated nodes for Kafka to achieve higher throughput. Performance will always depend on numerous factors including message throughput, message size, hardware, configuration settings, ... Performance may be linked to different focuses: Resilience: ensuring replication and not loosing data Throughput: ensuring message processing performance Payload size: support larger message Resilience \u00b6 When defining a topic, we need to specify the replicas factor to match the be at least 3 and then set the minimum number of in-sync replicas that specifies how may replicas must acknowledge a write to satisfy a producer that requests acknowledgments from all replicas. ( min.insync.replicas ). The replication of message data between brokers can consume a lot of network bandwidth so isolating replication traffic from application traffic can benefit performance. To achieve this, all replication traffic is configured to flow on a dedicated internal network. Throughput \u00b6 To achieve higher throughput the messages are not replicated across brokers and the acknowledgement can be set to only one broker. Expose resiliency to failures. The number of producers and consumers are aligned, and the number of partitions matches the number of consumers. All consumers are in the same consumer group. Measurement has to be done from the producer code. With 12 producers on a 3 brokers cluster and small payload (128 bytes), with 24 consumers the measured throughput is around 2.3 M messages / second. Payload size \u00b6 From measurement tests done using Kafka producer performance tool, there is a 1/log(s) curve, where below 10k bytes the performances are correct and then slowly degrade from 3000 msg /s (10k bytes msg) to 65 msg/s (515kb msg). To do performance test the event-streams-sample-producer github provides producer tool in Java, using a group of threads to run in multi cores machine. This project can be dockerized, and deployed in k8s. It uses the Kafka tool named: ProducerPerformance.java in the jar: <dependency> <groupId> org.apache.kafka </groupId> <artifactId> kafka-tools </artifactId> </dependency> Parameter considerations \u00b6 There are a lot of factors and parameters that needs to be tuned to improve performance at the brokers threading level ( num.replica.fetchers, num.io.threads, num.network.threads, log.cleaner.threads ) and the pod resources constraints. See the configuration documentation . Openshift specifics \u00b6 When exposing the Kafka broker via Routes, the traffic is encrypted with TLS, so client needs to deal with TLS certificates and encryption. Routes are exposed via DNS and HAProxy router. The router will act as middleman between Kafka clients and brokers, adding latency, and it can become bottleneck. The traffic generated by client needs to be sized and in case of the router needs to be scaled up, and even isolate the routing by adding a separate router for the Kafka routes. Disaster Recovery \u00b6 With the current implementation it is recommended to have one cluster per data center / availability zone. Consumers and producers are co-located to the brokers cluster. When there are needs to keep some part of the data replicated in both data centers, you need to assess what kind of data can be aggregated, and if Kafka mirroring tool can be used. The tool consumes from a source cluster, from a given topic, and produces to a destination cluster with the same named topic. It keeps the message key for partitioning, so order is preserved. The above diagram is using Kafka MirrorMaker 2 with a master to slave deployment. Within the data center 2, the brokers are here to manage the topics and events. When there is no consumer running, nothing happen. Consumers and producers can be started when DC1 fails. This is the active/passive model. In fact, we could have consumers within the DC2 processing topics to manage a read-only model, keeping in memory their projection view, as presented in the CQRS pattern . The second solution is to use one MirrorMaker 2 in each site, for each topic. This is an active - active topology: consumers and producers are on both sites. But to avoid infinite loop, we need to use naming convention for the topic, or only produce in the cluster of the main topic. Consumers consume from the replicated topic. When you want to deploy solution that spreads over multiple regions to support global streaming, you need to address the following challenges: How do you make data available to applications across multiple data centers? How to serve data closer to the geography? How to be compliant on regulations, like GDPR? How to address no duplication of records? Kafka 2.4 introduces the capability for a consumer to read messages from the closest replica using some rack-id and specific algorithm. This capability will help to extend the cluster to multiple data center and avoid having consumers going over WAN communication. Solution Considerations \u00b6 There are a set of design considerations to assess for each Kafka solution: Topics \u00b6 Performance is more a function of number of partitions than topics. Expect that each topic has at least one partition. When considering latency you should aim for limiting to hundreds of topic-partition per broker node. What of the most important question is what topics to use?. What is an event type? Should we use one topic to support multiple event types? Let define that an event type is linked to a main business entity like an Order, a ship, a FridgeredContainer. OrderCreated, OrderCancelled, OrderUpdated, OrderClosed are events linked to the states of the Order. The order of those events matter. So the natural approach is to use one topic per data type or schema, specially when using the topic as Event Sourcing where event order is important to build the audit log. You will use a unique partition to support that. The orderID is the partition key and all events related to the order are in the same topic. The important requirement to consider is the sequencing or event order. When event order is very important then use a unique partition, and use the entity unique identifier as key. Ordering is not preserved across partitions. When dealing with entity, independent entities may be in separate topics, when strongly related one may stay together. Other best practices: When event order is important use the same topic and use the entity unique identifier as partition key. When two entities are related together by containment relationship then they can be in the same topic. Different entities are separated to different topics. It is possible to group topics in coarse grained one when we discover that several consumers are listening to the same topics. Clearly define the partition key as it could be an compound key based on multiple entities. With Kafka stream, state store or KTable, you should separate the changelog topic from the others. Producers \u00b6 When developing a record producer you need to assess the following: What is the expected throughput to send events? Event size * average throughput combined with the expected latency help to compute buffer size. Can the producer batch events together to send them in batch over one send operation? Is there a risk for loosing communication? Tune the RETRIES_CONFIG and buffer size Assess once to exactly once delivery requirement. Look at idempotent producer. See implementation considerations discussion Consumers \u00b6 From the consumer point of view a set of items need to be addressed during design phase: Do you need to group consumers for parallel consumption of events? What is the processing done once the record is processed out of the topic? And how a record is supposed to be consumed?. How to persist consumer committed position? (the last offset that has been stored securely) Assess if offsets need to be persisted outside of Kafka?. From version 0.9 offset management is more efficient, and synchronous or asynchronous operations can be done from the consumer code. Does record time sensitive, and it is possible that consumers fall behind, so when a consumer restarts he can bypass missed records? Do the consumer needs to perform joins, aggregations between multiple partitions? See implementation consideration discussion","title":"Advanced Concepts"},{"location":"technology/advanced-kafka/#kafka-advanced-concepts","text":"","title":"Kafka Advanced Concepts"},{"location":"technology/advanced-kafka/#high-availability","text":"As a distributed cluster, Kafka brokers ensure high availability to process records. Topic has replication factor to support not loosing data in case of broker failure. You need at least 3 brokers to ensure availability and a replication factor set to 3 for each topic, so no data should be lost. But for production deployment, it is strongly recommended to use 5 brokers cluster to ensure the quorum is always set, even when doing product upgrade. Replica factor can still be set to 3. The brokers need to run on separate physical machines, and when cluster extends over multiple availability zones, a rack awareness configuration can be defined. The rack concept can represent an availability zone, data center, or an actual rack in your data center. Enabling rack awareness is done in a cluster definition (see Strimzi): spec : kafka : rack : topologyKey : topology.kubernetes.io/zone For Zookeeper 3,5,7 nodes are needed depending of the number of objects to manage. You can start with 3 and add nodes overtime.","title":"High Availability"},{"location":"technology/advanced-kafka/#replication-and-partition-leadership","text":"Partition enables data locality, elasticity, scalability, high performance, parallelism, and fault tolerance. Each partition is replicated at least 3 times and allocated in different brokers. One replicas is the leader . In the case of broker failure (broker 1 in figure below), one of the existing replica in the remaining running brokers will take the leader role as soon as the broker is identified as not responding: The record key in each record sent by producer, determines the partition allocation in Kafka : the records with the same key (hashcode of the key to be exact) will be in the same partition (It is possible to use more complex behavior via configuration and even code). As Kafka is keeping its cluster states in Apache Zookeeper , you also need to have at least a three node cluster for zookeeper. From version 2.8, it is possible to do not use Zookeeper, but not yet for production deployment: the brokers are using a Raft quorum algorithm, and an internal topic to keep state and metadata (See the process.roles property from the broker configuration). Writes to Zookeeper are only be performed on changes to the membership of consumer groups or on changes to the Kafka cluster topology itself. Assuming you are using the most recent Kafka version, it is possible to have a unique zookeeper cluster for multiple kafka clusters. But the latency between Kafka and Zookeeper needs to be under few milliseconds (< 15ms) anyway. Zookeepers and Brokers should have high availability communication via dual network, and each broker and node allocated on different racks and blades. To contact the cluster, consumers and producers are using a list of bootstrap server names (also named advertiser.listeners ). The list is used for cluster discovery, it does not need to keep the full set of server names or IP addresses. A Kafka cluster has exactly one broker that acts as the controller. Per design Kafka aims to run within a single data center. But it is still recommended to use multiple racks connected with low latency dual networks. With multiple racks you will have better fault tolerance, as one rack failure will impact only one broker. There is a configuration property to assign kafka broker using rack awareness. (See the broker configuration from the product documentation). As introduced on the topic introduction section , data are replicated between brokers. The following diagram illustrates the best case scenario where followers fetch data from the partition leader, and acknowledge the replications: Usually replicas is done in-sync, and the configuration settings specify the number of in-sync replicas needed: for example, a replica 3 can have a minimum in-sync of 2, to tolerate 1 (= 3-2) out of sync replica (1 broker outage). The leader maintains a set of in-sync-replicas (ISR) broker list: all the nodes which are up-to-date with the leader\u2019s log, and actively acknowledging new writes. Every write goes through the leader and is propagated to every node in the In Sync Replica set. Followers consume messages from the leader just as a normal Kafka consumer would and apply them to their own log. Having the followers pull from the leader has the nice property of allowing the follower to naturally batch together log entries they are applying to their log. Once all nodes in the ISR have acknowledged the request, the partition leader broker considers it committed, and can acknowledge to the producer. A message is considered committed when all in-sync replicas for that partition have applied this message to their log. If a leader fails, followers elect a new one. The partition leadership is dynamic and changes as servers come and go. Applications do not need to take specific actions to handle the change in the leadership of a partition. The Kafka client library automatically reconnects to the new leader, although you will see increased latency while the cluster settles: later we will see this problem with partition rebalancing. Any replica in the ISR is eligible to be elected leader. When a leader waits to get acknowledge before committing a message there will be more potential leaders. With (#failure + 1) replicas there is no data lost. But there is a risk of having the single broker separated from the Zookeeper cluster when network partition occurs. To tolerate f failures, both the majority vote and the ISR approach will wait for the same number of replicas to acknowledge before committing a message. Having higher replicas number like 5, will duplicate 5 times the data (more disk used) and will impact throughput as data is sent 1+4 times over the network. Get acknowledgement will take a little bit more time too. Another important design distinction is that Kafka does not require that crashed nodes recover with all their data intact. Kafka protocol by allowing a replica to rejoin the ISR, ensures that before rejoining, it must fully re-sync again even if it lost unflushed data in its crash. When a producer sends message, it can control how to get the response from the committed message: wait for all replicas to succeed, wait for one acknowledge, fire and forget. Consumers receive only committed messages. Always assess the latency requirements and consumers needs. Throughput is linked to the number of partitions within a topic and having more consumers running in parallel. Consumers and producers should better run on separate servers than the brokers nodes. Running in parallel, also means the order of event arrivals will be lost. Most of the time, consumers are processing events from a unique partition and Kafka records to partition assignment will guarantee that records with the same key hashcode will be in the same partition. So orders are preserved within a partition. But if consumer needs to read from multiple partitions and if ordered records is needed, the consumer needs to rebuild the order by implementing adhoc logic, based on time stamp. For high availability, assess any potential single point of failure, such as server, rack, network, power supply... We recommend reading this event stream article for planning your kafka on Kubernetes installation. For the consumers code maintenance, the re-creation of the consumer instance within the consumer group will trigger a partition rebalancing. This includes all the state of the aggregated data calculations that were persisted on disk. Until this process is finished real-time events are not processed. It is possible to limit this impact by setting the group.initial.rebalance.delay.ms to delay the rebalancing process once one instance of the consumer dies. Nevertheless the rebalancing will still occur when the updated consumer will rejoin the consumer group. When consumers are stream processing using Kafka streams, it is important to note that during the rollover the downstream processing will see a lag in event arrival: the time for the consumer to reread from the last committed offset. So if the end-to-end timing is becoming important, we need to setup a standby consumer group (group B). This consumer group has different name, but does the same processing logic, and is consuming the same events from the same topic as the active consumer group (group A). The difference is that they do not send events to the downstream topic until they are set up active. So the process is to set group B active while cluster A is set inactive. The downstream processing will not be impacted. Finally to be exhaustive, the control of the segment size for the change log topic, may be considered to avoid having the stream processing doing a lot of computation to reload its state when it restarts. To add new broker, you can deploy the runtime to a new server / rack / blade, and give a unique broker ID. Broker will process new topic, but it is possible to use thr kafka-reassign-partitions tool to migrate some existing topic/ partitions to the new server. The tool is used to reassign partitions across brokers. An ideal partition distribution would ensure even data load and partition sizes across all brokers.","title":"Replication and partition leadership"},{"location":"technology/advanced-kafka/#high-availability-in-the-context-of-kubernetes-deployment","text":"The combination of Kafka with Kubernetes seems to be a sound approach, but it is not that easy to achieve. Kubernetes workloads prefer to be stateless, Kafka is a stateful platform and manages its own brokers, and replications across known servers. It knows the underlying infrastructure. In Kubernetes, nodes and pods may change dynamically. Clients need to be able to access each of the broker directly once they get the connection metadata. Having a kubernetes service object which will round robin across all brokers in the cluster will not work with Kafka. The figure below illustrates a Kubernetes deployment, where Zookeeper and kafka brokers are allocated to 3 worker nodes, with some event driven microservices deployed in separate worker nodes. Those microservices are consumers and producers of events from one to many topics. The advantages of deploying Kafka on Kubernetes cluster is to facilitate the management of stateful sets, by scheduling both the persistence volume and broker pods in a clean rolling rehydration. Services add a logical name to access brokers for any deployed workload within the cluster. The virtual network also enables transparent TLS communication between components. For any Kubernetes deployment real high availability is constrained by the application / workload deployed on it. The Kubernetes platform supports high availability by having at least the following configuration: At least three master nodes (always an odd number of nodes). One is active at master, the others are in standby. The election of the master is using the quorum algorithm. At least three worker nodes, but with Zookeeper and Kafka clusters, we may need to have at least three more nodes as we do not want to have Zookeeper and Kafka brokers sharing the same host with other pods if the Kakfa traffic is supposed to grow. Externalize the management stack to three manager nodes Shared storage outside of the cluster to support private image registry, audit logs, and statefulset data persistence (like the Kakfa broker file systems). Use etcd cluster: See recommendations from this article . The virtual IP manager assigns virtual IP addresses to master and proxy nodes and monitors the health of the cluster. It leverages etcd for storing information, so it is important that etcd is high available too and connected to low latency network below 10ms. Traditionally disaster recovery and high availability were always consider separated subjects. Now active/active deployment where workloads are deployed in different data centers, is becoming a common IT's request. For sure, you need multiple Kafka Brokers, which will connect to the same ZooKeeper Ensemble running at least five nodes (you can tolerate the loss of one server during the planned maintenance of another server). One Zookeeper server acts as a lead and the two others as stand-by. The diagram above illustrates a simple deployment where Zookeeper servers and Kafka brokers are running in pods, in different worker nodes. It is a viable solution to start deploying solution on top of Kafka. When you have bigger cluster, it may be interesting to separate Zookeeper from Kafka nodes to limit the risk of failover, as Zookeeper keeps state of the Kafka cluster topology and metadata. You will limit to have both the Zookeeper leader and one Kafka broker dying at the same time. We use Kubernetes anti-affinity to ensure they are scheduled onto separate worker nodes that the ones used by Zookeeper. It uses the labels on pods with a rule like: **Kafka** pod should not run on same node as zookeeper pods . Here is an example of such spec: apiVersion : v1 kind : Pod metadata : name : with-pod-affinity spec : affinity : podAntiAffinity : requiredDuringSchedulingIgnoredDuringExecution : labelSelector : matchExpressions : - key : name operator : In values : - gc-zookeeper topologyKey : kubernetes.io/hostname We recommend reading the \"running zookeeper in k8s tutorial\" for understanding such configuration. For optimum performance, provision a fast storage class for persistence volume. Kafka uses the log.dirs property to configure the driver to persist logs. So you need to define multiple volumes/ drives to support log.dirs . Zookeeper should not be used by other applications deployed in k8s cluster, it has to be dedicated for one Kafka cluster only. In a multi-cluster configuration being used for disaster recovery purposes, messages sent between clusters will have different offsets in the two clusters. It is usual to use timestamps for position information when restarting applications for recovery after a disaster. For Kafka streaming with stateful processing like joins, event aggregation and correlation coming from multiple partitions, it is not easy to achieve high availability cross clusters: in the strictest case every event must be processed by the streaming service exactly once. Which means: producer emits data to different sites and be able to re-emit in case of failure. Brokers are known by producer via a list of hostnames and port numbers. communications between Zookeepers and cluster nodes are redundant and safe for data losses consumers ensure idempotence... They have to tolerate data duplication and manage data integrity in their persistence layer. Within Kafka's boundary, data will not be lost, when doing proper configuration, also to support high availability the complexity moves to the producer and the consumer implementation. Kafka configuration is an art and you need to tune the parameters by use case: Partition replication for at least 3 replicas. Recall that in case of node failure, coordination of partition re-assignments is provided with ZooKeeper. End to end latency needs to be measured from producer (when a message is sent) to consumer (when it is read). A consumer is able to get a message when the brokers finish replicating to all in-synch replicas. Use the producer buffering capability to pace the message to the broker. Can use memory or time based threshold via producer properties. Define the number of partitions to drive consumer parallelism. More consumers running in parallel the higher is the throughput. When using multiple partitions the global ordering of message is lost. Assess the retention hours to control when old messages in topic can be deleted. It is possible to keep messages forever, and for some application it makes fully sense. Control the maximum message size the server can receive. Zookeeper is not CPU intensive and each server should have a least 2 GB of heap space and 4GB reserved. Two CPUs per server should be sufficient. Servers keep their entire state machine in memory, and write every mutation to a durable WAL (Write Ahead Log) on persistent storage. To prevent the WAL from growing without bound, ZooKeeper servers periodically snapshot their in memory state to storage. Use fast and dynamically provisioned persistence storage for both WAL and snapshot.","title":"High Availability in the context of Kubernetes deployment"},{"location":"technology/advanced-kafka/#performance-considerations","text":"Performance will vary depending of the current Kafka broker nodes load: in Kubernetes deployment, with small production topology, nodes may be shared with other pods. It is recommended to control the environment with dedicated nodes for Kafka to achieve higher throughput. Performance will always depend on numerous factors including message throughput, message size, hardware, configuration settings, ... Performance may be linked to different focuses: Resilience: ensuring replication and not loosing data Throughput: ensuring message processing performance Payload size: support larger message","title":"Performance Considerations"},{"location":"technology/advanced-kafka/#resilience","text":"When defining a topic, we need to specify the replicas factor to match the be at least 3 and then set the minimum number of in-sync replicas that specifies how may replicas must acknowledge a write to satisfy a producer that requests acknowledgments from all replicas. ( min.insync.replicas ). The replication of message data between brokers can consume a lot of network bandwidth so isolating replication traffic from application traffic can benefit performance. To achieve this, all replication traffic is configured to flow on a dedicated internal network.","title":"Resilience"},{"location":"technology/advanced-kafka/#throughput","text":"To achieve higher throughput the messages are not replicated across brokers and the acknowledgement can be set to only one broker. Expose resiliency to failures. The number of producers and consumers are aligned, and the number of partitions matches the number of consumers. All consumers are in the same consumer group. Measurement has to be done from the producer code. With 12 producers on a 3 brokers cluster and small payload (128 bytes), with 24 consumers the measured throughput is around 2.3 M messages / second.","title":"Throughput"},{"location":"technology/advanced-kafka/#payload-size","text":"From measurement tests done using Kafka producer performance tool, there is a 1/log(s) curve, where below 10k bytes the performances are correct and then slowly degrade from 3000 msg /s (10k bytes msg) to 65 msg/s (515kb msg). To do performance test the event-streams-sample-producer github provides producer tool in Java, using a group of threads to run in multi cores machine. This project can be dockerized, and deployed in k8s. It uses the Kafka tool named: ProducerPerformance.java in the jar: <dependency> <groupId> org.apache.kafka </groupId> <artifactId> kafka-tools </artifactId> </dependency>","title":"Payload size"},{"location":"technology/advanced-kafka/#parameter-considerations","text":"There are a lot of factors and parameters that needs to be tuned to improve performance at the brokers threading level ( num.replica.fetchers, num.io.threads, num.network.threads, log.cleaner.threads ) and the pod resources constraints. See the configuration documentation .","title":"Parameter considerations"},{"location":"technology/advanced-kafka/#openshift-specifics","text":"When exposing the Kafka broker via Routes, the traffic is encrypted with TLS, so client needs to deal with TLS certificates and encryption. Routes are exposed via DNS and HAProxy router. The router will act as middleman between Kafka clients and brokers, adding latency, and it can become bottleneck. The traffic generated by client needs to be sized and in case of the router needs to be scaled up, and even isolate the routing by adding a separate router for the Kafka routes.","title":"Openshift specifics"},{"location":"technology/advanced-kafka/#disaster-recovery","text":"With the current implementation it is recommended to have one cluster per data center / availability zone. Consumers and producers are co-located to the brokers cluster. When there are needs to keep some part of the data replicated in both data centers, you need to assess what kind of data can be aggregated, and if Kafka mirroring tool can be used. The tool consumes from a source cluster, from a given topic, and produces to a destination cluster with the same named topic. It keeps the message key for partitioning, so order is preserved. The above diagram is using Kafka MirrorMaker 2 with a master to slave deployment. Within the data center 2, the brokers are here to manage the topics and events. When there is no consumer running, nothing happen. Consumers and producers can be started when DC1 fails. This is the active/passive model. In fact, we could have consumers within the DC2 processing topics to manage a read-only model, keeping in memory their projection view, as presented in the CQRS pattern . The second solution is to use one MirrorMaker 2 in each site, for each topic. This is an active - active topology: consumers and producers are on both sites. But to avoid infinite loop, we need to use naming convention for the topic, or only produce in the cluster of the main topic. Consumers consume from the replicated topic. When you want to deploy solution that spreads over multiple regions to support global streaming, you need to address the following challenges: How do you make data available to applications across multiple data centers? How to serve data closer to the geography? How to be compliant on regulations, like GDPR? How to address no duplication of records? Kafka 2.4 introduces the capability for a consumer to read messages from the closest replica using some rack-id and specific algorithm. This capability will help to extend the cluster to multiple data center and avoid having consumers going over WAN communication.","title":"Disaster Recovery"},{"location":"technology/advanced-kafka/#solution-considerations","text":"There are a set of design considerations to assess for each Kafka solution:","title":"Solution Considerations"},{"location":"technology/advanced-kafka/#topics","text":"Performance is more a function of number of partitions than topics. Expect that each topic has at least one partition. When considering latency you should aim for limiting to hundreds of topic-partition per broker node. What of the most important question is what topics to use?. What is an event type? Should we use one topic to support multiple event types? Let define that an event type is linked to a main business entity like an Order, a ship, a FridgeredContainer. OrderCreated, OrderCancelled, OrderUpdated, OrderClosed are events linked to the states of the Order. The order of those events matter. So the natural approach is to use one topic per data type or schema, specially when using the topic as Event Sourcing where event order is important to build the audit log. You will use a unique partition to support that. The orderID is the partition key and all events related to the order are in the same topic. The important requirement to consider is the sequencing or event order. When event order is very important then use a unique partition, and use the entity unique identifier as key. Ordering is not preserved across partitions. When dealing with entity, independent entities may be in separate topics, when strongly related one may stay together. Other best practices: When event order is important use the same topic and use the entity unique identifier as partition key. When two entities are related together by containment relationship then they can be in the same topic. Different entities are separated to different topics. It is possible to group topics in coarse grained one when we discover that several consumers are listening to the same topics. Clearly define the partition key as it could be an compound key based on multiple entities. With Kafka stream, state store or KTable, you should separate the changelog topic from the others.","title":"Topics"},{"location":"technology/advanced-kafka/#producers","text":"When developing a record producer you need to assess the following: What is the expected throughput to send events? Event size * average throughput combined with the expected latency help to compute buffer size. Can the producer batch events together to send them in batch over one send operation? Is there a risk for loosing communication? Tune the RETRIES_CONFIG and buffer size Assess once to exactly once delivery requirement. Look at idempotent producer. See implementation considerations discussion","title":"Producers"},{"location":"technology/advanced-kafka/#consumers","text":"From the consumer point of view a set of items need to be addressed during design phase: Do you need to group consumers for parallel consumption of events? What is the processing done once the record is processed out of the topic? And how a record is supposed to be consumed?. How to persist consumer committed position? (the last offset that has been stored securely) Assess if offsets need to be persisted outside of Kafka?. From version 0.9 offset management is more efficient, and synchronous or asynchronous operations can be done from the consumer code. Does record time sensitive, and it is possible that consumers fall behind, so when a consumer restarts he can bypass missed records? Do the consumer needs to perform joins, aggregations between multiple partitions? See implementation consideration discussion","title":"Consumers"},{"location":"technology/avro-schemas/","text":"Updates 6/20/2022 This chapter describes what and why Avro and Schema registry are important elements of any event-driven solutions. Why this is important \u00b6 Loosely coupling and asynchronous communication between applications does not mean there is no contract to enforce some constraint between producer and consumer. When we talk about contract we can first think about schema as we did with XSD. In the world of JSON, JSON schema and Avro schemas can be used to define data structure of the message. As there is a need to get metadata around messaging, cloudevents is well accepted and adopted as a specification to describe event data. Also AsyncAPI establishes standards for events and messaging in the asynchronous world with an API view, so combining message schema, channels and binding definitions so we have most of the needed information for a consumer to access a data stream or queue. So the contract is defined with a schema. From an EDA design point of view, the producer owns the definition of the schema as it owns the main business entity life cycle, events are generated from. Producer will make sure the message complies with the schema at hand for serializing. On top of those specifications, there are technologies to support the contract management in the form of schema registry and API manager. The following figure gives us the foundations for integration between producer, schema registry and consumers. Schema Registry provides producer and consumer APIs so that these can project whether the event they are about to produce or consume is compatible with previous versions or compatible with the version they are expecting. For that, both producers and consumers require the schema definition at hand at serialization and deserialization time. This can be done either by: Reading the schema from a local local resource to your producer such as a file, variable, property (or kubernetes construct such as configmap or secret). Retrieving the schema definition from the Schema Registry given a name/ID. When the producer wants to send an event to a Kafka topic, two things happen: The producer makes sure the event to be sent complies to the schema. Otherwise, it errors out. Project whether the event they are about to produce or consume is compatible with previous versions or compatible with the version they are expecting. For the first action, the producer already holds the two things it needs: the schema definition the event to be sent needs to comply with and the event itself. The producer is therefore able to carry out that compliance check. However, for the second action, where the producer needs to make sure the schema definition your event complies with is compatible with the existing schema definition for the topic in question (if any), the producer might need to contact the schema registry. Producers (and consumers) maintain a local cache with the schema definitions (and their versions) along with their unique global IDs (all of these retrieved from the schema registry) for the topics they want to produce/consume events to/from. If the producer has a schema definition in its local cache that matches the schema definition at hand for the serialization of the event, it simply sends the event as the schema definitions match and the event complies with such schema definition. However, if the producer does not have any schema definition in its local cache that matches the schema defintion at hand for the serialization of the event, whether because these are different versions or the producer simply does not have any schema definition in its cache, it contacts the schema registry. If a schema defintion for the topic in question that matches the schema definition at hand at serialization time in the producer already exists in the schema registry, the producer simply retrieves the global unique ID for that schema definition to locally cache it along with the schema definition itself for future events to be sent so that it avoids contacting the schema registry again. If a schema definition for the topic in question that matches the schema definition at hand at serialization time in the producer does not already exist in the schema registry, the producer must be able to register the schema definition it has got at hand for the topic in question at serialization time in the schema registry so that such schema definition is now available for any consumer wanting or needing to consume events that comply with such schema definition. For that, the producer must be configured to be able to auto-register schemas and also be provided with the appropriate credentials to register schema definitions from the schema registry perspective if this implements any type of RBAC mechanism (as it is the case for IBM Event Streams). If the producer is not configured to auto-register schema definitions or its credentials does not allow it to register schema definitions, then the send event action will fail until there is a schema definition for the topic in question that mateches the schema definition at hand at serialization time in the producer registered in the schema registry. If the producer is configured to auto-register schema definitions and its credentials allows it to register schema definitions, the schema registry validates the compatibility of the schema definition at hand at serialization time in the producer with existing schema definitions for the topic in questions (if any) to make sure that, if this schema definition to be registered is a newer version of any existing schema definition, it is complatible so that no consumer gets affected by this new schema definition version. Afer the schema definition or newer version of an exisint schema definition is registered in the schema registry, the producer retrieves its global unique ID to mantain its local cache up to date. Warning We stongly recommend that producers are not allowed to register schema definitions in the schema registry for better governance and management of schema definitions. It is highly recommended that schema definitions registration is done by someone responsible for such task with appropriate role (such as an admin, an API manager, an asynchronous API manager, a development manager, an operator, etc). Once the producer has a schema definition in its local cache, along with its global unique id, that matches the schema definition at hand for serialization of the event to be sent, it produces the event to the Kafka topic using the appropriate AvroKafkaSerializer class. The schema definition global unique ID gets serialized along with the event so that the event does not need to travel along with its schema definition as it was the case in older messaging or eventing techonolgies and systems. By default, when a consumer reads an event, the schema definition for such event is retrieved from the schema registry by the deserializer using the global unique ID, which is specified in the event being consumed. The schema definition for an event is retrieved from the schema registry only once, when an event comes with a global unique ID that can not be found in the schema definition cache the consumer maintains locally. The schema definition global unique ID can be located in the event headers or in the event payload, depending on the configuration of the producer application. When locating the global unique ID in the event payload, the format of the data begins with a magic byte, used as a signal to consumers, followed by the global unique ID, and the message data as normal. For example: # ... [MAGIC_BYTE] [GLOBAL_UNIQUE_ID] [MESSAGE DATA] Be aware that non Java consumers use a C library that might require the schema definition at hand for the deserializer too (see https://github.com/confluentinc/confluent-kafka-python/issues/834) as opposed to letting the deserializer retrieve the schema definition from the schema registry as explained above. The strategy would be to have the schema loaded from the schema registry via API. Schema Registry \u00b6 With a pure open-source strategy, Event Streams within Cloud Pak for integration is using Apicu.io as schema registry. The Event Streams product documentation is doing an excellent job to present the schema registry, we do not need to rewrite the story, just give you some summary from Apicur.io and links to code samples. Apicurio \u00b6 Apicur.io includes a schema registry to store schema definitions. It supports Avro, json, protobuf schemas, and an API registry to manage OpenApi and AsynchAPI. It is a Cloud-native Quarkus Java runtime for low memory footprint and fast deployment times. It supports different persistences like Kafka, Postgresql, Infinispan and supports different deployment models. Registry Characteristics \u00b6 Apicurio Registry is a datastore for sharing standard event schemas and API designs across API and event-driven architectures. In the messaging and event streaming world, data that are published to topics and queues often must be serialized or validated using a Schema. The registry supports adding, removing, and updating the following types of artifacts: OpenAPI, AsyncAPI, GraphQL, Apache Avro, Google protocol buffers, JSON Schema, Kafka Connect schema, WSDL, XML Schema (XSD). Schema can be created via Web Console, core REST API or Maven plugin It includes configurable rules to control the validity and compatibility. Client applications can dynamically push or pull the latest schema updates to or from Apicurio Registry at runtime. Apicurio is compatible with existing Confluent schema registry client applications. It includes client serializers/deserializers (Serdes) to validate Kafka and other message types at runtime. Operator-based installation of Apicurio Registry on OpenShift Use the concept of artifact group to collect schema and APIs logically related. Support search for artifacts by label, name, group, and description When using Kafka as persistence, special Kafka topic <kafkastore.topic> (default _schemas ), with a single partition, is used as a highly available write ahead log. All schemas, subject/version and ID metadata, and compatibility settings are appended as messages to this log. A Schema Registry instance therefore both produces and consumes messages under the _schemas topic. It produces messages to the log when, for example, new schemas are registered under a subject, or when updates to compatibility settings are registered. Schema Registry consumes from the _schemas log in a background thread, and updates its local caches on consumption of each new _schemas message to reflect the newly added schema or compatibility setting. Updating local state from the Kafka log in this manner ensures durability, ordering, and easy recoverability. The way Event Streams / Apicur.io has to handle schema association to topics is by schema name. Given we have a topic called orders, the schemas that will apply to it are avros-key (when using composite key) and orders-value (most likely based on cloudevents and then custom payload). Apache Avro \u00b6 Avro is an open source data serialization system that helps with data exchange between systems, programming languages, and processing frameworks. Avro helps define a binary format for your data, as well as map it to the programming language of your choice. Why Apache Avro \u00b6 There are several websites that discuss the Apache Avro data serialization system benefits over other messaging data protocols. A simple google search will list dozens of them. Here, we will highlight just a few from a Confluent blog post : It has a direct mapping to and from JSON It has a very compact format. The bulk of JSON, repeating every field name with every single record, is what makes JSON inefficient for high-volume usage. It is very fast. It has great bindings for a wide variety of programming languages so you can generate Java objects that make working with event data easier, but it does not require code generation so tools can be written generically for any data stream. It has a rich, extensible schema language defined in pure JSON It has the best notion of compatibility for evolving your data over time. Data Schemas \u00b6 Avro relies on schemas. When Avro data is produced or read, the Avro schema for such piece of data is always present. This permits each datum to be written with no per-value overheads, making serialization both fast and small. An Avro schema defines the structure of the Avro data format. How does a data schema look like? \u00b6 Let's see how a data schema to define a person's profile in a bank could look like: { \"namespace\" : \"banking.schemas.demo\" , \"name\" : \"profile\" , \"type\" : \"record\" , \"doc\" : \"Data schema to represent a profile for a banking entity\" , \"fields \" : [ { \"name\" : \"name\" , \"type\" : \"string\" }, { \"name\" : \"surname\" , \"type\" : \"string\" }, { \"name\" : \"age\" , \"type\" : \"int\" }, { \"name\" : \"account\" , \"type\" : \"banking.schemas.demo.account\" }, { \"name\" : \"gender\" , \"type\" : { \"type\" : \"enum\" , \"name\" : \"genderEnum\" , \"symbols\" : [ \"male\" , \"female\" ] } } ] } Notice: There are primitive data types like string and int but also complex types like record or enum . Complex type record requires a name attribute but it also can go along with a namespace attribute which is a JSON string that qualifies the name. Data schemas can be nested as you can see for the account data attribute. See below. { \"namespace\" : \"banking.schemas.demo\" , \"name\" : \"account\" , \"type\" : \"record\" , \"doc\" : \"Data schema to represent a customer account with the credit cards associated to it\" , \"fields\" : [ { \"name\" : \"id\" , \"type\" : \"string\" }, { \"name\" : \"savings\" , \"type\" : \"long\" }, { \"name\" : \"cards\" , \"type\" : { \"type\" : \"array\" , \"items\" : \"int\" } } ] } In the picture below we see two messages, one complies with the above Apache Avro data schema and the other does not: You might start realising by now the benefits of having the data flowing into your Apache Kafka event backbone validated against a schema. See next section for more. For more information on the Apache Avro Data Schema specification see https://avro.apache.org/docs/current/spec.html Benefits of using Data Schemas \u00b6 Clarity and Semantics : They document the usage of the event and the meaning of each field in the \"doc\" fields. Robustness : They protect downstream data consumers from malformed data, as only valid data will be permitted in the topic. They let the producers or consumers of data streams know the right fields are need in an event and what type each field is (contract for microservices). Compatibility : model and handle change in data format. Avro, Kafka and Schema Registry \u00b6 In this section we try to put all the pieces together for the common flow of sending and receiving messages through an event backbone such as kafka having those messages serialized using the Apache Avro data serialization system and complying with their respective messages that are stored and managed by a schema registry. Avro relies on schemas. When Avro data is produced or read, the Avro schema for such piece of data is always present. An Avro schema defines the structure of the Avro data format. Schema Registry defines a scope in which schemas can evolve, and that scope is the subject. The name of the subject depends on the configured subject name strategy, which by default is set to derive subject name from topic name. In this case, the messages are serialized using Avro and sent to a kafka topic. Each message is a key-value pair. Either the message key or the message value, or both, can be serialized as Avro. Integration with Schema Registry means that Kafka messages do not need to be written with the entire Avro schema. Instead, Kafka messages are written with the schema id . The producers writing the messages and the consumers reading the messages must be using the same Schema Registry to get the same mapping between a schema and schema id. More reading \u00b6 Articles and product documentation \u00b6 IBM Event Streams- Schemas overview Apicur.io schema registry documentation Confluent schema registry overview Producer code with reactive messaging and apicurio schema registry Consumer code with reactive messaging and apicurio schema registry Labs \u00b6 We have developed two labs, one for the IBM Event Streams product that comes with the IBM CloudPak for Integration installed on a RedHat OpenShift cluster and the other for the IBM Event Streams on IBM Cloud offering, to get hands-on experience working with Apache Avro, data schemas and the IBM Event Streams Schema Registry: IBM Event Streams on IBM Cloud lab IBM Event Streams from IBM CloudPak for Integration lab","title":"Avro Schema"},{"location":"technology/avro-schemas/#why-this-is-important","text":"Loosely coupling and asynchronous communication between applications does not mean there is no contract to enforce some constraint between producer and consumer. When we talk about contract we can first think about schema as we did with XSD. In the world of JSON, JSON schema and Avro schemas can be used to define data structure of the message. As there is a need to get metadata around messaging, cloudevents is well accepted and adopted as a specification to describe event data. Also AsyncAPI establishes standards for events and messaging in the asynchronous world with an API view, so combining message schema, channels and binding definitions so we have most of the needed information for a consumer to access a data stream or queue. So the contract is defined with a schema. From an EDA design point of view, the producer owns the definition of the schema as it owns the main business entity life cycle, events are generated from. Producer will make sure the message complies with the schema at hand for serializing. On top of those specifications, there are technologies to support the contract management in the form of schema registry and API manager. The following figure gives us the foundations for integration between producer, schema registry and consumers. Schema Registry provides producer and consumer APIs so that these can project whether the event they are about to produce or consume is compatible with previous versions or compatible with the version they are expecting. For that, both producers and consumers require the schema definition at hand at serialization and deserialization time. This can be done either by: Reading the schema from a local local resource to your producer such as a file, variable, property (or kubernetes construct such as configmap or secret). Retrieving the schema definition from the Schema Registry given a name/ID. When the producer wants to send an event to a Kafka topic, two things happen: The producer makes sure the event to be sent complies to the schema. Otherwise, it errors out. Project whether the event they are about to produce or consume is compatible with previous versions or compatible with the version they are expecting. For the first action, the producer already holds the two things it needs: the schema definition the event to be sent needs to comply with and the event itself. The producer is therefore able to carry out that compliance check. However, for the second action, where the producer needs to make sure the schema definition your event complies with is compatible with the existing schema definition for the topic in question (if any), the producer might need to contact the schema registry. Producers (and consumers) maintain a local cache with the schema definitions (and their versions) along with their unique global IDs (all of these retrieved from the schema registry) for the topics they want to produce/consume events to/from. If the producer has a schema definition in its local cache that matches the schema definition at hand for the serialization of the event, it simply sends the event as the schema definitions match and the event complies with such schema definition. However, if the producer does not have any schema definition in its local cache that matches the schema defintion at hand for the serialization of the event, whether because these are different versions or the producer simply does not have any schema definition in its cache, it contacts the schema registry. If a schema defintion for the topic in question that matches the schema definition at hand at serialization time in the producer already exists in the schema registry, the producer simply retrieves the global unique ID for that schema definition to locally cache it along with the schema definition itself for future events to be sent so that it avoids contacting the schema registry again. If a schema definition for the topic in question that matches the schema definition at hand at serialization time in the producer does not already exist in the schema registry, the producer must be able to register the schema definition it has got at hand for the topic in question at serialization time in the schema registry so that such schema definition is now available for any consumer wanting or needing to consume events that comply with such schema definition. For that, the producer must be configured to be able to auto-register schemas and also be provided with the appropriate credentials to register schema definitions from the schema registry perspective if this implements any type of RBAC mechanism (as it is the case for IBM Event Streams). If the producer is not configured to auto-register schema definitions or its credentials does not allow it to register schema definitions, then the send event action will fail until there is a schema definition for the topic in question that mateches the schema definition at hand at serialization time in the producer registered in the schema registry. If the producer is configured to auto-register schema definitions and its credentials allows it to register schema definitions, the schema registry validates the compatibility of the schema definition at hand at serialization time in the producer with existing schema definitions for the topic in questions (if any) to make sure that, if this schema definition to be registered is a newer version of any existing schema definition, it is complatible so that no consumer gets affected by this new schema definition version. Afer the schema definition or newer version of an exisint schema definition is registered in the schema registry, the producer retrieves its global unique ID to mantain its local cache up to date. Warning We stongly recommend that producers are not allowed to register schema definitions in the schema registry for better governance and management of schema definitions. It is highly recommended that schema definitions registration is done by someone responsible for such task with appropriate role (such as an admin, an API manager, an asynchronous API manager, a development manager, an operator, etc). Once the producer has a schema definition in its local cache, along with its global unique id, that matches the schema definition at hand for serialization of the event to be sent, it produces the event to the Kafka topic using the appropriate AvroKafkaSerializer class. The schema definition global unique ID gets serialized along with the event so that the event does not need to travel along with its schema definition as it was the case in older messaging or eventing techonolgies and systems. By default, when a consumer reads an event, the schema definition for such event is retrieved from the schema registry by the deserializer using the global unique ID, which is specified in the event being consumed. The schema definition for an event is retrieved from the schema registry only once, when an event comes with a global unique ID that can not be found in the schema definition cache the consumer maintains locally. The schema definition global unique ID can be located in the event headers or in the event payload, depending on the configuration of the producer application. When locating the global unique ID in the event payload, the format of the data begins with a magic byte, used as a signal to consumers, followed by the global unique ID, and the message data as normal. For example: # ... [MAGIC_BYTE] [GLOBAL_UNIQUE_ID] [MESSAGE DATA] Be aware that non Java consumers use a C library that might require the schema definition at hand for the deserializer too (see https://github.com/confluentinc/confluent-kafka-python/issues/834) as opposed to letting the deserializer retrieve the schema definition from the schema registry as explained above. The strategy would be to have the schema loaded from the schema registry via API.","title":"Why this is important"},{"location":"technology/avro-schemas/#schema-registry","text":"With a pure open-source strategy, Event Streams within Cloud Pak for integration is using Apicu.io as schema registry. The Event Streams product documentation is doing an excellent job to present the schema registry, we do not need to rewrite the story, just give you some summary from Apicur.io and links to code samples.","title":"Schema Registry"},{"location":"technology/avro-schemas/#apicurio","text":"Apicur.io includes a schema registry to store schema definitions. It supports Avro, json, protobuf schemas, and an API registry to manage OpenApi and AsynchAPI. It is a Cloud-native Quarkus Java runtime for low memory footprint and fast deployment times. It supports different persistences like Kafka, Postgresql, Infinispan and supports different deployment models.","title":"Apicurio"},{"location":"technology/avro-schemas/#registry-characteristics","text":"Apicurio Registry is a datastore for sharing standard event schemas and API designs across API and event-driven architectures. In the messaging and event streaming world, data that are published to topics and queues often must be serialized or validated using a Schema. The registry supports adding, removing, and updating the following types of artifacts: OpenAPI, AsyncAPI, GraphQL, Apache Avro, Google protocol buffers, JSON Schema, Kafka Connect schema, WSDL, XML Schema (XSD). Schema can be created via Web Console, core REST API or Maven plugin It includes configurable rules to control the validity and compatibility. Client applications can dynamically push or pull the latest schema updates to or from Apicurio Registry at runtime. Apicurio is compatible with existing Confluent schema registry client applications. It includes client serializers/deserializers (Serdes) to validate Kafka and other message types at runtime. Operator-based installation of Apicurio Registry on OpenShift Use the concept of artifact group to collect schema and APIs logically related. Support search for artifacts by label, name, group, and description When using Kafka as persistence, special Kafka topic <kafkastore.topic> (default _schemas ), with a single partition, is used as a highly available write ahead log. All schemas, subject/version and ID metadata, and compatibility settings are appended as messages to this log. A Schema Registry instance therefore both produces and consumes messages under the _schemas topic. It produces messages to the log when, for example, new schemas are registered under a subject, or when updates to compatibility settings are registered. Schema Registry consumes from the _schemas log in a background thread, and updates its local caches on consumption of each new _schemas message to reflect the newly added schema or compatibility setting. Updating local state from the Kafka log in this manner ensures durability, ordering, and easy recoverability. The way Event Streams / Apicur.io has to handle schema association to topics is by schema name. Given we have a topic called orders, the schemas that will apply to it are avros-key (when using composite key) and orders-value (most likely based on cloudevents and then custom payload).","title":"Registry Characteristics"},{"location":"technology/avro-schemas/#apache-avro","text":"Avro is an open source data serialization system that helps with data exchange between systems, programming languages, and processing frameworks. Avro helps define a binary format for your data, as well as map it to the programming language of your choice.","title":"Apache Avro"},{"location":"technology/avro-schemas/#why-apache-avro","text":"There are several websites that discuss the Apache Avro data serialization system benefits over other messaging data protocols. A simple google search will list dozens of them. Here, we will highlight just a few from a Confluent blog post : It has a direct mapping to and from JSON It has a very compact format. The bulk of JSON, repeating every field name with every single record, is what makes JSON inefficient for high-volume usage. It is very fast. It has great bindings for a wide variety of programming languages so you can generate Java objects that make working with event data easier, but it does not require code generation so tools can be written generically for any data stream. It has a rich, extensible schema language defined in pure JSON It has the best notion of compatibility for evolving your data over time.","title":"Why Apache Avro"},{"location":"technology/avro-schemas/#data-schemas","text":"Avro relies on schemas. When Avro data is produced or read, the Avro schema for such piece of data is always present. This permits each datum to be written with no per-value overheads, making serialization both fast and small. An Avro schema defines the structure of the Avro data format.","title":"Data Schemas"},{"location":"technology/avro-schemas/#how-does-a-data-schema-look-like","text":"Let's see how a data schema to define a person's profile in a bank could look like: { \"namespace\" : \"banking.schemas.demo\" , \"name\" : \"profile\" , \"type\" : \"record\" , \"doc\" : \"Data schema to represent a profile for a banking entity\" , \"fields \" : [ { \"name\" : \"name\" , \"type\" : \"string\" }, { \"name\" : \"surname\" , \"type\" : \"string\" }, { \"name\" : \"age\" , \"type\" : \"int\" }, { \"name\" : \"account\" , \"type\" : \"banking.schemas.demo.account\" }, { \"name\" : \"gender\" , \"type\" : { \"type\" : \"enum\" , \"name\" : \"genderEnum\" , \"symbols\" : [ \"male\" , \"female\" ] } } ] } Notice: There are primitive data types like string and int but also complex types like record or enum . Complex type record requires a name attribute but it also can go along with a namespace attribute which is a JSON string that qualifies the name. Data schemas can be nested as you can see for the account data attribute. See below. { \"namespace\" : \"banking.schemas.demo\" , \"name\" : \"account\" , \"type\" : \"record\" , \"doc\" : \"Data schema to represent a customer account with the credit cards associated to it\" , \"fields\" : [ { \"name\" : \"id\" , \"type\" : \"string\" }, { \"name\" : \"savings\" , \"type\" : \"long\" }, { \"name\" : \"cards\" , \"type\" : { \"type\" : \"array\" , \"items\" : \"int\" } } ] } In the picture below we see two messages, one complies with the above Apache Avro data schema and the other does not: You might start realising by now the benefits of having the data flowing into your Apache Kafka event backbone validated against a schema. See next section for more. For more information on the Apache Avro Data Schema specification see https://avro.apache.org/docs/current/spec.html","title":"How does a data schema look like?"},{"location":"technology/avro-schemas/#benefits-of-using-data-schemas","text":"Clarity and Semantics : They document the usage of the event and the meaning of each field in the \"doc\" fields. Robustness : They protect downstream data consumers from malformed data, as only valid data will be permitted in the topic. They let the producers or consumers of data streams know the right fields are need in an event and what type each field is (contract for microservices). Compatibility : model and handle change in data format.","title":"Benefits of using Data Schemas"},{"location":"technology/avro-schemas/#avro-kafka-and-schema-registry","text":"In this section we try to put all the pieces together for the common flow of sending and receiving messages through an event backbone such as kafka having those messages serialized using the Apache Avro data serialization system and complying with their respective messages that are stored and managed by a schema registry. Avro relies on schemas. When Avro data is produced or read, the Avro schema for such piece of data is always present. An Avro schema defines the structure of the Avro data format. Schema Registry defines a scope in which schemas can evolve, and that scope is the subject. The name of the subject depends on the configured subject name strategy, which by default is set to derive subject name from topic name. In this case, the messages are serialized using Avro and sent to a kafka topic. Each message is a key-value pair. Either the message key or the message value, or both, can be serialized as Avro. Integration with Schema Registry means that Kafka messages do not need to be written with the entire Avro schema. Instead, Kafka messages are written with the schema id . The producers writing the messages and the consumers reading the messages must be using the same Schema Registry to get the same mapping between a schema and schema id.","title":"Avro, Kafka and Schema Registry"},{"location":"technology/avro-schemas/#more-reading","text":"","title":"More reading"},{"location":"technology/avro-schemas/#articles-and-product-documentation","text":"IBM Event Streams- Schemas overview Apicur.io schema registry documentation Confluent schema registry overview Producer code with reactive messaging and apicurio schema registry Consumer code with reactive messaging and apicurio schema registry","title":"Articles and product documentation"},{"location":"technology/avro-schemas/#labs","text":"We have developed two labs, one for the IBM Event Streams product that comes with the IBM CloudPak for Integration installed on a RedHat OpenShift cluster and the other for the IBM Event Streams on IBM Cloud offering, to get hands-on experience working with Apache Avro, data schemas and the IBM Event Streams Schema Registry: IBM Event Streams on IBM Cloud lab IBM Event Streams from IBM CloudPak for Integration lab","title":"Labs"},{"location":"technology/event-streams/","text":"To start playing with Event Streams we propose two set of labs: Use IBM Event Streams managed service on IBM Cloud Use Event Streams as part of the Cloud Pak for Integration Demonstrate Event Streams on OpenShift from A to Z \u00b6 Script in separate note IBM Event Streams within the IBM Cloud Pak for Integration \u00b6 The product documentation on how to install Event Streams on Openshift is where to go to get the last updates. Lab exercise: A detail step-by-step Event Streams installation on OpenShift Lab exercise: Demonstrate Starter Application Lab exercise: Schema Registry on OpenShift Container Platform Lab exercise: Monitoring IBM Event Streams on OpenShift Container Platform IBM Event Streams on IBM Cloud \u00b6 Lab exercise: Provision IBM Event Streams as Managed Services using IBM Cloud console Lab exercise: Addressing Security and access control Lab exercise: How to use Schema Registry with an entreprise plan Lab exercise: Monitoring IBM Event Streams on Cloud","title":"Event Streams"},{"location":"technology/event-streams/#demonstrate-event-streams-on-openshift-from-a-to-z","text":"Script in separate note","title":"Demonstrate Event Streams on OpenShift from A to Z"},{"location":"technology/event-streams/#ibm-event-streams-within-the-ibm-cloud-pak-for-integration","text":"The product documentation on how to install Event Streams on Openshift is where to go to get the last updates. Lab exercise: A detail step-by-step Event Streams installation on OpenShift Lab exercise: Demonstrate Starter Application Lab exercise: Schema Registry on OpenShift Container Platform Lab exercise: Monitoring IBM Event Streams on OpenShift Container Platform","title":"IBM Event Streams within the IBM Cloud Pak for Integration"},{"location":"technology/event-streams/#ibm-event-streams-on-ibm-cloud","text":"Lab exercise: Provision IBM Event Streams as Managed Services using IBM Cloud console Lab exercise: Addressing Security and access control Lab exercise: How to use Schema Registry with an entreprise plan Lab exercise: Monitoring IBM Event Streams on Cloud","title":"IBM Event Streams on IBM Cloud"},{"location":"technology/event-streams/starter-app/","text":"This section details walking through the generation of a starter application for usage with IBM Event Streams, as documented in the official product documentation . The Starter application is an excellent way to demonstrate sending and consuming messages. Prepare the starter app configuration \u00b6 Log into the IBM Event Streams Dashboard, and from the home page, click the Try the starter application button from the Getting Started page Click Download JAR from GitHub . This will open a new window to https://github.com/ibm-messaging/kafka-java-vertx-starter/releases Click the link for demo-all.jar from the latest release available. At the time of this writing, the latest version was 1.0.0 . Return to the Configure & run starter application window and click Generate properties . In dialog that pops up from the right-hand side of the screen, enter the following information: Starter application name: starter-app-[your-initials] Leave New topic selected and enter a Topic name of starter-app-[your-initials] . Click Generate and download .zip The figure above illustrates that you can download a zip file containing the properties of the application according to the Event-Streams cluster configuration, and a `p12` TLS certificate to be added to a local folder. In a Terminal window, unzip the generated ZIP file from the previous window and move demo-all.jar file into the same folder. Review the extracted kafka.properties to understand how Event Streams has generated credentials and configuration information for this sample application to connect. Run starter application \u00b6 This Starter application will run locally to the user's laptop with the command: java -jar target/demo-all.jar -Dproperties_path = ./kafka.properties 1. As an alternate method, we have packaged this app in a docker image: quay.io/ibmcase/es-demo docker run -ti -p 8080 :8080 -v $( pwd ) /kafka.properties:/deployments/kafka.properties -v $( pwd ) /truststore.p12:/deployments/truststore.p12 quay.io/ibmcase/es-demo Wait until you see the string Application started in X ms in the output and then visit the application's user interface via http://localhost:8080 . Once in the User Interface, enter a message to be contained for the Kafka record value then click Start producing . Wait a few moments until the UI updates to show some of the confirmed produced messages and offsets, then click on Start consuming on the right side of the application. In the IBM Event Streams user interface, go to the topic where you send the messages to and make sure messages have actually made it. You can leave the application running for the rest of the lab or you can do the following actions on the application If you would like to stop the application from producing, you can click Stop producing . If you would like to stop the application from consuming, you can click Stop consuming . If you would like to stop the application entirely, you can input Control+C in the Terminal session where the application is running. An alternative sample application can be leveraged from the official documentation to generate higher amounts of load. Deploy to OpenShift \u00b6 This application can also be deployed to OpenShift. Here are the steps: Use the same kafka.properties and truststore.p12 files you have downloaded with the starter application to create two kubernetes secrets holding these files in your OpenShift cluster oc create secret generic demo-app-secret --from-file = ./kafka.properties oc create secret generic truststore-cert --from-file = ./truststore.p12 Clone the following GitHub repo that contains the Kubernetes artifacts that will run the starter application. git clone https://github.com/ibm-cloud-architecture/eda-quickstarts.git Change directory to where those Kubernetes artefacts are. cd eda-quickstarts/kafka-java-vertz-starter Deploy the Kubernetes artefacts. oc apply -k app-deployment Get the route to the starter application running on your OpenShift cluster. oc get route es-demo -o = jsonpath = '{.status.ingress[].host}' Point your browser to that url to work with the IBM Event Streams Starter Application. The source code for this application is in this git repo: ibm-messaging/kafka-java-vertx-starter . Even though the application is running internally in OpenShift, it uses the external kafka listener as that is how the kafka.properties are provided by IBM Event Streams by default. In an attempt to not overcomplicate this task, it is used what IBM Event Streams provides out of the box.","title":"Starter App"},{"location":"technology/event-streams/starter-app/#prepare-the-starter-app-configuration","text":"Log into the IBM Event Streams Dashboard, and from the home page, click the Try the starter application button from the Getting Started page Click Download JAR from GitHub . This will open a new window to https://github.com/ibm-messaging/kafka-java-vertx-starter/releases Click the link for demo-all.jar from the latest release available. At the time of this writing, the latest version was 1.0.0 . Return to the Configure & run starter application window and click Generate properties . In dialog that pops up from the right-hand side of the screen, enter the following information: Starter application name: starter-app-[your-initials] Leave New topic selected and enter a Topic name of starter-app-[your-initials] . Click Generate and download .zip The figure above illustrates that you can download a zip file containing the properties of the application according to the Event-Streams cluster configuration, and a `p12` TLS certificate to be added to a local folder. In a Terminal window, unzip the generated ZIP file from the previous window and move demo-all.jar file into the same folder. Review the extracted kafka.properties to understand how Event Streams has generated credentials and configuration information for this sample application to connect.","title":"Prepare the starter app configuration"},{"location":"technology/event-streams/starter-app/#run-starter-application","text":"This Starter application will run locally to the user's laptop with the command: java -jar target/demo-all.jar -Dproperties_path = ./kafka.properties 1. As an alternate method, we have packaged this app in a docker image: quay.io/ibmcase/es-demo docker run -ti -p 8080 :8080 -v $( pwd ) /kafka.properties:/deployments/kafka.properties -v $( pwd ) /truststore.p12:/deployments/truststore.p12 quay.io/ibmcase/es-demo Wait until you see the string Application started in X ms in the output and then visit the application's user interface via http://localhost:8080 . Once in the User Interface, enter a message to be contained for the Kafka record value then click Start producing . Wait a few moments until the UI updates to show some of the confirmed produced messages and offsets, then click on Start consuming on the right side of the application. In the IBM Event Streams user interface, go to the topic where you send the messages to and make sure messages have actually made it. You can leave the application running for the rest of the lab or you can do the following actions on the application If you would like to stop the application from producing, you can click Stop producing . If you would like to stop the application from consuming, you can click Stop consuming . If you would like to stop the application entirely, you can input Control+C in the Terminal session where the application is running. An alternative sample application can be leveraged from the official documentation to generate higher amounts of load.","title":"Run starter application"},{"location":"technology/event-streams/starter-app/#deploy-to-openshift","text":"This application can also be deployed to OpenShift. Here are the steps: Use the same kafka.properties and truststore.p12 files you have downloaded with the starter application to create two kubernetes secrets holding these files in your OpenShift cluster oc create secret generic demo-app-secret --from-file = ./kafka.properties oc create secret generic truststore-cert --from-file = ./truststore.p12 Clone the following GitHub repo that contains the Kubernetes artifacts that will run the starter application. git clone https://github.com/ibm-cloud-architecture/eda-quickstarts.git Change directory to where those Kubernetes artefacts are. cd eda-quickstarts/kafka-java-vertz-starter Deploy the Kubernetes artefacts. oc apply -k app-deployment Get the route to the starter application running on your OpenShift cluster. oc get route es-demo -o = jsonpath = '{.status.ingress[].host}' Point your browser to that url to work with the IBM Event Streams Starter Application. The source code for this application is in this git repo: ibm-messaging/kafka-java-vertx-starter . Even though the application is running internally in OpenShift, it uses the external kafka listener as that is how the kafka.properties are provided by IBM Event Streams by default. In an attempt to not overcomplicate this task, it is used what IBM Event Streams provides out of the box.","title":"Deploy to OpenShift"},{"location":"technology/event-streams/es-cp4i/","text":"Info Updated 10/03/2022 In this tutorial you will learn how to install Event Streams on OpenShift, using a the Administration Console, or using CLI. We propose two installation tutorials: one using the OpenShift Admin console, then the Event Streams console to create Kafka topics and use the Starter Application, to validate the installation. one using oc CLI from your terminal or from Cloud Shell Terminal. The Kafka Cluster configuration is for a development or staging environment with no persistence. Once the installation is successful, the final pods running in the eventstreams project are: sh dev-entity-operator-69c6b7cf87-bfxbw 2/2 Running 0 43s dev-kafka-0 1/1 Running 0 2m dev-kafka-1 1/1 Running 0 2m dev-kafka-2 1/1 Running 0 2m dev-zookeeper-0 1/1 Running 0 2m46s dev-zookeeper-1 1/1 Running 0 3h23m dev-zookeeper-2 1/1 Running 0 3h23m Updated October 07/2021 - Operator Release 2.4 - Product Release 10.4 - Kafka 2.8 We recommend to read the following \"structuring your deployment\" chapter from product documentation to give you more insight of the things to consider for deployment. In this tutorial we select to deploy Event Streams in one namespace, and the Operator to monitor multiple namespaces. Prerequisites \u00b6 Get access to an OpenShift Cluster. Get oc CLI from OpenShift Admin Console Install Cloudctl CLIs \u00b6 Install IBM Cloud Pak CLI: Cloudctl is a command line tool to manage Container Application Software for Enterprises (CASEs). This CLI will allow us to manage Cloud Pak related components as well as software, like IBM Event Streams, installed through any IBM Cloud Pak. In order to install it, execute the following commands in your IBM Cloud Shell: Download the IBM Cloud Pak CLI (example below is for Mac but other downloads are available here ): curl -L https://github.com/IBM/cloud-pak-cli/releases/latest/download/cloudctl-darwin-amd64.tar.gz -o cloudctl.tar.gz * Untar it tar -xvf cloudctl.tar.gz Rename it as cloudctl and move to a folder within your PATH: mv cloudctl-darwin-amd64 cloudctl Make sure your IBM Cloud Pak CLI is in the path: which cloudctl Make sure your IBM Cloud Pak CLI works: cloudctl help See also the product documentation for prerequisites details Install IBM catalog \u00b6 Use the EDA Gitops project with the different operator subscriptions and IBM catalog to define the IBM Operator catalog. # List existing catalog oc get catalogsource -n openshift-marketplace # Verifi ibm-operator-catalog is present in the list # if not add IBM catalog oc apply -f https://raw.githubusercontent.com/ibm-cloud-architecture/eda-gitops-catalog/main/ibm-catalog/catalog_source.yaml Get IBM Software entitlement key and create a secret \u00b6 Obtain IBM license entitlement key . Verify you can access the IBM Image repository with docker login cp.icr.io --username cp --password <key-copied> Create a OpenShift secret in the openshift-operator project # Get on good project oc project openshift-operators # Verify if secret exists oc describe secret ibm-entitlement-key # Create if needed oc create secret docker-registry ibm-entitlement-key \\ --docker-username = cp \\ --docker-server = cp.icr.io \\ --namespace = eventstreams \\ --docker-password = your_entitlement_key Attention This secret needs to be present in any namespace where an Event Streams cluster will be deployed. Install IBM foundational services \u00b6 When using security with IAM and other services, Event Streams needs IBM foundations services. It will install it automatically if you install the operator at the cluster level. In case you need to control the deployment and if Cloud Pak for Integration is not installed yet, you need to install the IBM foundational services operator for that, follow these simple instructions . Create the same ibm-entitlement-key in the ibm-common-services namespace. # verify operator installed oc get operators -n ibm-common-services # -> response: ibm-common-service-operator.openshift-operators ibm-namespace-scope-operator.ibm-common-services ibm-odlm.ibm-common-services Those are the deployments the foundational services are creating: NAME READY UP-TO-DATE AVAILABLE AGE ibm-common-service-webhook 1/1 1 1 68m ibm-namespace-scope-operator 1/1 1 1 69m operand-deployment-lifecycle-manager 1/1 1 1 68m secretshare 1/1 1 1 68m Recalls that operands are what operators manage. Install Event Streams Using OpenShift Console \u00b6 As Cluster Administrator, create a project using Home > Projects, and create button, and enter eventstreams as project name. The goal is to create a shareable Kafka cluster. As Administrator, use Openshift console -> Operators > OperatorHub to search for Event Streams operator, then install the operator by selecting the All namespaces on the cluster , the version v2.4. The Operator will monitor all namespaces. The installation of this operator will also include pre-requisites operators like Cloud Pak foundational services (3.11) Once the Operator is ready, go to the Installed Operator under the eventstreams project Create an Event Streams cluster instance using the operator user interface: Enter the minimum information for a development cluster, like name, license,.. Or go to the Yaml view and select one of the proposed Sample: The Development configuration should be enough to get you started. For production deployment see this article . Do not forget to set spec.license.accept to true. Before creating the instance, we recommend you read the security summary so you can relate the authentication configured by default with what application needs to use to connect to the Kafka cluster. For example the following declaration stipulates to use TLS authentication for internal communication, and SCRAM for external connection. ```yaml listeners: plain: port: 9092 type: internal tls: false external: authentication: type: scram-sha-512 type: route tls: authentication: type: tls ``` The first deployment of Event Streams, there may be creation of new deployments into the ibm-common-services if those services were not present before. Here is the updated list of ibm-common-services deployments: NAME READY UP-TO-DATE AVAILABLE AGE auth-idp 1 /1 1 1 27m auth-pap 1 /1 1 1 32m auth-pdp 1 /1 1 1 32m cert-manager-cainjector 1 /1 1 1 31m cert-manager-controller 1 /1 1 1 31m cert-manager-webhook 1 /1 1 1 30m common-web-ui 1 /1 1 1 20m configmap-watcher 1 /1 1 1 31m default-http-backend 1 /1 1 1 31m iam-policy-controller 1 /1 1 1 32m ibm-cert-manager-operator 1 /1 1 1 32m ibm-common-service-webhook 1 /1 1 1 3h12m ibm-commonui-operator 1 /1 1 1 32m ibm-iam-operator 1 /1 1 1 33m ibm-ingress-nginx-operator 1 /1 1 1 32m ibm-management-ingress-operator 1 /1 1 1 33m ibm-mongodb-operator 1 /1 1 1 32m ibm-monitoring-grafana 1 /1 1 1 32m ibm-monitoring-grafana-operator 1 /1 1 1 33m ibm-namespace-scope-operator 1 /1 1 1 3h12m ibm-platform-api-operator 1 /1 1 1 31m management-ingress 1 /1 1 1 23m nginx-ingress-controller 1 /1 1 1 31m oidcclient-watcher 1 /1 1 1 32m operand-deployment-lifecycle-manager 1 /1 1 1 3h11m platform-api 1 /1 1 1 31m secret-watcher 1 /1 1 1 32m secretshare 1 /1 1 1 3h12m It could take few minutes to get these pods up and running. The result of this cluster creation should looks like: with the list of pod as illustrated at the top of this article. See also Cloud Pak for integration documentation for other deployment considerations. Adding users and teams \u00b6 You need to be a platform administrator to create users and teams in IAM. To get the admin user credential, use the following command: oc get secret platform-auth-idp-credentials -o jsonpath = '{.data.admin_password}' -n ibm-common-services | base64 && echo \"\" Log into Event Streams \u00b6 You can get the User Interface end point by doing the following command. oc get route dev-ibm-es-ui -o jsonpath = '{.spec.host}' Change dev prefix with the name of your Kafka cluster you defined earlier. Or using the Admin Console Select the Event Streams Operator in the project where you install it Select Click on the IBM Event Streams Operator and then on the Event Streams option listed at the top bar Click on the IBM Event Streams cluster instance you want to access to its console and see the Admin UI attribute that displays the route to this IBM Event Streams instance's console. Click on the route link and enter your IBM Event Streams credentials. Here is the Console home page: Create Event Streams Topics \u00b6 This section is a generic example of the steps to proceed to define a topic with Event Streams on OpenShift. The example is to define a topic named INBOUND with 1 partition and a replica set to 3. Log into your IBM Event Streams instance through the UI as explained in the previous section. Click on the Topics option on the navigation bar on the left. In the topics page, click on the Create topic blue button on the top right corner Provide a name for your topic. Leave Partitions at 1. Depending on how long you want messages to persist you can change this. You can leave Replication Factor at the default 3. Click Create. Make sure the topic has been created by navigating to the topics section on the IBM Event Streams user inteface you can find an option for in the left hand side menu bar. Run starter application \u00b6 See separate note Install Event Streams Using CLIs \u00b6 This section is an alternate of using OpenShift Console. We are using our GitOps catalog repository which defines the different operators and scripts we can use to install Event Streams and other services via scripts. All can be automatized with ArgoCD / OpenShift GitOps. Create a project to host Event Streams cluster: oc new-project eventstreams * Clone our eda-gitops-catalog project git clone https://github.com/ibm-cloud-architecture/eda-gitops-catalog.git * Create the ibm-entitlement-key secret in this project. oc create secret docker-registry ibm-entitlement-key \\ --docker-username = cp \\ --docker-server = cp.icr.io \\ --namespace = eventstreams \\ --docker-password = your_entitlement_key Install Event Streams Operator subscriptions oc apply -k cp4i-operators/event-streams/operator/overlays/v2.4/ Install one Event Streams instance: Instances of Event Streams can be created after the Event Streams operator is installed. You can use te OpenShift console or our predefined cluster definition: oc apply -k cp4i-operators/event-streams/instances/dev/ If you want to do the same thing for a production cluster oc apply -k cp4i-operators/event-streams/instances/prod-small/ \u00b6 Common operations to perform on cluster \u00b6 We list here a set of common operations to perform on top of Event Streams Cluster. Download ES CLI plugin \u00b6 From Event Streams Console, go to the \"Find more in the Toolbox\" tile, and then select IBM Event Streams command-line interface , then select your target operating system: Initialize the Event Streams CLI plugin (make sure you provide the namespace where your IBM Event Streams instance is installed on as the command will fail if you dont have cluster wide admin permissions) $ cloudctl es init -n eventstreams IBM Cloud Platform Common Services endpoint: https://cp-console.apps.eda-sandbox.gse-ocp.net Namespace: integration Name: kafka IBM Cloud Pak for Integration UI address: https://integration-navigator-pn-integration.apps.eda-sandbox.gse-ocp.net Event Streams API endpoint: https://kafka-ibm-es-admapi-external-integration.apps.eda-sandbox.gse-ocp.net Event Streams API status: OK Event Streams UI address: https://kafka-ibm-es-ui-integration.apps.eda-sandbox.gse-ocp.net Apicurio Registry endpoint: https://kafka-ibm-es-ac-reg-external-integration.apps.eda-sandbox.gse-ocp.net Event Streams bootstrap address: kafka-kafka-bootstrap-integration.apps.eda-sandbox.gse-ocp.net:443 OK Access to Event Streams Console using CLI \u00b6 In order to log into IBM Event Streams console through the CLI, we are going to use the oc OpenShift CLI, the cloudctl Cloud Pak CLI and the es Cloud Pak CLI plugin. We assume you are already logged into your OpenShift cluster. Get your Cloud Pak Console route (you may need cluster wide admin permissions to do so as the Cloud Pak Console is usually installed in the ibm-common-services namespace by the cluster admins) $ oc get routes -n ibm-common-services | grep console cp-console cp-console.apps.eda-sandbox.gse-ocp.net icp-management-ingress https reencrypt/Redirect None Log into IBM Event Streams using the Cloud Pak console route from the previous step: $ cloudctl login -a https://cp-console.apps.eda-sandbox.gse-ocp.net --skip-ssl-validation Username> user50 Password> Authenticating... OK Targeted account mycluster Account Enter a namespace > integration Targeted namespace integration Configuring kubectl ... Property \"clusters.mycluster\" unset. Property \"users.mycluster-user\" unset. Property \"contexts.mycluster-context\" unset. Cluster \"mycluster\" set. User \"mycluster-user\" set. Context \"mycluster-context\" created. Switched to context \"mycluster-context\" . OK Configuring helm: /Users/user/.helm OK Create Topic with es CLI \u00b6 Create the topic with the desi specification. cloudctl es topic-create --name INBOUND --partitions 1 --replication-factor 3 Make sure the topic has been created by listing the topics. $ cloudctl es topics Topic name INBOUND OK Get Kafka Bootstrap Url \u00b6 For an application to access the Kafka Broker, we need to get the bootstrap URL. UI \u00b6 You can find your IBM Event Streams Kakfa bootstrap url if you log into the IBM Event Streams user interface, and click on the Connect to this cluster option displayed on the dashboard. This will display a menu where you will see a url to the left of the Generate SCAM credentials button. Make sure that you are on the External Connection . CLI \u00b6 You can find your IBM Event Streams and Kakfa bootstrap url when you init the IBM Event Streams Cloud Pak CLI plugin on the last line: $ cloudctl es init -n eventstreams IBM Cloud Platform Common Services endpoint: https://cp-console.apps.eda-sandbox-delta.gse-ocp.net Namespace: integration Name: kafka IBM Cloud Pak for Integration UI address: https://integration-navigator-pn-integration.apps.eda-sandbox-delta.gse-ocp.net Event Streams API endpoint: https://kafka-ibm-es-admapi-external-integration.apps.eda-sandbox-delta.gse-ocp.net Event Streams API status: OK Event Streams UI address: https://kafka-ibm-es-ui-integration.apps.eda-sandbox-delta.gse-ocp.net Apicurio Registry endpoint: https://kafka-ibm-es-ac-reg-external-integration.apps.eda-sandbox-delta.gse-ocp.net Event Streams bootstrap address: kafka-kafka-bootstrap-integration.apps.eda-sandbox-delta.gse-ocp.net:443 OK Generate SCRAM Service Credentials \u00b6 For an application to connect to an Event Streams instance through the secured external listener, it needs SCRAM credentials to act as service credentials to authenticate the application. We also need TLS certificate to encrypt the communication with the brokers. UI \u00b6 Log into the IBM Event Streams user interface as explained previously in this readme and click on the Connect to this cluster option displayed on the dashboard. This will display a menu. Make sure that you are on the External Connection . Click on the Generate SCRAM credentials button. Introduce a name for your credentials and choose the option that better suits the needs of your applications (this will create RBAC permissions for you credentials so that a service credentials can do only what it needs to do). For this demo, select Produce messages, consume messages and create topics and schemas last option. Decide whether your service credentials need to have the ability to access all topics or certain topics only. For this demo, select All Topics and then click Next. Decide whether your service credentials need to have the ability to access all consumer groups or certain specific consumer groups only. For this demo, select All Consumer Groups and click Next. Decide whether your service credentials need to have the ability to access all transactional IDs or certain specific transactional IDs only. For this demo, select All transaction IDs and click on Generate credentials . Take note of the set of credentials displayed on screen. You will need to provide your applications with these in order to get authenticated and authorized with your IBM Event Streams instance. If you did not take note of your SCRAM credentials or you forgot these, the above will create a KafkaUser object in OpenShift that is interpreted by the IBM Event Streams Operator. You can see this KafkaUser if you go to the OpenShift console, click on Operators --> Installed Operators on the right hand side menu, then click on the IBM Event Streams operator and finally click on Kafka Users at the top bar menu. If you click on your Kafka User , you will see what is the Kubernetes secret behind holding your SCRAM credentials details. Click on that secret and you will be able to see again your SCRAM password (your SCRAM username is the same name as the Kafka User created or the secret holding your SCRAM password ) CLI \u00b6 Log into your IBM Event Streams instance through the CLI. Create your SCRAM service credentials with the following command (adjust the topics, consumer groups, transaction IDs, etc permissions your SCRAM service credentials should have in order to satisfy your application requirements): $ cloudctl es kafka-user-create --name test-credentials-cli --consumer --producer --schema-topic-create --all-topics --all-groups --all-txnids --auth-type scram-sha-512 KafkaUser name Authentication Authorization Username Secret test-credentials-cli scram-sha-512 simple EntityOperator has not created corresponding username EntityOperator has not created corresponding secret Resource type Name Pattern type Host Operation topic * literal * Read topic __schema_ prefix * Read topic * literal * Write topic * literal * Create topic __schema_ prefix * Alter group * literal * Read transactionalId * literal * Write Created KafkaUser test-credentials-cli. OK List the KafkaUser objects to make sure yours has been created: $ cloudctl es kafka-users KafkaUser name Authentication Authorization test-credentials scram-sha-512 simple test-credentials-cli scram-sha-512 simple OK To retrieve your credentials execute the following command: $ cloudctl es kafka-user test-credentials-cli KafkaUser name Authentication Authorization Username Secret test-credentials-cli scram-sha-512 simple test-credentials-cli test-credentials-cli Resource type Name Pattern type Host Operation topic * literal * Read topic __schema_ prefix * Read topic * literal * Write topic * literal * Create topic __schema_ prefix * Alter group * literal * Read transactionalId * literal * Write OK Above you can see your SCRAM username under Username and the secret holding your SCRAM password under Secret . In order to retrieve the password, execute the following command: $ oc get secret test-credentials-cli -o jsonpath = '{.data.password}' | base64 --decode ******* NEXT: For more information about how to connect to your cluste, read the IBM Event Streams product documentation Get Event Streams TLS Certificates \u00b6 In this section we are going to see how to download the TLS certificats to securely connect to our IBM Event Streams instance. UI \u00b6 Log into the IBM Event Streams console user interface as explained before in this readme. Click on the Connect to this cluster option displayed on the dashboard. This will display a menu where you will see a Certificates section: Depending on what language your application is written into, you will need a PKCS12 certificate or a PEM certificate . Click on Download certificate for any of the options you need. If it is the PKCS12 certificate bear in mind it comes with a password for the truststore. You don't need to write this down as it will display any time you click on Download certificate button. CLI \u00b6 Log into IBM Event Streams through the CLI as already explained before in this readme. To retrieve the PKCS12 certificate execute the following command: $ cloudctl es certificates --format p12 Trustore password is ******** Certificate successfully written to /Users/testUser/Downloads/es-cert.p12. OK To retrieve the PEM certificate execute the following command: $ cloudctl es certificates --format pem Certificate successfully written to /Users/testUser/Downloads/es-cert.pem. OK","title":"Deploy Event-Streams"},{"location":"technology/event-streams/es-cp4i/#prerequisites","text":"Get access to an OpenShift Cluster. Get oc CLI from OpenShift Admin Console","title":"Prerequisites"},{"location":"technology/event-streams/es-cp4i/#install-cloudctl-clis","text":"Install IBM Cloud Pak CLI: Cloudctl is a command line tool to manage Container Application Software for Enterprises (CASEs). This CLI will allow us to manage Cloud Pak related components as well as software, like IBM Event Streams, installed through any IBM Cloud Pak. In order to install it, execute the following commands in your IBM Cloud Shell: Download the IBM Cloud Pak CLI (example below is for Mac but other downloads are available here ): curl -L https://github.com/IBM/cloud-pak-cli/releases/latest/download/cloudctl-darwin-amd64.tar.gz -o cloudctl.tar.gz * Untar it tar -xvf cloudctl.tar.gz Rename it as cloudctl and move to a folder within your PATH: mv cloudctl-darwin-amd64 cloudctl Make sure your IBM Cloud Pak CLI is in the path: which cloudctl Make sure your IBM Cloud Pak CLI works: cloudctl help See also the product documentation for prerequisites details","title":"Install Cloudctl CLIs"},{"location":"technology/event-streams/es-cp4i/#install-ibm-catalog","text":"Use the EDA Gitops project with the different operator subscriptions and IBM catalog to define the IBM Operator catalog. # List existing catalog oc get catalogsource -n openshift-marketplace # Verifi ibm-operator-catalog is present in the list # if not add IBM catalog oc apply -f https://raw.githubusercontent.com/ibm-cloud-architecture/eda-gitops-catalog/main/ibm-catalog/catalog_source.yaml","title":"Install IBM catalog"},{"location":"technology/event-streams/es-cp4i/#get-ibm-software-entitlement-key-and-create-a-secret","text":"Obtain IBM license entitlement key . Verify you can access the IBM Image repository with docker login cp.icr.io --username cp --password <key-copied> Create a OpenShift secret in the openshift-operator project # Get on good project oc project openshift-operators # Verify if secret exists oc describe secret ibm-entitlement-key # Create if needed oc create secret docker-registry ibm-entitlement-key \\ --docker-username = cp \\ --docker-server = cp.icr.io \\ --namespace = eventstreams \\ --docker-password = your_entitlement_key Attention This secret needs to be present in any namespace where an Event Streams cluster will be deployed.","title":"Get IBM Software entitlement key and create a secret"},{"location":"technology/event-streams/es-cp4i/#install-ibm-foundational-services","text":"When using security with IAM and other services, Event Streams needs IBM foundations services. It will install it automatically if you install the operator at the cluster level. In case you need to control the deployment and if Cloud Pak for Integration is not installed yet, you need to install the IBM foundational services operator for that, follow these simple instructions . Create the same ibm-entitlement-key in the ibm-common-services namespace. # verify operator installed oc get operators -n ibm-common-services # -> response: ibm-common-service-operator.openshift-operators ibm-namespace-scope-operator.ibm-common-services ibm-odlm.ibm-common-services Those are the deployments the foundational services are creating: NAME READY UP-TO-DATE AVAILABLE AGE ibm-common-service-webhook 1/1 1 1 68m ibm-namespace-scope-operator 1/1 1 1 69m operand-deployment-lifecycle-manager 1/1 1 1 68m secretshare 1/1 1 1 68m Recalls that operands are what operators manage.","title":"Install IBM foundational services"},{"location":"technology/event-streams/es-cp4i/#install-event-streams-using-openshift-console","text":"As Cluster Administrator, create a project using Home > Projects, and create button, and enter eventstreams as project name. The goal is to create a shareable Kafka cluster. As Administrator, use Openshift console -> Operators > OperatorHub to search for Event Streams operator, then install the operator by selecting the All namespaces on the cluster , the version v2.4. The Operator will monitor all namespaces. The installation of this operator will also include pre-requisites operators like Cloud Pak foundational services (3.11) Once the Operator is ready, go to the Installed Operator under the eventstreams project Create an Event Streams cluster instance using the operator user interface: Enter the minimum information for a development cluster, like name, license,.. Or go to the Yaml view and select one of the proposed Sample: The Development configuration should be enough to get you started. For production deployment see this article . Do not forget to set spec.license.accept to true. Before creating the instance, we recommend you read the security summary so you can relate the authentication configured by default with what application needs to use to connect to the Kafka cluster. For example the following declaration stipulates to use TLS authentication for internal communication, and SCRAM for external connection. ```yaml listeners: plain: port: 9092 type: internal tls: false external: authentication: type: scram-sha-512 type: route tls: authentication: type: tls ``` The first deployment of Event Streams, there may be creation of new deployments into the ibm-common-services if those services were not present before. Here is the updated list of ibm-common-services deployments: NAME READY UP-TO-DATE AVAILABLE AGE auth-idp 1 /1 1 1 27m auth-pap 1 /1 1 1 32m auth-pdp 1 /1 1 1 32m cert-manager-cainjector 1 /1 1 1 31m cert-manager-controller 1 /1 1 1 31m cert-manager-webhook 1 /1 1 1 30m common-web-ui 1 /1 1 1 20m configmap-watcher 1 /1 1 1 31m default-http-backend 1 /1 1 1 31m iam-policy-controller 1 /1 1 1 32m ibm-cert-manager-operator 1 /1 1 1 32m ibm-common-service-webhook 1 /1 1 1 3h12m ibm-commonui-operator 1 /1 1 1 32m ibm-iam-operator 1 /1 1 1 33m ibm-ingress-nginx-operator 1 /1 1 1 32m ibm-management-ingress-operator 1 /1 1 1 33m ibm-mongodb-operator 1 /1 1 1 32m ibm-monitoring-grafana 1 /1 1 1 32m ibm-monitoring-grafana-operator 1 /1 1 1 33m ibm-namespace-scope-operator 1 /1 1 1 3h12m ibm-platform-api-operator 1 /1 1 1 31m management-ingress 1 /1 1 1 23m nginx-ingress-controller 1 /1 1 1 31m oidcclient-watcher 1 /1 1 1 32m operand-deployment-lifecycle-manager 1 /1 1 1 3h11m platform-api 1 /1 1 1 31m secret-watcher 1 /1 1 1 32m secretshare 1 /1 1 1 3h12m It could take few minutes to get these pods up and running. The result of this cluster creation should looks like: with the list of pod as illustrated at the top of this article. See also Cloud Pak for integration documentation for other deployment considerations.","title":"Install Event Streams Using OpenShift Console"},{"location":"technology/event-streams/es-cp4i/#adding-users-and-teams","text":"You need to be a platform administrator to create users and teams in IAM. To get the admin user credential, use the following command: oc get secret platform-auth-idp-credentials -o jsonpath = '{.data.admin_password}' -n ibm-common-services | base64 && echo \"\"","title":"Adding users and teams"},{"location":"technology/event-streams/es-cp4i/#log-into-event-streams","text":"You can get the User Interface end point by doing the following command. oc get route dev-ibm-es-ui -o jsonpath = '{.spec.host}' Change dev prefix with the name of your Kafka cluster you defined earlier. Or using the Admin Console Select the Event Streams Operator in the project where you install it Select Click on the IBM Event Streams Operator and then on the Event Streams option listed at the top bar Click on the IBM Event Streams cluster instance you want to access to its console and see the Admin UI attribute that displays the route to this IBM Event Streams instance's console. Click on the route link and enter your IBM Event Streams credentials. Here is the Console home page:","title":"Log into Event Streams"},{"location":"technology/event-streams/es-cp4i/#create-event-streams-topics","text":"This section is a generic example of the steps to proceed to define a topic with Event Streams on OpenShift. The example is to define a topic named INBOUND with 1 partition and a replica set to 3. Log into your IBM Event Streams instance through the UI as explained in the previous section. Click on the Topics option on the navigation bar on the left. In the topics page, click on the Create topic blue button on the top right corner Provide a name for your topic. Leave Partitions at 1. Depending on how long you want messages to persist you can change this. You can leave Replication Factor at the default 3. Click Create. Make sure the topic has been created by navigating to the topics section on the IBM Event Streams user inteface you can find an option for in the left hand side menu bar.","title":"Create Event Streams Topics"},{"location":"technology/event-streams/es-cp4i/#run-starter-application","text":"See separate note","title":"Run starter application"},{"location":"technology/event-streams/es-cp4i/#install-event-streams-using-clis","text":"This section is an alternate of using OpenShift Console. We are using our GitOps catalog repository which defines the different operators and scripts we can use to install Event Streams and other services via scripts. All can be automatized with ArgoCD / OpenShift GitOps. Create a project to host Event Streams cluster: oc new-project eventstreams * Clone our eda-gitops-catalog project git clone https://github.com/ibm-cloud-architecture/eda-gitops-catalog.git * Create the ibm-entitlement-key secret in this project. oc create secret docker-registry ibm-entitlement-key \\ --docker-username = cp \\ --docker-server = cp.icr.io \\ --namespace = eventstreams \\ --docker-password = your_entitlement_key Install Event Streams Operator subscriptions oc apply -k cp4i-operators/event-streams/operator/overlays/v2.4/ Install one Event Streams instance: Instances of Event Streams can be created after the Event Streams operator is installed. You can use te OpenShift console or our predefined cluster definition: oc apply -k cp4i-operators/event-streams/instances/dev/ If you want to do the same thing for a production cluster","title":"Install Event Streams Using CLIs"},{"location":"technology/event-streams/es-cp4i/#oc-apply-k-cp4i-operatorsevent-streamsinstancesprod-small","text":"","title":"oc apply -k cp4i-operators/event-streams/instances/prod-small/\n"},{"location":"technology/event-streams/es-cp4i/#common-operations-to-perform-on-cluster","text":"We list here a set of common operations to perform on top of Event Streams Cluster.","title":"Common operations to perform on cluster"},{"location":"technology/event-streams/es-cp4i/#download-es-cli-plugin","text":"From Event Streams Console, go to the \"Find more in the Toolbox\" tile, and then select IBM Event Streams command-line interface , then select your target operating system: Initialize the Event Streams CLI plugin (make sure you provide the namespace where your IBM Event Streams instance is installed on as the command will fail if you dont have cluster wide admin permissions) $ cloudctl es init -n eventstreams IBM Cloud Platform Common Services endpoint: https://cp-console.apps.eda-sandbox.gse-ocp.net Namespace: integration Name: kafka IBM Cloud Pak for Integration UI address: https://integration-navigator-pn-integration.apps.eda-sandbox.gse-ocp.net Event Streams API endpoint: https://kafka-ibm-es-admapi-external-integration.apps.eda-sandbox.gse-ocp.net Event Streams API status: OK Event Streams UI address: https://kafka-ibm-es-ui-integration.apps.eda-sandbox.gse-ocp.net Apicurio Registry endpoint: https://kafka-ibm-es-ac-reg-external-integration.apps.eda-sandbox.gse-ocp.net Event Streams bootstrap address: kafka-kafka-bootstrap-integration.apps.eda-sandbox.gse-ocp.net:443 OK","title":"Download ES CLI plugin"},{"location":"technology/event-streams/es-cp4i/#access-to-event-streams-console-using-cli","text":"In order to log into IBM Event Streams console through the CLI, we are going to use the oc OpenShift CLI, the cloudctl Cloud Pak CLI and the es Cloud Pak CLI plugin. We assume you are already logged into your OpenShift cluster. Get your Cloud Pak Console route (you may need cluster wide admin permissions to do so as the Cloud Pak Console is usually installed in the ibm-common-services namespace by the cluster admins) $ oc get routes -n ibm-common-services | grep console cp-console cp-console.apps.eda-sandbox.gse-ocp.net icp-management-ingress https reencrypt/Redirect None Log into IBM Event Streams using the Cloud Pak console route from the previous step: $ cloudctl login -a https://cp-console.apps.eda-sandbox.gse-ocp.net --skip-ssl-validation Username> user50 Password> Authenticating... OK Targeted account mycluster Account Enter a namespace > integration Targeted namespace integration Configuring kubectl ... Property \"clusters.mycluster\" unset. Property \"users.mycluster-user\" unset. Property \"contexts.mycluster-context\" unset. Cluster \"mycluster\" set. User \"mycluster-user\" set. Context \"mycluster-context\" created. Switched to context \"mycluster-context\" . OK Configuring helm: /Users/user/.helm OK","title":"Access to Event Streams Console using CLI"},{"location":"technology/event-streams/es-cp4i/#create-topic-with-es-cli","text":"Create the topic with the desi specification. cloudctl es topic-create --name INBOUND --partitions 1 --replication-factor 3 Make sure the topic has been created by listing the topics. $ cloudctl es topics Topic name INBOUND OK","title":"Create Topic with es CLI"},{"location":"technology/event-streams/es-cp4i/#get-kafka-bootstrap-url","text":"For an application to access the Kafka Broker, we need to get the bootstrap URL.","title":"Get Kafka Bootstrap Url"},{"location":"technology/event-streams/es-cp4i/#ui","text":"You can find your IBM Event Streams Kakfa bootstrap url if you log into the IBM Event Streams user interface, and click on the Connect to this cluster option displayed on the dashboard. This will display a menu where you will see a url to the left of the Generate SCAM credentials button. Make sure that you are on the External Connection .","title":"UI"},{"location":"technology/event-streams/es-cp4i/#cli","text":"You can find your IBM Event Streams and Kakfa bootstrap url when you init the IBM Event Streams Cloud Pak CLI plugin on the last line: $ cloudctl es init -n eventstreams IBM Cloud Platform Common Services endpoint: https://cp-console.apps.eda-sandbox-delta.gse-ocp.net Namespace: integration Name: kafka IBM Cloud Pak for Integration UI address: https://integration-navigator-pn-integration.apps.eda-sandbox-delta.gse-ocp.net Event Streams API endpoint: https://kafka-ibm-es-admapi-external-integration.apps.eda-sandbox-delta.gse-ocp.net Event Streams API status: OK Event Streams UI address: https://kafka-ibm-es-ui-integration.apps.eda-sandbox-delta.gse-ocp.net Apicurio Registry endpoint: https://kafka-ibm-es-ac-reg-external-integration.apps.eda-sandbox-delta.gse-ocp.net Event Streams bootstrap address: kafka-kafka-bootstrap-integration.apps.eda-sandbox-delta.gse-ocp.net:443 OK","title":"CLI"},{"location":"technology/event-streams/es-cp4i/#generate-scram-service-credentials","text":"For an application to connect to an Event Streams instance through the secured external listener, it needs SCRAM credentials to act as service credentials to authenticate the application. We also need TLS certificate to encrypt the communication with the brokers.","title":"Generate SCRAM Service Credentials"},{"location":"technology/event-streams/es-cp4i/#ui_1","text":"Log into the IBM Event Streams user interface as explained previously in this readme and click on the Connect to this cluster option displayed on the dashboard. This will display a menu. Make sure that you are on the External Connection . Click on the Generate SCRAM credentials button. Introduce a name for your credentials and choose the option that better suits the needs of your applications (this will create RBAC permissions for you credentials so that a service credentials can do only what it needs to do). For this demo, select Produce messages, consume messages and create topics and schemas last option. Decide whether your service credentials need to have the ability to access all topics or certain topics only. For this demo, select All Topics and then click Next. Decide whether your service credentials need to have the ability to access all consumer groups or certain specific consumer groups only. For this demo, select All Consumer Groups and click Next. Decide whether your service credentials need to have the ability to access all transactional IDs or certain specific transactional IDs only. For this demo, select All transaction IDs and click on Generate credentials . Take note of the set of credentials displayed on screen. You will need to provide your applications with these in order to get authenticated and authorized with your IBM Event Streams instance. If you did not take note of your SCRAM credentials or you forgot these, the above will create a KafkaUser object in OpenShift that is interpreted by the IBM Event Streams Operator. You can see this KafkaUser if you go to the OpenShift console, click on Operators --> Installed Operators on the right hand side menu, then click on the IBM Event Streams operator and finally click on Kafka Users at the top bar menu. If you click on your Kafka User , you will see what is the Kubernetes secret behind holding your SCRAM credentials details. Click on that secret and you will be able to see again your SCRAM password (your SCRAM username is the same name as the Kafka User created or the secret holding your SCRAM password )","title":"UI"},{"location":"technology/event-streams/es-cp4i/#cli_1","text":"Log into your IBM Event Streams instance through the CLI. Create your SCRAM service credentials with the following command (adjust the topics, consumer groups, transaction IDs, etc permissions your SCRAM service credentials should have in order to satisfy your application requirements): $ cloudctl es kafka-user-create --name test-credentials-cli --consumer --producer --schema-topic-create --all-topics --all-groups --all-txnids --auth-type scram-sha-512 KafkaUser name Authentication Authorization Username Secret test-credentials-cli scram-sha-512 simple EntityOperator has not created corresponding username EntityOperator has not created corresponding secret Resource type Name Pattern type Host Operation topic * literal * Read topic __schema_ prefix * Read topic * literal * Write topic * literal * Create topic __schema_ prefix * Alter group * literal * Read transactionalId * literal * Write Created KafkaUser test-credentials-cli. OK List the KafkaUser objects to make sure yours has been created: $ cloudctl es kafka-users KafkaUser name Authentication Authorization test-credentials scram-sha-512 simple test-credentials-cli scram-sha-512 simple OK To retrieve your credentials execute the following command: $ cloudctl es kafka-user test-credentials-cli KafkaUser name Authentication Authorization Username Secret test-credentials-cli scram-sha-512 simple test-credentials-cli test-credentials-cli Resource type Name Pattern type Host Operation topic * literal * Read topic __schema_ prefix * Read topic * literal * Write topic * literal * Create topic __schema_ prefix * Alter group * literal * Read transactionalId * literal * Write OK Above you can see your SCRAM username under Username and the secret holding your SCRAM password under Secret . In order to retrieve the password, execute the following command: $ oc get secret test-credentials-cli -o jsonpath = '{.data.password}' | base64 --decode ******* NEXT: For more information about how to connect to your cluste, read the IBM Event Streams product documentation","title":"CLI"},{"location":"technology/event-streams/es-cp4i/#get-event-streams-tls-certificates","text":"In this section we are going to see how to download the TLS certificats to securely connect to our IBM Event Streams instance.","title":"Get Event Streams TLS Certificates"},{"location":"technology/event-streams/es-cp4i/#ui_2","text":"Log into the IBM Event Streams console user interface as explained before in this readme. Click on the Connect to this cluster option displayed on the dashboard. This will display a menu where you will see a Certificates section: Depending on what language your application is written into, you will need a PKCS12 certificate or a PEM certificate . Click on Download certificate for any of the options you need. If it is the PKCS12 certificate bear in mind it comes with a password for the truststore. You don't need to write this down as it will display any time you click on Download certificate button.","title":"UI"},{"location":"technology/event-streams/es-cp4i/#cli_2","text":"Log into IBM Event Streams through the CLI as already explained before in this readme. To retrieve the PKCS12 certificate execute the following command: $ cloudctl es certificates --format p12 Trustore password is ******** Certificate successfully written to /Users/testUser/Downloads/es-cert.p12. OK To retrieve the PEM certificate execute the following command: $ cloudctl es certificates --format pem Certificate successfully written to /Users/testUser/Downloads/es-cert.pem. OK","title":"CLI"},{"location":"technology/event-streams/es-maas/es-cloud/","text":"This documentation aims to be a introductory hands-on lab on IBM Event Streams on Cloud with topic creation. Index \u00b6 Pre-requisites \u00b6 This lab requires the following components to work against: An IBM Cloud account. Get a IBM Cloud Account by using the register link in https://cloud.ibm.com/login Create a new account is free of charge. On your development workstation you will need: IBM Cloud CLI ( https://cloud.ibm.com/docs/cli?topic=cloud-cli-getting-started ) IBM CLoud CLI Event Streams plugin ( ibmcloud plugin install event-streams ) Login CLI \u00b6 ibmcoud login Account and resource group concepts \u00b6 As any other IBM Cloud services, Event Streams can be part of a resources group, is controlled by user roles, and is accessible via API keys. To get familiar with those concepts, it is recommended to study the concepts of IBM account and how it is related to resource group and services. The following diagram is a summary of the objects managed in IBM Cloud: To summarize: Account represents the billable entity, and can have multiple users. Users are given access to resource groups. Applications are identified with a service ID. To restrict permissions for using specific services, you can assign specific access policies to the service ID and user ID Resource groups are here to organize any type of resources (services, clusters, VMs...) that are managed by Identity and Access Management (IAM). Resource groups are not scoped by location Access group are used to organize a set of users and service IDs into a single entity and easily assign permissions Create a Event Streams service instance \u00b6 From the IBM Cloud Dashboard page, you can create a new resource, using the right top button Create resource . which leads to the service and feature catalog. From there in the services view, select the integration category and then the Event Streams tile: You can access this screen from this URL: https://cloud.ibm.com/catalog/event-streams . Plan characteristics \u00b6 Within the first page for the Event Streams creation, you need to select the region, the pricing plan, a service name and the resource group. For the region, it is important to note that the 'lite' plan is available only in Dallas, and it used to do some proof of concept. It is recommended to select a region close to your on-premise data center. For the Plan description, the product documentation goes over the different plans in details. The 'multi-tenancy' means the Kafka cluster is shared with other people. The cluster topology is covering multi availability zones inside the same data center. The following diagram illustrates a simple view of this topology with the different network zones and availability zones: We will address fine-grained access control in the security lab . As described in the Kafka concept introduction , topic may have partitions. Partitions are used to improve throughput as consumer can run in parallel, and producer can publish to multiple partitions. The plan set a limit on the total number of partitions. Each partition records, are persisted in the file system and so the maximum time records are kept on disks is controlled by the maximum retention period and total size. Those Kafka configurations are described in the topic and broker documentation . Fro the Standard plan, the first page has also a price estimator. The two important concepts used for pricing are the number of partition instances and the number of GB consumed: each consumer reading from a topic/partition will increase the number of byte consumed. The cost is per month. Creating Event Streams instance with IBM Cloud CLI \u00b6 Go to IBM Cloud and click on the user avatar on the top right corner. Then, click on Log in to CLI and API option: Copy the IBM Cloud CLI login command Open a terminal window, paste and execute the command: $ ibmcloud login -a https://cloud.ibm.com -u passcode -p XsgEKGb84Z API endpoint: https://cloud.ibm.com Authenticating... OK Targeted account bill s Account ( b63... ) <-> 195 ... Select a region ( or press enter to skip ) : 1 . au-syd 2 . in -che 3 . jp-osa 4 . jp-tok 5 . kr-seo 6 . eu-de 7 . eu-gb 8 . us-south 9 . us-south-test 10 . us-east Enter a number> 6 Targeted region eu-de API endpoint: https://cloud.ibm.com Region: eu-de User: A<> Account: Bill s Account ( b63... ) <-> 195 ... Resource group: No resource group targeted, use ibmcloud target -g RESOURCE_GROUP CF API endpoint: Org: Space: List your services with ibmcloud resource service-instances and make sure your IBM Event Streams instance is listed: $ ibmcloud resource service-instances Retrieving instances with type service_instance in all resource groups in all locations under account Kedar Kulkarni ' s Account as ALMARAZJ@ie.ibm.com... OK Name Location State Type IBM Cloud Monitoring with Sysdig-rgd us-south active service_instance apikey for simple toolchain us-east active service_instance aapoc-event-streams us-south active service_instance Event Streams-wn eu-de active service_instance We can see our instance called: Event Streams-wn Create an Event Streams instance using CLI ibmcloud resource service-instance-create EventStreamsEDA2 messagehub standard us-south List your IBM Event Streams instance details with ibmcloud resource service-instance <instance_name> : $ ibmcloud resource service-instance Event \\ Streams-wn Retrieving service instance Event Streams-wn in all resource groups under account Kedar Kulkarni ' s Account as ALMARAZJ@ie.ibm.com... OK Name: Event Streams-wn ID: crn:v1:bluemix:public:messagehub:eu-de:a/b636d1d8...cfa:b05be9...2e687a:: GUID: b05be932...e687a Location: eu-de Service Name: messagehub Service Plan Name: enterprise-3nodes-2tb Resource Group Name: State: active Type: service_instance Sub Type: Created at: 2020 -05-11T15:54:48Z Created by: bob.the.builder@someemail.com Updated at: 2020 -05-11T16:49:18Z Last Operation: Status sync succeeded Message Synchronized the instance Mind the \\ character in your IBM Event Streams instance. Initialize your IBM Event Streams plugin for the IBM Cloud CLI with ibmcloud es init : $ ibmcloud es init Select an Event Streams instance: 1 . Event Streams-2t 2 . Event Streams-wn 3 . aapoc-event-streams 4 . tutorial Enter a number> 2 API Endpoint: https://mh-tcqsppdpzlrkdmkb.....175-0000.eu-de.containers.appdomain.cloud OK Check all the CLI commands available to you to manage and interact with your IBM Event Streams instance with $ ibmcloud es : $ ibmcloud es NAME: ibmcloud es - Plugin for IBM Event Streams ( build 1908221834 ) USAGE: ibmcloud es command [ arguments... ] [ command options ] COMMANDS: broker Display details of a broker. broker-config Display broker configuration. cluster Display details of the cluster. group Display details of a consumer group. group-delete Delete a consumer group. group-reset Reset the offsets for a consumer group. groups List the consumer groups. init Initialize the IBM Event Streams plugin. topic Display details of a topic. topic-create Create a new topic. topic-delete Delete a topic. topic-delete-records Delete records from a topic before a given offset. topic-partitions-set Set the partitions for a topic. topic-update Update the configuration for a topic. topics List the topics. help, h Show help Enter 'ibmcloud es help [command]' for more information about a command. List your cluster configuration with $ ibmcloud es cluster : $ ibmcloud es cluster Details for cluster Cluster ID Controller mh-tcqsppdpzlrkdmkbgmgl-4c20...361c6f175-0000 0 Details for brokers ID Host Port Rack 0 kafka-0.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d......22e361c6f175-0000.eu-de.containers.appdomain.cloud 9093 fra05 1 kafka-1.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d......22e361c6f175-0000.eu-de.containers.appdomain.cloud 9093 fra02 2 kafka-2.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d......22e361c6f175-0000.eu-de.containers.appdomain.cloud 9093 fra04 No cluster-wide dynamic configurations found. 1. Looking at broker details: ibmcloud es broker 0 : ibmcloud es broker 0 Details for broker ID Host Port Rack 0 broker-0-t19zgvnykgdqy1zl.kafka.svc02.us-south.eventstreams.cloud.ibm.com 9093 dal10 Details for broker configuration Name Value Sensitive? broker.id 0 false broker.rack dal10 false advertised.listeners SASL_EXTERNAL://broker-0-t19zgvnykgdqy1zl.kafka.svc02.us-south.eventstreams.cloud.ibm.com:9093 false OK Get detail view of a broker configuration: ibmcloud es broker-config 0 We will see other CLI commands in future labs. Coming back another time \u00b6 When coming back to the IBM Cloud dashboard the simplest way to find the Event Streams service is to go to the Services : Click on your IBM Event Streams instance: Click on Launch Dashboard button to open the IBM Event Streams dashboard Main Event Streams Dashboard page \u00b6 Once the instance is created, or when you come back to the service, you reach the manage panel, as illustrated in previous figure. From the Dashboard we can access the Topics and Consumer groups panels. Create topic \u00b6 In this section we are going to see how to create, list and delete topics both using the User Interface and then the IBM Event Streams CLI. Open the IBM Event Streams user interface (go into your IBM Event Streams service within your IBM Cloud portal and click on the launch dashboard button). Once there, click on the Topics tab from the top menu: Let create a demo-topic-ui topic. If you need to revisit the topic concepts, you can read this note . When you go to the topics view you get the list of existing topics. From this list an administrator can delete an existing topic or create new one. The 'create topic' button leads to the step by step process. Switch to the Advanced mode to get access to the complete set of parameters. The first panel is here to define the core configuration Some parameters to understand: Number of partitions : the default value should be 1. If the data can be partitioned without loosing semantic, you can increase the number of partitions. Retention time : This is how long messages are retained before they are deleted to free up space. If your messages are not read by a consumer within this time, they will be missed. It is mapped to the retention.ms kafka topic configuration. The bottom part of the configuration page, includes logs, cleanup and indexing . The partition's log parameter section includes a cleanup policy that could be: delete: discard old segments when their retention time or size limit has been reached compact: retain at least the last known value for each message key within the log of data for a single topic partition. The topic looks like a table in DB. compact, delete: compact the log and remove old records retention bytes : represents the maximum size a partition (which consists of log segments) can grow to, before old log segments will be discarded to free up space. log segment size is the maximum size in bytes of a single log file. Cleanup segment time - segment.ms controls the period of time after which Kafka will force the log to roll, even if the segment file isn't full, this is to ensure that retention can delete or compact old data. Index - segment.index.bytes controls the size of the index that maps offsets to file positions. The log cleaner policy is supported by a log cleaner, which are threads that recopy log segment files, removing records whose key appears in the head of the log. The number of replications is set to three with a min-in-sync replicas of two. A message is considered committed when all in sync replicas for that partition have applied it to their log. The leader maintains a set of in-sync-replicas: all the nodes which are up-to-date with the leader\u2019s log, and actively acknowledging new writes. Every write goes through the leader and is propagated to every node in the In Sync Replica set, or ISR. We can now see our new topic: To delete a topic, click on the topic options button at the right end of a topic, click on Delete this topic and then on the Delete button in the confirmation pop-up window: The topic should now be deleted: Create topic with CLI \u00b6 List your topics with $ ibmcloud es topics : $ ibmcloud es topics OK No topics found. 1. Create a topic: (Default 1 partition - 3 replicas) $ ibmcloud es topic-create --name demo-topic Created topic demo-topic OK * Execute $ ibmcloud es topic-create --help for more further configuration of your topic creation List topics: $ ibmcloud es topics Topic name demo-topic OK Display details of a topic: $ ibmcloud es topic demo-topic Details for topic demo-topic Topic name Internal? Partition count Replication factor demo-topic false 1 3 Partition details for topic demo-topic Partition ID Leader Replicas In-sync 0 2 [ 2 1 0 ] [ 2 1 0 ] Configuration parameters for topic demo-topic Name Value cleanup.policy delete min.insync.replicas 2 segment.bytes 536870912 retention.ms 86400000 retention.bytes 1073741824 OK Delete records in a topic (in the command below, we want to delete record on a partition 0 offset 5 and partition 1 from offset 0): $ ibmcloud es topic-delete-records --name demo-topic --partition-offset 1 :0 ; 0 :5 --force Add partitions to an existing topic, by setting the new target number of partition: $ ibmcloud es topic-partition-set --name demo-topic --partitions 30 Delete a topic: $ ibmcloud es topic-delete demo-topic Really delete topic 'demo-topic' ? [ y/N ] > y Topic demo-topic deleted successfully OK List topics: $ ibmcloud es topics OK No topics found. For the last list of commands see the CLI Reference manual . Getting started applications \u00b6 From the manage dashboard we can download a getting started application that has two processes: one consumer and one producer, or we can use a second application that we have in this repository Using the Event Streams on cloud getting started app \u00b6 To be able to build the code you need to get gradle installed or use the docker image: docker run --rm -u gradle -v \"$PWD\":/home/gradle/project -w /home/gradle/project gradle gradle <gradle-task> The instructions are in this documentation and can be summarized as: Clone the github repository: git clone https://github.com/ibm-messaging/event-streams-samples.git Build the code: Using the gradle CLI cd kafka-java-console-sample gradle clean && gradle build or the gradle docker image docker run --rm -u gradle -v \" $PWD \" :/home/gradle/project -w /home/gradle/project gradle gradle build Start consumer java -jar ./build/libs/kafka-java-console-sample-2.0.jar broker-0-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093 am_rbb9e794mMwhE-KGPYo0hhW3h91e28OhT8IlruFe5 -consumer Start producer java -jar ./build/libs/kafka-java-console-sample-2.0.jar broker-0-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093 am_rbb9e794mMwhE-KGPYo0hhW3h91e28OhT8IlruFe5 -producer","title":"Event Streams on Cloud hands on lab"},{"location":"technology/event-streams/es-maas/es-cloud/#index","text":"","title":"Index"},{"location":"technology/event-streams/es-maas/es-cloud/#pre-requisites","text":"This lab requires the following components to work against: An IBM Cloud account. Get a IBM Cloud Account by using the register link in https://cloud.ibm.com/login Create a new account is free of charge. On your development workstation you will need: IBM Cloud CLI ( https://cloud.ibm.com/docs/cli?topic=cloud-cli-getting-started ) IBM CLoud CLI Event Streams plugin ( ibmcloud plugin install event-streams )","title":"Pre-requisites"},{"location":"technology/event-streams/es-maas/es-cloud/#login-cli","text":"ibmcoud login","title":"Login CLI"},{"location":"technology/event-streams/es-maas/es-cloud/#account-and-resource-group-concepts","text":"As any other IBM Cloud services, Event Streams can be part of a resources group, is controlled by user roles, and is accessible via API keys. To get familiar with those concepts, it is recommended to study the concepts of IBM account and how it is related to resource group and services. The following diagram is a summary of the objects managed in IBM Cloud: To summarize: Account represents the billable entity, and can have multiple users. Users are given access to resource groups. Applications are identified with a service ID. To restrict permissions for using specific services, you can assign specific access policies to the service ID and user ID Resource groups are here to organize any type of resources (services, clusters, VMs...) that are managed by Identity and Access Management (IAM). Resource groups are not scoped by location Access group are used to organize a set of users and service IDs into a single entity and easily assign permissions","title":"Account and resource group concepts"},{"location":"technology/event-streams/es-maas/es-cloud/#create-a-event-streams-service-instance","text":"From the IBM Cloud Dashboard page, you can create a new resource, using the right top button Create resource . which leads to the service and feature catalog. From there in the services view, select the integration category and then the Event Streams tile: You can access this screen from this URL: https://cloud.ibm.com/catalog/event-streams .","title":"Create a Event Streams service instance"},{"location":"technology/event-streams/es-maas/es-cloud/#plan-characteristics","text":"Within the first page for the Event Streams creation, you need to select the region, the pricing plan, a service name and the resource group. For the region, it is important to note that the 'lite' plan is available only in Dallas, and it used to do some proof of concept. It is recommended to select a region close to your on-premise data center. For the Plan description, the product documentation goes over the different plans in details. The 'multi-tenancy' means the Kafka cluster is shared with other people. The cluster topology is covering multi availability zones inside the same data center. The following diagram illustrates a simple view of this topology with the different network zones and availability zones: We will address fine-grained access control in the security lab . As described in the Kafka concept introduction , topic may have partitions. Partitions are used to improve throughput as consumer can run in parallel, and producer can publish to multiple partitions. The plan set a limit on the total number of partitions. Each partition records, are persisted in the file system and so the maximum time records are kept on disks is controlled by the maximum retention period and total size. Those Kafka configurations are described in the topic and broker documentation . Fro the Standard plan, the first page has also a price estimator. The two important concepts used for pricing are the number of partition instances and the number of GB consumed: each consumer reading from a topic/partition will increase the number of byte consumed. The cost is per month.","title":"Plan characteristics"},{"location":"technology/event-streams/es-maas/es-cloud/#creating-event-streams-instance-with-ibm-cloud-cli","text":"Go to IBM Cloud and click on the user avatar on the top right corner. Then, click on Log in to CLI and API option: Copy the IBM Cloud CLI login command Open a terminal window, paste and execute the command: $ ibmcloud login -a https://cloud.ibm.com -u passcode -p XsgEKGb84Z API endpoint: https://cloud.ibm.com Authenticating... OK Targeted account bill s Account ( b63... ) <-> 195 ... Select a region ( or press enter to skip ) : 1 . au-syd 2 . in -che 3 . jp-osa 4 . jp-tok 5 . kr-seo 6 . eu-de 7 . eu-gb 8 . us-south 9 . us-south-test 10 . us-east Enter a number> 6 Targeted region eu-de API endpoint: https://cloud.ibm.com Region: eu-de User: A<> Account: Bill s Account ( b63... ) <-> 195 ... Resource group: No resource group targeted, use ibmcloud target -g RESOURCE_GROUP CF API endpoint: Org: Space: List your services with ibmcloud resource service-instances and make sure your IBM Event Streams instance is listed: $ ibmcloud resource service-instances Retrieving instances with type service_instance in all resource groups in all locations under account Kedar Kulkarni ' s Account as ALMARAZJ@ie.ibm.com... OK Name Location State Type IBM Cloud Monitoring with Sysdig-rgd us-south active service_instance apikey for simple toolchain us-east active service_instance aapoc-event-streams us-south active service_instance Event Streams-wn eu-de active service_instance We can see our instance called: Event Streams-wn Create an Event Streams instance using CLI ibmcloud resource service-instance-create EventStreamsEDA2 messagehub standard us-south List your IBM Event Streams instance details with ibmcloud resource service-instance <instance_name> : $ ibmcloud resource service-instance Event \\ Streams-wn Retrieving service instance Event Streams-wn in all resource groups under account Kedar Kulkarni ' s Account as ALMARAZJ@ie.ibm.com... OK Name: Event Streams-wn ID: crn:v1:bluemix:public:messagehub:eu-de:a/b636d1d8...cfa:b05be9...2e687a:: GUID: b05be932...e687a Location: eu-de Service Name: messagehub Service Plan Name: enterprise-3nodes-2tb Resource Group Name: State: active Type: service_instance Sub Type: Created at: 2020 -05-11T15:54:48Z Created by: bob.the.builder@someemail.com Updated at: 2020 -05-11T16:49:18Z Last Operation: Status sync succeeded Message Synchronized the instance Mind the \\ character in your IBM Event Streams instance. Initialize your IBM Event Streams plugin for the IBM Cloud CLI with ibmcloud es init : $ ibmcloud es init Select an Event Streams instance: 1 . Event Streams-2t 2 . Event Streams-wn 3 . aapoc-event-streams 4 . tutorial Enter a number> 2 API Endpoint: https://mh-tcqsppdpzlrkdmkb.....175-0000.eu-de.containers.appdomain.cloud OK Check all the CLI commands available to you to manage and interact with your IBM Event Streams instance with $ ibmcloud es : $ ibmcloud es NAME: ibmcloud es - Plugin for IBM Event Streams ( build 1908221834 ) USAGE: ibmcloud es command [ arguments... ] [ command options ] COMMANDS: broker Display details of a broker. broker-config Display broker configuration. cluster Display details of the cluster. group Display details of a consumer group. group-delete Delete a consumer group. group-reset Reset the offsets for a consumer group. groups List the consumer groups. init Initialize the IBM Event Streams plugin. topic Display details of a topic. topic-create Create a new topic. topic-delete Delete a topic. topic-delete-records Delete records from a topic before a given offset. topic-partitions-set Set the partitions for a topic. topic-update Update the configuration for a topic. topics List the topics. help, h Show help Enter 'ibmcloud es help [command]' for more information about a command. List your cluster configuration with $ ibmcloud es cluster : $ ibmcloud es cluster Details for cluster Cluster ID Controller mh-tcqsppdpzlrkdmkbgmgl-4c20...361c6f175-0000 0 Details for brokers ID Host Port Rack 0 kafka-0.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d......22e361c6f175-0000.eu-de.containers.appdomain.cloud 9093 fra05 1 kafka-1.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d......22e361c6f175-0000.eu-de.containers.appdomain.cloud 9093 fra02 2 kafka-2.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d......22e361c6f175-0000.eu-de.containers.appdomain.cloud 9093 fra04 No cluster-wide dynamic configurations found. 1. Looking at broker details: ibmcloud es broker 0 : ibmcloud es broker 0 Details for broker ID Host Port Rack 0 broker-0-t19zgvnykgdqy1zl.kafka.svc02.us-south.eventstreams.cloud.ibm.com 9093 dal10 Details for broker configuration Name Value Sensitive? broker.id 0 false broker.rack dal10 false advertised.listeners SASL_EXTERNAL://broker-0-t19zgvnykgdqy1zl.kafka.svc02.us-south.eventstreams.cloud.ibm.com:9093 false OK Get detail view of a broker configuration: ibmcloud es broker-config 0 We will see other CLI commands in future labs.","title":"Creating Event Streams instance with IBM Cloud CLI"},{"location":"technology/event-streams/es-maas/es-cloud/#coming-back-another-time","text":"When coming back to the IBM Cloud dashboard the simplest way to find the Event Streams service is to go to the Services : Click on your IBM Event Streams instance: Click on Launch Dashboard button to open the IBM Event Streams dashboard","title":"Coming back another time"},{"location":"technology/event-streams/es-maas/es-cloud/#main-event-streams-dashboard-page","text":"Once the instance is created, or when you come back to the service, you reach the manage panel, as illustrated in previous figure. From the Dashboard we can access the Topics and Consumer groups panels.","title":"Main Event Streams Dashboard page"},{"location":"technology/event-streams/es-maas/es-cloud/#create-topic","text":"In this section we are going to see how to create, list and delete topics both using the User Interface and then the IBM Event Streams CLI. Open the IBM Event Streams user interface (go into your IBM Event Streams service within your IBM Cloud portal and click on the launch dashboard button). Once there, click on the Topics tab from the top menu: Let create a demo-topic-ui topic. If you need to revisit the topic concepts, you can read this note . When you go to the topics view you get the list of existing topics. From this list an administrator can delete an existing topic or create new one. The 'create topic' button leads to the step by step process. Switch to the Advanced mode to get access to the complete set of parameters. The first panel is here to define the core configuration Some parameters to understand: Number of partitions : the default value should be 1. If the data can be partitioned without loosing semantic, you can increase the number of partitions. Retention time : This is how long messages are retained before they are deleted to free up space. If your messages are not read by a consumer within this time, they will be missed. It is mapped to the retention.ms kafka topic configuration. The bottom part of the configuration page, includes logs, cleanup and indexing . The partition's log parameter section includes a cleanup policy that could be: delete: discard old segments when their retention time or size limit has been reached compact: retain at least the last known value for each message key within the log of data for a single topic partition. The topic looks like a table in DB. compact, delete: compact the log and remove old records retention bytes : represents the maximum size a partition (which consists of log segments) can grow to, before old log segments will be discarded to free up space. log segment size is the maximum size in bytes of a single log file. Cleanup segment time - segment.ms controls the period of time after which Kafka will force the log to roll, even if the segment file isn't full, this is to ensure that retention can delete or compact old data. Index - segment.index.bytes controls the size of the index that maps offsets to file positions. The log cleaner policy is supported by a log cleaner, which are threads that recopy log segment files, removing records whose key appears in the head of the log. The number of replications is set to three with a min-in-sync replicas of two. A message is considered committed when all in sync replicas for that partition have applied it to their log. The leader maintains a set of in-sync-replicas: all the nodes which are up-to-date with the leader\u2019s log, and actively acknowledging new writes. Every write goes through the leader and is propagated to every node in the In Sync Replica set, or ISR. We can now see our new topic: To delete a topic, click on the topic options button at the right end of a topic, click on Delete this topic and then on the Delete button in the confirmation pop-up window: The topic should now be deleted:","title":"Create topic"},{"location":"technology/event-streams/es-maas/es-cloud/#create-topic-with-cli","text":"List your topics with $ ibmcloud es topics : $ ibmcloud es topics OK No topics found. 1. Create a topic: (Default 1 partition - 3 replicas) $ ibmcloud es topic-create --name demo-topic Created topic demo-topic OK * Execute $ ibmcloud es topic-create --help for more further configuration of your topic creation List topics: $ ibmcloud es topics Topic name demo-topic OK Display details of a topic: $ ibmcloud es topic demo-topic Details for topic demo-topic Topic name Internal? Partition count Replication factor demo-topic false 1 3 Partition details for topic demo-topic Partition ID Leader Replicas In-sync 0 2 [ 2 1 0 ] [ 2 1 0 ] Configuration parameters for topic demo-topic Name Value cleanup.policy delete min.insync.replicas 2 segment.bytes 536870912 retention.ms 86400000 retention.bytes 1073741824 OK Delete records in a topic (in the command below, we want to delete record on a partition 0 offset 5 and partition 1 from offset 0): $ ibmcloud es topic-delete-records --name demo-topic --partition-offset 1 :0 ; 0 :5 --force Add partitions to an existing topic, by setting the new target number of partition: $ ibmcloud es topic-partition-set --name demo-topic --partitions 30 Delete a topic: $ ibmcloud es topic-delete demo-topic Really delete topic 'demo-topic' ? [ y/N ] > y Topic demo-topic deleted successfully OK List topics: $ ibmcloud es topics OK No topics found. For the last list of commands see the CLI Reference manual .","title":"Create topic with CLI"},{"location":"technology/event-streams/es-maas/es-cloud/#getting-started-applications","text":"From the manage dashboard we can download a getting started application that has two processes: one consumer and one producer, or we can use a second application that we have in this repository","title":"Getting started applications"},{"location":"technology/event-streams/es-maas/es-cloud/#using-the-event-streams-on-cloud-getting-started-app","text":"To be able to build the code you need to get gradle installed or use the docker image: docker run --rm -u gradle -v \"$PWD\":/home/gradle/project -w /home/gradle/project gradle gradle <gradle-task> The instructions are in this documentation and can be summarized as: Clone the github repository: git clone https://github.com/ibm-messaging/event-streams-samples.git Build the code: Using the gradle CLI cd kafka-java-console-sample gradle clean && gradle build or the gradle docker image docker run --rm -u gradle -v \" $PWD \" :/home/gradle/project -w /home/gradle/project gradle gradle build Start consumer java -jar ./build/libs/kafka-java-console-sample-2.0.jar broker-0-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093 am_rbb9e794mMwhE-KGPYo0hhW3h91e28OhT8IlruFe5 -consumer Start producer java -jar ./build/libs/kafka-java-console-sample-2.0.jar broker-0-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093 am_rbb9e794mMwhE-KGPYo0hhW3h91e28OhT8IlruFe5 -producer","title":"Using the Event Streams on cloud getting started app"},{"location":"technology/event-streams/es-maas/security/","text":"This documentation aims to be a introductory hands-on lab for the security feature of IBM Event Streams. Security considerations \u00b6 There are a lot of security requirements to address to secure platform and solution, in this section we will list some of them Secure data access which include data in motion or at rest encryption, connection and isolation via private network. Support to Virtual Private Network and bring your own keys. Control access to Kafka cluster and resources like topics, using role based access control, SAML tokens and TLS certificates. Bring your own keys. Monitor and audit to provide audit log and secured APIs access User onboarding : define application specific service accounts, and admin users for devOps staff. IAM Concept Summary \u00b6 To undertand the Identity and access management you can read this article . To summarize: Account represents the billable entity, and can have multiple users. Users are given access to resource groups. _ Identity_ concept consists of user identities, service and app identities, API keys, and resources. Applications and IBM Cloud Services are identified with a service ID. To restrict permissions for using specific services, you can assign specific access policies to the service ID and user ID Resource groups are here to organize any type of resources (services, clusters, VMs...) that are managed by Identity and Access Management (IAM). Resource groups are not scoped by location. API keys can be use to authenticate user or a service / application. To control access three components are used: access groups, resources and access policies. Access group are used to organize a set of users and service IDs into a single entity and easily assign permissions via access policies Policies give permission to access account resources. Policies include a subject (user, service, access group), target (resource), and role. Policy can be set to all resources in a resource group There are two types of access roles : platform management and service access. Here is the main page for the IAM service on IBM Cloud where an account owner can manage the different components of the security control: Event Streams access management concepts \u00b6 This section is a quick overview of the Managing access to your Event Streams resources article. Roles \u00b6 Users can perform specific tasks when they are assigned to a role for which access policies are defined. The roles defined for Event Streams are Reader, Writer and Manager . Reader : Users can view Event Streams resources, and applications can only consume records Writer : Users can edit resource, and applications can produce and consume records Manager : Users can do privileged actions. Assign access \u00b6 The product documentation addresses how to assign access in this section . The type of Kafka resources that may be secured are: cluster, topic, group, or transaction id . In this lab we will give topic access to users within a group and application by using service ID. Pre-requisites \u00b6 This lab requires the following components to work against: An IBM Cloud account. Get a IBM Cloud Account by using the register link in https://cloud.ibm.com/login Create a new account is free of charge. IBM Cloud CLI ( https://cloud.ibm.com/docs/cli?topic=cloud-cli-getting-started ) IBM CLoud CLI Event Streams plugin ( ibmcloud plugin install event-streams ) Add access group to the account using IAM \u00b6 The goal of this step is to create an access group to access Event Streams services as administrator. From the IBM Cloud main page, go to the Manage > IAM menu on top right of IBM Cloud Dashboard page: This should lead you to the main IAM page as illustrated in first figure above. Under the Access Group, you can create a new group of users. This will be an administrator group: Then add users to this newly created group, by selecting users from the list, (those users were invited to join the account at some time) and Click on Add to group link. Finally, you want now to add Access Policies to control access to Event Streams clusters. For that click Assign access button: Then select the type of resource (Event Streams), you want to define the access policy on: Then specify that the group can manage all instances of Event Streams service. The condition applies to the service instance with a Manager role: Add the policy and assign it to the group. You could stay in the same panel to add more target to the policy. The newly created, access policies for administer / manager to any Event Streams services is now listed in the access group: Any user who has a manager role for either 'All' services or 'All' Event Streams services in the same account will also have full access. Limiting topic access to group of users \u00b6 The product document illustrates some access control common scenarios in this section . We recommend reading those settings. In this step we are implementing one of the classical scenario: suppose we have a line of business that will create topics by applying a naming convention where topic name starts with a prefix like: bn-lob1-* . We want users and service ID to get read / write access to only those topics matching those prefix. To do so you need to: Add an access group to include member of the line of business: bn-lob1-group Define an access policy with the following criterias: Event streams as resource type All regions Specifying one of the Event Streams resource instance (the one you provisioned in this lab ) Select the service instance that hosts the target cluster Specify the resource type to be topic And the resource ID to matches bn-lob1-* Then add a second rule to enforce read access at the cluster level: Assign the two rules: The group has the two access policies: Authentication with API Keys \u00b6 To let an application to remotely authenticate itself, Event Streams uses API keys. The goal of this section, is to create API keys so applications, tools, scripts can connect to the newly created IBM Event Streams instance. You will create three keys for the different roles: creation of topics, read access only and read/write access. In your IBM Event Streams instance service page, click on Service credentials on the left hand side menu: Observe, there is no service credentials yet and click on the New credential button on the top right corner: Enter a name for your service, choose Manager role for now and click on Add : You should now see your new service credential and be able to inspect its details if you click on its dropdown arrow on it left: Using the same process you can add a credential for Writer Role. For the Reader role you will use the CLI in the next section. API Key is used for the sasl.jaas.config conncection properties in Kafka consumer or producer: security.protocol = SASL_SSL ssl.protocol = TLSv1.2 ssl.enabled.protocols = TLSv1.2 ssl.endpoint.identification.algorithm = HTTPS sasl.mechanism = PLAIN sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"am....5\"; Adding Keys with CLI \u00b6 You could create the service credentials using the CLI, so you will add Reader role API Keys: First you can explore the service credentials created previously, using the CLI with $ ibmcloud resource service-key <service_credentials_name> : $ ibmcloud resource service-key demo-serv-cred Retrieving service key demo-serv-cred in all resource groups under account bill ' s Account as A...... Name: demo-serv-cred ID: crn:v1:bluemix:public:messagehub:eu-de:a/b636d1d8.....8cfa:b05be932-2....02e687a:resource-key:4ba348d2-...-360e983d99c5 Created At: Tue May 12 10 :53:02 UTC 2020 State: active Credentials: api_key: ***** apikey: ***** iam_apikey_description: Auto-generated for key 4ba348d2-5fcf-4c13-a265-360e983d99c5 iam_apikey_name: demo-serv-cred iam_role_crn: crn:v1:bluemix:public:iam::::serviceRole:Manager iam_serviceid_crn: crn:v1:bluemix:public:iam-identity::a/b636d1d83e34d7ae7e904591ac248cfa::serviceid:ServiceId-380e866c-5914-4e01-85c4-d80bd1b8a899 instance_id: b05be932-2a60-4315-951d-a6dd902e687a kafka_admin_url: https://mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud kafka_brokers_sasl: [ kafka-2.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093 kafka-0.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093 kafka-1.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093 ] kafka_http_url: https://mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud password: ***** user: token Add a Reader role API key: # Get the name of the event streams service: ibmcloud resource service-instances ibmcloud resource service-key-create bn-lob1-app-reader Reader --instance-name \"Event Streams-wn\" OK Service key crn:v1:bluemix:public:messagehub:eu-de:a/b6...248cfa:b05...e687a:resource-key:7ee00.....15a2 was created. Name: bn-lob1-reader ID: crn:v1:bluemix:public:messagehub:eu-de:a/b636d..fa:b05b..7a:resource-key:7ee0042.....b15a2 Created At: Wed May 13 00 :33:49 UTC 2020 State: active Credentials: api_key: xrvMI4PQYmdOcUwPRUJXy6Xlo9UCY9xywNUPiU3jjpKH apikey: xrvMI4PQYmdOcUwPRUJXy6Xlo9UCY9xywNUPiU3jjpKH iam_apikey_description: Auto-generated for key 7ee0042f-572b-46f6-b9cc-912cc63b15a2 iam_apikey_name: bn-lob1-reader iam_role_crn: crn:v1:bluemix:public:iam::::serviceRole:Reader iam_serviceid_crn: crn:v1:bluemix:public:iam-identity::a/b636d1....48cfa::serviceid:ServiceId-b4d3....18af1 instance_id: b05....687a kafka_admin_url: https://mh-tcqsppdpzlrkdmkbgmgl-4c201a...1c6f175-0000.eu-de.containers.appdomain.cloud kafka_brokers_sasl: [ kafka-0.mh-tcqsppdpzlrkdmkbgmgl-4c2...175-0000.eu-de.containers.appdomain.cloud:9093.... ] kafka_http_url: https://mh-tcqsppdpzlrkdmkbgmgl-4c20...-0000.eu-de.containers.appdomain.cloud password: xr......KH user: token Control application access \u00b6 The last step of this lab is to add a specific IAM group for applications so you can link any future applications to a group so that producer and consumer codes using the Writer role API key as defined previously can read and write to specific topic as defined by an access policy. We want to give this Writer role API key to the bn-lob1-app group, so applications within this group can create topics and R/W on those topics with the name bn-lob1-* . To do so, we have to add to the Access Group bn-lob1-app , the service ID that maps the Key created above: In IAM Access groups select the service IDs tab and click on add service id button: Search for the service ID name: bn-lob1-writer and You should get this result: Kafka Streams specifics \u00b6 A lot of Kafka implementations use Kafka Streams API, in this case the applications need to have Manager role on topic resource, and Reader role for cluster and group with a Manager API Key. Connecting application using API Key \u00b6 To connect to Event Streams on cloud, we need to define consumer and producer common configuration as presented in the product documentation Here is an exemple of using reactive messaging in microprofile with the liberty kafka connector: mp.messaging.connector.liberty-kafka.security.protocol = SASL_SSL mp.messaging.connector.liberty-kafka.ssl.protocol = TLSv1.2 mp.messaging.connector.liberty-kafka.sasl.mechanism = PLAIN mp.messaging.connector.liberty-kafka.sasl.jaas.config = \"org.apache.kafka.common.security.plain.PlainLoginModule required username=token password=longapikeycomingfromservicecredential;\" These properties could in fact be part of a secret in Kubernetes and used by the pod. Encryption at rest \u00b6 Event Streams stores message data at rest and message logs on encrypted disks. Event Streams supports customer-managed encryption with provided key: bring you own key. This feature is available on the Enterprise plan only. The product documentation addresses in detail this capability. In this section, we just want to present some step by step to play with Key Protect, defining root key, and get Wrapped Keys to be used for encryption. Create a Key Protect instance \u00b6 In the Create resource from the IBM Cloud dashboard page, select a Key Protect service, add a name, and select a resource pool: Create a root key or upload yours \u00b6 Using the Manage menu of Key Protect, and Add key button: Specify to import your own key or create a new one. Give a name Keys are symmetric 256-bit keys, supported by the AES-CBC-PAD algorithm. For added security, keys are generated by FIPS 140-2 Level 3 certified hardware security modules (HSMs) that are located in secure IBM Cloud data centers Get the cloud resource name from the newly create key: Get a Wrapped Key with API: \u00b6 export GUID = $( ibmcloud resource service-instance eda-KeyProtect --id | grep crn | awk '{print $2}\u2019) export TOKEN=$(ibmcloud iam oauth-tokens | awk ' { print $4 } ' ) curl -X POST https://us-south.kms.cloud.ibm.com/api/v2/keys/34f2598f-e6e9-4822-8f51-cd9036e537e8?action=wrap -H ' accept: application/vnd.ibm.kms.key_action+json ' -H ' authorization: Bearer $TOKEN ' -H ' bluemix-instance: $GUID ' -H ' content-type: application/vnd.ibm.kms.key_action+json ' Authorize Event Streams to access Key Protect \u00b6 In IAM application, go to the Authorizations menu: And define event streams as the source service, you can specify a specific instance id or any event streams service. The target is the Key Protect instance. The service access is Reader. You should get something like: Now Event Streams could use your wrapped key to encrypte data at rest. Warning Temporarily des-authorizing Event Streams to access Key Protect, will block communication to Event Streams instance. Loosing any keys, will mean loosing the data. Restoring access by recreating the authorization between ES and Key Protect, will reopen traffic. When rotating the root key a new rewrapping of DEK is performed and the new key needs to be communicated to Event Streams.","title":"Kafka security"},{"location":"technology/event-streams/es-maas/security/#security-considerations","text":"There are a lot of security requirements to address to secure platform and solution, in this section we will list some of them Secure data access which include data in motion or at rest encryption, connection and isolation via private network. Support to Virtual Private Network and bring your own keys. Control access to Kafka cluster and resources like topics, using role based access control, SAML tokens and TLS certificates. Bring your own keys. Monitor and audit to provide audit log and secured APIs access User onboarding : define application specific service accounts, and admin users for devOps staff.","title":"Security considerations"},{"location":"technology/event-streams/es-maas/security/#iam-concept-summary","text":"To undertand the Identity and access management you can read this article . To summarize: Account represents the billable entity, and can have multiple users. Users are given access to resource groups. _ Identity_ concept consists of user identities, service and app identities, API keys, and resources. Applications and IBM Cloud Services are identified with a service ID. To restrict permissions for using specific services, you can assign specific access policies to the service ID and user ID Resource groups are here to organize any type of resources (services, clusters, VMs...) that are managed by Identity and Access Management (IAM). Resource groups are not scoped by location. API keys can be use to authenticate user or a service / application. To control access three components are used: access groups, resources and access policies. Access group are used to organize a set of users and service IDs into a single entity and easily assign permissions via access policies Policies give permission to access account resources. Policies include a subject (user, service, access group), target (resource), and role. Policy can be set to all resources in a resource group There are two types of access roles : platform management and service access. Here is the main page for the IAM service on IBM Cloud where an account owner can manage the different components of the security control:","title":"IAM Concept Summary"},{"location":"technology/event-streams/es-maas/security/#event-streams-access-management-concepts","text":"This section is a quick overview of the Managing access to your Event Streams resources article.","title":"Event Streams access management concepts"},{"location":"technology/event-streams/es-maas/security/#roles","text":"Users can perform specific tasks when they are assigned to a role for which access policies are defined. The roles defined for Event Streams are Reader, Writer and Manager . Reader : Users can view Event Streams resources, and applications can only consume records Writer : Users can edit resource, and applications can produce and consume records Manager : Users can do privileged actions.","title":"Roles"},{"location":"technology/event-streams/es-maas/security/#assign-access","text":"The product documentation addresses how to assign access in this section . The type of Kafka resources that may be secured are: cluster, topic, group, or transaction id . In this lab we will give topic access to users within a group and application by using service ID.","title":"Assign access"},{"location":"technology/event-streams/es-maas/security/#pre-requisites","text":"This lab requires the following components to work against: An IBM Cloud account. Get a IBM Cloud Account by using the register link in https://cloud.ibm.com/login Create a new account is free of charge. IBM Cloud CLI ( https://cloud.ibm.com/docs/cli?topic=cloud-cli-getting-started ) IBM CLoud CLI Event Streams plugin ( ibmcloud plugin install event-streams )","title":"Pre-requisites"},{"location":"technology/event-streams/es-maas/security/#add-access-group-to-the-account-using-iam","text":"The goal of this step is to create an access group to access Event Streams services as administrator. From the IBM Cloud main page, go to the Manage > IAM menu on top right of IBM Cloud Dashboard page: This should lead you to the main IAM page as illustrated in first figure above. Under the Access Group, you can create a new group of users. This will be an administrator group: Then add users to this newly created group, by selecting users from the list, (those users were invited to join the account at some time) and Click on Add to group link. Finally, you want now to add Access Policies to control access to Event Streams clusters. For that click Assign access button: Then select the type of resource (Event Streams), you want to define the access policy on: Then specify that the group can manage all instances of Event Streams service. The condition applies to the service instance with a Manager role: Add the policy and assign it to the group. You could stay in the same panel to add more target to the policy. The newly created, access policies for administer / manager to any Event Streams services is now listed in the access group: Any user who has a manager role for either 'All' services or 'All' Event Streams services in the same account will also have full access.","title":"Add access group to the account using IAM"},{"location":"technology/event-streams/es-maas/security/#limiting-topic-access-to-group-of-users","text":"The product document illustrates some access control common scenarios in this section . We recommend reading those settings. In this step we are implementing one of the classical scenario: suppose we have a line of business that will create topics by applying a naming convention where topic name starts with a prefix like: bn-lob1-* . We want users and service ID to get read / write access to only those topics matching those prefix. To do so you need to: Add an access group to include member of the line of business: bn-lob1-group Define an access policy with the following criterias: Event streams as resource type All regions Specifying one of the Event Streams resource instance (the one you provisioned in this lab ) Select the service instance that hosts the target cluster Specify the resource type to be topic And the resource ID to matches bn-lob1-* Then add a second rule to enforce read access at the cluster level: Assign the two rules: The group has the two access policies:","title":"Limiting topic access to group of users"},{"location":"technology/event-streams/es-maas/security/#authentication-with-api-keys","text":"To let an application to remotely authenticate itself, Event Streams uses API keys. The goal of this section, is to create API keys so applications, tools, scripts can connect to the newly created IBM Event Streams instance. You will create three keys for the different roles: creation of topics, read access only and read/write access. In your IBM Event Streams instance service page, click on Service credentials on the left hand side menu: Observe, there is no service credentials yet and click on the New credential button on the top right corner: Enter a name for your service, choose Manager role for now and click on Add : You should now see your new service credential and be able to inspect its details if you click on its dropdown arrow on it left: Using the same process you can add a credential for Writer Role. For the Reader role you will use the CLI in the next section. API Key is used for the sasl.jaas.config conncection properties in Kafka consumer or producer: security.protocol = SASL_SSL ssl.protocol = TLSv1.2 ssl.enabled.protocols = TLSv1.2 ssl.endpoint.identification.algorithm = HTTPS sasl.mechanism = PLAIN sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"am....5\";","title":"Authentication with API Keys"},{"location":"technology/event-streams/es-maas/security/#adding-keys-with-cli","text":"You could create the service credentials using the CLI, so you will add Reader role API Keys: First you can explore the service credentials created previously, using the CLI with $ ibmcloud resource service-key <service_credentials_name> : $ ibmcloud resource service-key demo-serv-cred Retrieving service key demo-serv-cred in all resource groups under account bill ' s Account as A...... Name: demo-serv-cred ID: crn:v1:bluemix:public:messagehub:eu-de:a/b636d1d8.....8cfa:b05be932-2....02e687a:resource-key:4ba348d2-...-360e983d99c5 Created At: Tue May 12 10 :53:02 UTC 2020 State: active Credentials: api_key: ***** apikey: ***** iam_apikey_description: Auto-generated for key 4ba348d2-5fcf-4c13-a265-360e983d99c5 iam_apikey_name: demo-serv-cred iam_role_crn: crn:v1:bluemix:public:iam::::serviceRole:Manager iam_serviceid_crn: crn:v1:bluemix:public:iam-identity::a/b636d1d83e34d7ae7e904591ac248cfa::serviceid:ServiceId-380e866c-5914-4e01-85c4-d80bd1b8a899 instance_id: b05be932-2a60-4315-951d-a6dd902e687a kafka_admin_url: https://mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud kafka_brokers_sasl: [ kafka-2.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093 kafka-0.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093 kafka-1.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093 ] kafka_http_url: https://mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud password: ***** user: token Add a Reader role API key: # Get the name of the event streams service: ibmcloud resource service-instances ibmcloud resource service-key-create bn-lob1-app-reader Reader --instance-name \"Event Streams-wn\" OK Service key crn:v1:bluemix:public:messagehub:eu-de:a/b6...248cfa:b05...e687a:resource-key:7ee00.....15a2 was created. Name: bn-lob1-reader ID: crn:v1:bluemix:public:messagehub:eu-de:a/b636d..fa:b05b..7a:resource-key:7ee0042.....b15a2 Created At: Wed May 13 00 :33:49 UTC 2020 State: active Credentials: api_key: xrvMI4PQYmdOcUwPRUJXy6Xlo9UCY9xywNUPiU3jjpKH apikey: xrvMI4PQYmdOcUwPRUJXy6Xlo9UCY9xywNUPiU3jjpKH iam_apikey_description: Auto-generated for key 7ee0042f-572b-46f6-b9cc-912cc63b15a2 iam_apikey_name: bn-lob1-reader iam_role_crn: crn:v1:bluemix:public:iam::::serviceRole:Reader iam_serviceid_crn: crn:v1:bluemix:public:iam-identity::a/b636d1....48cfa::serviceid:ServiceId-b4d3....18af1 instance_id: b05....687a kafka_admin_url: https://mh-tcqsppdpzlrkdmkbgmgl-4c201a...1c6f175-0000.eu-de.containers.appdomain.cloud kafka_brokers_sasl: [ kafka-0.mh-tcqsppdpzlrkdmkbgmgl-4c2...175-0000.eu-de.containers.appdomain.cloud:9093.... ] kafka_http_url: https://mh-tcqsppdpzlrkdmkbgmgl-4c20...-0000.eu-de.containers.appdomain.cloud password: xr......KH user: token","title":"Adding Keys with CLI"},{"location":"technology/event-streams/es-maas/security/#control-application-access","text":"The last step of this lab is to add a specific IAM group for applications so you can link any future applications to a group so that producer and consumer codes using the Writer role API key as defined previously can read and write to specific topic as defined by an access policy. We want to give this Writer role API key to the bn-lob1-app group, so applications within this group can create topics and R/W on those topics with the name bn-lob1-* . To do so, we have to add to the Access Group bn-lob1-app , the service ID that maps the Key created above: In IAM Access groups select the service IDs tab and click on add service id button: Search for the service ID name: bn-lob1-writer and You should get this result:","title":"Control application access"},{"location":"technology/event-streams/es-maas/security/#kafka-streams-specifics","text":"A lot of Kafka implementations use Kafka Streams API, in this case the applications need to have Manager role on topic resource, and Reader role for cluster and group with a Manager API Key.","title":"Kafka Streams specifics"},{"location":"technology/event-streams/es-maas/security/#connecting-application-using-api-key","text":"To connect to Event Streams on cloud, we need to define consumer and producer common configuration as presented in the product documentation Here is an exemple of using reactive messaging in microprofile with the liberty kafka connector: mp.messaging.connector.liberty-kafka.security.protocol = SASL_SSL mp.messaging.connector.liberty-kafka.ssl.protocol = TLSv1.2 mp.messaging.connector.liberty-kafka.sasl.mechanism = PLAIN mp.messaging.connector.liberty-kafka.sasl.jaas.config = \"org.apache.kafka.common.security.plain.PlainLoginModule required username=token password=longapikeycomingfromservicecredential;\" These properties could in fact be part of a secret in Kubernetes and used by the pod.","title":"Connecting application using API Key"},{"location":"technology/event-streams/es-maas/security/#encryption-at-rest","text":"Event Streams stores message data at rest and message logs on encrypted disks. Event Streams supports customer-managed encryption with provided key: bring you own key. This feature is available on the Enterprise plan only. The product documentation addresses in detail this capability. In this section, we just want to present some step by step to play with Key Protect, defining root key, and get Wrapped Keys to be used for encryption.","title":"Encryption at rest"},{"location":"technology/event-streams/es-maas/security/#create-a-key-protect-instance","text":"In the Create resource from the IBM Cloud dashboard page, select a Key Protect service, add a name, and select a resource pool:","title":"Create a Key Protect instance"},{"location":"technology/event-streams/es-maas/security/#create-a-root-key-or-upload-yours","text":"Using the Manage menu of Key Protect, and Add key button: Specify to import your own key or create a new one. Give a name Keys are symmetric 256-bit keys, supported by the AES-CBC-PAD algorithm. For added security, keys are generated by FIPS 140-2 Level 3 certified hardware security modules (HSMs) that are located in secure IBM Cloud data centers Get the cloud resource name from the newly create key:","title":"Create a root key or upload yours"},{"location":"technology/event-streams/es-maas/security/#get-a-wrapped-key-with-api","text":"export GUID = $( ibmcloud resource service-instance eda-KeyProtect --id | grep crn | awk '{print $2}\u2019) export TOKEN=$(ibmcloud iam oauth-tokens | awk ' { print $4 } ' ) curl -X POST https://us-south.kms.cloud.ibm.com/api/v2/keys/34f2598f-e6e9-4822-8f51-cd9036e537e8?action=wrap -H ' accept: application/vnd.ibm.kms.key_action+json ' -H ' authorization: Bearer $TOKEN ' -H ' bluemix-instance: $GUID ' -H ' content-type: application/vnd.ibm.kms.key_action+json '","title":"Get a Wrapped Key with API:"},{"location":"technology/event-streams/es-maas/security/#authorize-event-streams-to-access-key-protect","text":"In IAM application, go to the Authorizations menu: And define event streams as the source service, you can specify a specific instance id or any event streams service. The target is the Key Protect instance. The service access is Reader. You should get something like: Now Event Streams could use your wrapped key to encrypte data at rest. Warning Temporarily des-authorizing Event Streams to access Key Protect, will block communication to Event Streams instance. Loosing any keys, will mean loosing the data. Restoring access by recreating the authorization between ES and Key Protect, will reopen traffic. When rotating the root key a new rewrapping of DEK is performed and the new key needs to be communicated to Event Streams.","title":"Authorize Event Streams to access Key Protect"},{"location":"technology/faq/","text":"Updated 03/29/2022 Basic questions \u00b6 What is Kafka? \u00b6 pub/sub middleware to share data between applications Open source, started in 2011 by Linkedin based on append log to persist immutable records ordered by arrival. support data partitioning, distributed brokers, horizontal scaling, low-latency and high throughput. producer has no knowledge of consumer records stay even after being consumed durability with replication to avoid loosing data for high availability What are the major components? \u00b6 Topic, consumer, producer, brokers, cluster see this note for deep dive Rich API to control the producer semantic, and consumer Consumer groups. See this note for detail Kafka streams API to support data streaming with stateful operations and stream processing topology Kafka connect for source and sink connection to external systems Topic replication with Mirror Maker 2 What are major use cases? \u00b6 Modern data pipeline with buffering to data lake Data hub, to continuously expose business entities to event-driven applications and microservices Real time analytics with aggregate computation, and complex event processing The communication layer for Event-driven, reactive microservice. Why does Kafka use zookeeper? \u00b6 Kafka as a distributed system using cluster, it needs to keep cluster states, sharing configuration like topic, assess which node is still alive within the cluster, support registering new node added to the cluster, being able to support dynamic restart. Zookeeper is an orchestrator for distributed system, it maintains Kafka cluster integrity, select broker leader... Zookeeper is also used to manage offset commit, and to the leader selection process. Version 2.8 starts to get rid of Zookeeper so it uses another algorithm to define partition leadership and cluster health via one broker becoming the cluster controller. See this note on KIP 500 What is a replica? \u00b6 A lit of nodes responsible to participate into the data replication process for a given partition. It is a critical feature to ensure durability, be able to continue to consume records, or to ensure a certain level of data loss safety is guaranteed when producing records. What are a leader and follower in Kafka? \u00b6 Topic has 1 to many partition, which are append logs. Every partition in Kafka has a server that plays the role of leader . When replication is set in a topic, follower brokers will pull data from the leader to ensure replication, up to the specified replication factor. If the leader fails, one of the followers needs to take over as the leader\u2019s role. The leader election process involves zookeeper and assess which follower was the most in-synch with the leader. Leader is the end point for read and write operations on the partition. (Exception is the new feature to read from local follower). To get the list of In-synch Replication for a given topic the following tool can be used: kafka-topics.sh --bootstrap-server :9092 --describe --topic <topicname> What is Offset? \u00b6 A unique identifier of records inside a partition. It is automatically created by the broker, and producer can get it from the broker response. Consumer uses it to commit its read. It means, in case of consumer restarts, it will read from the last committed offset. What is a consumer group? \u00b6 It groups consumers of one to many topics. Each partition is consumed by exactly one consumer within each subscribing consumer group . Consumer group is specified via the group.id consumer's property, and when consumers subscribe to topic(s). There is a protocol to manage consumers within a group so that partition can be reallocated when a consumer lefts the group. The group leader is responsible to do the partition assignment. When using the group.instance.id properties, consumer is treated as a static member, which means there will be no partition rebalance when consumer lefts a group for a short time period. When not set the group coordinator (a broker) will allocate ids to group members, and reallocation will occur. For Kafka Streams application it is recommended to use static membership. Brokers keep offsets until a retention period within which consumer group can lose all its consumers. After that period, offsets are discarded. The consumer group can be deleted manually, or automatically when the last committed offset for that group expires. When the group coordinator receives an OffsetCommitRequest, it appends the request to a special compacted Kafka topic named __consumer_offsets. Ack from the broker is done once all replicas on this hidden topics are successful. The tool kafka-consumer-group.sh helps getting details of consumer group: # Inside a Kafka broker container bin/kafka-consumer-groups.sh --bootstrap-server kafka:9092 --describe --group order-group --members --verbose How to support multi-tenancy? \u00b6 Multi-tenant means multiple different groups of application can produce and consumer messages isolated from other. So by constructs, topics and brokers are multi-tenant. Now the control will be at the access control level policy, the use of service account, and naming convention on the topic name. Consumer and producer authenticate themselves using dedicated service account users, with SCRAM user or Mutual TLS user. Each topic can have security policy to control read, write, creation operations. How client access Kafka cluster metadata? \u00b6 Provide a list of Kafka brokers, minimum two, so the client API will get the metadata once connected to one of the broker. How to get at most once delivery? \u00b6 Set producer acknowledge level (acks) property to 0 or 1. How to support exactly once delivery? \u00b6 The goal is to address that if a producer sends a message twice the system will send only one message to the consumer, and once the consumer commits the read offset, it will not receive the message again even if it restarts. See the section in the producer implementation considerations note . The consumer needs to always read from its last committed offset. Also it is important to note that the Kafka Stream API supports exactly once semantics with the config: processing.guarantee=exactly_once . Each task within a read-process-write flow may fail so this setting is important to be sure the right answer is delivered, even in case of task failure, and the process is executed exactly once. Exactly-once delivery for other destination systems generally requires cooperation with such systems which may be possible by using the offset processing. Retention time for topic what does it mean? \u00b6 The message sent to a cluster is kept for a max period of time or until a max size is reached. Those topic properties are: retention.ms and retention.bytes . Messages stay in the log even if they are consumed. The oldest messages are marked for deletion or compaction depending of the cleanup policy (delete or compact) set to cleanup.policy topic's parameter. See the Kafka documentation on topic configuration parameters . Here is a command to create a topic with specific retention properties: bin/kafka-configs --zookeeper XX.XX.XX.XX:2181 --entity-type topics --entity-name orders --alter --add-config retention.ms = 55000 --add-config retention.byte = 100000 But there is also the offsets.retention.minutes property, set at the cluster level to control when the offset information will be deleted. It is defaulted to 1 day, but the max possible value is 7 days. This is to avoid keeping too much information in the broker memory and avoid to miss data when consumers do not run continuously. So consumers need to commit their offset. If the consumer settings define: auto.offset.reset=earliest , the consumer will reprocess all the events each time it restarts, (or skips to the latest if set to latest ). When using latest , if the consumers are offline for more than the offsets retention time window, they will lose events. What are the topic characteristics I need to define during requirements? \u00b6 This is a requirement gathering related question, to understand what need to be done for configuration topic configuration but also consumer and producer configuration, as well as retention strategy. Number of brokers in the cluster retention time and size Need for HA, set replicas to number of broker or at least the value of 3, with in-synch replica to 2 Type of data to transport to assess message size Plan to use schema management to control change to the payload definition volume per day with peak and average Need to do geo replication to other Kafka cluster Network filesystem used on the target Kubernetes cluster and current storage class What are the impacts of having not enough resource for Kafka? \u00b6 The table in this Event Streams product documentation illustrates the resource requirements for a getting started cluster. When resources start to be at stress, then Kafka communication to ZooKeeper and/or other Kafka brokers can suffer resulting in out-of-sync partitions and container restarts perpetuating the issue. Resource constraints is one of the first things we consider when diagnosing ES issues. Security configuration \u00b6 On Kubernetes, Kafka can be configured with external and internal URLs. With Strimzi internal URLs are using TLS or Plain authentication, then TLS for encryption. If no authentication property is specified then the listener does not authenticate clients which connect through that listener. The listener will accept all connections without authentication. Mutual TLS authentication for internal communication looks like: - name : tls port : 9093 type : internal tls : true authentication : type : tls To connect any app (producer, consumer) we need a TLS user like: piVersion : kafka.strimzi.io/v1beta2 kind : KafkaUser metadata : name : tls-user labels : strimzi.io/cluster : vaccine-kafka spec : authentication : type : tls Then the following configurations need to be done for each app. For example in Quarkus app, we need to specify where to find the client certificate (for each Kafka TLS user a secret is created with the certificate (ca.crt) and a user password) oc describe secret tls-user Data ==== ca.crt: 1164 bytes user.crt: 1009 bytes user.key: 1704 bytes user.p12: 2374 bytes user.password: 12 bytes For Java client we need the following security settings, to specify from which secret to get the keystore password and certificate. The certificate will be mounted to /deployments/certs/user . %prod.kafka.security.protocol = SSL %prod.kafka.ssl.keystore.location = /deployments/certs/user/user.p12 %prod.kafka.ssl.keystore.type = PKCS12 quarkus.openshift.env.mapping.KAFKA_SSL_KEYSTORE_PASSWORD.from-secret = ${ KAFKA_USER : tls -user } quarkus.openshift.env.mapping.KAFKA_SSL_KEYSTORE_PASSWORD.with-key = user.password quarkus.openshift.mounts.user-cert.path = /deployments/certs/user quarkus.openshift.secret-volumes.user-cert.secret-name = ${ KAFKA_USER : tls -user } # To validate server side certificate we will mount it too with the following declaration quarkus.openshift.env.mapping.KAFKA_SSL_TRUSTSTORE_PASSWORD.from-secret = ${ KAFKA_CA_CERT_NAME : kafka -cluster-ca-cert } quarkus.openshift.env.mapping.KAFKA_SSL_TRUSTSTORE_PASSWORD.with-key = ca.password quarkus.openshift.mounts.kafka-cert.path = /deployments/certs/server quarkus.openshift.secret-volumes.kafka-cert.secret-name = ${ KAFKA_CA_CERT_NAME : kafka -cluster-ca-cert } For the server side certificate, it will be in a truststore, which is mounted to /deployments/certs/server and from a secret (this secret is created at the cluster level). Also because we also use TLS for encryption we need: %prod.kafka.ssl.protocol=TLSv1.2 Mutual TLS authentication is always used for the communication between Kafka brokers and ZooKeeper pods. For mutual, or two-way, authentication, both the server and the client present certificates. SCRAM: (Salted Challenge Response Authentication Mechanism) is an authentication protocol that can establish mutual authentication using passwords. Strimzi can configure Kafka to use SASL (Simple Authentication and Security Layer) SCRAM-SHA-512 to provide authentication on both unencrypted and encrypted client connections. The listener declaration: - name : external port : 9094 type : route tls : true authentication : type : scram-sha-512 Need a scram-user: apiVersion : kafka.strimzi.io/v1beta2 kind : KafkaUser metadata : name : scram-user labels : strimzi.io/cluster : vaccine-kafka spec : authentication : type : scram-sha-512 Then the app properties need to have: security.protocol = SASL_SSL %prod.quarkus.openshift.env.mapping.KAFKA_SSL_TRUSTSTORE_PASSWORD.from-secret = ${ KAFKA_CA_CERT_NAME : kafka -cluster-ca-cert } %prod.quarkus.openshift.env.mapping.KAFKA_SSL_TRUSTSTORE_PASSWORD.with-key = ca.password %prod.quarkus.openshift.env.mapping.KAFKA_SCRAM_PWD.from-secret = ${ KAFKA_USER : scram -user } %prod.quarkus.openshift.env.mapping.KAFKA_SCRAM_PWD.with-key = password %prod.quarkus.openshift.mounts.kafka-cert.path = /deployments/certs/server %prod.quarkus.openshift.secret-volumes.kafka-cert.secret-name = ${ KAFKA_CA_CERT_NAME : kafka -cluster-ca-cert } Verify consumer connection \u00b6 Here is an example of TLS authentication for Event streams ConsumerConfig values: bootstrap.servers = [eda-dev-kafka-bootstrap.eventstreams.svc:9093] check.crcs = true client.dns.lookup = default client.id = cold-chain-agent-c2c11228-d876-4db2-a16a-ea7826e358d2-StreamThread-1-restore-consumer client.rack = connections.max.idle.ms = 540000 default.api.timeout.ms = 60000 enable.auto.commit = false exclude.internal.topics = true fetch.max.bytes = 52428800 fetch.max.wait.ms = 500 fetch.min.bytes = 1 group.id = null group.instance.id = null heartbeat.interval.ms = 3000 interceptor.classes = [] internal.leave.group.on.close = false isolation.level = read_uncommitted key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer sasl.client.callback.handler.class = null sasl.jaas.config = null sasl.kerberos.kinit.cmd = /usr/bin/kinit sasl.kerberos.min.time.before.relogin = 60000 sasl.kerberos.service.name = null sasl.kerberos.ticket.renew.jitter = 0.05 sasl.kerberos.ticket.renew.window.factor = 0.8 sasl.login.callback.handler.class = null sasl.login.class = null sasl.login.refresh.buffer.seconds = 300 sasl.login.refresh.min.period.seconds = 60 sasl.login.refresh.window.factor = 0.8 sasl.login.refresh.window.jitter = 0.05 sasl.mechanism = GSSAPI security.protocol = SSL security.providers = null send.buffer.bytes = 131072 session.timeout.ms = 10000 ssl.cipher.suites = null ssl.enabled.protocols = [TLSv1.2] ssl.endpoint.identification.algorithm = https ssl.key.password = null ssl.keymanager.algorithm = SunX509 ssl.keystore.location = /deployments/certs/user/user.p12 ssl.keystore.password = [hidden] ssl.keystore.type = PKCS12 ssl.protocol = TLSv1.2 ssl.provider = null ssl.secure.random.implementation = null ssl.trustmanager.algorithm = PKIX ssl.truststore.location = /deployments/certs/server/ca.p12 ssl.truststore.password = [hidden] ssl.truststore.type = PKCS12 value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer What Security support in Kafka \u00b6 Encrypt data in transit between producer and Kafka brokers Client authentication Client authorization How to protect data at rest? \u00b6 Use encrypted file system for each brokers Encrypt data at the producer level, using some API, and then decode at the consumer level. The data in the appeld log will be encrypted. More advanced concepts \u00b6 What is range partition assignment strategy? \u00b6 There are multiple partition assignment strategy for a consumer, part of a consumer group , to get its partition to fetch data from. Members of the consumer group subscribe to the topics they are interested in and forward their subscriptions to a Kafka broker serving as the group coordinator. The coordinator selects one member to perform the group assignment and propagates the subscriptions of all members to it. Then assign(Cluster, GroupSubscription) is called to perform the assignment and the results are forwarded back to each respective members. Range assignor works on a per-topic basis: it lays out the available partitions in numeric order and the consumers in lexicographic order, and assign partition to each consumer so partition with the same id will be in the same consumer: topic-1-part-0 and topic-2-part-0 will be processed by consumer-0 What is sticky assignor? \u00b6 The CooperativeStickyAssignor helps supporting incremental cooperative rebalancing to the clients' group protocol, which allows consumers to keep all of their assigned partitions during a rebalance and at the end revoke only those which must be migrated to another consumer for overall cluster balance. The goal is to reduce unnecessary downtime due to unnecessary partition migration, by leveraging the sticky assignor which link consumer to partition id. See KIP 429 for details. How to get an homogeneous distribution of message to partitions? \u00b6 Design the message key and hash coding for even distributed. Or implement a customer partitioner by implementing the Partitioner interface. How to ensure efficient join between two topics? \u00b6 Need to use co-partitioning, which means having the same key in both topic, the same number of partitions and the same producer partitioner, which most likely should be the default one that uses the following formula: partition = hash(key) % numPartitions . What is transaction in Kafka? \u00b6 Producer can use transaction begin, commit and rollback API while publishing events to a multi partition topic. This is done by setting a unique transactionId as part of its configuration (with idempotence and min inflight record set to 1). Either all messages are successfully written or none of them are. There are some producer exception to consider to abort the transaction: any KafkaException for sure, but also OutOfSequenceTx which may happen when the PID is greater than the last one seen by the producer. See explanations here . And the KIP 98 What is the high watermark? \u00b6 The high watermark offset is the offset of the last message that was successfully copied to all of the log\u2019s replicas. A consumer can only read up to the high watermark offset to prevent reading un-replicated messages. What should we do for queue full exception or timeout exception on producer? \u00b6 The brokers are running behind, so we need to add more brokers and redistribute partitions. How to send large messages? \u00b6 We can set some properties at the broker, topic, consumer and producer level: Broker: consider the message.max.bytes and replica.fetch.max.bytes Consumer: max.partition.fetch.bytes . Records are fetched in batches by the consumer, so this properties gives the max amount of data per partition the server will return. Default 1 Megabyte How to maximize throughput? \u00b6 For producer if you want to maximize throughput over low latency, set batch.size and linger.ms to higher value. Linger delay producer, it will wait for up to the given delay to allow other records to be sent so that the sends can be batched together. Why Kafka Stream applications may impact cluster performance? \u00b6 They may use internal hidden topics to persist their states for Ktable and GlobalKTable. Process input and output topics How message schema version is propagated? \u00b6 The record includes a byte with the version number from the schema registry. Consumers do not see message in topic, what happens? \u00b6 The brokers may have an issue on this partition. If a broker, part of the ISR list fails, then new leader election may delay the broker commit from a producer. The consumer has a communication issue, or fails, so the consumer group rebalance is underway. How compression schema used is known by the consumer? \u00b6 The record header includes such metadata. So it is possible to have different schema per record. What does out-of-synch partition mean and when it occurs? \u00b6 With partition leader and replication to the followers, the number of in-synch replicas is at least the number of expected replicas. For example for a replicas = 3 the in-synch is set to 2, and it represents the minimum number of replicas that must acknowledge a write for the write to be considered successful. The record is considered \u201ccommitted\u201d when all ISRs for a partition wrote to their log. Only committed records are readable from consumer. So out-of-synch will happen if the followers are not able to send their acknowledge to the replica leader as quickly as expected. Run Kafka Test Container with TopologyTestDriver \u00b6 Topology Test Driver is used without kafka, so there is no real need to use test container. How to remove personal identifying information? \u00b6 From the source connector, it is possible to add processing class to process the records before publishing them to Kafka topic, so that any Kafka Streams apps will not see PII. How to handle variable workload with Kafka Connector source connector? \u00b6 Increase and decrease the number of Kafka connect workers based upon current application load. Derived products related questions \u00b6 Competitors to Kafka \u00b6 NATS Redpanda a Modern streaming platform for mission critical workloads, and is compatible with Kafka API. It is a cluster of brokers without any zookeepers. It also leverage the SSD technology to improve I/O operations. AWS Kinesis Cloud service, managed by AWS staff, paid as you go, proportional to the shard (like partition) used. 24h to 7 days persistence Number of shards are adaptable with throughput. Uses the concept of Kinesis data streams, which uses shards: data records are composed of a sequence number, a partition key and a data blob. restrictions on message size (1 MB) and consumption rate of messages (5 reads /s, < 2MB per shard, 1000 write /s) Server side encryption using master key managed by AWS KMS GCP Pub/sub Solace Active MQ: Java based messaging server to be the JMS reference implementation, so it supports transactional messaging. various messaging protocols including AMQP, STOMP, and MQTT It maintains the delivery state of every message resulting in lower throughput. Can apply JMS message selector to consumer specific message Point to point or pub/sub, but servers push messages to consumer/subscribers Performance of both queue and topic degrades as the number of consumers rises Rabbit MQ: Support queues, with messages removed once consumed Add the concept of Exchange to route message to queues Limited throughput, but can send large message Support JMS, AMQP protocols, and participation to transaction Smart broker / dumb consumer model that focuses on consistently delivering messages to consumers. Differences between AMQ Streams and Confluent \u00b6 AMQ Streams and Confluent are based on the open source Kafka, but Confluent as the main contributer to Kafka, is adding proprietary features to make the product more marketable, so we will not do a pure features comparison a generic features comparison: Feature Confluent AMQ Streams Kafka open source Aligned within a month to the Kafka release Within 2 months after Kafka release Kafka API Same Same k8s / OpenShift deployment Helm \"operator\" Real Kubernetes Operator based on open source Strimzi Kafka Connectors Connectors hub to reference any connectors on the market, with some certified for Confluent. Open source connectors supported. Apache Camel offers a set of connectors not directly supported by Red Hat but useful in a BYO connectors. Fuse and Debezium can be used. Schema registry Proprietary API and schema Solution may leverage open source Apicur.io schema registry which is compatible with Confluent API. Cruise control for auto cluster balancing Adds on Available via Operator Mirroring between clusters Replicator tool Mirror Maker 2 deployable and managed by Strimzi operator Multi region cluster Supported Supported Role Based access control Supported Supported with explicit user manifest, integrated with Red Hat SSO and OPA. ksql Open sourced licensed by Confluent Customer can use open source version of kSQL but meed to verify licensing for cloud deployment. SQL processing on Kafka Records may also being done with Apache Flink Kafka Streams Supported from Kafka Open Source Supported from Kafka Open Source. Also moving CEP and Streams processing to an external tool makes a lot more sense. Apache Flink should be considered. Not directly supported by Red Hat Storage NFS and tiered storage Block storage with replication to s3 buckets for long persisence using Kafka connector. S3 Connector is not supported by Red Hat. As a managed service Proprietary solution Same with: IBM Event Streams and Red Hat AMQ streams as a service Integration with IBM mainframe Not strategic - Weak Strong with IBM connector and deployment on Z and P Admin User Interface Control center Operator in OpenShift and 3nd party open-source user interface like Kafdrop, Kowl, work with AMQ Streams but without direct support from Red Hat As Kafka adoption is a strategic investment, it is important to grow the competency and skill set to manage kafka clusters. Zookeeper was an important element to complexify the cluster management, as 2.8 it is removed, so it should be simpler to manage cluster. With customers cann influence the product roadmap, but it will kill the open source approach if only Confluent committers prioritize the feature requets. It is important to keep competitions and multi committers. Event streams resource requirements \u00b6 See the detailed tables in the product documentation. Differences between Akka and Kafka? \u00b6 Akka is a open source toolkit for Scala or Java to simplify multithreading programming and makes application more reactive by adopting an asynchronous mechanism to access to io: database or HTTP request. To support asynchronous communication between 'actors', it uses messaging, internal to the JVM. Kafka is part of the architecture, while Akka is an implementation choice for one of the component of the business application deployed inside the architecture. vert.x is another open source implementation of such internal messaging mechanism but supporting more language: Java, Groovy, Ruby, JavaScript, Ceylon, Scala, and Kotlin. Is is possible to stream video to kafka? \u00b6 Yes it is possible, but need to do that with care and real justification. If the goal is to classify streamed images, then it is possible to do so, and need to assess if it will be important to be streams versus video at rest. The following article, from Neeraj Krishna illustrates a python implementation to send video frame every 3 images, do image classification using ResNet50 model trained on ImageNet, embbeded in python consumer. The results are saved in mongodb with the metadata needed to query after processing. Other FAQs \u00b6 IBM Event streams on Cloud FAQ FAQ from Confluent","title":"Kafka FAQ"},{"location":"technology/faq/#basic-questions","text":"","title":"Basic questions"},{"location":"technology/faq/#what-is-kafka","text":"pub/sub middleware to share data between applications Open source, started in 2011 by Linkedin based on append log to persist immutable records ordered by arrival. support data partitioning, distributed brokers, horizontal scaling, low-latency and high throughput. producer has no knowledge of consumer records stay even after being consumed durability with replication to avoid loosing data for high availability","title":"What is Kafka?"},{"location":"technology/faq/#what-are-the-major-components","text":"Topic, consumer, producer, brokers, cluster see this note for deep dive Rich API to control the producer semantic, and consumer Consumer groups. See this note for detail Kafka streams API to support data streaming with stateful operations and stream processing topology Kafka connect for source and sink connection to external systems Topic replication with Mirror Maker 2","title":"What are the major components?"},{"location":"technology/faq/#what-are-major-use-cases","text":"Modern data pipeline with buffering to data lake Data hub, to continuously expose business entities to event-driven applications and microservices Real time analytics with aggregate computation, and complex event processing The communication layer for Event-driven, reactive microservice.","title":"What are major use cases?"},{"location":"technology/faq/#why-does-kafka-use-zookeeper","text":"Kafka as a distributed system using cluster, it needs to keep cluster states, sharing configuration like topic, assess which node is still alive within the cluster, support registering new node added to the cluster, being able to support dynamic restart. Zookeeper is an orchestrator for distributed system, it maintains Kafka cluster integrity, select broker leader... Zookeeper is also used to manage offset commit, and to the leader selection process. Version 2.8 starts to get rid of Zookeeper so it uses another algorithm to define partition leadership and cluster health via one broker becoming the cluster controller. See this note on KIP 500","title":"Why does Kafka use zookeeper?"},{"location":"technology/faq/#what-is-a-replica","text":"A lit of nodes responsible to participate into the data replication process for a given partition. It is a critical feature to ensure durability, be able to continue to consume records, or to ensure a certain level of data loss safety is guaranteed when producing records.","title":"What is a replica?"},{"location":"technology/faq/#what-are-a-leader-and-follower-in-kafka","text":"Topic has 1 to many partition, which are append logs. Every partition in Kafka has a server that plays the role of leader . When replication is set in a topic, follower brokers will pull data from the leader to ensure replication, up to the specified replication factor. If the leader fails, one of the followers needs to take over as the leader\u2019s role. The leader election process involves zookeeper and assess which follower was the most in-synch with the leader. Leader is the end point for read and write operations on the partition. (Exception is the new feature to read from local follower). To get the list of In-synch Replication for a given topic the following tool can be used: kafka-topics.sh --bootstrap-server :9092 --describe --topic <topicname>","title":"What are a leader and follower in Kafka?"},{"location":"technology/faq/#what-is-offset","text":"A unique identifier of records inside a partition. It is automatically created by the broker, and producer can get it from the broker response. Consumer uses it to commit its read. It means, in case of consumer restarts, it will read from the last committed offset.","title":"What is Offset?"},{"location":"technology/faq/#what-is-a-consumer-group","text":"It groups consumers of one to many topics. Each partition is consumed by exactly one consumer within each subscribing consumer group . Consumer group is specified via the group.id consumer's property, and when consumers subscribe to topic(s). There is a protocol to manage consumers within a group so that partition can be reallocated when a consumer lefts the group. The group leader is responsible to do the partition assignment. When using the group.instance.id properties, consumer is treated as a static member, which means there will be no partition rebalance when consumer lefts a group for a short time period. When not set the group coordinator (a broker) will allocate ids to group members, and reallocation will occur. For Kafka Streams application it is recommended to use static membership. Brokers keep offsets until a retention period within which consumer group can lose all its consumers. After that period, offsets are discarded. The consumer group can be deleted manually, or automatically when the last committed offset for that group expires. When the group coordinator receives an OffsetCommitRequest, it appends the request to a special compacted Kafka topic named __consumer_offsets. Ack from the broker is done once all replicas on this hidden topics are successful. The tool kafka-consumer-group.sh helps getting details of consumer group: # Inside a Kafka broker container bin/kafka-consumer-groups.sh --bootstrap-server kafka:9092 --describe --group order-group --members --verbose","title":"What is a consumer group?"},{"location":"technology/faq/#how-to-support-multi-tenancy","text":"Multi-tenant means multiple different groups of application can produce and consumer messages isolated from other. So by constructs, topics and brokers are multi-tenant. Now the control will be at the access control level policy, the use of service account, and naming convention on the topic name. Consumer and producer authenticate themselves using dedicated service account users, with SCRAM user or Mutual TLS user. Each topic can have security policy to control read, write, creation operations.","title":"How to support multi-tenancy?"},{"location":"technology/faq/#how-client-access-kafka-cluster-metadata","text":"Provide a list of Kafka brokers, minimum two, so the client API will get the metadata once connected to one of the broker.","title":"How client access Kafka cluster metadata?"},{"location":"technology/faq/#how-to-get-at-most-once-delivery","text":"Set producer acknowledge level (acks) property to 0 or 1.","title":"How to get at most once delivery?"},{"location":"technology/faq/#how-to-support-exactly-once-delivery","text":"The goal is to address that if a producer sends a message twice the system will send only one message to the consumer, and once the consumer commits the read offset, it will not receive the message again even if it restarts. See the section in the producer implementation considerations note . The consumer needs to always read from its last committed offset. Also it is important to note that the Kafka Stream API supports exactly once semantics with the config: processing.guarantee=exactly_once . Each task within a read-process-write flow may fail so this setting is important to be sure the right answer is delivered, even in case of task failure, and the process is executed exactly once. Exactly-once delivery for other destination systems generally requires cooperation with such systems which may be possible by using the offset processing.","title":"How to support exactly once delivery?"},{"location":"technology/faq/#retention-time-for-topic-what-does-it-mean","text":"The message sent to a cluster is kept for a max period of time or until a max size is reached. Those topic properties are: retention.ms and retention.bytes . Messages stay in the log even if they are consumed. The oldest messages are marked for deletion or compaction depending of the cleanup policy (delete or compact) set to cleanup.policy topic's parameter. See the Kafka documentation on topic configuration parameters . Here is a command to create a topic with specific retention properties: bin/kafka-configs --zookeeper XX.XX.XX.XX:2181 --entity-type topics --entity-name orders --alter --add-config retention.ms = 55000 --add-config retention.byte = 100000 But there is also the offsets.retention.minutes property, set at the cluster level to control when the offset information will be deleted. It is defaulted to 1 day, but the max possible value is 7 days. This is to avoid keeping too much information in the broker memory and avoid to miss data when consumers do not run continuously. So consumers need to commit their offset. If the consumer settings define: auto.offset.reset=earliest , the consumer will reprocess all the events each time it restarts, (or skips to the latest if set to latest ). When using latest , if the consumers are offline for more than the offsets retention time window, they will lose events.","title":"Retention time for topic what does it mean?"},{"location":"technology/faq/#what-are-the-topic-characteristics-i-need-to-define-during-requirements","text":"This is a requirement gathering related question, to understand what need to be done for configuration topic configuration but also consumer and producer configuration, as well as retention strategy. Number of brokers in the cluster retention time and size Need for HA, set replicas to number of broker or at least the value of 3, with in-synch replica to 2 Type of data to transport to assess message size Plan to use schema management to control change to the payload definition volume per day with peak and average Need to do geo replication to other Kafka cluster Network filesystem used on the target Kubernetes cluster and current storage class","title":"What are the topic characteristics I need to define during requirements?"},{"location":"technology/faq/#what-are-the-impacts-of-having-not-enough-resource-for-kafka","text":"The table in this Event Streams product documentation illustrates the resource requirements for a getting started cluster. When resources start to be at stress, then Kafka communication to ZooKeeper and/or other Kafka brokers can suffer resulting in out-of-sync partitions and container restarts perpetuating the issue. Resource constraints is one of the first things we consider when diagnosing ES issues.","title":"What are the impacts of having not enough resource for Kafka?"},{"location":"technology/faq/#security-configuration","text":"On Kubernetes, Kafka can be configured with external and internal URLs. With Strimzi internal URLs are using TLS or Plain authentication, then TLS for encryption. If no authentication property is specified then the listener does not authenticate clients which connect through that listener. The listener will accept all connections without authentication. Mutual TLS authentication for internal communication looks like: - name : tls port : 9093 type : internal tls : true authentication : type : tls To connect any app (producer, consumer) we need a TLS user like: piVersion : kafka.strimzi.io/v1beta2 kind : KafkaUser metadata : name : tls-user labels : strimzi.io/cluster : vaccine-kafka spec : authentication : type : tls Then the following configurations need to be done for each app. For example in Quarkus app, we need to specify where to find the client certificate (for each Kafka TLS user a secret is created with the certificate (ca.crt) and a user password) oc describe secret tls-user Data ==== ca.crt: 1164 bytes user.crt: 1009 bytes user.key: 1704 bytes user.p12: 2374 bytes user.password: 12 bytes For Java client we need the following security settings, to specify from which secret to get the keystore password and certificate. The certificate will be mounted to /deployments/certs/user . %prod.kafka.security.protocol = SSL %prod.kafka.ssl.keystore.location = /deployments/certs/user/user.p12 %prod.kafka.ssl.keystore.type = PKCS12 quarkus.openshift.env.mapping.KAFKA_SSL_KEYSTORE_PASSWORD.from-secret = ${ KAFKA_USER : tls -user } quarkus.openshift.env.mapping.KAFKA_SSL_KEYSTORE_PASSWORD.with-key = user.password quarkus.openshift.mounts.user-cert.path = /deployments/certs/user quarkus.openshift.secret-volumes.user-cert.secret-name = ${ KAFKA_USER : tls -user } # To validate server side certificate we will mount it too with the following declaration quarkus.openshift.env.mapping.KAFKA_SSL_TRUSTSTORE_PASSWORD.from-secret = ${ KAFKA_CA_CERT_NAME : kafka -cluster-ca-cert } quarkus.openshift.env.mapping.KAFKA_SSL_TRUSTSTORE_PASSWORD.with-key = ca.password quarkus.openshift.mounts.kafka-cert.path = /deployments/certs/server quarkus.openshift.secret-volumes.kafka-cert.secret-name = ${ KAFKA_CA_CERT_NAME : kafka -cluster-ca-cert } For the server side certificate, it will be in a truststore, which is mounted to /deployments/certs/server and from a secret (this secret is created at the cluster level). Also because we also use TLS for encryption we need: %prod.kafka.ssl.protocol=TLSv1.2 Mutual TLS authentication is always used for the communication between Kafka brokers and ZooKeeper pods. For mutual, or two-way, authentication, both the server and the client present certificates. SCRAM: (Salted Challenge Response Authentication Mechanism) is an authentication protocol that can establish mutual authentication using passwords. Strimzi can configure Kafka to use SASL (Simple Authentication and Security Layer) SCRAM-SHA-512 to provide authentication on both unencrypted and encrypted client connections. The listener declaration: - name : external port : 9094 type : route tls : true authentication : type : scram-sha-512 Need a scram-user: apiVersion : kafka.strimzi.io/v1beta2 kind : KafkaUser metadata : name : scram-user labels : strimzi.io/cluster : vaccine-kafka spec : authentication : type : scram-sha-512 Then the app properties need to have: security.protocol = SASL_SSL %prod.quarkus.openshift.env.mapping.KAFKA_SSL_TRUSTSTORE_PASSWORD.from-secret = ${ KAFKA_CA_CERT_NAME : kafka -cluster-ca-cert } %prod.quarkus.openshift.env.mapping.KAFKA_SSL_TRUSTSTORE_PASSWORD.with-key = ca.password %prod.quarkus.openshift.env.mapping.KAFKA_SCRAM_PWD.from-secret = ${ KAFKA_USER : scram -user } %prod.quarkus.openshift.env.mapping.KAFKA_SCRAM_PWD.with-key = password %prod.quarkus.openshift.mounts.kafka-cert.path = /deployments/certs/server %prod.quarkus.openshift.secret-volumes.kafka-cert.secret-name = ${ KAFKA_CA_CERT_NAME : kafka -cluster-ca-cert }","title":"Security configuration"},{"location":"technology/faq/#verify-consumer-connection","text":"Here is an example of TLS authentication for Event streams ConsumerConfig values: bootstrap.servers = [eda-dev-kafka-bootstrap.eventstreams.svc:9093] check.crcs = true client.dns.lookup = default client.id = cold-chain-agent-c2c11228-d876-4db2-a16a-ea7826e358d2-StreamThread-1-restore-consumer client.rack = connections.max.idle.ms = 540000 default.api.timeout.ms = 60000 enable.auto.commit = false exclude.internal.topics = true fetch.max.bytes = 52428800 fetch.max.wait.ms = 500 fetch.min.bytes = 1 group.id = null group.instance.id = null heartbeat.interval.ms = 3000 interceptor.classes = [] internal.leave.group.on.close = false isolation.level = read_uncommitted key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer sasl.client.callback.handler.class = null sasl.jaas.config = null sasl.kerberos.kinit.cmd = /usr/bin/kinit sasl.kerberos.min.time.before.relogin = 60000 sasl.kerberos.service.name = null sasl.kerberos.ticket.renew.jitter = 0.05 sasl.kerberos.ticket.renew.window.factor = 0.8 sasl.login.callback.handler.class = null sasl.login.class = null sasl.login.refresh.buffer.seconds = 300 sasl.login.refresh.min.period.seconds = 60 sasl.login.refresh.window.factor = 0.8 sasl.login.refresh.window.jitter = 0.05 sasl.mechanism = GSSAPI security.protocol = SSL security.providers = null send.buffer.bytes = 131072 session.timeout.ms = 10000 ssl.cipher.suites = null ssl.enabled.protocols = [TLSv1.2] ssl.endpoint.identification.algorithm = https ssl.key.password = null ssl.keymanager.algorithm = SunX509 ssl.keystore.location = /deployments/certs/user/user.p12 ssl.keystore.password = [hidden] ssl.keystore.type = PKCS12 ssl.protocol = TLSv1.2 ssl.provider = null ssl.secure.random.implementation = null ssl.trustmanager.algorithm = PKIX ssl.truststore.location = /deployments/certs/server/ca.p12 ssl.truststore.password = [hidden] ssl.truststore.type = PKCS12 value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer","title":"Verify consumer connection"},{"location":"technology/faq/#what-security-support-in-kafka","text":"Encrypt data in transit between producer and Kafka brokers Client authentication Client authorization","title":"What Security support in Kafka"},{"location":"technology/faq/#how-to-protect-data-at-rest","text":"Use encrypted file system for each brokers Encrypt data at the producer level, using some API, and then decode at the consumer level. The data in the appeld log will be encrypted.","title":"How to protect data at rest?"},{"location":"technology/faq/#more-advanced-concepts","text":"","title":"More advanced concepts"},{"location":"technology/faq/#what-is-range-partition-assignment-strategy","text":"There are multiple partition assignment strategy for a consumer, part of a consumer group , to get its partition to fetch data from. Members of the consumer group subscribe to the topics they are interested in and forward their subscriptions to a Kafka broker serving as the group coordinator. The coordinator selects one member to perform the group assignment and propagates the subscriptions of all members to it. Then assign(Cluster, GroupSubscription) is called to perform the assignment and the results are forwarded back to each respective members. Range assignor works on a per-topic basis: it lays out the available partitions in numeric order and the consumers in lexicographic order, and assign partition to each consumer so partition with the same id will be in the same consumer: topic-1-part-0 and topic-2-part-0 will be processed by consumer-0","title":"What is range partition assignment strategy?"},{"location":"technology/faq/#what-is-sticky-assignor","text":"The CooperativeStickyAssignor helps supporting incremental cooperative rebalancing to the clients' group protocol, which allows consumers to keep all of their assigned partitions during a rebalance and at the end revoke only those which must be migrated to another consumer for overall cluster balance. The goal is to reduce unnecessary downtime due to unnecessary partition migration, by leveraging the sticky assignor which link consumer to partition id. See KIP 429 for details.","title":"What is sticky assignor?"},{"location":"technology/faq/#how-to-get-an-homogeneous-distribution-of-message-to-partitions","text":"Design the message key and hash coding for even distributed. Or implement a customer partitioner by implementing the Partitioner interface.","title":"How to get an homogeneous distribution of message to partitions?"},{"location":"technology/faq/#how-to-ensure-efficient-join-between-two-topics","text":"Need to use co-partitioning, which means having the same key in both topic, the same number of partitions and the same producer partitioner, which most likely should be the default one that uses the following formula: partition = hash(key) % numPartitions .","title":"How to ensure efficient join between two topics?"},{"location":"technology/faq/#what-is-transaction-in-kafka","text":"Producer can use transaction begin, commit and rollback API while publishing events to a multi partition topic. This is done by setting a unique transactionId as part of its configuration (with idempotence and min inflight record set to 1). Either all messages are successfully written or none of them are. There are some producer exception to consider to abort the transaction: any KafkaException for sure, but also OutOfSequenceTx which may happen when the PID is greater than the last one seen by the producer. See explanations here . And the KIP 98","title":"What is transaction in Kafka?"},{"location":"technology/faq/#what-is-the-high-watermark","text":"The high watermark offset is the offset of the last message that was successfully copied to all of the log\u2019s replicas. A consumer can only read up to the high watermark offset to prevent reading un-replicated messages.","title":"What is the high watermark?"},{"location":"technology/faq/#what-should-we-do-for-queue-full-exception-or-timeout-exception-on-producer","text":"The brokers are running behind, so we need to add more brokers and redistribute partitions.","title":"What should we do for queue full exception or timeout exception on producer?"},{"location":"technology/faq/#how-to-send-large-messages","text":"We can set some properties at the broker, topic, consumer and producer level: Broker: consider the message.max.bytes and replica.fetch.max.bytes Consumer: max.partition.fetch.bytes . Records are fetched in batches by the consumer, so this properties gives the max amount of data per partition the server will return. Default 1 Megabyte","title":"How to send large messages?"},{"location":"technology/faq/#how-to-maximize-throughput","text":"For producer if you want to maximize throughput over low latency, set batch.size and linger.ms to higher value. Linger delay producer, it will wait for up to the given delay to allow other records to be sent so that the sends can be batched together.","title":"How to maximize throughput?"},{"location":"technology/faq/#why-kafka-stream-applications-may-impact-cluster-performance","text":"They may use internal hidden topics to persist their states for Ktable and GlobalKTable. Process input and output topics","title":"Why Kafka Stream applications may impact cluster performance?"},{"location":"technology/faq/#how-message-schema-version-is-propagated","text":"The record includes a byte with the version number from the schema registry.","title":"How message schema version is propagated?"},{"location":"technology/faq/#consumers-do-not-see-message-in-topic-what-happens","text":"The brokers may have an issue on this partition. If a broker, part of the ISR list fails, then new leader election may delay the broker commit from a producer. The consumer has a communication issue, or fails, so the consumer group rebalance is underway.","title":"Consumers do not see message in topic, what happens?"},{"location":"technology/faq/#how-compression-schema-used-is-known-by-the-consumer","text":"The record header includes such metadata. So it is possible to have different schema per record.","title":"How compression schema used is known by the consumer?"},{"location":"technology/faq/#what-does-out-of-synch-partition-mean-and-when-it-occurs","text":"With partition leader and replication to the followers, the number of in-synch replicas is at least the number of expected replicas. For example for a replicas = 3 the in-synch is set to 2, and it represents the minimum number of replicas that must acknowledge a write for the write to be considered successful. The record is considered \u201ccommitted\u201d when all ISRs for a partition wrote to their log. Only committed records are readable from consumer. So out-of-synch will happen if the followers are not able to send their acknowledge to the replica leader as quickly as expected.","title":"What does out-of-synch partition mean and when it occurs?"},{"location":"technology/faq/#run-kafka-test-container-with-topologytestdriver","text":"Topology Test Driver is used without kafka, so there is no real need to use test container.","title":"Run Kafka Test Container with TopologyTestDriver"},{"location":"technology/faq/#how-to-remove-personal-identifying-information","text":"From the source connector, it is possible to add processing class to process the records before publishing them to Kafka topic, so that any Kafka Streams apps will not see PII.","title":"How to remove personal identifying information?"},{"location":"technology/faq/#how-to-handle-variable-workload-with-kafka-connector-source-connector","text":"Increase and decrease the number of Kafka connect workers based upon current application load.","title":"How to handle variable workload with Kafka Connector source connector?"},{"location":"technology/faq/#derived-products-related-questions","text":"","title":"Derived products related questions"},{"location":"technology/faq/#competitors-to-kafka","text":"NATS Redpanda a Modern streaming platform for mission critical workloads, and is compatible with Kafka API. It is a cluster of brokers without any zookeepers. It also leverage the SSD technology to improve I/O operations. AWS Kinesis Cloud service, managed by AWS staff, paid as you go, proportional to the shard (like partition) used. 24h to 7 days persistence Number of shards are adaptable with throughput. Uses the concept of Kinesis data streams, which uses shards: data records are composed of a sequence number, a partition key and a data blob. restrictions on message size (1 MB) and consumption rate of messages (5 reads /s, < 2MB per shard, 1000 write /s) Server side encryption using master key managed by AWS KMS GCP Pub/sub Solace Active MQ: Java based messaging server to be the JMS reference implementation, so it supports transactional messaging. various messaging protocols including AMQP, STOMP, and MQTT It maintains the delivery state of every message resulting in lower throughput. Can apply JMS message selector to consumer specific message Point to point or pub/sub, but servers push messages to consumer/subscribers Performance of both queue and topic degrades as the number of consumers rises Rabbit MQ: Support queues, with messages removed once consumed Add the concept of Exchange to route message to queues Limited throughput, but can send large message Support JMS, AMQP protocols, and participation to transaction Smart broker / dumb consumer model that focuses on consistently delivering messages to consumers.","title":"Competitors to Kafka"},{"location":"technology/faq/#differences-between-amq-streams-and-confluent","text":"AMQ Streams and Confluent are based on the open source Kafka, but Confluent as the main contributer to Kafka, is adding proprietary features to make the product more marketable, so we will not do a pure features comparison a generic features comparison: Feature Confluent AMQ Streams Kafka open source Aligned within a month to the Kafka release Within 2 months after Kafka release Kafka API Same Same k8s / OpenShift deployment Helm \"operator\" Real Kubernetes Operator based on open source Strimzi Kafka Connectors Connectors hub to reference any connectors on the market, with some certified for Confluent. Open source connectors supported. Apache Camel offers a set of connectors not directly supported by Red Hat but useful in a BYO connectors. Fuse and Debezium can be used. Schema registry Proprietary API and schema Solution may leverage open source Apicur.io schema registry which is compatible with Confluent API. Cruise control for auto cluster balancing Adds on Available via Operator Mirroring between clusters Replicator tool Mirror Maker 2 deployable and managed by Strimzi operator Multi region cluster Supported Supported Role Based access control Supported Supported with explicit user manifest, integrated with Red Hat SSO and OPA. ksql Open sourced licensed by Confluent Customer can use open source version of kSQL but meed to verify licensing for cloud deployment. SQL processing on Kafka Records may also being done with Apache Flink Kafka Streams Supported from Kafka Open Source Supported from Kafka Open Source. Also moving CEP and Streams processing to an external tool makes a lot more sense. Apache Flink should be considered. Not directly supported by Red Hat Storage NFS and tiered storage Block storage with replication to s3 buckets for long persisence using Kafka connector. S3 Connector is not supported by Red Hat. As a managed service Proprietary solution Same with: IBM Event Streams and Red Hat AMQ streams as a service Integration with IBM mainframe Not strategic - Weak Strong with IBM connector and deployment on Z and P Admin User Interface Control center Operator in OpenShift and 3nd party open-source user interface like Kafdrop, Kowl, work with AMQ Streams but without direct support from Red Hat As Kafka adoption is a strategic investment, it is important to grow the competency and skill set to manage kafka clusters. Zookeeper was an important element to complexify the cluster management, as 2.8 it is removed, so it should be simpler to manage cluster. With customers cann influence the product roadmap, but it will kill the open source approach if only Confluent committers prioritize the feature requets. It is important to keep competitions and multi committers.","title":"Differences between AMQ Streams and Confluent"},{"location":"technology/faq/#event-streams-resource-requirements","text":"See the detailed tables in the product documentation.","title":"Event streams resource requirements"},{"location":"technology/faq/#differences-between-akka-and-kafka","text":"Akka is a open source toolkit for Scala or Java to simplify multithreading programming and makes application more reactive by adopting an asynchronous mechanism to access to io: database or HTTP request. To support asynchronous communication between 'actors', it uses messaging, internal to the JVM. Kafka is part of the architecture, while Akka is an implementation choice for one of the component of the business application deployed inside the architecture. vert.x is another open source implementation of such internal messaging mechanism but supporting more language: Java, Groovy, Ruby, JavaScript, Ceylon, Scala, and Kotlin.","title":"Differences between Akka and Kafka?"},{"location":"technology/faq/#is-is-possible-to-stream-video-to-kafka","text":"Yes it is possible, but need to do that with care and real justification. If the goal is to classify streamed images, then it is possible to do so, and need to assess if it will be important to be streams versus video at rest. The following article, from Neeraj Krishna illustrates a python implementation to send video frame every 3 images, do image classification using ResNet50 model trained on ImageNet, embbeded in python consumer. The results are saved in mongodb with the metadata needed to query after processing.","title":"Is is possible to stream video to kafka?"},{"location":"technology/faq/#other-faqs","text":"IBM Event streams on Cloud FAQ FAQ from Confluent","title":"Other FAQs"},{"location":"technology/flink/","text":"Warning Updated 08/20/2022- Work in progress Why Flink? \u00b6 In classical IT architecture, we can see two types of data processing: transactional and analytics. With 'monolytics' application, the database system serves multiple applications which sometimes access the same database instances and tables. This approach cause problems to support evolution and scaling. Microservice architecture addresses part of those problems by isolating data storage per service. To get insight from the data, the traditional approach is to develop data warehouse and ETL jobs to copy and transform data from the transactional systems to the warehouse. ETL process extracts data from a transactional database, transforms data into a common representation that might include validation, value normalization, encoding, deduplication, and schema transformation, and finally loads the new record into the target analytical database. They are batches and run periodically. From the data warehouse, the analysts build queries, metrics, and dashboards / reports to address a specific business question. Massive storage is needed, which uses different protocol such as: NFS, S3, HDFS... Today, there is a new way to think about data by seeing they are created as continuous streams of events, which can be processed in real time, and serve as the foundation for stateful stream processing application: the analytics move to the real data stream. We can define three classes of applications implemented with stateful stream processing: Event-driven applications : to adopt the reactive manifesto for scaling, resilience, responsive application, leveraging messaging as communication system. Data pipeline applications : replace ETL with low latency stream processing. Data analytics applications : immediatly act on the data and query live updated reports. For more real industry use cases content see the Flink Forward web site. The What \u00b6 Apache Flink (2016) is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Flink supports batch (data set )and graph (data stream) processing. It is very good at: Very low latency processing event time semantics to get consistent and accurate results even in case of out of order events Exactly once state consistency Millisecond latencies while processing millions of events per second Expressive and easy-to-use APIs: map, reduce, join, window, split, and connect. SQL support to implement user friendly streaming queries Fault tolerance, and high availability: supports worker and master failover, eliminating any single point of failure A lot of connectors to integrate with Kafka, Cassandra, Elastic Search, JDBC, S3... Support container and deployment on Kubernetes Support updating the application code and migrate jobs to different Flink clusters without losing the state of the application Also support batch processing The figure below illustrates those different models combined with Zepellin as a multi purpose notebook to develop data analytic projects on top of Spark, Python or Flink. Flink architecture \u00b6 Flink consists of a Job Manager and n Task Managers . The JobManager controls the execution of a single application. It receives an application for execution and builds a Task Execution Graph from the defined Job Graph. It manages job submission and the job lifecycle then allocates work to Task Managers The Resource Manager manages Task Slots and leverages underlying orchestrator, like Kubernetes or Yarn. A Task slot is the unit of work executed on CPU. The Task Managers execute the actual stream processing logic. There are multiple task managers running in a cluster. The number of slots limits the number of tasks a TaskManager can execute. After it has been started, a TaskManager registers its slots to the ResourceManager The Disparcher exposes API to submit applications for execution. It hosts the user interface too. Only one Job Manager is active at a given point of time, and there may be n Task Managers. There are different deployment models : Deploy on executing cluster, this is the session mode . Use session cluster to run multiple jobs: we need a JobManager container. Per job mode, spin up a cluster per job submission. More k8s oriented. This provides better resource isolation. Application mode creates a cluster per app with the main() function executed on the JobManager. It can include multiple jobs but they run inside the app. It allows for saving the required CPU cycles, but also save the bandwidth required for downloading the dependencies locally. Flink can run on any common resource manager like Hadoop Yarn, Mesos, or Kubernetes. For development purpose, we can use docker images to deploy a Session or Job cluster . Batch processing \u00b6 Process all the data in one job with bounded dataset. It is used when we need all the data for assessing trend, develop AI model, and with a focus on throughput instead of latency. Hadoop was designed to do batch processing. Flink has capability to replace Hadoop map reduce processing. High Availability \u00b6 With Task managers running in parallel, if one fails the number of available slots drops by the JobManager asks the Resource Manager to get new processing slots. The application's restart strategy determines how often the JobManager restarts the application and how long it waits between restarts. Flink uses Zookeeper to manage multiple JobManagers and select the leader to control the execution of the streaming application. Application's tasks checkpoints and other states are saved in a remote storage, but metadata are saved in Zookeeper. When a JobManager fails, all tasks that belong to its application are automatically cancelled. A new JobManager that takes over the work by getting information of the storage from Zookeeper, and then restarts the process with the JobManager. Stream processing concepts \u00b6 In Flink , applications are composed of streaming dataflows that may be transformed by user-defined operators. These dataflows form directed graphs that start with one or more sources, and end in one or more sinks. The data flows between operations. The figure below, from product documentation, summarizes the simple APIs used to develop a data stream processing flow: src: apache Flink product doc Stream processing includes a set of functions to transform data, to produce a new output stream. Intermediate steps compute rolling aggregations like min, max, mean, or collect and buffer records in time window to compute metrics on finite set of events. To properly define window operator semantics, we need to determine both how events are assigned to buckets and how often the window produces a result. Flink's streaming model is based on windowing and checkpointing, it uses controlled cyclic dependency graph as its execution engine. The following figure is showing integration of stream processing runtime with an append log system, like Kafka, with internal local state persistence and continuous checkpoint to remote storage as HA support: As part of the checkpointing process, Flink saves the 'offset read commit' information of the append log, so in case of a failure, Flink recovers a stateful streaming application by restoring its state from a previous checkpoint and resetting the read position on the append log. The evolution of microservice is to become more event-driven, which are stateful streaming applications that ingest event streams and process the events with application-specific business logic. This logic can be done in flow defined in Flink and executed in the clustered runtime. A lot of predefined connectors exist to connect to specific source and sink. Transform operators can be chained. Dataflow can consume from Kafka, Kinesis, Queue, and any data sources. A typical high level view of Flink app is presented in figure below: src: apache Flink product doc Programs in Flink are inherently parallel and distributed. During execution, a stream has one or more stream partitions, and each operator has one or more operator subtasks. src: apache Flink site A Flink application, can be stateful, run in parallel on a distributed cluster. The various parallel instances of a given operator will execute independently, in separate threads, and in general will be running on different machines. State is always accessed local, which helps Flink applications achieve high throughput and low-latency. You can choose to keep state on the JVM heap, or if it is too large, saves it in efficiently organized on-disk data structures. This is the Job Manager component which parallelizes the job and distributes slices of the Data Stream flow, you defined, to the Task Managers for execution. Each parallel slice of your job will be executed in a task slot . Once Flink is started (for example with the docker image), Flink Dashboard http://localhost:8081/#/overview presents the execution reporting of those components: The execution is from one of the training examples, the number of task slot was set to 4, and one job is running. Spark is not a true real time processing while Flink is. Flink and Spark support batch processing too. Stateless \u00b6 Some applications support data loss and expect fast recovery times in case of failure and always consuming the latest incoming data. Alerting applications where only low latency alerts are useful, or application where only the last data received is relevant. When checkpointing is turned off Flink offers no inherent guarantees in case of failures. This means that you can either have data loss or duplicate messages combined always with a loss of application state. Statefulness \u00b6 When using aggregates or windows operators, states need to be kept. For fault tolerant Flink uses checkpoints and savepoints. Checkpoints represent a snapshot of where the input data stream is with each operator's state. A streaming dataflow can be resumed from a checkpoint while maintaining consistency (exactly-once processing semantics) by restoring the state of the operators and by replaying the records from the point of the checkpoint. In case of failure of a parallel execution, Flink stops the stream flow, then restarts operators from the last checkpoints. When doing the reallocation of data partition for processing, states are reallocated too. States are saved on distributed file systems. When coupled with Kafka as data source, the committed read offset will be part of the checkpoint data. Flink uses the concept of Checkpoint Barriers , which represents a separation of records, so records received since the last snapshot are part of the future snapshot. Barrier can be seen as a mark, a tag in the data stream that close a snapshot. In Kafka, it will be the last committed read offset. The barrier flows with the stream so can be distributed. Once a sink operator (the end of a streaming DAG) has received the barrier n from all of its input streams, it acknowledges that snapshot n to the checkpoint coordinator. After all sinks have acknowledged a snapshot, it is considered completed. Once snapshot n has been completed, the job will never ask the source for records before such snapshot. State snapshots are save in a state backend (in memory, HDFS, RockDB). KeyedStream is a key-value store. Key match the key in the stream, state update does not need transaction. For DataSet (Batch processing) there is no checkpoint, so in case of failure the stream is replayed. When addressing exactly once processing it is very important to consider the following: the read from the source apply the processing logic like window aggregation generate the results to a sink 1 and 2 can be done exactly once, using Flink source connector and checkpointing but generating one unique result to a sink is more complex and is dependant of the target technology. After reading records from Kafka, do the processing and generate results, in case of failure Flink will reload the record from the read offset and may generate duplicate in the Sink. As duplicates will occur, we always need to assess idempotent support from downstream applications. A lot of distributed key-value storages support consistent result event after retries. To support end-to-end exactly one delivery we need to have a sink that supports transaction and two-phase commit. In case of failure we need to rollback the output generated. It is important to note transactional output impacts latency. Flink takes checkpoints periodically, like every 10 seconds, which leads to the minimum latency we can expect at the sink level. For Kafka Sink connector, as kafka producer, we need to set the transactionId , and the delivery type: new KafkaSinkBuilder < String > () . setBootstrapServers ( bootstrapURL ) . setDeliverGuarantee ( DeliveryGuarantee . EXACTLY_ONCE ) . setTransactionalIdPrefix ( \"store-sol\" ) With transaction ID, a sequence number is sent by the kafka producer API to the broker, and so the partition leader will be able to remove duplicate retries. When the checkpointing period is set, we need to also configure transaction.max.timeout.ms of the Kafka broker and transaction.timeout.ms for the producer (sink connector) to a higher timeout than the checkpointing interval plus the max expected Flink downtime. If not the Kafka broker will consider the connection has fail and will remove its state management. State management \u00b6 All data maintained by a task and used to compute the results of a function belong to the state of the task. While processing the data, the task can read and update its state and compute its result based on its input data and state. State management includes address very large states, and no state is lost in case of failures. Each operator needs to register its state. Operator State is scoped to an operator task: all records processed by the same parallel task have access to the same state Keyed state is maintained and accessed with respect to a key defined in the records of an operator\u2019s input stream. Flink maintains one state instance per key value and Flink partitions all records with the same key to the operator task that maintains the state for this key. The key-value map is sharded across all parallel tasks: Each task maintains its state locally to improve latency. For small state, the state backends will use JVM heap, but for larger state RocksDB is used. A state backend takes care of checkpointing the state of a task to a remote and persistent storage. With stateful distributed processing, scaling stateful operators, enforces state repartitioning and assigning to more or fewer parallel tasks. Keys are organized in key-groups, and key groups are assigned to tasks. Operators with operator list state are scaled by redistributing the list entries. Operators with operator union list state are scaled by broadcasting the full list of state entries to each task. Flink uses Checkpointing to periodically store the state of the various stream processing operators on durable storage. When recovering from a failure, the stream processing job can resume from the latest checkpoint. Checkpointing is coordinated by the Job Manager, it knows the location of the latest completed checkpoint which will get important later on. This checkpointing and recovery mechanism can provide exactly-once consistency for application state, given that all operators checkpoint and restore all of their states and that all input streams are reset to the position up to which they were consumed when the checkpoint was taken. This will work perfectly with Kafka, but not with sockets or queues where messages are lost once consumed. Therefore exactly-once state consistency can be ensured only if all input streams are from resettable data sources. During the recovery and depending on the sink operators of an application, some result records might be emitted multiple times to downstream systems. Windowing \u00b6 Windows are buckets within a Stream and can be defined with times, or count of elements. Tumbling window assign events into nonoverlapping buckets of fixed size. When the window border is passed, all the events are sent to an evaluation function for processing. Count-based tumbling windows define how many events are collected before triggering evaluation. Time based timbling window define time interval of n seconds. Amount of the data vary in a window. .keyBy(...).window(TumblingProcessingTimeWindows.of(Time.seconds(2))) Sliding window: same but windows can overlap. An event might belong to multiple buckets. So there is a window sliding time parameter: .keyBy(...).window(SlidingProcessingTimeWindows.of(Time.seconds(2), Time.seconds(1))) Session window: Starts when the data stream processes records and stop when there is inactivity, so the timer set this threshold: .keyBy(...).window(ProcessingTimeSessionWindows.withGap(Time.seconds(5))) . The operator creates one window for each data element received. Global window: one window per key and never close. The processing is done with Trigger: . keyBy ( 0 ) . window ( GlobalWindows . create ()) . trigger ( CountTrigger . of ( 5 )) KeyStream can help to run in parallel, each window will have the same key. Time is central to the stream processing, and the time is a parameter of the flow / environment and can take different meanings: ProcessingTime = system time of the machine executing the task: best performance and low latency EventTime = the time at the source level, embedded in the record. Deliver consistent and deterministic results regardless of order IngestionTime = time when getting into Flink. See example TumblingWindowOnSale.java and to test it, do the following: # Start the SaleDataServer that starts a server on socket 9181 and will read the avg.txt file and send each line to the socket java -cp target/my-flink-1.0.0-SNAPSHOT.jar jbcodeforce.sale.SaleDataServer # inside the job manager container start with ` flink run -d -c jbcodeforce.windows.TumblingWindowOnSale /home/my-flink/target/my-flink-1.0.0-SNAPSHOT.jar ` . # The job creates the data/profitPerMonthWindowed.txt file with accumulated sale and number of record in a 2 seconds tumbling time window ( June,Bat,Category5,154,6 ) ( August,PC,Category5,74,2 ) ( July,Television,Category1,50,1 ) ( June,Tablet,Category2,142,5 ) ( July,Steamer,Category5,123,6 ) ... Trigger \u00b6 Trigger determines when a window is ready to be processed. All windows have default trigger. For example tumbling window has a 2s trigger. Global window has explicit trigger. We can implement our own triggers by implementing the Trigger interface with different methods to implement: onElement(..), onEventTime(...), onProcessingTime(...) Default triggers: EventTimeTrigger: fires based upon progress of event time ProcessingTimeTrigger: fires based upon progress of processing time CountTrigger: fires when # of element in a window > parameter PurgingTrigger Eviction \u00b6 Evictor is used to remove elements from a window after the trigger fires and before or after the window function is applied. The logic to remove is app specific. The predefined evictors: CountEvictor, DeltaEvictor and TimeEvictor. Watermark \u00b6 Watermark is the mechanism to keep how the event time has progressed: with windowing operator, event time stamp is used, but windows are defined on elapse time, for example, 10 minutes, so watermark helps to track te point of time where no more delayed events will arrive. The Flink API expects a WatermarkStrategy that contains both a TimestampAssigner and WatermarkGenerator. A TimestampAssigner is a simple function that extracts a field from an event. A number of common strategies are available out of the box as static methods on WatermarkStrategy, so reference to the documentation and examples. Watermark is crucial for out of order events, and when dealing with multi sources. Kafka topic partitions can be a challenge without watermark. With IoT device and network latency, it is possible to get an event with an earlier timestamp, while the operator has already processed such event timestamp from other sources. It is possible to configure to accept late events, with the allowed lateness time by which element can be late before being dropped. Flink keeps a state of Window until the allowed lateness time expires. Resources \u00b6 Product documentation . Official training Base docker image is: https://hub.docker.com/_/flink Flink docker setup and the docker-compose files in this repo. FAQ Cloudera flink stateful tutorial : very good example for inventory transaction and queries on item considered as stream Building real-time dashboard applications with Apache Flink, Elasticsearch, and Kibana Udemy Apache Flink a real time hands-on: do not recommend this one !.","title":"Apache Flink"},{"location":"technology/flink/#why-flink","text":"In classical IT architecture, we can see two types of data processing: transactional and analytics. With 'monolytics' application, the database system serves multiple applications which sometimes access the same database instances and tables. This approach cause problems to support evolution and scaling. Microservice architecture addresses part of those problems by isolating data storage per service. To get insight from the data, the traditional approach is to develop data warehouse and ETL jobs to copy and transform data from the transactional systems to the warehouse. ETL process extracts data from a transactional database, transforms data into a common representation that might include validation, value normalization, encoding, deduplication, and schema transformation, and finally loads the new record into the target analytical database. They are batches and run periodically. From the data warehouse, the analysts build queries, metrics, and dashboards / reports to address a specific business question. Massive storage is needed, which uses different protocol such as: NFS, S3, HDFS... Today, there is a new way to think about data by seeing they are created as continuous streams of events, which can be processed in real time, and serve as the foundation for stateful stream processing application: the analytics move to the real data stream. We can define three classes of applications implemented with stateful stream processing: Event-driven applications : to adopt the reactive manifesto for scaling, resilience, responsive application, leveraging messaging as communication system. Data pipeline applications : replace ETL with low latency stream processing. Data analytics applications : immediatly act on the data and query live updated reports. For more real industry use cases content see the Flink Forward web site.","title":"Why Flink?"},{"location":"technology/flink/#the-what","text":"Apache Flink (2016) is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Flink supports batch (data set )and graph (data stream) processing. It is very good at: Very low latency processing event time semantics to get consistent and accurate results even in case of out of order events Exactly once state consistency Millisecond latencies while processing millions of events per second Expressive and easy-to-use APIs: map, reduce, join, window, split, and connect. SQL support to implement user friendly streaming queries Fault tolerance, and high availability: supports worker and master failover, eliminating any single point of failure A lot of connectors to integrate with Kafka, Cassandra, Elastic Search, JDBC, S3... Support container and deployment on Kubernetes Support updating the application code and migrate jobs to different Flink clusters without losing the state of the application Also support batch processing The figure below illustrates those different models combined with Zepellin as a multi purpose notebook to develop data analytic projects on top of Spark, Python or Flink.","title":"The What"},{"location":"technology/flink/#flink-architecture","text":"Flink consists of a Job Manager and n Task Managers . The JobManager controls the execution of a single application. It receives an application for execution and builds a Task Execution Graph from the defined Job Graph. It manages job submission and the job lifecycle then allocates work to Task Managers The Resource Manager manages Task Slots and leverages underlying orchestrator, like Kubernetes or Yarn. A Task slot is the unit of work executed on CPU. The Task Managers execute the actual stream processing logic. There are multiple task managers running in a cluster. The number of slots limits the number of tasks a TaskManager can execute. After it has been started, a TaskManager registers its slots to the ResourceManager The Disparcher exposes API to submit applications for execution. It hosts the user interface too. Only one Job Manager is active at a given point of time, and there may be n Task Managers. There are different deployment models : Deploy on executing cluster, this is the session mode . Use session cluster to run multiple jobs: we need a JobManager container. Per job mode, spin up a cluster per job submission. More k8s oriented. This provides better resource isolation. Application mode creates a cluster per app with the main() function executed on the JobManager. It can include multiple jobs but they run inside the app. It allows for saving the required CPU cycles, but also save the bandwidth required for downloading the dependencies locally. Flink can run on any common resource manager like Hadoop Yarn, Mesos, or Kubernetes. For development purpose, we can use docker images to deploy a Session or Job cluster .","title":"Flink architecture"},{"location":"technology/flink/#batch-processing","text":"Process all the data in one job with bounded dataset. It is used when we need all the data for assessing trend, develop AI model, and with a focus on throughput instead of latency. Hadoop was designed to do batch processing. Flink has capability to replace Hadoop map reduce processing.","title":"Batch processing"},{"location":"technology/flink/#high-availability","text":"With Task managers running in parallel, if one fails the number of available slots drops by the JobManager asks the Resource Manager to get new processing slots. The application's restart strategy determines how often the JobManager restarts the application and how long it waits between restarts. Flink uses Zookeeper to manage multiple JobManagers and select the leader to control the execution of the streaming application. Application's tasks checkpoints and other states are saved in a remote storage, but metadata are saved in Zookeeper. When a JobManager fails, all tasks that belong to its application are automatically cancelled. A new JobManager that takes over the work by getting information of the storage from Zookeeper, and then restarts the process with the JobManager.","title":"High Availability"},{"location":"technology/flink/#stream-processing-concepts","text":"In Flink , applications are composed of streaming dataflows that may be transformed by user-defined operators. These dataflows form directed graphs that start with one or more sources, and end in one or more sinks. The data flows between operations. The figure below, from product documentation, summarizes the simple APIs used to develop a data stream processing flow: src: apache Flink product doc Stream processing includes a set of functions to transform data, to produce a new output stream. Intermediate steps compute rolling aggregations like min, max, mean, or collect and buffer records in time window to compute metrics on finite set of events. To properly define window operator semantics, we need to determine both how events are assigned to buckets and how often the window produces a result. Flink's streaming model is based on windowing and checkpointing, it uses controlled cyclic dependency graph as its execution engine. The following figure is showing integration of stream processing runtime with an append log system, like Kafka, with internal local state persistence and continuous checkpoint to remote storage as HA support: As part of the checkpointing process, Flink saves the 'offset read commit' information of the append log, so in case of a failure, Flink recovers a stateful streaming application by restoring its state from a previous checkpoint and resetting the read position on the append log. The evolution of microservice is to become more event-driven, which are stateful streaming applications that ingest event streams and process the events with application-specific business logic. This logic can be done in flow defined in Flink and executed in the clustered runtime. A lot of predefined connectors exist to connect to specific source and sink. Transform operators can be chained. Dataflow can consume from Kafka, Kinesis, Queue, and any data sources. A typical high level view of Flink app is presented in figure below: src: apache Flink product doc Programs in Flink are inherently parallel and distributed. During execution, a stream has one or more stream partitions, and each operator has one or more operator subtasks. src: apache Flink site A Flink application, can be stateful, run in parallel on a distributed cluster. The various parallel instances of a given operator will execute independently, in separate threads, and in general will be running on different machines. State is always accessed local, which helps Flink applications achieve high throughput and low-latency. You can choose to keep state on the JVM heap, or if it is too large, saves it in efficiently organized on-disk data structures. This is the Job Manager component which parallelizes the job and distributes slices of the Data Stream flow, you defined, to the Task Managers for execution. Each parallel slice of your job will be executed in a task slot . Once Flink is started (for example with the docker image), Flink Dashboard http://localhost:8081/#/overview presents the execution reporting of those components: The execution is from one of the training examples, the number of task slot was set to 4, and one job is running. Spark is not a true real time processing while Flink is. Flink and Spark support batch processing too.","title":"Stream processing concepts"},{"location":"technology/flink/#stateless","text":"Some applications support data loss and expect fast recovery times in case of failure and always consuming the latest incoming data. Alerting applications where only low latency alerts are useful, or application where only the last data received is relevant. When checkpointing is turned off Flink offers no inherent guarantees in case of failures. This means that you can either have data loss or duplicate messages combined always with a loss of application state.","title":"Stateless"},{"location":"technology/flink/#statefulness","text":"When using aggregates or windows operators, states need to be kept. For fault tolerant Flink uses checkpoints and savepoints. Checkpoints represent a snapshot of where the input data stream is with each operator's state. A streaming dataflow can be resumed from a checkpoint while maintaining consistency (exactly-once processing semantics) by restoring the state of the operators and by replaying the records from the point of the checkpoint. In case of failure of a parallel execution, Flink stops the stream flow, then restarts operators from the last checkpoints. When doing the reallocation of data partition for processing, states are reallocated too. States are saved on distributed file systems. When coupled with Kafka as data source, the committed read offset will be part of the checkpoint data. Flink uses the concept of Checkpoint Barriers , which represents a separation of records, so records received since the last snapshot are part of the future snapshot. Barrier can be seen as a mark, a tag in the data stream that close a snapshot. In Kafka, it will be the last committed read offset. The barrier flows with the stream so can be distributed. Once a sink operator (the end of a streaming DAG) has received the barrier n from all of its input streams, it acknowledges that snapshot n to the checkpoint coordinator. After all sinks have acknowledged a snapshot, it is considered completed. Once snapshot n has been completed, the job will never ask the source for records before such snapshot. State snapshots are save in a state backend (in memory, HDFS, RockDB). KeyedStream is a key-value store. Key match the key in the stream, state update does not need transaction. For DataSet (Batch processing) there is no checkpoint, so in case of failure the stream is replayed. When addressing exactly once processing it is very important to consider the following: the read from the source apply the processing logic like window aggregation generate the results to a sink 1 and 2 can be done exactly once, using Flink source connector and checkpointing but generating one unique result to a sink is more complex and is dependant of the target technology. After reading records from Kafka, do the processing and generate results, in case of failure Flink will reload the record from the read offset and may generate duplicate in the Sink. As duplicates will occur, we always need to assess idempotent support from downstream applications. A lot of distributed key-value storages support consistent result event after retries. To support end-to-end exactly one delivery we need to have a sink that supports transaction and two-phase commit. In case of failure we need to rollback the output generated. It is important to note transactional output impacts latency. Flink takes checkpoints periodically, like every 10 seconds, which leads to the minimum latency we can expect at the sink level. For Kafka Sink connector, as kafka producer, we need to set the transactionId , and the delivery type: new KafkaSinkBuilder < String > () . setBootstrapServers ( bootstrapURL ) . setDeliverGuarantee ( DeliveryGuarantee . EXACTLY_ONCE ) . setTransactionalIdPrefix ( \"store-sol\" ) With transaction ID, a sequence number is sent by the kafka producer API to the broker, and so the partition leader will be able to remove duplicate retries. When the checkpointing period is set, we need to also configure transaction.max.timeout.ms of the Kafka broker and transaction.timeout.ms for the producer (sink connector) to a higher timeout than the checkpointing interval plus the max expected Flink downtime. If not the Kafka broker will consider the connection has fail and will remove its state management.","title":"Statefulness"},{"location":"technology/flink/#state-management","text":"All data maintained by a task and used to compute the results of a function belong to the state of the task. While processing the data, the task can read and update its state and compute its result based on its input data and state. State management includes address very large states, and no state is lost in case of failures. Each operator needs to register its state. Operator State is scoped to an operator task: all records processed by the same parallel task have access to the same state Keyed state is maintained and accessed with respect to a key defined in the records of an operator\u2019s input stream. Flink maintains one state instance per key value and Flink partitions all records with the same key to the operator task that maintains the state for this key. The key-value map is sharded across all parallel tasks: Each task maintains its state locally to improve latency. For small state, the state backends will use JVM heap, but for larger state RocksDB is used. A state backend takes care of checkpointing the state of a task to a remote and persistent storage. With stateful distributed processing, scaling stateful operators, enforces state repartitioning and assigning to more or fewer parallel tasks. Keys are organized in key-groups, and key groups are assigned to tasks. Operators with operator list state are scaled by redistributing the list entries. Operators with operator union list state are scaled by broadcasting the full list of state entries to each task. Flink uses Checkpointing to periodically store the state of the various stream processing operators on durable storage. When recovering from a failure, the stream processing job can resume from the latest checkpoint. Checkpointing is coordinated by the Job Manager, it knows the location of the latest completed checkpoint which will get important later on. This checkpointing and recovery mechanism can provide exactly-once consistency for application state, given that all operators checkpoint and restore all of their states and that all input streams are reset to the position up to which they were consumed when the checkpoint was taken. This will work perfectly with Kafka, but not with sockets or queues where messages are lost once consumed. Therefore exactly-once state consistency can be ensured only if all input streams are from resettable data sources. During the recovery and depending on the sink operators of an application, some result records might be emitted multiple times to downstream systems.","title":"State management"},{"location":"technology/flink/#windowing","text":"Windows are buckets within a Stream and can be defined with times, or count of elements. Tumbling window assign events into nonoverlapping buckets of fixed size. When the window border is passed, all the events are sent to an evaluation function for processing. Count-based tumbling windows define how many events are collected before triggering evaluation. Time based timbling window define time interval of n seconds. Amount of the data vary in a window. .keyBy(...).window(TumblingProcessingTimeWindows.of(Time.seconds(2))) Sliding window: same but windows can overlap. An event might belong to multiple buckets. So there is a window sliding time parameter: .keyBy(...).window(SlidingProcessingTimeWindows.of(Time.seconds(2), Time.seconds(1))) Session window: Starts when the data stream processes records and stop when there is inactivity, so the timer set this threshold: .keyBy(...).window(ProcessingTimeSessionWindows.withGap(Time.seconds(5))) . The operator creates one window for each data element received. Global window: one window per key and never close. The processing is done with Trigger: . keyBy ( 0 ) . window ( GlobalWindows . create ()) . trigger ( CountTrigger . of ( 5 )) KeyStream can help to run in parallel, each window will have the same key. Time is central to the stream processing, and the time is a parameter of the flow / environment and can take different meanings: ProcessingTime = system time of the machine executing the task: best performance and low latency EventTime = the time at the source level, embedded in the record. Deliver consistent and deterministic results regardless of order IngestionTime = time when getting into Flink. See example TumblingWindowOnSale.java and to test it, do the following: # Start the SaleDataServer that starts a server on socket 9181 and will read the avg.txt file and send each line to the socket java -cp target/my-flink-1.0.0-SNAPSHOT.jar jbcodeforce.sale.SaleDataServer # inside the job manager container start with ` flink run -d -c jbcodeforce.windows.TumblingWindowOnSale /home/my-flink/target/my-flink-1.0.0-SNAPSHOT.jar ` . # The job creates the data/profitPerMonthWindowed.txt file with accumulated sale and number of record in a 2 seconds tumbling time window ( June,Bat,Category5,154,6 ) ( August,PC,Category5,74,2 ) ( July,Television,Category1,50,1 ) ( June,Tablet,Category2,142,5 ) ( July,Steamer,Category5,123,6 ) ...","title":"Windowing"},{"location":"technology/flink/#trigger","text":"Trigger determines when a window is ready to be processed. All windows have default trigger. For example tumbling window has a 2s trigger. Global window has explicit trigger. We can implement our own triggers by implementing the Trigger interface with different methods to implement: onElement(..), onEventTime(...), onProcessingTime(...) Default triggers: EventTimeTrigger: fires based upon progress of event time ProcessingTimeTrigger: fires based upon progress of processing time CountTrigger: fires when # of element in a window > parameter PurgingTrigger","title":"Trigger"},{"location":"technology/flink/#eviction","text":"Evictor is used to remove elements from a window after the trigger fires and before or after the window function is applied. The logic to remove is app specific. The predefined evictors: CountEvictor, DeltaEvictor and TimeEvictor.","title":"Eviction"},{"location":"technology/flink/#watermark","text":"Watermark is the mechanism to keep how the event time has progressed: with windowing operator, event time stamp is used, but windows are defined on elapse time, for example, 10 minutes, so watermark helps to track te point of time where no more delayed events will arrive. The Flink API expects a WatermarkStrategy that contains both a TimestampAssigner and WatermarkGenerator. A TimestampAssigner is a simple function that extracts a field from an event. A number of common strategies are available out of the box as static methods on WatermarkStrategy, so reference to the documentation and examples. Watermark is crucial for out of order events, and when dealing with multi sources. Kafka topic partitions can be a challenge without watermark. With IoT device and network latency, it is possible to get an event with an earlier timestamp, while the operator has already processed such event timestamp from other sources. It is possible to configure to accept late events, with the allowed lateness time by which element can be late before being dropped. Flink keeps a state of Window until the allowed lateness time expires.","title":"Watermark"},{"location":"technology/flink/#resources","text":"Product documentation . Official training Base docker image is: https://hub.docker.com/_/flink Flink docker setup and the docker-compose files in this repo. FAQ Cloudera flink stateful tutorial : very good example for inventory transaction and queries on item considered as stream Building real-time dashboard applications with Apache Flink, Elasticsearch, and Kibana Udemy Apache Flink a real time hands-on: do not recommend this one !.","title":"Resources"},{"location":"technology/kafka-connect/","text":"Kafka connect is an open source component for easily integrate external systems with Kafka. It works with any Kafka product like IBM Event Streams, Strimzi, AMQ Streams, Confluent. It uses the concepts of source and sink connectors to ingest or deliver data to / from Kafka topics. The general concepts are detailed in the IBM Event streams product documentation , and Robin Moffatt's video . Here is a quick summary: Connector represents a logical job to move data from / to kafka to / from external systems. A lot of existing connectors , Apache Camel Kafka connectors can be reused, or you can implement your own . Workers are JVMs running the connectors. For production deployment workers run in cluster or \"distributed mode\", and leverage the Kafka consumer group management protocol to scale tasks horizontally. Tasks : each worker coordinates a set of tasks to copy data. In distributed mode, task states are saved in Kafka topics. They can be started, stopped at any time to support resilience, and scalable data pipeline. REST API to configure the connectors and monitors the tasks. The following figure illustrates a classical 'distributed' deployment of a Kafka Connect cluster. Workers are the running processes to execute connectors and tasks. Each Connector is responsible for defining and updating a set of Tasks that actually copy the data. Tasks are threads in a JVM. For fault tolerance and offset management, Kafka Connect uses Kafka topics (suffix name as -offsets, -config, -status ) to persist its states. When a connector is first submitted to the cluster, the workers rebalance the full set of connectors in the cluster with their tasks so that each worker has approximately the same amount of work. Connector and tasks are not guaranteed to run on the same instance in the cluster, especially if you have multiple tasks and multiple instances in your kafka connect cluster. The connector may be configured to add Converters (code used to translate data between Connect and the system sending or receiving data), and Transforms : simple logic to alter each message produced by or sent to a connector. Connector keeps state into three topics, which may be created when the connectors start are: connect-configs : This topic stores the connector and task configurations. connect-offsets : This topic stores offsets for Kafka Connect. connect-status : This topic stores status updates of connectors and tasks. Characteristics \u00b6 Copy vast quantity of data from source to kafka: work at the datasource level. So when the source is a database, it uses JDBC API for example. Support streaming and batch. Scale from standalone, mono connector approach to start small, to run in parallel on distributed cluster. Copy data, externalizing transformation in other framework. Kafka Connect defines three models: data model, worker model and connector model. Worker model allows Kafka Connect to scale the application. Kafka Connect cluster can serve multiple applications and so may be organized as a service. Connector cluster configuration \u00b6 The following configurations are important to review: group.id : one per connect cluster. It is ised by source connectors only. heartbeat.interval.ms : The expected time between heartbeats to the group coordinator when using Kafka\u2019s group management facilities. Fault tolerance \u00b6 When a worker fails: Tasks allocated in the failed worker are reallocated to existing workers, and the task's state, read offsets, source record mapping to offset are reloaded from the different topics. Both figure above are illustrating a MongoDB sink connector. MQ Source connector \u00b6 The source code is in this repo and uses JMS as protocol to integrate with MQ. When the connector encounters a message that it cannot process, it stops rather than throwing the message away. The MQ source connector does not currently make much use of message keys. It is possible to use CorrelationID as a key by defining MQ source mq.record.builder.key.header property: key.converter: org.apache.kafka.connect.storage.StringConverter value.converter: org.apache.kafka.connect.converters.ByteArrayConverter mq.record.builder: com.ibm.eventstreams.connect.mqsource.builders.DefaultRecordBuilder mq.connection.mode: client mq.message.body.jms: true mq.record.builder.key.header: JMSCorrelationID The record builder helps to transform the input message to a kafka record, using or not a schema. Always keep the coherence between body.jms, record builder and data converter. The MQ source task starts a unique JMS Reader that will read n messages from the queue. The `poll() function returns the list of MQ source records, and will commit to JMS if the number of message read match the batch size or if there is no more records. Once the Kafka Producer gets acknowledge that records are received by Brokers then use callback on the source task to commit MQ transaction for example. Any producer configuration can be modified in the source connector configuration: producer.override.acks : 1 Installation \u00b6 The Kafka connect framework fits well into a kubernetes deployment. In 2021 we have different options for that deployment: the Strimzi Kafka connect operator , IBM Event Streams Connector , Red Hat AMQ Streams (2021.Q3) connector or one of the Confluent connector . IBM Event Streams Cloud Pak for Integration \u00b6 If you are using IBM Event Streams 2021.x on Cloud Pak for Integration, the connectors setup is part of the user admin console toolbox: Deploying connectors against an IBM Event Streams cluster, you need to have a Kafka user with Manager role, to be able to create topic, produce and consume messages for all topics. As an extendable framework, Kafka Connect, can have new connector plugins. To deploy new connector, you need to use the kafka docker image which needs to be updated with the connector jars and redeployed to kubernetes cluster or to other environment. With IBM Event Streams on Openshift, the toolbox includes a kafka connect environment packaging, that defines a Dockerfile and configuration files to build your own image with the connectors jar files you need. The configuration files defines the properties to connect to Event Streams kafka brokers using API keys and SASL. The following public IBM messaging github account includes supported, open sourced, connectors (search for connector ). Here is the list of supported connectors for IBM Event Streams. Event Stream Kafka connector use custom resource definition defined by Strimzi. So configuration for Strimzi works for Event Streams. Normally you define one Kafka connect cluster, with a custom docker image which has all the necessary jars file for any connector you want to use. Then you configure each connector so they can start processing events or producing events. A Kafka connect cluster is identified with a group.id and then it saves its states in topics. The example below are for the configuration in cluster, also named distributed. config : group.id : connect-cluster offset.storage.topic : connect-cluster-offsets config.storage.topic : connect-cluster-configs status.storage.topic : connect-cluster-status The real-time inventory gitops repository includes a MQ Source connector to push message to Kafka. It uses ArgoCD to maintain states of Kafka Cluster, topics, users, and Kafka connector. Once the connector pods are running we need to start the connector tasks. Strimzi \u00b6 KafkaConnector resources allow you to create and manage connector instances for Kafka Connect in a Kubernetes-native way. To manage connectors, you can use the Kafka Connect REST API, or use KafkaConnector custom resources. In case of GitOps methodology we will define connector cluster and connector instance as yamls. Connector configuration is passed to Kafka Connect as part of an HTTP request and stored within Kafka itself. Further Readings \u00b6 Apache Kafka connect documentation Confluent Connector Documentation IBM Event Streams Connectors List of supported connectors by Event Streams MongoDB Connector for Apache Kafka","title":"Kafka Connect"},{"location":"technology/kafka-connect/#characteristics","text":"Copy vast quantity of data from source to kafka: work at the datasource level. So when the source is a database, it uses JDBC API for example. Support streaming and batch. Scale from standalone, mono connector approach to start small, to run in parallel on distributed cluster. Copy data, externalizing transformation in other framework. Kafka Connect defines three models: data model, worker model and connector model. Worker model allows Kafka Connect to scale the application. Kafka Connect cluster can serve multiple applications and so may be organized as a service.","title":"Characteristics"},{"location":"technology/kafka-connect/#connector-cluster-configuration","text":"The following configurations are important to review: group.id : one per connect cluster. It is ised by source connectors only. heartbeat.interval.ms : The expected time between heartbeats to the group coordinator when using Kafka\u2019s group management facilities.","title":"Connector cluster configuration"},{"location":"technology/kafka-connect/#fault-tolerance","text":"When a worker fails: Tasks allocated in the failed worker are reallocated to existing workers, and the task's state, read offsets, source record mapping to offset are reloaded from the different topics. Both figure above are illustrating a MongoDB sink connector.","title":"Fault tolerance"},{"location":"technology/kafka-connect/#mq-source-connector","text":"The source code is in this repo and uses JMS as protocol to integrate with MQ. When the connector encounters a message that it cannot process, it stops rather than throwing the message away. The MQ source connector does not currently make much use of message keys. It is possible to use CorrelationID as a key by defining MQ source mq.record.builder.key.header property: key.converter: org.apache.kafka.connect.storage.StringConverter value.converter: org.apache.kafka.connect.converters.ByteArrayConverter mq.record.builder: com.ibm.eventstreams.connect.mqsource.builders.DefaultRecordBuilder mq.connection.mode: client mq.message.body.jms: true mq.record.builder.key.header: JMSCorrelationID The record builder helps to transform the input message to a kafka record, using or not a schema. Always keep the coherence between body.jms, record builder and data converter. The MQ source task starts a unique JMS Reader that will read n messages from the queue. The `poll() function returns the list of MQ source records, and will commit to JMS if the number of message read match the batch size or if there is no more records. Once the Kafka Producer gets acknowledge that records are received by Brokers then use callback on the source task to commit MQ transaction for example. Any producer configuration can be modified in the source connector configuration: producer.override.acks : 1","title":"MQ Source connector"},{"location":"technology/kafka-connect/#installation","text":"The Kafka connect framework fits well into a kubernetes deployment. In 2021 we have different options for that deployment: the Strimzi Kafka connect operator , IBM Event Streams Connector , Red Hat AMQ Streams (2021.Q3) connector or one of the Confluent connector .","title":"Installation"},{"location":"technology/kafka-connect/#ibm-event-streams-cloud-pak-for-integration","text":"If you are using IBM Event Streams 2021.x on Cloud Pak for Integration, the connectors setup is part of the user admin console toolbox: Deploying connectors against an IBM Event Streams cluster, you need to have a Kafka user with Manager role, to be able to create topic, produce and consume messages for all topics. As an extendable framework, Kafka Connect, can have new connector plugins. To deploy new connector, you need to use the kafka docker image which needs to be updated with the connector jars and redeployed to kubernetes cluster or to other environment. With IBM Event Streams on Openshift, the toolbox includes a kafka connect environment packaging, that defines a Dockerfile and configuration files to build your own image with the connectors jar files you need. The configuration files defines the properties to connect to Event Streams kafka brokers using API keys and SASL. The following public IBM messaging github account includes supported, open sourced, connectors (search for connector ). Here is the list of supported connectors for IBM Event Streams. Event Stream Kafka connector use custom resource definition defined by Strimzi. So configuration for Strimzi works for Event Streams. Normally you define one Kafka connect cluster, with a custom docker image which has all the necessary jars file for any connector you want to use. Then you configure each connector so they can start processing events or producing events. A Kafka connect cluster is identified with a group.id and then it saves its states in topics. The example below are for the configuration in cluster, also named distributed. config : group.id : connect-cluster offset.storage.topic : connect-cluster-offsets config.storage.topic : connect-cluster-configs status.storage.topic : connect-cluster-status The real-time inventory gitops repository includes a MQ Source connector to push message to Kafka. It uses ArgoCD to maintain states of Kafka Cluster, topics, users, and Kafka connector. Once the connector pods are running we need to start the connector tasks.","title":"IBM Event Streams Cloud Pak for Integration"},{"location":"technology/kafka-connect/#strimzi","text":"KafkaConnector resources allow you to create and manage connector instances for Kafka Connect in a Kubernetes-native way. To manage connectors, you can use the Kafka Connect REST API, or use KafkaConnector custom resources. In case of GitOps methodology we will define connector cluster and connector instance as yamls. Connector configuration is passed to Kafka Connect as part of an HTTP request and stored within Kafka itself.","title":"Strimzi"},{"location":"technology/kafka-connect/#further-readings","text":"Apache Kafka connect documentation Confluent Connector Documentation IBM Event Streams Connectors List of supported connectors by Event Streams MongoDB Connector for Apache Kafka","title":"Further Readings"},{"location":"technology/kafka-consumers/","text":"Info Updated 05/05/2022 Understanding Kafka Consumers \u00b6 This chapter includes some technology summary and best practices about Kafka consumer. It may be useful for beginner or seasoned developers who want a refresh after some time far away from Kafka... Consumer group \u00b6 Consumers belong to consumer groups . You specify the group name as part of the consumer connection parameters using the group.id configuration: properties . put ( ConsumerConfig . GROUP_ID_CONFIG , groupid ); Consumer groups are grouping consumers to cooperate to consume messages from one or more topics. Consumers can run in separate hosts and separate processes. The figure below represents 2 consumer apps belonging to the same consumer group. Consumer 1 is getting data from 2 partitions, while consumer 2 is getting from one partition. When a consumer is unique in a group, it will get data from all partitions. There is always at least one consumer per partition. One broker is responsible to be the consumer group coordinator which is responsible for assigning partitions to the consumers in the group. The first consumer to join the group will be the group leader. It will get the list of consumers and it is responsible for assigning a subset of partitions to each consumer. Membership in a consumer group is maintained dynamically. Consumers send hearbeats to the group coordinator broker (see configuration like heartbeat.interval.ms ) and session.timeout.ms . Partition assignement is done by different strategies from range, round robin, sticky and cooperative sticky (See partition assignement strategy ). When a consumer fails, the partitions assigned to it will be reassigned to an other consumer in the same group. When a new consumer joins the group, partitions will be moved from existing consumers to the new one. Group rebalancing is also used when new partitions are added to one of the subscribed topics. The group will automatically detect the new partitions through periodic metadata refreshes and assign them to members of the group. During a rebalance, depending of the strategy, consumers may not consume messages (Need Kafka 2.4+ to get cooperative balancing feature). Kafka automatically detects failed consumers so that it can reassign partitions to working consumers. The consumer can take time to process records, so to avoid the consumer group controler removing consumer taking too long, it is possible to set the max.poll.interval.ms consumer property. If poll() is not called before expiration of this timeout, then the consumer is considered failed and the group will rebalance in order to reassign the partitions to another member. The second mechanism is the heartbeat consumers send to the group cordinator to show they are alive. The session.timeout.ms specifies the max value to consider before removing a non responding consumer. Implementing a Topic consumer is using the kafka KafkaConsumer class which the API documentation is a must read. It is interesting to note that: To support the same semantic of a queue processing like other integration messaging systems, you need to have all the consumers assigned to a single consumer group, so that each record delivery would be balanced over the group like with a queue. To support pub/sub like other messaging systems, each consumer would have its own consumer group, and subscribes to all the records published to the topic. With client.rack setting a consumer can consume from a local replica, which will have better latency when using a stretched cluster or multiple availability zones. For a single thread consumer, the implementation code follow the following pattern: prepare the consumer properties create an instance of KafkaConsumer to subscribe to at least one topic loop on polling events: the consumer ensures its liveness with the broker via the poll API. It will get n records per poll. process the ConsumerRecords and commit the offset by code or by using the autocommit attribute of the consumer As long as the consumer continues to call poll(), it will stay in the group and continues to receive messages from the partitions it was assigned to. When the consumer does not send heartbeats for a duration of session.timeout.ms , then it is considered unresponsive and its partitions will be reassigned. Examples of Java consumers can be found in the order management microservice project under the order-command-ms folder. We are proposing a deep dive study on this manual offset commit in this consumer code . Example of Javascript implementation is in this repository/folder But the complexity comes from the offset management and multithreading needs. So the following important considerations need to be addressed while implementing a consumer. Assess number of consumers needed \u00b6 The KafkaConsumer is not thread safe so it is recommended to run it in a unique thread. If really, needed you can implement a multi-threads solution, but as each thread will open a TCP connection to the Kafka brokers, be sure to close the connection to avoid memory leak. The alternate is to start n processes (JVM process) with a mono thread. If you need multiple consumers running in parallel to scale horizontally, you have to define multiple partitions while configuring the topic and use fine-grained control over offset persistence. The consumer-per-partition pattern maximizes throughput. When consumers run in parallel and you use multiple threads per consumer you need to be sure the total number of threads across all instances do not exceed the total number of partitions in the topic. Also, a consumer can subscribe to multiple topics. The brokers are doing rebalancing of the assignment of topic-partition to a consumer that belong to a group. Offset management \u00b6 Recall that offset is just a numeric identifier of a consumer position of the last record read within a partition. Consumers periodically need to commit the offsets they have received, to present a recovery point in case of failure. To commit offset (via API or automatically) the consumer sends a message to kafka broker to the special topic named __consumer_offsets to keep the committed offset for each partition. (When there is a committed offset, the auto.offset.reset property is not used) Consumers do a read commit for the last processed record: When a consumer starts, it receives a partition to consume, and it starts at its group's committed offset or the latest or earliest offset as specified in the auto.offset.reset property. If a consumer fails after processing a message but before committing its offset, the committed offset information will not reflect the processing of the message. This means that the message will be processed again by the next consumer, in that group, to be assigned the partition. In the case where consumers are set to auto commit, it means the offset if committed at the poll() function and if the service crashed while processing of this record as: then the record (partition 0 - offset 4) will never be processed. But it is not lost. As shown in the figure below, in case of consumer failure, it is possible to get duplicates. When the last message processed by the consumer, before crashing, is younger than the last committed offset, the consumer will get this record again. This case may happen when using a time based commit strategy. Source: Kafka definitive guide book from Todd Palino, Gwen Shapira In the opposite, if the last committed offset is after the last processed messages and there were multiple messages returned in the poll, then those messages may be lost (in term of consumer processing not lost in kafka). This will happen with autocommit set up at the time of the read operation, and the last offset of the poll is the committed offset. See the enable.auto.commit property. Limiting to poll one message at the time, will help to avoid this problem, but will impact throughput. It is possible to commit by calling API so developer can control when to commit the read. For manual commit, we can use one of the two approaches: offsets\u2014synchronous commit: send the offset number for the records read using consumer.commitSync(Collections.singletonMap(partition, new OffsetAndMetadata(lastOffset + 1))) method asynchronous As soon as you are coding manual commit, it is strongly recommended to implement the ConsumerRebalanceListener interface to be able to do state modifications when the topic is rebalanced. Assess if it is acceptable to loose messages from topic. If so, when a consumer restarts, it will start consuming the topic from the latest committed offset within the partition allocated to itself. As storing a message to an external system and storing the offsets are two separate operations, and in case of failure between them, it is possible to have stale offsets, which will introduce duplicate messages when consumers restart to process from last known committed offset. In this case, consumer's idempotence is needed to support updating the same row in the table, or use the event timestamp as update timestamp in the database record or use other clever solution. As presented in the producer coding practice, using transaction to support \"exactly-once\", also means the consumers should read committed data only. This can be achieved by setting the isolation.level=read_committed in the consumer's configuration. The last offset will be the first message in the partition belonging to an open not yet committed transaction. This offset is known as the 'Last Stable Offset'(LSO). Producer transaction \u00b6 When consuming from a Kafka topic and producing to another topic, like in Kafka Streams programming approach, we can use the producer's transaction feature to send the committed offset message and the new records in the second topic in the same transaction. This can be seen as a consume-transform-produce loop pattern so that every input event is processed exactly once. An example of such pattern in done in the order management microservice - command part . Consumer lag \u00b6 The consumer lag for a partition is the difference between the offset of the most recently published message and the consumer's committed offset. If the lag starts to grow, it means the consumer is not able to keep up with the producer's pace. The risk, is that slow consumer may fall behind, and when partition management may remove old log segments, leading the consumer to jump forward to continnue on the next log segment. Consumer may have lost messages. You can use the kafka-consumer-groups tool to see and manage the consumer lag. You can use the kafka-consumer-groups tool to see the consumer lag, or use the Event Streams User Interface: The group can be extended to see how each consumer, within the group, performs on a multi partitions topic: Reset a group \u00b6 Sometime it is needed to reprocess the messages. The easiest way is to change the groupid of the consumers to get an implicit offsets reset, but it is also possible to reset for some topic to the earliest offset: kafka-consumer-groups \\ --bootstrap-server kafkahost:9092 \\ --group ordercmd-command-consumer-grp \\ --reset-offsets \\ --all-topics \\ --to-earliest \\ --execute Kafka useful Consumer APIs \u00b6 KafkaConsumer a topic consumer which support: transparently handles brokers failure transparently adapt to partition migration within the cluster support grouping for load balancing among consumers maintains TCP connections to the necessary brokers to fetch data subscribe to multiple topics and being part of consumer groups each partition is assigned to exactly one consumer in the group if a process fails, the partitions assigned to it will be reassigned to other consumers in the same group ConsumerRecords holds the list ConsumerRecord per partition for a particular topic. ConsumerRecord A key/value pair to be received from Kafka. This also consists of a topic name and a partition number from which the record is being received, an offset that points to the record in a Kafka partition, and a timestamp Repositories with consumer code \u00b6 Within the Reefer ontainer shipment solution we have a order events consumer: order event agent Quarkus app with Kafka streams Nodejs kafka consumers and producers A lot of python consumer codes in the integration tests, with or without Avro schema References \u00b6 IBM Event Streams - Consuming messages KafkaConsumer class","title":"Kafka Consumers"},{"location":"technology/kafka-consumers/#understanding-kafka-consumers","text":"This chapter includes some technology summary and best practices about Kafka consumer. It may be useful for beginner or seasoned developers who want a refresh after some time far away from Kafka...","title":"Understanding Kafka Consumers"},{"location":"technology/kafka-consumers/#consumer-group","text":"Consumers belong to consumer groups . You specify the group name as part of the consumer connection parameters using the group.id configuration: properties . put ( ConsumerConfig . GROUP_ID_CONFIG , groupid ); Consumer groups are grouping consumers to cooperate to consume messages from one or more topics. Consumers can run in separate hosts and separate processes. The figure below represents 2 consumer apps belonging to the same consumer group. Consumer 1 is getting data from 2 partitions, while consumer 2 is getting from one partition. When a consumer is unique in a group, it will get data from all partitions. There is always at least one consumer per partition. One broker is responsible to be the consumer group coordinator which is responsible for assigning partitions to the consumers in the group. The first consumer to join the group will be the group leader. It will get the list of consumers and it is responsible for assigning a subset of partitions to each consumer. Membership in a consumer group is maintained dynamically. Consumers send hearbeats to the group coordinator broker (see configuration like heartbeat.interval.ms ) and session.timeout.ms . Partition assignement is done by different strategies from range, round robin, sticky and cooperative sticky (See partition assignement strategy ). When a consumer fails, the partitions assigned to it will be reassigned to an other consumer in the same group. When a new consumer joins the group, partitions will be moved from existing consumers to the new one. Group rebalancing is also used when new partitions are added to one of the subscribed topics. The group will automatically detect the new partitions through periodic metadata refreshes and assign them to members of the group. During a rebalance, depending of the strategy, consumers may not consume messages (Need Kafka 2.4+ to get cooperative balancing feature). Kafka automatically detects failed consumers so that it can reassign partitions to working consumers. The consumer can take time to process records, so to avoid the consumer group controler removing consumer taking too long, it is possible to set the max.poll.interval.ms consumer property. If poll() is not called before expiration of this timeout, then the consumer is considered failed and the group will rebalance in order to reassign the partitions to another member. The second mechanism is the heartbeat consumers send to the group cordinator to show they are alive. The session.timeout.ms specifies the max value to consider before removing a non responding consumer. Implementing a Topic consumer is using the kafka KafkaConsumer class which the API documentation is a must read. It is interesting to note that: To support the same semantic of a queue processing like other integration messaging systems, you need to have all the consumers assigned to a single consumer group, so that each record delivery would be balanced over the group like with a queue. To support pub/sub like other messaging systems, each consumer would have its own consumer group, and subscribes to all the records published to the topic. With client.rack setting a consumer can consume from a local replica, which will have better latency when using a stretched cluster or multiple availability zones. For a single thread consumer, the implementation code follow the following pattern: prepare the consumer properties create an instance of KafkaConsumer to subscribe to at least one topic loop on polling events: the consumer ensures its liveness with the broker via the poll API. It will get n records per poll. process the ConsumerRecords and commit the offset by code or by using the autocommit attribute of the consumer As long as the consumer continues to call poll(), it will stay in the group and continues to receive messages from the partitions it was assigned to. When the consumer does not send heartbeats for a duration of session.timeout.ms , then it is considered unresponsive and its partitions will be reassigned. Examples of Java consumers can be found in the order management microservice project under the order-command-ms folder. We are proposing a deep dive study on this manual offset commit in this consumer code . Example of Javascript implementation is in this repository/folder But the complexity comes from the offset management and multithreading needs. So the following important considerations need to be addressed while implementing a consumer.","title":"Consumer group"},{"location":"technology/kafka-consumers/#assess-number-of-consumers-needed","text":"The KafkaConsumer is not thread safe so it is recommended to run it in a unique thread. If really, needed you can implement a multi-threads solution, but as each thread will open a TCP connection to the Kafka brokers, be sure to close the connection to avoid memory leak. The alternate is to start n processes (JVM process) with a mono thread. If you need multiple consumers running in parallel to scale horizontally, you have to define multiple partitions while configuring the topic and use fine-grained control over offset persistence. The consumer-per-partition pattern maximizes throughput. When consumers run in parallel and you use multiple threads per consumer you need to be sure the total number of threads across all instances do not exceed the total number of partitions in the topic. Also, a consumer can subscribe to multiple topics. The brokers are doing rebalancing of the assignment of topic-partition to a consumer that belong to a group.","title":"Assess number of consumers needed"},{"location":"technology/kafka-consumers/#offset-management","text":"Recall that offset is just a numeric identifier of a consumer position of the last record read within a partition. Consumers periodically need to commit the offsets they have received, to present a recovery point in case of failure. To commit offset (via API or automatically) the consumer sends a message to kafka broker to the special topic named __consumer_offsets to keep the committed offset for each partition. (When there is a committed offset, the auto.offset.reset property is not used) Consumers do a read commit for the last processed record: When a consumer starts, it receives a partition to consume, and it starts at its group's committed offset or the latest or earliest offset as specified in the auto.offset.reset property. If a consumer fails after processing a message but before committing its offset, the committed offset information will not reflect the processing of the message. This means that the message will be processed again by the next consumer, in that group, to be assigned the partition. In the case where consumers are set to auto commit, it means the offset if committed at the poll() function and if the service crashed while processing of this record as: then the record (partition 0 - offset 4) will never be processed. But it is not lost. As shown in the figure below, in case of consumer failure, it is possible to get duplicates. When the last message processed by the consumer, before crashing, is younger than the last committed offset, the consumer will get this record again. This case may happen when using a time based commit strategy. Source: Kafka definitive guide book from Todd Palino, Gwen Shapira In the opposite, if the last committed offset is after the last processed messages and there were multiple messages returned in the poll, then those messages may be lost (in term of consumer processing not lost in kafka). This will happen with autocommit set up at the time of the read operation, and the last offset of the poll is the committed offset. See the enable.auto.commit property. Limiting to poll one message at the time, will help to avoid this problem, but will impact throughput. It is possible to commit by calling API so developer can control when to commit the read. For manual commit, we can use one of the two approaches: offsets\u2014synchronous commit: send the offset number for the records read using consumer.commitSync(Collections.singletonMap(partition, new OffsetAndMetadata(lastOffset + 1))) method asynchronous As soon as you are coding manual commit, it is strongly recommended to implement the ConsumerRebalanceListener interface to be able to do state modifications when the topic is rebalanced. Assess if it is acceptable to loose messages from topic. If so, when a consumer restarts, it will start consuming the topic from the latest committed offset within the partition allocated to itself. As storing a message to an external system and storing the offsets are two separate operations, and in case of failure between them, it is possible to have stale offsets, which will introduce duplicate messages when consumers restart to process from last known committed offset. In this case, consumer's idempotence is needed to support updating the same row in the table, or use the event timestamp as update timestamp in the database record or use other clever solution. As presented in the producer coding practice, using transaction to support \"exactly-once\", also means the consumers should read committed data only. This can be achieved by setting the isolation.level=read_committed in the consumer's configuration. The last offset will be the first message in the partition belonging to an open not yet committed transaction. This offset is known as the 'Last Stable Offset'(LSO).","title":"Offset management"},{"location":"technology/kafka-consumers/#producer-transaction","text":"When consuming from a Kafka topic and producing to another topic, like in Kafka Streams programming approach, we can use the producer's transaction feature to send the committed offset message and the new records in the second topic in the same transaction. This can be seen as a consume-transform-produce loop pattern so that every input event is processed exactly once. An example of such pattern in done in the order management microservice - command part .","title":"Producer transaction"},{"location":"technology/kafka-consumers/#consumer-lag","text":"The consumer lag for a partition is the difference between the offset of the most recently published message and the consumer's committed offset. If the lag starts to grow, it means the consumer is not able to keep up with the producer's pace. The risk, is that slow consumer may fall behind, and when partition management may remove old log segments, leading the consumer to jump forward to continnue on the next log segment. Consumer may have lost messages. You can use the kafka-consumer-groups tool to see and manage the consumer lag. You can use the kafka-consumer-groups tool to see the consumer lag, or use the Event Streams User Interface: The group can be extended to see how each consumer, within the group, performs on a multi partitions topic:","title":"Consumer lag"},{"location":"technology/kafka-consumers/#reset-a-group","text":"Sometime it is needed to reprocess the messages. The easiest way is to change the groupid of the consumers to get an implicit offsets reset, but it is also possible to reset for some topic to the earliest offset: kafka-consumer-groups \\ --bootstrap-server kafkahost:9092 \\ --group ordercmd-command-consumer-grp \\ --reset-offsets \\ --all-topics \\ --to-earliest \\ --execute","title":"Reset a group"},{"location":"technology/kafka-consumers/#kafka-useful-consumer-apis","text":"KafkaConsumer a topic consumer which support: transparently handles brokers failure transparently adapt to partition migration within the cluster support grouping for load balancing among consumers maintains TCP connections to the necessary brokers to fetch data subscribe to multiple topics and being part of consumer groups each partition is assigned to exactly one consumer in the group if a process fails, the partitions assigned to it will be reassigned to other consumers in the same group ConsumerRecords holds the list ConsumerRecord per partition for a particular topic. ConsumerRecord A key/value pair to be received from Kafka. This also consists of a topic name and a partition number from which the record is being received, an offset that points to the record in a Kafka partition, and a timestamp","title":"Kafka useful Consumer APIs"},{"location":"technology/kafka-consumers/#repositories-with-consumer-code","text":"Within the Reefer ontainer shipment solution we have a order events consumer: order event agent Quarkus app with Kafka streams Nodejs kafka consumers and producers A lot of python consumer codes in the integration tests, with or without Avro schema","title":"Repositories with consumer code"},{"location":"technology/kafka-consumers/#references","text":"IBM Event Streams - Consuming messages KafkaConsumer class","title":"References"},{"location":"technology/kafka-mirrormaker/","text":"This section introduces Mirror Maker 2.0 , the new replication feature of Kafka 2.4, and how it can be used, along with best practices, for data replication between two Kafka clusters. Mirror Maker 2.0 was defined as part of the Kafka Improvement Process - KIP 382 and can be used for disaster recovery (active / passive) or for more complex topology with 3 data centers to support always on. Overview \u00b6 We recommend to start by reading the IBM Event Streams product documentation on geo-replication to understand the main concepts. Some core principles we use in this article: We have two data centers in different region and each region has multiple availability zones OpenShift clusters are defined in both region and spread among the three data center. For a better view of the golden topology for OpenShift see this diagram with master, worker nodes deployment. At least two Event Streams / Kafka clusters are defined, one as source in the active region and one target for disaster recovery or passive region. Source cluster has producer and consumer applications deployed in the same OpenShift Cluster or deployed on VMs and accessing the Kafka brokers via network load balancer. Producer, consumer or streaming applications deployed within OpenShift use the bootstrap URL to kafka broker via internal service definition. Something like es-prod-kafka-bootstrap.ibm-eventstreams.svc . With such configuration their setting will be the same on the target cluster. The target cluster has the mirror maker cluster which is based on the Kafka connect framework. The following diagram illustrates those principles: When zooming into what need to be replicated, we can see source topics from the blue cluster to target topics on the green cluster. This configuration is for disaster recovery, with a active - passive model, where only the left side has active applications producing and consuming records from Kafka Topics. As the mirroring is over longer internet distance, then expect some latency in the data mirroring. We can extend this deployment by using Mirror Maker 2 to replicate data over multiple clusters with a more active - active deployment which the following diagram illustrates the concepts for an \"always-on\" deployment: This model can also being used between cloud providers. In active - active mode the clusters get data injected in local cluster and replicated data injected from remote cluster. The topic names are prefixed with the original cluster name. In the figure below, the cluster on the right has green local producers and consumers, topics are replicated to the left, the blue cluster. Same for blue topic from the left to the right. Consumers on both sides are getting data from the 'order' topics (local and replicated) to get a complete view of all the orders created on both sides. The following diagram zooms into a classical Web based solution design where mobile or web apps are going to a web tier to serve single page application, static content, and APIs. Then a set of microservices implement the business logic, some of those services are event-driven, so they produce and consumer events from topics. When active-active replication is in place it, means the same topology is deployed in another data center and data from the same topic (business entity) arrive to the replicated topic. The service can save the record in its own database and cache. (The service Tier is not detailed with the expected replicas, neither the application load balancer displays routes to other data center) If there is a failure on one of the side of the data replication, the data are transparently available. A read model query will return the good result on both side. In replication, data in topic, topic states and metadata are replicated. IBM Event Streams release 10.0 is supporting Mirror Maker 2 as part of the geo-replication feature . Mirror Maker 2 components \u00b6 Mirror maker 2.0 is the solution to replicate data in topics from one Kafka cluster to another. It uses the Kafka Connect framework to simplify configuration, parallel execution and horizontal scaling. The figure below illustrates the MirrorMaker 2.0 internal components running within Kafka Connect. MirrorMaker 2 uses the cluster name or identifier as prefix for topic, and uses the concept of source topic and target topic. It runs in standalone mode, which can be used for development and test purpose, or in distributed mode (cluster) for production deployment. With distribution mode, MirrorMaker 2.0 creates the following topics on the cluster it is connected to (See later the property connectCluster ): ...-configs.source.internal : This topic is used to store the connector and task configuration. ...-offsets.source.internal : This topic is used to store offsets for Kafka Connect. ...-status.source.internal : This topic is used to store status updates of connectors and tasks. source.heartbeats : to check that the remote cluster is available and the clusters are connected **source.checkpoints.internal*: MirrorCheckpointConnector tracks and maps offsets for specified consumer groups using an offset sync topic and checkpoint topic. A typical MirrorMaker 2.0 configuration is done via a property file and defines the replication source and target clusters with their connection properties and the replication flow definition. Here is a simple example for a local cluster replicating to a remote IBM Event Streams cluster using TLS v1.2 for connection encryption and SASL authentication protocol. The IBM Event Streams instance runs on the Cloud. clusters = source, target source.bootstrap.servers = ${KAFKA_SOURCE_BROKERS} target.bootstrap.servers = ${KAFKA_TARGET_BROKERS} target.security.protocol = SASL_SSL target.ssl.protocol = TLSv1.2 target.ssl.endpoint.identification.algorithm = https target.sasl.mechanism = PLAIN target.sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=${KAFKA_TARGET_APIKEY}; # enable and configure individual replication flows source->target.enabled = true source->target.topics = products tasks.max = 10 Topics to be replicated are configured via a whitelist that may include regular expression for pattern matching on the topic name. So if you use naming convention for your topic, you could do fine grained selection of the replicated topic. It is possible to specify topics you do not want to replicate via the blacklist property. White listed topics are set with the source->target.topics attribute of the replication flow and uses Java regular expression syntax. The default blacklisted topics are Kafka internal topic: blacklist = [follower\\.replication\\.throttled\\.replicas, leader\\.replication\\.throttled\\.replicas, message\\.timestamp\\.difference\\.max\\.ms, message\\.timestamp\\.type, unclean\\.leader\\.election\\.enable, min\\.insync\\.replicas] We can also define the blacklist with the properties: topics.blacklist . Comma-separated lists and Java Regular Expressions are supported. Internally, MirrorSourceConnector and MirrorCheckpointConnector will create multiple Kafka tasks (up to the value of tasks.max property), and MirrorHeartbeatConnector creates an additional task. MirrorSourceConnector will have one task per topic-partition combination to replicate, while MirrorCheckpointConnector will have one task per consumer group. The Kafka Connect framework uses the coordinator API, with the assign() API, so there is no consumer group used while fetching data from source topics. There is no call to commit() either; rebalancing occurs only when there is a new topic created that matches the whitelist pattern. Mirror Maker 2 can run on VM, bare metal or within containers deployed on kubernetes cluster. Why replicating? \u00b6 The classical needs for replication between clusters can be listed as: Disaster recovery when one secondary cluster is passive while the producer and consumers are on the active cluster in the primary data center: The following article goes over those principals. Active-active cluster mirroring for inter services communication: consumers and producers are on both sides and consume or produce to their local cluster. Moving data to a read only cluster as a front door to data lake, or to do cross data centers aggregation on the different event streams: Fan-in to get holistic data view. GDPR compliance to isolate data in country and geography Hybrid cloud operations to share data between on-premise cluster and managed service clusters. Deployment examples \u00b6 We encourage you to go over our Mirror maker 2 labs which addresses different replication scenarios. The Connect column defines where the Mirror Maker 2 runs. Scenario Source Target Connect Lab Getting Started Lab 1 Event Streams on Cloud Local Kafka Local on localhost Kafka Mirror Maker 2 - Lab 1 Lab 2 Using Mirror Maker 2 from Event Streams on premise to Event stream on cloud On OCP OCP Kafka Mirror Maker 2 - Lab 2 Replication considerations \u00b6 Topic metadata replication \u00b6 It is possible to disable the topic metadata replication. We do not encourage to do so. Per design topic can be added dynamically, specially when developing with Kafka Streams where intermediate topics are created by the stream topology semantic, and topic configuration can be altered to increase the number of partitions. Changes to the source topic are dynamically propagated to the target avoiding maintenance nightmare. By synchronizing configuration properties, the need for rebalancing is reduced. When doing manual configuration, even if the initial topic configuration is duplicated, any dynamic changes to the topic properties are not going to be automatically propagated and the administrator needs to change the target topic. If the throughput on the source topic has increased and the number of partition was increased to support the load, then the target cluster will not have the same downstream capability which may lead to overloading (disk space or memory capacity). With a GitOps approach, this risk is mitigated as topic definition in the GitOps repository could be propagated to the target and source cluster mostly at the same time. Also if the consumer of a partition is expecting to process the events in order within the partition, then changing the number of partitions between source and target will make the ordering not valid any more. If the replication factor are set differently between the two clusters then the availability guaranty of the replicated data may be impacted and bad settings with broker failure will lead to data lost. Finally, it is important to consider that changes to topic configuration triggers a consumer rebalance which stalls the mirroring process and creates a backlog in the pipeline and increases the end to end latency observed by the downstream application. Naming convention \u00b6 Mirror maker 2 sets the prefix for the name of the replicated topic with the name of the source cluster. This is an important and simple solution to avoid infinite loop when doing bi-directional mirroring. At the consumer side the subscribe() function supports regular expression for topic name. So a code like: kafkaConsumer . subscribe ( \"^.*accounts\" ) will listen to all the topics in the cluster having cluster name prefixed topics and the local accounts topic. This could be useful when we want to aggregate data from different data centers / clusters. Offset management \u00b6 Mirror maker 2 tracks offset per consumer group. There are two topics created on the target cluster to manage the offset mapping between the source and target clusters and the checkpoints of the last committed offset in the source topic/partitions/consumer group. When a producer sends its records, it gets the offsets in the partition the records were created. In the diagram below we have a source topic A/partition 1 with the last write offset done by a producer to be 5, and the last read committed offset by the consumer assigned to partition 1 being 3. The last replicated offset 3 is mapped as downstream offset 12 in the target partition. Offset numbers do not match between replicated partitions. So if the blue consumer needs to reconnect to the green target cluster it will read from the last committed offset which is 12 in this environment. This information is saved in the checkpoint topic. Offset synch are emitted at the beginning of the replication and when there is a situation which leads that the numbering sequencing diverges. For example, the normal behavior is to increase the offset by one 2,3,4,5,6,7, which is mapped to 12,13,14,15,16,... on target cluster. If the write operation for offset 20 at the source is a 17 on the target then MM 2 emits a new offset synch records to the offset-synch topic. The checkpoint and offset_synch topics enable replication to be fully restored from the correct offset position on failover. On the following diagram, once the cluster source is down, the consumers on the target cluster are restarted, and they will start from the last committed offset of the source, which was offset 3 that is in fact offset 12 on target replicated topic. No record skipped. Record duplication \u00b6 Exactly-once delivery is difficult to achieve in distributed system. In the case of Kafka, producer, brokers, and consumers are working together to ensure only one message is processed end to end. With coding practice and configuration settings, within a unique cluster, Kafka can guarantee exactly once processing. No duplicated records between producer and broker, and committed reads, on consumer side, are not reprocessed in case of consumer restarts. But for cross cluster replications, the semantic is based on at least once approach. Duplicates can happen when the mirror maker source task stops before committing its offset to the source topic. A restart will load records from the last committed offset which can generate duplicates. The following diagram illustrate this case, record offset 26 on target topic is a duplicate of record 25. Also Mirror Maker 2 is a generic topic consumer, it will not participate to the \"read-committed\" process, if the topic includes duplicate messages it will propagate them to the target. In the future MM2 will be able to support exactly once by using the checkpoint topic on the target cluster to keep the state of the committed offset from the consumer side, and write with an atomic transaction between the target topic and the checkpoint topic, and commit the source read offset as part of the same transaction. MM2 topology \u00b6 In this section we want to address horizontal scalability and how to organize the MirrorMaker 2 topology for multi-tenancy. The simplest approach is to use one Mirror Maker instance per family of topics: the classification of family of topic can be anything, from line of business, to team, to application. Suppose an application is using 1000 topic - partitions, for data replication it may make sense to have one MM2 instance for this application. The configuration will define the groupId to match the application name for example. The following diagram illustrates this kind of topology by using regular expression on the topic white list selection, there are three Mirror Maker 2 instances mirroring the different topics with name starting with topic-name-A*, topic-name-B*, topic-name-C*, respectively. Each connect instance is a JVM workers that replicate the topic/partitions and has different group.id. For Bi-directional replication for the same topic name, Mirror Maker 2 will use the cluster name as prefix. With MM2, we do not need to have 2 MM2 clusters but only one and bidirectional definitions. The following example is showing the configuration for a MM2 bidirectional settings, with accounts topic to be replicated on both cluster: apiVersion: kafka.strimzi.io/v1alpha1 kind: KafkaMirrorMaker2 ... mirrors: - sourceCluster: \"event-streams-wdc\" targetCluster: \"kafka-on-premise\" ... topicsPattern: \"accounts,orders\" - sourceCluster: \"kafka-on-premise\" targetCluster: \"event-streams-wdc\" ... topicsPattern: \"accounts,customers\" Consumer coding \u00b6 We recommend to review the producer implementation best practices and the consumer considerations . Capacity planning \u00b6 For platform sizing, the main metric to assess, is the number of partitions to replicate. The number of partitions and number of brokers are somehow connected as getting a high number of partitions involves increasing the number of brokers. For Mirror Maker 2, as it is based on Kafka connect, there is a unique cluster and each partition mirroring is supported by a task within the JVM so the first constraint is the memory allocated to the container and the heap size. To address capacity planning, we need to review some characteristic of the Kafka Connect framework: For each topic/partition there will be a task running. We can see in the trace that tasks are mapped to threads inside the JVM. So the parallelism will be bound by the number of CPUs the JVM runs on. The parameters max.tasks specifies the max parallel processing we can have per JVM. So for each Topic we need to assess the number of partitions to be replicated. Each task is using the consumer API and is part of the same consumer group, the partition within a group are balanced by an internal controller. With Kafka connect any changes to the topic topology triggers a partition rebalancing. In MM2 each consumer / task is assigned a partition by the controller. So the rebalancing is done internally. Still adding a broker node into the cluster will generate rebalancing. The task processing is stateless, consume - produce wait for acknowledge, commit offset. In this case, the CPU and network performance are key. For platform tuning activity, we need to monitor operating system performance metrics. If the CPU becomes the bottleneck, we can allocate more CPU or start to scale horizontally by adding more Mirror Maker 2 instances. If the network at the server level is the bottleneck, then adding more servers will help. Kafka will automatically balance the load among all the tasks running on all the machines. The size of the message impacts also the throughput as with small message the throughput is CPU bounded. With 100 bytes messages or more we can observe network saturation. The parameters to consider for sizing are the following: Parameter Description Impact Number of topic/ partition Each task processes one partition For pure parallel processing max.tasks is set around the number of CPU Record size Size of the message in each partition in average Memory usage and Throughput: the # of records/s decrease when size increase, while MB/s throughput increases in logarithmic Expected input throughput The producer writing to the source topic throughput Be sure the consumers inside MM2 absorb the demand Network latency This is where positioning MM2 close to the target cluster may help improve latency Version migration \u00b6 Once the Mirror Maker cluster is up and running, it may be needed to update the underlying code when a new product version is released. Based on Kafka Connect distributed mode multiple workers JVM coordinate the topic / partition repartition among themselves. If a worker process dies, the cluster is rebalanced to distribute the work fairly over the remaining workers. If a new worker starts work, a rebalance ensures it takes over some work from the existing workers. Using the REST API it is possible to stop and restart a connector. As of now the recommendation is to start a new MirrorMaker instance with the new version and the same groupId as the existing workers you want to migrate. Then stop the existing version. As each MirrorMaker workers are part of the same group, the internal worker controller will coordinate with the other workers the 'consumer' task to partition assignment. We have presented a similar approach in this section , where we tested that each instance of MirrorMaker 2 could assume the replication. First we will stop the Node 1 instance, upgrade it to the latest version, then start it again. Then we\u2019ll repeat the same procedure on Node 2. We\u2019ll continue to watch the Consumer VM window to note that replication should not stop at any point. We\u2019ve now upgraded to Kafka 2.5 including the latest MirrorMaker 2. Meanwhile, replication was uninterrupted due to the second instance of MirrorMaker 2: Now we\u2019ll restart the Node 1 instance of MirrorMaker 2, stop the Node 2 instance, we can still see replication occurring on the upgraded Node 1 instance of MirrorMaker 2. We upgrade Node 2\u2019s instance of MirrorMaker 2 exactly as on Node 1, and start it again, and once again, replication is still going. When using Strimzi, if the update applies to the MM2 Custom Resource Definition, just reapplying the CRD should be enough. Be sure to verify the product documentation as new version may enforce to have new topics. It was the case when Kafka connect added the config topic in a recent version. Resources \u00b6 IBM Event Streams product documentation IBM Event Streams managed service - Disaster recovery example scenario Strimzi configuration for Mirror Maker 2 Getting started with Mirror Maker 2 - Tech Academy Using MirrorMaker2 from Dale Lane","title":"Mirror Maker 2"},{"location":"technology/kafka-mirrormaker/#overview","text":"We recommend to start by reading the IBM Event Streams product documentation on geo-replication to understand the main concepts. Some core principles we use in this article: We have two data centers in different region and each region has multiple availability zones OpenShift clusters are defined in both region and spread among the three data center. For a better view of the golden topology for OpenShift see this diagram with master, worker nodes deployment. At least two Event Streams / Kafka clusters are defined, one as source in the active region and one target for disaster recovery or passive region. Source cluster has producer and consumer applications deployed in the same OpenShift Cluster or deployed on VMs and accessing the Kafka brokers via network load balancer. Producer, consumer or streaming applications deployed within OpenShift use the bootstrap URL to kafka broker via internal service definition. Something like es-prod-kafka-bootstrap.ibm-eventstreams.svc . With such configuration their setting will be the same on the target cluster. The target cluster has the mirror maker cluster which is based on the Kafka connect framework. The following diagram illustrates those principles: When zooming into what need to be replicated, we can see source topics from the blue cluster to target topics on the green cluster. This configuration is for disaster recovery, with a active - passive model, where only the left side has active applications producing and consuming records from Kafka Topics. As the mirroring is over longer internet distance, then expect some latency in the data mirroring. We can extend this deployment by using Mirror Maker 2 to replicate data over multiple clusters with a more active - active deployment which the following diagram illustrates the concepts for an \"always-on\" deployment: This model can also being used between cloud providers. In active - active mode the clusters get data injected in local cluster and replicated data injected from remote cluster. The topic names are prefixed with the original cluster name. In the figure below, the cluster on the right has green local producers and consumers, topics are replicated to the left, the blue cluster. Same for blue topic from the left to the right. Consumers on both sides are getting data from the 'order' topics (local and replicated) to get a complete view of all the orders created on both sides. The following diagram zooms into a classical Web based solution design where mobile or web apps are going to a web tier to serve single page application, static content, and APIs. Then a set of microservices implement the business logic, some of those services are event-driven, so they produce and consumer events from topics. When active-active replication is in place it, means the same topology is deployed in another data center and data from the same topic (business entity) arrive to the replicated topic. The service can save the record in its own database and cache. (The service Tier is not detailed with the expected replicas, neither the application load balancer displays routes to other data center) If there is a failure on one of the side of the data replication, the data are transparently available. A read model query will return the good result on both side. In replication, data in topic, topic states and metadata are replicated. IBM Event Streams release 10.0 is supporting Mirror Maker 2 as part of the geo-replication feature .","title":"Overview"},{"location":"technology/kafka-mirrormaker/#mirror-maker-2-components","text":"Mirror maker 2.0 is the solution to replicate data in topics from one Kafka cluster to another. It uses the Kafka Connect framework to simplify configuration, parallel execution and horizontal scaling. The figure below illustrates the MirrorMaker 2.0 internal components running within Kafka Connect. MirrorMaker 2 uses the cluster name or identifier as prefix for topic, and uses the concept of source topic and target topic. It runs in standalone mode, which can be used for development and test purpose, or in distributed mode (cluster) for production deployment. With distribution mode, MirrorMaker 2.0 creates the following topics on the cluster it is connected to (See later the property connectCluster ): ...-configs.source.internal : This topic is used to store the connector and task configuration. ...-offsets.source.internal : This topic is used to store offsets for Kafka Connect. ...-status.source.internal : This topic is used to store status updates of connectors and tasks. source.heartbeats : to check that the remote cluster is available and the clusters are connected **source.checkpoints.internal*: MirrorCheckpointConnector tracks and maps offsets for specified consumer groups using an offset sync topic and checkpoint topic. A typical MirrorMaker 2.0 configuration is done via a property file and defines the replication source and target clusters with their connection properties and the replication flow definition. Here is a simple example for a local cluster replicating to a remote IBM Event Streams cluster using TLS v1.2 for connection encryption and SASL authentication protocol. The IBM Event Streams instance runs on the Cloud. clusters = source, target source.bootstrap.servers = ${KAFKA_SOURCE_BROKERS} target.bootstrap.servers = ${KAFKA_TARGET_BROKERS} target.security.protocol = SASL_SSL target.ssl.protocol = TLSv1.2 target.ssl.endpoint.identification.algorithm = https target.sasl.mechanism = PLAIN target.sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=${KAFKA_TARGET_APIKEY}; # enable and configure individual replication flows source->target.enabled = true source->target.topics = products tasks.max = 10 Topics to be replicated are configured via a whitelist that may include regular expression for pattern matching on the topic name. So if you use naming convention for your topic, you could do fine grained selection of the replicated topic. It is possible to specify topics you do not want to replicate via the blacklist property. White listed topics are set with the source->target.topics attribute of the replication flow and uses Java regular expression syntax. The default blacklisted topics are Kafka internal topic: blacklist = [follower\\.replication\\.throttled\\.replicas, leader\\.replication\\.throttled\\.replicas, message\\.timestamp\\.difference\\.max\\.ms, message\\.timestamp\\.type, unclean\\.leader\\.election\\.enable, min\\.insync\\.replicas] We can also define the blacklist with the properties: topics.blacklist . Comma-separated lists and Java Regular Expressions are supported. Internally, MirrorSourceConnector and MirrorCheckpointConnector will create multiple Kafka tasks (up to the value of tasks.max property), and MirrorHeartbeatConnector creates an additional task. MirrorSourceConnector will have one task per topic-partition combination to replicate, while MirrorCheckpointConnector will have one task per consumer group. The Kafka Connect framework uses the coordinator API, with the assign() API, so there is no consumer group used while fetching data from source topics. There is no call to commit() either; rebalancing occurs only when there is a new topic created that matches the whitelist pattern. Mirror Maker 2 can run on VM, bare metal or within containers deployed on kubernetes cluster.","title":"Mirror Maker 2 components"},{"location":"technology/kafka-mirrormaker/#why-replicating","text":"The classical needs for replication between clusters can be listed as: Disaster recovery when one secondary cluster is passive while the producer and consumers are on the active cluster in the primary data center: The following article goes over those principals. Active-active cluster mirroring for inter services communication: consumers and producers are on both sides and consume or produce to their local cluster. Moving data to a read only cluster as a front door to data lake, or to do cross data centers aggregation on the different event streams: Fan-in to get holistic data view. GDPR compliance to isolate data in country and geography Hybrid cloud operations to share data between on-premise cluster and managed service clusters.","title":"Why replicating?"},{"location":"technology/kafka-mirrormaker/#deployment-examples","text":"We encourage you to go over our Mirror maker 2 labs which addresses different replication scenarios. The Connect column defines where the Mirror Maker 2 runs. Scenario Source Target Connect Lab Getting Started Lab 1 Event Streams on Cloud Local Kafka Local on localhost Kafka Mirror Maker 2 - Lab 1 Lab 2 Using Mirror Maker 2 from Event Streams on premise to Event stream on cloud On OCP OCP Kafka Mirror Maker 2 - Lab 2","title":"Deployment examples"},{"location":"technology/kafka-mirrormaker/#replication-considerations","text":"","title":"Replication considerations"},{"location":"technology/kafka-mirrormaker/#topic-metadata-replication","text":"It is possible to disable the topic metadata replication. We do not encourage to do so. Per design topic can be added dynamically, specially when developing with Kafka Streams where intermediate topics are created by the stream topology semantic, and topic configuration can be altered to increase the number of partitions. Changes to the source topic are dynamically propagated to the target avoiding maintenance nightmare. By synchronizing configuration properties, the need for rebalancing is reduced. When doing manual configuration, even if the initial topic configuration is duplicated, any dynamic changes to the topic properties are not going to be automatically propagated and the administrator needs to change the target topic. If the throughput on the source topic has increased and the number of partition was increased to support the load, then the target cluster will not have the same downstream capability which may lead to overloading (disk space or memory capacity). With a GitOps approach, this risk is mitigated as topic definition in the GitOps repository could be propagated to the target and source cluster mostly at the same time. Also if the consumer of a partition is expecting to process the events in order within the partition, then changing the number of partitions between source and target will make the ordering not valid any more. If the replication factor are set differently between the two clusters then the availability guaranty of the replicated data may be impacted and bad settings with broker failure will lead to data lost. Finally, it is important to consider that changes to topic configuration triggers a consumer rebalance which stalls the mirroring process and creates a backlog in the pipeline and increases the end to end latency observed by the downstream application.","title":"Topic metadata replication"},{"location":"technology/kafka-mirrormaker/#naming-convention","text":"Mirror maker 2 sets the prefix for the name of the replicated topic with the name of the source cluster. This is an important and simple solution to avoid infinite loop when doing bi-directional mirroring. At the consumer side the subscribe() function supports regular expression for topic name. So a code like: kafkaConsumer . subscribe ( \"^.*accounts\" ) will listen to all the topics in the cluster having cluster name prefixed topics and the local accounts topic. This could be useful when we want to aggregate data from different data centers / clusters.","title":"Naming convention"},{"location":"technology/kafka-mirrormaker/#offset-management","text":"Mirror maker 2 tracks offset per consumer group. There are two topics created on the target cluster to manage the offset mapping between the source and target clusters and the checkpoints of the last committed offset in the source topic/partitions/consumer group. When a producer sends its records, it gets the offsets in the partition the records were created. In the diagram below we have a source topic A/partition 1 with the last write offset done by a producer to be 5, and the last read committed offset by the consumer assigned to partition 1 being 3. The last replicated offset 3 is mapped as downstream offset 12 in the target partition. Offset numbers do not match between replicated partitions. So if the blue consumer needs to reconnect to the green target cluster it will read from the last committed offset which is 12 in this environment. This information is saved in the checkpoint topic. Offset synch are emitted at the beginning of the replication and when there is a situation which leads that the numbering sequencing diverges. For example, the normal behavior is to increase the offset by one 2,3,4,5,6,7, which is mapped to 12,13,14,15,16,... on target cluster. If the write operation for offset 20 at the source is a 17 on the target then MM 2 emits a new offset synch records to the offset-synch topic. The checkpoint and offset_synch topics enable replication to be fully restored from the correct offset position on failover. On the following diagram, once the cluster source is down, the consumers on the target cluster are restarted, and they will start from the last committed offset of the source, which was offset 3 that is in fact offset 12 on target replicated topic. No record skipped.","title":"Offset management"},{"location":"technology/kafka-mirrormaker/#record-duplication","text":"Exactly-once delivery is difficult to achieve in distributed system. In the case of Kafka, producer, brokers, and consumers are working together to ensure only one message is processed end to end. With coding practice and configuration settings, within a unique cluster, Kafka can guarantee exactly once processing. No duplicated records between producer and broker, and committed reads, on consumer side, are not reprocessed in case of consumer restarts. But for cross cluster replications, the semantic is based on at least once approach. Duplicates can happen when the mirror maker source task stops before committing its offset to the source topic. A restart will load records from the last committed offset which can generate duplicates. The following diagram illustrate this case, record offset 26 on target topic is a duplicate of record 25. Also Mirror Maker 2 is a generic topic consumer, it will not participate to the \"read-committed\" process, if the topic includes duplicate messages it will propagate them to the target. In the future MM2 will be able to support exactly once by using the checkpoint topic on the target cluster to keep the state of the committed offset from the consumer side, and write with an atomic transaction between the target topic and the checkpoint topic, and commit the source read offset as part of the same transaction.","title":"Record duplication"},{"location":"technology/kafka-mirrormaker/#mm2-topology","text":"In this section we want to address horizontal scalability and how to organize the MirrorMaker 2 topology for multi-tenancy. The simplest approach is to use one Mirror Maker instance per family of topics: the classification of family of topic can be anything, from line of business, to team, to application. Suppose an application is using 1000 topic - partitions, for data replication it may make sense to have one MM2 instance for this application. The configuration will define the groupId to match the application name for example. The following diagram illustrates this kind of topology by using regular expression on the topic white list selection, there are three Mirror Maker 2 instances mirroring the different topics with name starting with topic-name-A*, topic-name-B*, topic-name-C*, respectively. Each connect instance is a JVM workers that replicate the topic/partitions and has different group.id. For Bi-directional replication for the same topic name, Mirror Maker 2 will use the cluster name as prefix. With MM2, we do not need to have 2 MM2 clusters but only one and bidirectional definitions. The following example is showing the configuration for a MM2 bidirectional settings, with accounts topic to be replicated on both cluster: apiVersion: kafka.strimzi.io/v1alpha1 kind: KafkaMirrorMaker2 ... mirrors: - sourceCluster: \"event-streams-wdc\" targetCluster: \"kafka-on-premise\" ... topicsPattern: \"accounts,orders\" - sourceCluster: \"kafka-on-premise\" targetCluster: \"event-streams-wdc\" ... topicsPattern: \"accounts,customers\"","title":"MM2 topology"},{"location":"technology/kafka-mirrormaker/#consumer-coding","text":"We recommend to review the producer implementation best practices and the consumer considerations .","title":"Consumer coding"},{"location":"technology/kafka-mirrormaker/#capacity-planning","text":"For platform sizing, the main metric to assess, is the number of partitions to replicate. The number of partitions and number of brokers are somehow connected as getting a high number of partitions involves increasing the number of brokers. For Mirror Maker 2, as it is based on Kafka connect, there is a unique cluster and each partition mirroring is supported by a task within the JVM so the first constraint is the memory allocated to the container and the heap size. To address capacity planning, we need to review some characteristic of the Kafka Connect framework: For each topic/partition there will be a task running. We can see in the trace that tasks are mapped to threads inside the JVM. So the parallelism will be bound by the number of CPUs the JVM runs on. The parameters max.tasks specifies the max parallel processing we can have per JVM. So for each Topic we need to assess the number of partitions to be replicated. Each task is using the consumer API and is part of the same consumer group, the partition within a group are balanced by an internal controller. With Kafka connect any changes to the topic topology triggers a partition rebalancing. In MM2 each consumer / task is assigned a partition by the controller. So the rebalancing is done internally. Still adding a broker node into the cluster will generate rebalancing. The task processing is stateless, consume - produce wait for acknowledge, commit offset. In this case, the CPU and network performance are key. For platform tuning activity, we need to monitor operating system performance metrics. If the CPU becomes the bottleneck, we can allocate more CPU or start to scale horizontally by adding more Mirror Maker 2 instances. If the network at the server level is the bottleneck, then adding more servers will help. Kafka will automatically balance the load among all the tasks running on all the machines. The size of the message impacts also the throughput as with small message the throughput is CPU bounded. With 100 bytes messages or more we can observe network saturation. The parameters to consider for sizing are the following: Parameter Description Impact Number of topic/ partition Each task processes one partition For pure parallel processing max.tasks is set around the number of CPU Record size Size of the message in each partition in average Memory usage and Throughput: the # of records/s decrease when size increase, while MB/s throughput increases in logarithmic Expected input throughput The producer writing to the source topic throughput Be sure the consumers inside MM2 absorb the demand Network latency This is where positioning MM2 close to the target cluster may help improve latency","title":"Capacity planning"},{"location":"technology/kafka-mirrormaker/#version-migration","text":"Once the Mirror Maker cluster is up and running, it may be needed to update the underlying code when a new product version is released. Based on Kafka Connect distributed mode multiple workers JVM coordinate the topic / partition repartition among themselves. If a worker process dies, the cluster is rebalanced to distribute the work fairly over the remaining workers. If a new worker starts work, a rebalance ensures it takes over some work from the existing workers. Using the REST API it is possible to stop and restart a connector. As of now the recommendation is to start a new MirrorMaker instance with the new version and the same groupId as the existing workers you want to migrate. Then stop the existing version. As each MirrorMaker workers are part of the same group, the internal worker controller will coordinate with the other workers the 'consumer' task to partition assignment. We have presented a similar approach in this section , where we tested that each instance of MirrorMaker 2 could assume the replication. First we will stop the Node 1 instance, upgrade it to the latest version, then start it again. Then we\u2019ll repeat the same procedure on Node 2. We\u2019ll continue to watch the Consumer VM window to note that replication should not stop at any point. We\u2019ve now upgraded to Kafka 2.5 including the latest MirrorMaker 2. Meanwhile, replication was uninterrupted due to the second instance of MirrorMaker 2: Now we\u2019ll restart the Node 1 instance of MirrorMaker 2, stop the Node 2 instance, we can still see replication occurring on the upgraded Node 1 instance of MirrorMaker 2. We upgrade Node 2\u2019s instance of MirrorMaker 2 exactly as on Node 1, and start it again, and once again, replication is still going. When using Strimzi, if the update applies to the MM2 Custom Resource Definition, just reapplying the CRD should be enough. Be sure to verify the product documentation as new version may enforce to have new topics. It was the case when Kafka connect added the config topic in a recent version.","title":"Version migration"},{"location":"technology/kafka-mirrormaker/#resources","text":"IBM Event Streams product documentation IBM Event Streams managed service - Disaster recovery example scenario Strimzi configuration for Mirror Maker 2 Getting started with Mirror Maker 2 - Tech Academy Using MirrorMaker2 from Dale Lane","title":"Resources"},{"location":"technology/kafka-monitoring/","text":"Monitoring Kafka with Prometheus and Grafana \u00b6 Info Updated 09/08/2022 Overview \u00b6 A comprehensive Kafka monitoring plan should collect metrics from the following components: Kafka Broker(s) ZooKeeper metrics as Kafka relies on it to maintain its state Producer(s) / Consumer(s), in general sense, which includes Kafka Connector cluster Kafka Broker, Zookeeper and Java clients (producer/consumer) expose metrics via JMX (Java Management Extensions) and can be configured to report stats back to Prometheus using the JMX exporter maintained by Prometheus. There is also a number of exporters maintained by the community to explore. Some of them can be used in addition to the JMX export. To monitor Kafka, for example, the JMX exporter is often used to provide broker level metrics, while community exporters claim to provide more accurate cluster level metrics (e.g. Kafka exporter , Kafka Zookeeper Exporter by CloudFlare , and others). Alternatively, you can consider writing your own custom exporter . What to monitor \u00b6 A long list of metrics is made available by Kafka ( here ) and Zookeeper ( here ). The easiest way to see the available metrics is to fire up jconsole and point it at a running kafka client or Kafka/Prometheus server; this will allow browsing all metrics with JMX. But you are still left to figure out which ones you want to actively monitor and the ones that you want to be actively alerted. An simple way to get started would be to start with the Grafana\u2019s sample dashboards for the Prometheus exporters you chose to use and then modify them as you learn more about the available metrics and/or your environment. The Monitoring Kafka metrics article by DataDog and How to monitor Kafka by Server Density provides guidance on key Kafka and Prometheus metrics, reasoning to why you should care about them and suggestions on thresholds to trigger alerts. In the next section, we will demonstrate exactly that; we will start with sample dashboards and make few modifications to exemplify how to configure key Kafka metrics to display in the dashboard. Here are a set of helpful links for Event Streams monitoring: Monitoring deployment health using User Interface, or OpenShift CLI Cluster health : Event Streams has a preconfigured monitoring dashboard, but other tools could be used as it exports a set of metrics via standard like JMX. Prometheus Server and scrape jobs \u00b6 Prometheus uses a configuration file in YAML format to define the scraping jobs and their instances . You can also use the configuration file to define recording rules and alerting rules : Recording rules allow you to precompute frequently needed or computationally expensive expressions and save their result as a new set of time series. Querying the precomputed result will then often be much faster than executing the original expression every time it is needed. This is especially useful for dashboards, which need to query the same expression repeatedly every time they refresh. Alerting rules allow you to define alert conditions based on Prometheus expression language expressions and to send notifications about firing alerts to an external service. Alerting rules in Prometheus servers send alerts to an Alertmanager. The Alertmanager then manages those alerts, including silencing, inhibition, aggregation and sending out notifications via methods such as email, PagerDuty and others. Below, we will go through the steps to stand-up a local Prometheus server as a Docker container and to modify the configuration file to scrape Kafka metrics: Create/run a docker container using Prometheus official image from DockerHub docker run -d -p 9090 :9090 prom/prometheus Obtain the IP address of the Kafka container docker inspect kafka_c | grep IPAddress Edit the prometheus.yml to add Kafka as a target docker exec -it prometheus_c \\s h vi /etc/prometheus/prometheus.yml Locate the scrape_configs section in the properties file and add the lines below to define the Kafka job, where the IP should be the IP of the kafka container - job_name : 'kafka' static_configs : - targets : [ '172.17.0.4:7071' ] Reload the configuration file ps -ef kill -HUP <prometheus PID> You can now verify that Kafka is listed as a target job in Prometheus. On a Browser, open the http://localhost:9090/targets URL. Grafana Server and dashboards \u00b6 We will use Grafana for visualization of the metrics scraped by Prometheus for that, we will need to: Stand-up a local Grafana server as a Docker container Configure Prometheus as a data source in Grafana Import sample dashboards provided by Grafana and/or community Modify the sample dashboards as we see fit Let\u2019s get started: Create a docker container using Prometheus official image from DockerHub docker run -d --name = grafana_c -p 3000 :3000 grafana/grafana On a Browser, open the http://localhost:3000 URL. Login as admin/admin . You will be prompted to change the password. Once logged in, Grafana provides visual guidance on what the next steps are: a) Add data sources b) Create first dashboard and others Configure Prometheus as a data source: Enter a Name for the data source (e.g. Prometheus) Select Prometheus as Type Enter http://localhost:9090 for HTTP URL In our simple server configuration, select Browser for HTTP Access Click Save and Test to validate configuration Back to Home, click Dashboards -> Manage to import sample dashboards Click the +Import button and paste this URL https://grafana.com/dashboards/721 Make sure to select Prometheus as the data source. NOTE: You can also explore other sample dashboard options at https://grafana.com/dashboards . For instance, there is a Kubernetes Kafka resource metrics sample dashboard that you could use instead as the starting point when configuring Kafka monitoring on ICP. The six graphs displayed in the dashboard are configured as follows: NOTE: You might want to go back to your Kafka Docker container and push messages into the topics you have created above to see changes to the graph. Or, if you have already pushed messages, you can change the Quick Range from last 5 minutes to something else (e.g. last 6 hours ) on the top right hand corner of the dashboard. Graph Formula Format As CPU Usage rate(process_cpu_seconds_total{job=\"kafka\"}[1m]) Time Series JVM Memory Used sum without(area)(jvm_memory_bytes_used{job=\"kafka\"}) Time Series Time spent in GC sum without(gc)(rate(jvm_gc_collection_seconds_sum{job=\"kafka\"}[5m])) Time Series Messages In per Topic sum without(instance)(rate(kafka_server_brokertopicmetrics_messagesin_total{job=\"kafka\",topic!=\"\"}[5m])) Time Series Bytes In per Topic sum without(instance)(rate(kafka_server_brokertopicmetrics_bytesin_total{job=\"kafka\",topic!=\"\"}[5m])) Time Series Bytes Out per Topic sum without(instance)(rate(kafka_server_brokertopicmetrics_bytesout_total{job=\"kafka\",topic!=\"\"}[5m])) Time Series Prometheus provides a functional expression language that lets the user select and aggregate time series data in real time. Before proceeding review the information on these pages to gain basic understanding of: Prometheus Expression language - http://docs.grafana.org/features/datasources/prometheus/ Grafana Query Editor - http://docs.grafana.org/features/datasources/prometheus/ As you make modifications to the dashboard it is also important to understand the data returned by the scrape jobs in the first place. For two of the metrics above, this is what the Kafka JMX exporter returns. You can go to https://localhost:7071/metrics to inspect others returned in /metrics endpoint response: Messages in Per Topic Time spent in GC","title":"Kafka Monitoring"},{"location":"technology/kafka-monitoring/#monitoring-kafka-with-prometheus-and-grafana","text":"Info Updated 09/08/2022","title":"Monitoring Kafka with Prometheus and Grafana"},{"location":"technology/kafka-monitoring/#overview","text":"A comprehensive Kafka monitoring plan should collect metrics from the following components: Kafka Broker(s) ZooKeeper metrics as Kafka relies on it to maintain its state Producer(s) / Consumer(s), in general sense, which includes Kafka Connector cluster Kafka Broker, Zookeeper and Java clients (producer/consumer) expose metrics via JMX (Java Management Extensions) and can be configured to report stats back to Prometheus using the JMX exporter maintained by Prometheus. There is also a number of exporters maintained by the community to explore. Some of them can be used in addition to the JMX export. To monitor Kafka, for example, the JMX exporter is often used to provide broker level metrics, while community exporters claim to provide more accurate cluster level metrics (e.g. Kafka exporter , Kafka Zookeeper Exporter by CloudFlare , and others). Alternatively, you can consider writing your own custom exporter .","title":"Overview"},{"location":"technology/kafka-monitoring/#what-to-monitor","text":"A long list of metrics is made available by Kafka ( here ) and Zookeeper ( here ). The easiest way to see the available metrics is to fire up jconsole and point it at a running kafka client or Kafka/Prometheus server; this will allow browsing all metrics with JMX. But you are still left to figure out which ones you want to actively monitor and the ones that you want to be actively alerted. An simple way to get started would be to start with the Grafana\u2019s sample dashboards for the Prometheus exporters you chose to use and then modify them as you learn more about the available metrics and/or your environment. The Monitoring Kafka metrics article by DataDog and How to monitor Kafka by Server Density provides guidance on key Kafka and Prometheus metrics, reasoning to why you should care about them and suggestions on thresholds to trigger alerts. In the next section, we will demonstrate exactly that; we will start with sample dashboards and make few modifications to exemplify how to configure key Kafka metrics to display in the dashboard. Here are a set of helpful links for Event Streams monitoring: Monitoring deployment health using User Interface, or OpenShift CLI Cluster health : Event Streams has a preconfigured monitoring dashboard, but other tools could be used as it exports a set of metrics via standard like JMX.","title":"What to monitor"},{"location":"technology/kafka-monitoring/#prometheus-server-and-scrape-jobs","text":"Prometheus uses a configuration file in YAML format to define the scraping jobs and their instances . You can also use the configuration file to define recording rules and alerting rules : Recording rules allow you to precompute frequently needed or computationally expensive expressions and save their result as a new set of time series. Querying the precomputed result will then often be much faster than executing the original expression every time it is needed. This is especially useful for dashboards, which need to query the same expression repeatedly every time they refresh. Alerting rules allow you to define alert conditions based on Prometheus expression language expressions and to send notifications about firing alerts to an external service. Alerting rules in Prometheus servers send alerts to an Alertmanager. The Alertmanager then manages those alerts, including silencing, inhibition, aggregation and sending out notifications via methods such as email, PagerDuty and others. Below, we will go through the steps to stand-up a local Prometheus server as a Docker container and to modify the configuration file to scrape Kafka metrics: Create/run a docker container using Prometheus official image from DockerHub docker run -d -p 9090 :9090 prom/prometheus Obtain the IP address of the Kafka container docker inspect kafka_c | grep IPAddress Edit the prometheus.yml to add Kafka as a target docker exec -it prometheus_c \\s h vi /etc/prometheus/prometheus.yml Locate the scrape_configs section in the properties file and add the lines below to define the Kafka job, where the IP should be the IP of the kafka container - job_name : 'kafka' static_configs : - targets : [ '172.17.0.4:7071' ] Reload the configuration file ps -ef kill -HUP <prometheus PID> You can now verify that Kafka is listed as a target job in Prometheus. On a Browser, open the http://localhost:9090/targets URL.","title":"Prometheus Server and scrape jobs"},{"location":"technology/kafka-monitoring/#grafana-server-and-dashboards","text":"We will use Grafana for visualization of the metrics scraped by Prometheus for that, we will need to: Stand-up a local Grafana server as a Docker container Configure Prometheus as a data source in Grafana Import sample dashboards provided by Grafana and/or community Modify the sample dashboards as we see fit Let\u2019s get started: Create a docker container using Prometheus official image from DockerHub docker run -d --name = grafana_c -p 3000 :3000 grafana/grafana On a Browser, open the http://localhost:3000 URL. Login as admin/admin . You will be prompted to change the password. Once logged in, Grafana provides visual guidance on what the next steps are: a) Add data sources b) Create first dashboard and others Configure Prometheus as a data source: Enter a Name for the data source (e.g. Prometheus) Select Prometheus as Type Enter http://localhost:9090 for HTTP URL In our simple server configuration, select Browser for HTTP Access Click Save and Test to validate configuration Back to Home, click Dashboards -> Manage to import sample dashboards Click the +Import button and paste this URL https://grafana.com/dashboards/721 Make sure to select Prometheus as the data source. NOTE: You can also explore other sample dashboard options at https://grafana.com/dashboards . For instance, there is a Kubernetes Kafka resource metrics sample dashboard that you could use instead as the starting point when configuring Kafka monitoring on ICP. The six graphs displayed in the dashboard are configured as follows: NOTE: You might want to go back to your Kafka Docker container and push messages into the topics you have created above to see changes to the graph. Or, if you have already pushed messages, you can change the Quick Range from last 5 minutes to something else (e.g. last 6 hours ) on the top right hand corner of the dashboard. Graph Formula Format As CPU Usage rate(process_cpu_seconds_total{job=\"kafka\"}[1m]) Time Series JVM Memory Used sum without(area)(jvm_memory_bytes_used{job=\"kafka\"}) Time Series Time spent in GC sum without(gc)(rate(jvm_gc_collection_seconds_sum{job=\"kafka\"}[5m])) Time Series Messages In per Topic sum without(instance)(rate(kafka_server_brokertopicmetrics_messagesin_total{job=\"kafka\",topic!=\"\"}[5m])) Time Series Bytes In per Topic sum without(instance)(rate(kafka_server_brokertopicmetrics_bytesin_total{job=\"kafka\",topic!=\"\"}[5m])) Time Series Bytes Out per Topic sum without(instance)(rate(kafka_server_brokertopicmetrics_bytesout_total{job=\"kafka\",topic!=\"\"}[5m])) Time Series Prometheus provides a functional expression language that lets the user select and aggregate time series data in real time. Before proceeding review the information on these pages to gain basic understanding of: Prometheus Expression language - http://docs.grafana.org/features/datasources/prometheus/ Grafana Query Editor - http://docs.grafana.org/features/datasources/prometheus/ As you make modifications to the dashboard it is also important to understand the data returned by the scrape jobs in the first place. For two of the metrics above, this is what the Kafka JMX exporter returns. You can go to https://localhost:7071/metrics to inspect others returned in /metrics endpoint response: Messages in Per Topic Time spent in GC","title":"Grafana Server and dashboards"},{"location":"technology/kafka-overview/","text":"In this article we are summarizing what Apache Kafka is and grouping some references, notes and tips we gathered working with Kafka while producing the different assets for this Event Driven Architecture references. This content does not replace the excellent introduction every developer using Kafka should read. Introduction \u00b6 Kafka is a distributed real time event streaming platform with the following key capabilities: Publish and subscribe streams of records. Data are stored so consuming applications can pull the information they need, and keep track of what they have seen so far. It can handle hundreds of read and write operations per second from many producers and consumers. Atomic broadcast, send a record once, every subscriber gets it once. Store streams of data records on disk and replicate them within the distributed cluster for fault-tolerance. Persist data for a given time period before delete. Can grow elastically and transparently with no downtime. Built on top of the ZooKeeper synchronization service to keep topic, partitions and metadata highly available. Kafka Components \u00b6 The diagram below presents Kafka's key components: Brokers \u00b6 Kafka runs as a cluster of broker servers that can, in theory, span multiple data centers. Each brokers manages data replication, topic/partition management, offset management. To cover multiple data centers within the same cluster, the network latency between data centers needs to be very low, at the 15ms or less, as there is a lot of communication between kafka brokers and between kafka brokers and zookeeper servers. The Kafka cluster stores streams of records in topics . Topic is referenced by producer to send data to, and subscribed by consumers to get data. Data in topic is persisted to file systems for a retention time period (Defined at the topic level). The file system can be network based. In the figure above, the Kafka brokers are allocated on three servers, with data within the topic are replicated two times. In production, it is recommended to use at least five nodes to authorize planned failure and un-planned failure, and when doing replicas, use a replica factor at least equals to three. Zookeeper \u00b6 Zookeeper is used to persist the component and platform states and it runs in cluster to ensure high availability. One zookeeper server is the leader and other are used in backup. Kafka does not keep state regarding consumers and producers. Depends on kafka version, offsets are maintained in Zookeeper or in Kafka : newer versions use an internal Kafka topic called __consumer_offsets. In any case consumers can read next message (or from a specific offset) correctly even during broker server outrages. Access Controls are saved in Zookeeper As of Kafka 2.8+ Zookeeper is becoming optional. Topics \u00b6 Topics represent end points to publish and consume records. Each record consists of a key, a value (the data payload as byte array), a timestamp and some metadata. Producers publish data records to topic and consumers subscribe to topics. When a record is produced without specifying a partition, a partition will be chosen using a hash of the key. If the record did not provide a timestamp, the producer will stamp the record with its current time (creation time or log append time). Producers hold a pool of buffers to keep records not yet transmitted to the server. Kafka store log data in its log.dir and topic maps to subdirectories in this log directory. Kafka uses topics with a pub/sub combined with queue model: it uses the concept of consumer group to divide the processing over a collection of consumer processes, running in parallel, and messages can be broadcasted to multiple groups. Consumer performs asynchronous pull to the connected brokers via the subscription to a topic. The figure below illustrates one topic having multiple partitions, replicated within the broker cluster: Partitions \u00b6 Partitions are basically used to parallelize the event processing when a single server would not be able to process all events, using the broker clustering. So to manage increase in the load of messages, Kafka uses partitions. Each broker may have zero or more partitions per topic. When creating topic we specify the number of partition to use. Kafka tolerates up to N-1 server failures without losing any messages. N is the replication factor for a given partition. Each partition is a time ordered immutable sequence of records, that are persisted for a long time period. It is a log. Topic is a labelled log. Consumers see messages in the order they are stored in the log. Each partition is replicated across a configurable number of servers for fault tolerance. The number of partition will depend on characteristics like the number of consumers, the traffic pattern, etc... You can have 2000 partitions per broker. Each partitioned message has a unique sequence id called offset (\"abcde, ab, a ...\" in the figure above are offsets). Those offset ids are defined when events arrived at the broker level, and are local to the partition. They are immutable. When a consumer reads a topic, it actually reads data from all the partitions. As a consumer reads data from a partition, it advances its offset. To read an event the consumer needs to use the topic name, the partition number and the last offset to read from. Brokers keep offset information in an hidden topic. Partitions guarantee that data with the same keys will be sent to the same consumer and in order. Partitions are saved to disk as append log. The older records are deleted after a given time period or if the size of log goes over a limit. It is possible to compact the log. The log compaction means, the last known value for each message key is kept. Compacted Topics are used in Streams processing for stateful operator to keep aggregate or grouping by key. You can read more about log compaction from the kafka doc . Replication \u00b6 Each partition can be replicated across a number of servers. The replication factor is captured by the number of brokers to be used for replication. To ensure high availability it should be set to at least a value of three. Partitions have one leader and zero or more followers. The leader manages all the read and write requests for the partition. The followers replicate the leader content. We are addressing data replication in the high availability section below. Consumer group \u00b6 This is the way to group consumers so the processing of event is parallelized. The number of consumers in a group is the same as the number of partition defined in a topic. We are detailing consumer group implementation in this note .","title":"Kafka Overview"},{"location":"technology/kafka-overview/#introduction","text":"Kafka is a distributed real time event streaming platform with the following key capabilities: Publish and subscribe streams of records. Data are stored so consuming applications can pull the information they need, and keep track of what they have seen so far. It can handle hundreds of read and write operations per second from many producers and consumers. Atomic broadcast, send a record once, every subscriber gets it once. Store streams of data records on disk and replicate them within the distributed cluster for fault-tolerance. Persist data for a given time period before delete. Can grow elastically and transparently with no downtime. Built on top of the ZooKeeper synchronization service to keep topic, partitions and metadata highly available.","title":"Introduction"},{"location":"technology/kafka-overview/#kafka-components","text":"The diagram below presents Kafka's key components:","title":"Kafka Components"},{"location":"technology/kafka-overview/#brokers","text":"Kafka runs as a cluster of broker servers that can, in theory, span multiple data centers. Each brokers manages data replication, topic/partition management, offset management. To cover multiple data centers within the same cluster, the network latency between data centers needs to be very low, at the 15ms or less, as there is a lot of communication between kafka brokers and between kafka brokers and zookeeper servers. The Kafka cluster stores streams of records in topics . Topic is referenced by producer to send data to, and subscribed by consumers to get data. Data in topic is persisted to file systems for a retention time period (Defined at the topic level). The file system can be network based. In the figure above, the Kafka brokers are allocated on three servers, with data within the topic are replicated two times. In production, it is recommended to use at least five nodes to authorize planned failure and un-planned failure, and when doing replicas, use a replica factor at least equals to three.","title":"Brokers"},{"location":"technology/kafka-overview/#zookeeper","text":"Zookeeper is used to persist the component and platform states and it runs in cluster to ensure high availability. One zookeeper server is the leader and other are used in backup. Kafka does not keep state regarding consumers and producers. Depends on kafka version, offsets are maintained in Zookeeper or in Kafka : newer versions use an internal Kafka topic called __consumer_offsets. In any case consumers can read next message (or from a specific offset) correctly even during broker server outrages. Access Controls are saved in Zookeeper As of Kafka 2.8+ Zookeeper is becoming optional.","title":"Zookeeper"},{"location":"technology/kafka-overview/#topics","text":"Topics represent end points to publish and consume records. Each record consists of a key, a value (the data payload as byte array), a timestamp and some metadata. Producers publish data records to topic and consumers subscribe to topics. When a record is produced without specifying a partition, a partition will be chosen using a hash of the key. If the record did not provide a timestamp, the producer will stamp the record with its current time (creation time or log append time). Producers hold a pool of buffers to keep records not yet transmitted to the server. Kafka store log data in its log.dir and topic maps to subdirectories in this log directory. Kafka uses topics with a pub/sub combined with queue model: it uses the concept of consumer group to divide the processing over a collection of consumer processes, running in parallel, and messages can be broadcasted to multiple groups. Consumer performs asynchronous pull to the connected brokers via the subscription to a topic. The figure below illustrates one topic having multiple partitions, replicated within the broker cluster:","title":"Topics"},{"location":"technology/kafka-overview/#partitions","text":"Partitions are basically used to parallelize the event processing when a single server would not be able to process all events, using the broker clustering. So to manage increase in the load of messages, Kafka uses partitions. Each broker may have zero or more partitions per topic. When creating topic we specify the number of partition to use. Kafka tolerates up to N-1 server failures without losing any messages. N is the replication factor for a given partition. Each partition is a time ordered immutable sequence of records, that are persisted for a long time period. It is a log. Topic is a labelled log. Consumers see messages in the order they are stored in the log. Each partition is replicated across a configurable number of servers for fault tolerance. The number of partition will depend on characteristics like the number of consumers, the traffic pattern, etc... You can have 2000 partitions per broker. Each partitioned message has a unique sequence id called offset (\"abcde, ab, a ...\" in the figure above are offsets). Those offset ids are defined when events arrived at the broker level, and are local to the partition. They are immutable. When a consumer reads a topic, it actually reads data from all the partitions. As a consumer reads data from a partition, it advances its offset. To read an event the consumer needs to use the topic name, the partition number and the last offset to read from. Brokers keep offset information in an hidden topic. Partitions guarantee that data with the same keys will be sent to the same consumer and in order. Partitions are saved to disk as append log. The older records are deleted after a given time period or if the size of log goes over a limit. It is possible to compact the log. The log compaction means, the last known value for each message key is kept. Compacted Topics are used in Streams processing for stateful operator to keep aggregate or grouping by key. You can read more about log compaction from the kafka doc .","title":"Partitions"},{"location":"technology/kafka-overview/#replication","text":"Each partition can be replicated across a number of servers. The replication factor is captured by the number of brokers to be used for replication. To ensure high availability it should be set to at least a value of three. Partitions have one leader and zero or more followers. The leader manages all the read and write requests for the partition. The followers replicate the leader content. We are addressing data replication in the high availability section below.","title":"Replication"},{"location":"technology/kafka-overview/#consumer-group","text":"This is the way to group consumers so the processing of event is parallelized. The number of consumers in a group is the same as the number of partition defined in a topic. We are detailing consumer group implementation in this note .","title":"Consumer group"},{"location":"technology/kafka-producers/","text":"Understanding Kafka Producers \u00b6 A producer is a thread safe kafka client API that publishes records to the cluster. It uses buffers, thread pool, and serializers to send data. They are stateless: the consumers is responsible to manage the offsets of the message they read. When the producer connects via the initial bootstrap connection, it gets the metadata about the topic - partition and the leader broker to connect to. The assignment of messages to partition is done following different algorithms: round-robin if there is no key specified, using the hash code of the key, or custom defined. We recommend reading IBM Event streams producer guidelines to understand how producers work with its configuration parameters. Design considerations \u00b6 When developing a record producer you need to assess the followings: What is the event payload to send? Is is a root aggregate, as defined in domain driven design, with value objects? Does it need to be kept in sequence to be used as event sourcing? or order does not matter? Remember that when order is important, messages need to go to the same topic. When multiple partitions are used, the messages with the same key will go to the same partition to guaranty the order. See related discussions from Martin Kleppmann on confluent web site . Also to be exhaustive, it is possible to get a producer doing retries that could generate duplicate records as acknowledges may take time to come: within a batch of n records, if the producer did not get all the n acknowledges on time, it may resend the batch. This is where 'idempotence' becomes important (see later section). Is there a strong requirement to manage the schema definition? If using one topic to manage all events about a business entity, then be sure to support a flexible avro schema . What is the expected throughput to send events? Event size * average throughput combined with the expected latency help to compute buffer size. By default, the buffer size is set at 32Mb, but can be configured with buffer.memory property. (See producer configuration API ) Can the producer batches events together to send them in batch over one send operation? By design kafka producers batch events. Is there a risk for loosing communication? Tune the RETRIES_CONFIG and buffer size, and ensure to have at least 3 or even better 5, brokers within the cluster to maintain quorum in case of one failure. The client API is implemented to support reconnection. When deploying kafka on Kubernetes, it is important to proxy the broker URLs with a proxy server outside of kubernetes. The HAProxy needs to scale, and as the kafka traffic may be important, it may make sense to have a dedicated HAProxy for clients to brokers traffic. Assess exactly once delivery requirement. Look at idempotent producer: retries will not introduce duplicate records (see section below). Partitions help to scale the consumer processing of messages, but it also helps the producer to be more efficient as it can send message in parallel to different partition. Where the event timestamp comes from? Should the producer send operation set it or is it loaded from external data? Remember that LogAppendTime is considered to be processing time, and CreateTime is considered to be event time. Typical producer code structure \u00b6 The producer code, using java or python API, does the following steps: define producer properties create a producer instance Connect to the bootstrap URL, get a broker leader send event records and get resulting metadata. Producers are thread safe. The send() operation is asynchronous and returns immediately once record has been stored in the buffer of records, and it is possible to add a callback to process the broker acknowledgements. Here is an example of producer code from the quick start. Kafka useful Producer APIs \u00b6 Here is a list of common API to use in your producer and consumer code. KafkaProducer A Kafka client that publishes records to the Kafka cluster. The send method is asynchronous. A producer is thread safe so we can have per topic to interface. ProducerRecord to be published to a topic RecordMetadata metadata for a record that has been acknowledged by the server. Properties to consider \u00b6 The following properties are helpful to tune at each topic and producer and will vary depending on the requirements: Properties Description BOOTSTRAP_SERVERS_CONFIG A comma-separated list of host:port values for all the brokers deployed. So producer may use any brokers KEY_SERIALIZER_CLASS_CONFIG and VALUE_SERIALIZER_CLASS_CONFIG convert the keys and values into byte arrays. Using default String serializer should be a good solution for Json payload. For streaming app, use customer serializer. ACKS_CONFIG specifies the minimum number of acknowledgments from a broker that the producer will wait for before considering a record send completed. Values = all, 0, and 1. 0 is for fire and forget. RETRIES_CONFIG specifies the number of times to attempt to resend a batch of events. ENABLE_IDEMPOTENCE_CONFIG Set to true, the number of retries will be maximized, and the acks will be set to All . TRANSACTION_ID A unique identifier for a producer. In case of multiple producer instances, a same ID will mean a second producers can commit the transaction. Epoch number, linked to the process ID, avoid having two producers doing this commit. If no transaction ID is specified, the transaction will be valid within a single session. Advanced producer guidances \u00b6 How to support exactly once delivery \u00b6 Knowing that exactly once delivery is one of the hardest problems to solve in distributed systems, how kafka does it?. Broker can fail or a network may respond slowly while a producer is trying to send events. Producer can set acknowledge level to control the delivery semantic to ensure not loosing data. The following semantic is supported: At least once : means the producer set ACKS=1 and get an acknowledgement message when the message sent, has been written to at least one time in the cluster (assume replicas = 3). If the ack is not received, the producer may retry, which may generate duplicate records in case the broker stops after saving to the topic and before sending back the acknowledgement message. At most semantic : means the producer will not do retry in case of no acknowledge received. It may create log and compensation, but the message may be lost. Exactly once means even if the producer sends the message twice the system will send only one message to the consumer. Once the consumer commits the read offset, it will not receive the message again, even if it restarts. Consumer offset needs to be in sync with produced event. With acks = 1 it is possible to lose messages, as illustrated in the following diagram, where new messages were not replicated yet, and ack was already sent back to the producer. Losing messages will depend if a replica is taking the leader position or not, or if the failed broker goes back online before replicas election but has no fsynch to the disk before the crash. To avoid that we need to have ack=-1 , three replicas and in-sync-replica=2. Replicated messages are acknowledged, when broker fails, a new leader is selected with the replicated messagesm it becomes the partition leader and others start to replicate from him. Producer reconnect to the partition leader. Here is an example of cluster configuration with default set for all topic spec : strimziOverrides : kafka : config : default.replication.factor : 3 min.insync.replicas : 2 Or at the topic level: kind : KafkaTopic metadata : name : rt-store.inventory namespace : rt-inventory-dev labels : eventstreams.ibm.com/cluster : dev spec : partitions : 1 replicas : 3 config : min.insync.replicas : 1 At the best case scenario, with a replica factor set to 3, a broker responding on time to the producer, and with a consumer committing its offset and reading from the last committed offset it is possible to get only one message end to end. Sometime the brokers will not send acknowledge in expected time, and the producer may decide to send the records again, generating duplicate... To avoid duplicate message at the broker level, when acknowledge is set to ALL, the producer can also set idempotence flag: ENABLE_IDEMPOTENCE_CONFIG = true. With the idempotence property, the record sent, has a sequence number and a producer id, so that the broker keeps the last sequence number per producer and per partition. If a message is received with a lower sequence number, it means a producer is doing some retries on record already processed, so the broker will drop it, to avoid having duplicate records per partition. If the id is greater than current id known by the broker, the broker will create an OutOfSequence exception, which may be fatal as records may have been lost. The sequence number is persisted in a log so even in case of broker leader failure, the new leader will have a good view of the states of the system. The replication mechanism guarantees that, when a message is written to the leader replica, it will be replicated to all available replicas. As soon as you want to get acknowledge of all replicates, it is obvious to set idempotence to true. It does not impact performance. To add to this discussion, as topic may have multiple partitions, idempotent producers do not provide guarantees for writes across multiple Topic-Partition. For that Kafka supports atomic writes to all partitions, so that all records are saved or none of them are visible to consumers. This transaction control is done by using the producer transactional API, and a transacitional protocol with coordinator and control message. Here is an example of such configuration that can be done in a producer constructor method: producerProps . put ( \"enable.idempotence\" , \"true\" ); producerProps . put ( \"transactional.id\" , \"prod-1\" ); kafkaProducer . initTransactions () initTransactions() registers the producer with the broker as one that can use transaction, identifying it by its transactional.id and a sequence number, or epoch. Epoch is used to avoid an old producer to commit a transaction while a new producer instance was created for that and continues its work. Kafka streams with consume-process-produce loop requires transaction and exactly once. Even commiting its read offset is part of the transaction. So Producer API has a sendOffsetsToTransaction method . See the KIP 98 for details. In case of multiple partitions, the broker will store a list of all updated partitions for a given transaction. To support transaction a transaction coordinator keeps its states into an internal topic (TransactionLog). Control messages are added to the main topic but never exposed to the 'user', so that consumers have the knowledge if a transaction is committed or not. See the code in order command microservice. The consumer is also interested to configure the reading of the transactional messages by defining the isolation level. Consumer waits to read transactional messages until the associated transaction has been committed. Here is an example of consumer code and configuration consumerProps . put ( \"enable.auto.commit\" , \"false\" ); consumerProps . put ( \"isolation.level\" , \"read_committed\" ); With read_committed , no message that was written to the input topic in the same transaction will be read by this consumer until message replicas are all written. In consume-process-produce loop, producer commits its offset with code, and specifies the last offset to read. offsetsToCommit . put ( partition , new OffsetAndMetadata ( offset + 1 )) producer . sendOffsetsToTransaction ( offsetsToCommit , \"consumer-group-id\" ); The producer then commits the transaction. try { kafkaProducer . beginTransaction (); ProducerRecord < String , String > record = new ProducerRecord <> ( ApplicationConfig . ORDER_COMMAND_TOPIC , key , value ); Future < RecordMetadata > send = kafkaProducer . send ( record , callBackFunction ); kafkaProducer . commitTransaction (); } catch ( KafkaException e ){ kafkaProducer . abortTransaction (); } There is an interesting article from the Baeldung team about exactly once processing in kafka with code example which we have re-used to implement the order processing in our Reefer Container Shipment reference application and explained here Code Examples \u00b6 Order management with CQRS in Java EDA quickstart Quarkus Producer API Springboot with kafka template Event driven microservice template More readings \u00b6 Creating advanced kafka producer in java - Cloudurable Confluent blog: Exactly-once Semantics are Possible: Here\u2019s How Kafka Does it","title":"Kafka Producers"},{"location":"technology/kafka-producers/#understanding-kafka-producers","text":"A producer is a thread safe kafka client API that publishes records to the cluster. It uses buffers, thread pool, and serializers to send data. They are stateless: the consumers is responsible to manage the offsets of the message they read. When the producer connects via the initial bootstrap connection, it gets the metadata about the topic - partition and the leader broker to connect to. The assignment of messages to partition is done following different algorithms: round-robin if there is no key specified, using the hash code of the key, or custom defined. We recommend reading IBM Event streams producer guidelines to understand how producers work with its configuration parameters.","title":"Understanding Kafka Producers"},{"location":"technology/kafka-producers/#design-considerations","text":"When developing a record producer you need to assess the followings: What is the event payload to send? Is is a root aggregate, as defined in domain driven design, with value objects? Does it need to be kept in sequence to be used as event sourcing? or order does not matter? Remember that when order is important, messages need to go to the same topic. When multiple partitions are used, the messages with the same key will go to the same partition to guaranty the order. See related discussions from Martin Kleppmann on confluent web site . Also to be exhaustive, it is possible to get a producer doing retries that could generate duplicate records as acknowledges may take time to come: within a batch of n records, if the producer did not get all the n acknowledges on time, it may resend the batch. This is where 'idempotence' becomes important (see later section). Is there a strong requirement to manage the schema definition? If using one topic to manage all events about a business entity, then be sure to support a flexible avro schema . What is the expected throughput to send events? Event size * average throughput combined with the expected latency help to compute buffer size. By default, the buffer size is set at 32Mb, but can be configured with buffer.memory property. (See producer configuration API ) Can the producer batches events together to send them in batch over one send operation? By design kafka producers batch events. Is there a risk for loosing communication? Tune the RETRIES_CONFIG and buffer size, and ensure to have at least 3 or even better 5, brokers within the cluster to maintain quorum in case of one failure. The client API is implemented to support reconnection. When deploying kafka on Kubernetes, it is important to proxy the broker URLs with a proxy server outside of kubernetes. The HAProxy needs to scale, and as the kafka traffic may be important, it may make sense to have a dedicated HAProxy for clients to brokers traffic. Assess exactly once delivery requirement. Look at idempotent producer: retries will not introduce duplicate records (see section below). Partitions help to scale the consumer processing of messages, but it also helps the producer to be more efficient as it can send message in parallel to different partition. Where the event timestamp comes from? Should the producer send operation set it or is it loaded from external data? Remember that LogAppendTime is considered to be processing time, and CreateTime is considered to be event time.","title":"Design considerations"},{"location":"technology/kafka-producers/#typical-producer-code-structure","text":"The producer code, using java or python API, does the following steps: define producer properties create a producer instance Connect to the bootstrap URL, get a broker leader send event records and get resulting metadata. Producers are thread safe. The send() operation is asynchronous and returns immediately once record has been stored in the buffer of records, and it is possible to add a callback to process the broker acknowledgements. Here is an example of producer code from the quick start.","title":"Typical producer code structure"},{"location":"technology/kafka-producers/#kafka-useful-producer-apis","text":"Here is a list of common API to use in your producer and consumer code. KafkaProducer A Kafka client that publishes records to the Kafka cluster. The send method is asynchronous. A producer is thread safe so we can have per topic to interface. ProducerRecord to be published to a topic RecordMetadata metadata for a record that has been acknowledged by the server.","title":"Kafka useful Producer APIs"},{"location":"technology/kafka-producers/#properties-to-consider","text":"The following properties are helpful to tune at each topic and producer and will vary depending on the requirements: Properties Description BOOTSTRAP_SERVERS_CONFIG A comma-separated list of host:port values for all the brokers deployed. So producer may use any brokers KEY_SERIALIZER_CLASS_CONFIG and VALUE_SERIALIZER_CLASS_CONFIG convert the keys and values into byte arrays. Using default String serializer should be a good solution for Json payload. For streaming app, use customer serializer. ACKS_CONFIG specifies the minimum number of acknowledgments from a broker that the producer will wait for before considering a record send completed. Values = all, 0, and 1. 0 is for fire and forget. RETRIES_CONFIG specifies the number of times to attempt to resend a batch of events. ENABLE_IDEMPOTENCE_CONFIG Set to true, the number of retries will be maximized, and the acks will be set to All . TRANSACTION_ID A unique identifier for a producer. In case of multiple producer instances, a same ID will mean a second producers can commit the transaction. Epoch number, linked to the process ID, avoid having two producers doing this commit. If no transaction ID is specified, the transaction will be valid within a single session.","title":"Properties to consider"},{"location":"technology/kafka-producers/#advanced-producer-guidances","text":"","title":"Advanced producer guidances"},{"location":"technology/kafka-producers/#how-to-support-exactly-once-delivery","text":"Knowing that exactly once delivery is one of the hardest problems to solve in distributed systems, how kafka does it?. Broker can fail or a network may respond slowly while a producer is trying to send events. Producer can set acknowledge level to control the delivery semantic to ensure not loosing data. The following semantic is supported: At least once : means the producer set ACKS=1 and get an acknowledgement message when the message sent, has been written to at least one time in the cluster (assume replicas = 3). If the ack is not received, the producer may retry, which may generate duplicate records in case the broker stops after saving to the topic and before sending back the acknowledgement message. At most semantic : means the producer will not do retry in case of no acknowledge received. It may create log and compensation, but the message may be lost. Exactly once means even if the producer sends the message twice the system will send only one message to the consumer. Once the consumer commits the read offset, it will not receive the message again, even if it restarts. Consumer offset needs to be in sync with produced event. With acks = 1 it is possible to lose messages, as illustrated in the following diagram, where new messages were not replicated yet, and ack was already sent back to the producer. Losing messages will depend if a replica is taking the leader position or not, or if the failed broker goes back online before replicas election but has no fsynch to the disk before the crash. To avoid that we need to have ack=-1 , three replicas and in-sync-replica=2. Replicated messages are acknowledged, when broker fails, a new leader is selected with the replicated messagesm it becomes the partition leader and others start to replicate from him. Producer reconnect to the partition leader. Here is an example of cluster configuration with default set for all topic spec : strimziOverrides : kafka : config : default.replication.factor : 3 min.insync.replicas : 2 Or at the topic level: kind : KafkaTopic metadata : name : rt-store.inventory namespace : rt-inventory-dev labels : eventstreams.ibm.com/cluster : dev spec : partitions : 1 replicas : 3 config : min.insync.replicas : 1 At the best case scenario, with a replica factor set to 3, a broker responding on time to the producer, and with a consumer committing its offset and reading from the last committed offset it is possible to get only one message end to end. Sometime the brokers will not send acknowledge in expected time, and the producer may decide to send the records again, generating duplicate... To avoid duplicate message at the broker level, when acknowledge is set to ALL, the producer can also set idempotence flag: ENABLE_IDEMPOTENCE_CONFIG = true. With the idempotence property, the record sent, has a sequence number and a producer id, so that the broker keeps the last sequence number per producer and per partition. If a message is received with a lower sequence number, it means a producer is doing some retries on record already processed, so the broker will drop it, to avoid having duplicate records per partition. If the id is greater than current id known by the broker, the broker will create an OutOfSequence exception, which may be fatal as records may have been lost. The sequence number is persisted in a log so even in case of broker leader failure, the new leader will have a good view of the states of the system. The replication mechanism guarantees that, when a message is written to the leader replica, it will be replicated to all available replicas. As soon as you want to get acknowledge of all replicates, it is obvious to set idempotence to true. It does not impact performance. To add to this discussion, as topic may have multiple partitions, idempotent producers do not provide guarantees for writes across multiple Topic-Partition. For that Kafka supports atomic writes to all partitions, so that all records are saved or none of them are visible to consumers. This transaction control is done by using the producer transactional API, and a transacitional protocol with coordinator and control message. Here is an example of such configuration that can be done in a producer constructor method: producerProps . put ( \"enable.idempotence\" , \"true\" ); producerProps . put ( \"transactional.id\" , \"prod-1\" ); kafkaProducer . initTransactions () initTransactions() registers the producer with the broker as one that can use transaction, identifying it by its transactional.id and a sequence number, or epoch. Epoch is used to avoid an old producer to commit a transaction while a new producer instance was created for that and continues its work. Kafka streams with consume-process-produce loop requires transaction and exactly once. Even commiting its read offset is part of the transaction. So Producer API has a sendOffsetsToTransaction method . See the KIP 98 for details. In case of multiple partitions, the broker will store a list of all updated partitions for a given transaction. To support transaction a transaction coordinator keeps its states into an internal topic (TransactionLog). Control messages are added to the main topic but never exposed to the 'user', so that consumers have the knowledge if a transaction is committed or not. See the code in order command microservice. The consumer is also interested to configure the reading of the transactional messages by defining the isolation level. Consumer waits to read transactional messages until the associated transaction has been committed. Here is an example of consumer code and configuration consumerProps . put ( \"enable.auto.commit\" , \"false\" ); consumerProps . put ( \"isolation.level\" , \"read_committed\" ); With read_committed , no message that was written to the input topic in the same transaction will be read by this consumer until message replicas are all written. In consume-process-produce loop, producer commits its offset with code, and specifies the last offset to read. offsetsToCommit . put ( partition , new OffsetAndMetadata ( offset + 1 )) producer . sendOffsetsToTransaction ( offsetsToCommit , \"consumer-group-id\" ); The producer then commits the transaction. try { kafkaProducer . beginTransaction (); ProducerRecord < String , String > record = new ProducerRecord <> ( ApplicationConfig . ORDER_COMMAND_TOPIC , key , value ); Future < RecordMetadata > send = kafkaProducer . send ( record , callBackFunction ); kafkaProducer . commitTransaction (); } catch ( KafkaException e ){ kafkaProducer . abortTransaction (); } There is an interesting article from the Baeldung team about exactly once processing in kafka with code example which we have re-used to implement the order processing in our Reefer Container Shipment reference application and explained here","title":"How to support exactly once delivery"},{"location":"technology/kafka-producers/#code-examples","text":"Order management with CQRS in Java EDA quickstart Quarkus Producer API Springboot with kafka template Event driven microservice template","title":"Code Examples"},{"location":"technology/kafka-producers/#more-readings","text":"Creating advanced kafka producer in java - Cloudurable Confluent blog: Exactly-once Semantics are Possible: Here\u2019s How Kafka Does it","title":"More readings"},{"location":"technology/kafka-streams/","text":"Info Updated 09/06/2022 Kafka Streams is client API to build microservices with input and output data are in Kafka. It is based on programming a graph of processing nodes to support the business logic developer wants to apply on the event streams. We recommend reading this excellent introduction Kafka stream made simple from Jay Kreps from Confluent to get a good understanding of why Kafka stream was created. Concepts \u00b6 The business logic is implemented via topology that represents a graph of processing nodes. Each node within the graph, processes events from the parent node. To summarize, Kafka Streams has the following capabilities: Kafka Streams applications are built on top of producer and consumer APIs and are leveraging Kafka capabilities to do data parallelism processing, support distributed coordination of partition to task assignment, and being fault tolerant. Streams processing is helpful for handling out-of-order data, reprocessing input as code changes, and performing stateful computations, like real time analytics. It uses producer / consumer APIs, stateful storage and consumer groups. It treats both past and future data the same way. Kafka Streams is an embedded library to integrate in your Java application. No need for separate processing cluster. As deployable container it can scale horizontally easily within Kubernetes platform. It does not run in Kafka cluster. Topology consumes continuous real time flows of records and publishes new flows to one or more topics. A stream (represented by the KStream API) is a durable, partitioned sequence of immutable events. When a new event is added a stream, it's appended to the partition that its key belongs to. It can scale vertically, by increasing the number of threads for each Kafka Streams application on a single machine, and horizontally by adding additional machines or pods in kubernetes. Each deployed instance use the same value for the application.id kafka stream property. The assignment of stream partitions to stream tasks never changes, so task is the unit of parallelism. Task executes the topology, and is buffering records coming from the attached partitions. KTable is a durable, partitioned collection that models change over time. It's the mutable counterpart of KStreams. It represents what is true at the current moment. Each data record is considered a contextual update. Tables are saved in state store backed up with kafka topic and are queryables. Any operation on the table such as querying, inserting, or updating a row is carried out behind the scenes by a corresponding operation on the table\u2019s state store. These state stores are being materialized on local disk inside your application instances It supports exactly-once processing semantics to guarantee that each record is processed once and only once even when there is a failure. Stream APIs transform, aggregate and enrich data, per record with milli second latency, from one topic to another one. Supports stateful and windowing operations by processing one record at a time. An application's processor topology is scaled by breaking it into multiple tasks. Tasks can then instantiate their own processor topology based on the assigned partitions. Fault tolerance \u00b6 As KTables are persisted on state store, they are materialized on local to broker disk, as change log streams: In the case of a stream processing task fails, it can rebuild its internal, in memory state store from the kafka topic / change log. Once done it can reconsume messages. The system is fault tolerant. Scaling \u00b6 When topics have multiple partitions, each kafka streams task consumes a unique partition. If for any reasons, we need to scale by adding new instances of the application, so in term of kubernetes, adding more pods, then the system will rebalance the stream tasks allocation to new instances created. We can start as many threads of the application as there are input Kafka topic partitions. Another good example to illustrate threading, task and machine scaling is documented in this on Confluent article . Code structure \u00b6 In general the code for processing event does the following: Set a properties object to specify which brokers to connect to and what kind of key and value des/serialization mechanisms to use. Define a stream client: if you want to get the stream of records use KStream , if you want a changelog with the last value of a given key use KTable (For example, using KTable to keep a user profile data by userid key). Create a topology of input source and sink target and the set of actions to perform in between. Start the stream client to consume records. Programming with KStream and Ktable is not easy at first, as there are a lot of concepts for data manipulations, serialization and operations chaining. A stateful operator uses the streaming Domain Specific Language, with constructs for aggregation, join and time window operations. Stateful transformations require a state store associated with the stream processor. We recommend at this stage to do our exercise 1 from the Tech Academy tutorial to develop a first simple topology and test it without any Kafka cluster using the Topology test driver . The following code extract, is part of the Apache Kafka Word count example and is used to illustrate the programming model used: // Streams processing are created from a builder. final StreamsBuilder builder = new StreamsBuilder (); // pattern to extract word final Pattern pattern = Pattern . compile ( \"\\\\W+\" ); // source is a kafka topic, and materialized as a KStream KStream < String , String > textLines = builder . stream ( source ); // implement the logic to count words KTable < String , Long > wordCounts = textLines . flatMapValues ( textLine -> Arrays . asList ( pattern . split ( textLine . toLowerCase ()))) . print ( Printed . toSysOut ()) . groupBy (( key , word ) -> word ) . count ( Materialized . < String , Long , KeyValueStore < Bytes , byte []>> as ( \"counts-store\" )); // sink is another kafka topic. Produce for each word the number of occurrence in the given doc wordCounts . toStream (). to ( sink , Produced . with ( Serdes . String (), Serdes . Long ())); KafkaStreams streams = new KafkaStreams ( builder . build (), props ); streams . start (); KStream represents KeyValue records coming as event stream from the input topic. flatMapValues() transforms the value of each record in \"this\" stream into zero or more values with the same key in a new KStream (in memory). So here the text line is split into words. The parameter is a ValueMapper which applies transformation on values but keeps the key. Another important transformation is the KeyValueMapper . groupBy() Group the records of this KStream on a new key that is selected using the provided KeyValueMapper. So here it creates new KStream with the extracted word as key. count() counts the number of records in this stream by the grouped key. Materialized is an class to define a \"store\" to persist state and data. So here the state store is \"counts-store\". As store is a in-memory table, but it could also be persisted in external database. Could be the Facebook's RocksDB key value persistence or a log-compacted topic in Kafka. Produced defines how to provide the optional parameter types when producing to new topics. KTable is an abstraction of a changelog stream from a primary-keyed table. Important: map, flatMapValues and mapValues ... functions don\u2019t modify the object or value presented as a parameter. Available tutorials \u00b6 We found the following tutorial helpful to grow your competency on Kafka Streams: Word count Kafka Stream example from product documentation Use Quarkus and Kafka Streams to use groupBy, join with another Stream Quarkus and Kafka Streams guides Build an inventory aggregator with Quarkus, with kstreams, ktable and interactive queries, Mutiny , all deployable on OpenShift with quarkus kubernetes plugin. Interactive queries \u00b6 State store can be queried, and this is supported by the interactive queries . Result can be from the local store, if the key is in the local store, or a remote one. The metadata of the key to task allocation is maintained and shared between tasks. As a Kafka stream app runs on multiple instances, the entire state of the app is distributed among the instances. The Stream topology will transform the stream to table with one of the groupBy or aggregate operation: items . groupByKey ( ItemStream . buildGroupDefinition ()) . aggregate ( () -> new Inventory (), ( k , newItem , existingInventory ) -> existingInventory . updateStockQuantity ( k , newItem ), InventoryAggregate . materializeAsInventoryStore ()); which then it can be materialized as queryable key - value store. // class InventoryAggregate /** * Create a key value store named INVENTORY_STORE_NAME to persist store inventory */ public static Materialized < String , Inventory , KeyValueStore < Bytes , byte []>> materializeAsInventoryStore () { return Materialized . < String , Inventory , KeyValueStore < Bytes , byte []>> as ( INVENTORY_STORE_NAME ) . withKeySerde ( Serdes . String ()). withValueSerde ( inventorySerde ); } Each store is local to the instance it was created in: The storeID is the key used in the KTable. Once the Kafka Stream is started and store created (loop to get it ready) then it is easy to access it: @Inject KafkaStreams streams ; private ReadOnlyKeyValueStore < String , Inventory > getInventoryStockStore () { while ( true ) { try { StoreQueryParameters < ReadOnlyKeyValueStore < String , Inventory >> parameters = StoreQueryParameters . fromNameAndType ( InventoryAggregate . INVENTORY_STORE_NAME , QueryableStoreTypes . keyValueStore ()); return streams . store ( parameters ); ... // access one element of the store Inventory result = getInventoryStockStore (). get ( storeID ); To get access to remote store, we need to expose each store via an API. The easiest one is a REST api, but it could be any RPC protocol. Each instance is uniquely identified via the application.server property. When deploying on kubernetes it could be the pod IP address accessible via the $POD_IP environment variable. Below is a quarkus declaration: hostname = ${POD_IP:localhost} quarkus.kafka-streams.application-server = ${hostname}:8080 Now the design decision is to return the URL of the remote instance to the client doing the query call or do the call internally to the instnace reached to always returning a result. The knowledge of other application instance is done by sharing metadata. The following code example illustrates the access to metadata for store via the kafka stream context and then build a pipeline medata to share information about host, port and partition allocation. streams . allMetadataForStore ( ItemStream . ITEMS_STORE_NAME ) . stream () . map ( m -> new PipelineMetadata ( m . hostInfo (). host () + \":\" + m . hostInfo (). port (), m . topicPartitions () . stream () . map ( TopicPartition :: toString ) . collect ( Collectors . toSet ()))) . collect ( Collectors . toList ()); Design considerations \u00b6 Partitions are assigned to a StreamTask, and each StreamTask has its own state store. So it is important to use key and kafka will assign records with same key to same partition so lookup inside state store will work. Avoid external database lookup as part of the stream: As kafka can handle million of records per second, so a lookup to an external database to do a join between a primary key that is in the event and a table in the database to do a data enrichment, for example, is a bad practice. The approach will be to use Ktable, with state store and perform a join in memory. Reference data can be loaded inside a Ktable for event stream enrichment. Table and streams joins: we recommend reading this deep dive article on joining between streams and joining stream with table. The important points from this article: kstream - kstream joins are windowed to control the size of data to keep in memory to search for the matching records. Faust: a python library to do kafka streaming \u00b6 Faust is a python library to support stream processing. It does not have its own DSL as Kafka streams in Java has, but just python functions. It uses rocksdb to support tables. For the installation, in your python environment do a pipenv run pip install faust , or pip install faust . Then use faust as a CLI. So to start an agent as worker use: faust -A nameofthepythoncode -l info Multiple instances of a Faust worker can be started independently to distribute stream processing across machines and CPU cores. Further readings \u00b6 The API and product documentation . Kafka Streams concepts from Confluent Deep dive explanation for the differences between KStream and KTable from Michael Noll Our set of samples to getting started in coding kafka streams Distributed, Real-time Joins and Aggregations using Kafka Stream, from Michael Noll at Confluent Confluent Kafka Streams documentation Kafka Streams architecture article from Confluent . Andy Bryant's article on kafka stream work allocation and sub-topologies","title":"Kafka Streams"},{"location":"technology/kafka-streams/#concepts","text":"The business logic is implemented via topology that represents a graph of processing nodes. Each node within the graph, processes events from the parent node. To summarize, Kafka Streams has the following capabilities: Kafka Streams applications are built on top of producer and consumer APIs and are leveraging Kafka capabilities to do data parallelism processing, support distributed coordination of partition to task assignment, and being fault tolerant. Streams processing is helpful for handling out-of-order data, reprocessing input as code changes, and performing stateful computations, like real time analytics. It uses producer / consumer APIs, stateful storage and consumer groups. It treats both past and future data the same way. Kafka Streams is an embedded library to integrate in your Java application. No need for separate processing cluster. As deployable container it can scale horizontally easily within Kubernetes platform. It does not run in Kafka cluster. Topology consumes continuous real time flows of records and publishes new flows to one or more topics. A stream (represented by the KStream API) is a durable, partitioned sequence of immutable events. When a new event is added a stream, it's appended to the partition that its key belongs to. It can scale vertically, by increasing the number of threads for each Kafka Streams application on a single machine, and horizontally by adding additional machines or pods in kubernetes. Each deployed instance use the same value for the application.id kafka stream property. The assignment of stream partitions to stream tasks never changes, so task is the unit of parallelism. Task executes the topology, and is buffering records coming from the attached partitions. KTable is a durable, partitioned collection that models change over time. It's the mutable counterpart of KStreams. It represents what is true at the current moment. Each data record is considered a contextual update. Tables are saved in state store backed up with kafka topic and are queryables. Any operation on the table such as querying, inserting, or updating a row is carried out behind the scenes by a corresponding operation on the table\u2019s state store. These state stores are being materialized on local disk inside your application instances It supports exactly-once processing semantics to guarantee that each record is processed once and only once even when there is a failure. Stream APIs transform, aggregate and enrich data, per record with milli second latency, from one topic to another one. Supports stateful and windowing operations by processing one record at a time. An application's processor topology is scaled by breaking it into multiple tasks. Tasks can then instantiate their own processor topology based on the assigned partitions.","title":"Concepts"},{"location":"technology/kafka-streams/#fault-tolerance","text":"As KTables are persisted on state store, they are materialized on local to broker disk, as change log streams: In the case of a stream processing task fails, it can rebuild its internal, in memory state store from the kafka topic / change log. Once done it can reconsume messages. The system is fault tolerant.","title":"Fault tolerance"},{"location":"technology/kafka-streams/#scaling","text":"When topics have multiple partitions, each kafka streams task consumes a unique partition. If for any reasons, we need to scale by adding new instances of the application, so in term of kubernetes, adding more pods, then the system will rebalance the stream tasks allocation to new instances created. We can start as many threads of the application as there are input Kafka topic partitions. Another good example to illustrate threading, task and machine scaling is documented in this on Confluent article .","title":"Scaling"},{"location":"technology/kafka-streams/#code-structure","text":"In general the code for processing event does the following: Set a properties object to specify which brokers to connect to and what kind of key and value des/serialization mechanisms to use. Define a stream client: if you want to get the stream of records use KStream , if you want a changelog with the last value of a given key use KTable (For example, using KTable to keep a user profile data by userid key). Create a topology of input source and sink target and the set of actions to perform in between. Start the stream client to consume records. Programming with KStream and Ktable is not easy at first, as there are a lot of concepts for data manipulations, serialization and operations chaining. A stateful operator uses the streaming Domain Specific Language, with constructs for aggregation, join and time window operations. Stateful transformations require a state store associated with the stream processor. We recommend at this stage to do our exercise 1 from the Tech Academy tutorial to develop a first simple topology and test it without any Kafka cluster using the Topology test driver . The following code extract, is part of the Apache Kafka Word count example and is used to illustrate the programming model used: // Streams processing are created from a builder. final StreamsBuilder builder = new StreamsBuilder (); // pattern to extract word final Pattern pattern = Pattern . compile ( \"\\\\W+\" ); // source is a kafka topic, and materialized as a KStream KStream < String , String > textLines = builder . stream ( source ); // implement the logic to count words KTable < String , Long > wordCounts = textLines . flatMapValues ( textLine -> Arrays . asList ( pattern . split ( textLine . toLowerCase ()))) . print ( Printed . toSysOut ()) . groupBy (( key , word ) -> word ) . count ( Materialized . < String , Long , KeyValueStore < Bytes , byte []>> as ( \"counts-store\" )); // sink is another kafka topic. Produce for each word the number of occurrence in the given doc wordCounts . toStream (). to ( sink , Produced . with ( Serdes . String (), Serdes . Long ())); KafkaStreams streams = new KafkaStreams ( builder . build (), props ); streams . start (); KStream represents KeyValue records coming as event stream from the input topic. flatMapValues() transforms the value of each record in \"this\" stream into zero or more values with the same key in a new KStream (in memory). So here the text line is split into words. The parameter is a ValueMapper which applies transformation on values but keeps the key. Another important transformation is the KeyValueMapper . groupBy() Group the records of this KStream on a new key that is selected using the provided KeyValueMapper. So here it creates new KStream with the extracted word as key. count() counts the number of records in this stream by the grouped key. Materialized is an class to define a \"store\" to persist state and data. So here the state store is \"counts-store\". As store is a in-memory table, but it could also be persisted in external database. Could be the Facebook's RocksDB key value persistence or a log-compacted topic in Kafka. Produced defines how to provide the optional parameter types when producing to new topics. KTable is an abstraction of a changelog stream from a primary-keyed table. Important: map, flatMapValues and mapValues ... functions don\u2019t modify the object or value presented as a parameter.","title":"Code structure"},{"location":"technology/kafka-streams/#available-tutorials","text":"We found the following tutorial helpful to grow your competency on Kafka Streams: Word count Kafka Stream example from product documentation Use Quarkus and Kafka Streams to use groupBy, join with another Stream Quarkus and Kafka Streams guides Build an inventory aggregator with Quarkus, with kstreams, ktable and interactive queries, Mutiny , all deployable on OpenShift with quarkus kubernetes plugin.","title":"Available tutorials"},{"location":"technology/kafka-streams/#interactive-queries","text":"State store can be queried, and this is supported by the interactive queries . Result can be from the local store, if the key is in the local store, or a remote one. The metadata of the key to task allocation is maintained and shared between tasks. As a Kafka stream app runs on multiple instances, the entire state of the app is distributed among the instances. The Stream topology will transform the stream to table with one of the groupBy or aggregate operation: items . groupByKey ( ItemStream . buildGroupDefinition ()) . aggregate ( () -> new Inventory (), ( k , newItem , existingInventory ) -> existingInventory . updateStockQuantity ( k , newItem ), InventoryAggregate . materializeAsInventoryStore ()); which then it can be materialized as queryable key - value store. // class InventoryAggregate /** * Create a key value store named INVENTORY_STORE_NAME to persist store inventory */ public static Materialized < String , Inventory , KeyValueStore < Bytes , byte []>> materializeAsInventoryStore () { return Materialized . < String , Inventory , KeyValueStore < Bytes , byte []>> as ( INVENTORY_STORE_NAME ) . withKeySerde ( Serdes . String ()). withValueSerde ( inventorySerde ); } Each store is local to the instance it was created in: The storeID is the key used in the KTable. Once the Kafka Stream is started and store created (loop to get it ready) then it is easy to access it: @Inject KafkaStreams streams ; private ReadOnlyKeyValueStore < String , Inventory > getInventoryStockStore () { while ( true ) { try { StoreQueryParameters < ReadOnlyKeyValueStore < String , Inventory >> parameters = StoreQueryParameters . fromNameAndType ( InventoryAggregate . INVENTORY_STORE_NAME , QueryableStoreTypes . keyValueStore ()); return streams . store ( parameters ); ... // access one element of the store Inventory result = getInventoryStockStore (). get ( storeID ); To get access to remote store, we need to expose each store via an API. The easiest one is a REST api, but it could be any RPC protocol. Each instance is uniquely identified via the application.server property. When deploying on kubernetes it could be the pod IP address accessible via the $POD_IP environment variable. Below is a quarkus declaration: hostname = ${POD_IP:localhost} quarkus.kafka-streams.application-server = ${hostname}:8080 Now the design decision is to return the URL of the remote instance to the client doing the query call or do the call internally to the instnace reached to always returning a result. The knowledge of other application instance is done by sharing metadata. The following code example illustrates the access to metadata for store via the kafka stream context and then build a pipeline medata to share information about host, port and partition allocation. streams . allMetadataForStore ( ItemStream . ITEMS_STORE_NAME ) . stream () . map ( m -> new PipelineMetadata ( m . hostInfo (). host () + \":\" + m . hostInfo (). port (), m . topicPartitions () . stream () . map ( TopicPartition :: toString ) . collect ( Collectors . toSet ()))) . collect ( Collectors . toList ());","title":"Interactive queries"},{"location":"technology/kafka-streams/#design-considerations","text":"Partitions are assigned to a StreamTask, and each StreamTask has its own state store. So it is important to use key and kafka will assign records with same key to same partition so lookup inside state store will work. Avoid external database lookup as part of the stream: As kafka can handle million of records per second, so a lookup to an external database to do a join between a primary key that is in the event and a table in the database to do a data enrichment, for example, is a bad practice. The approach will be to use Ktable, with state store and perform a join in memory. Reference data can be loaded inside a Ktable for event stream enrichment. Table and streams joins: we recommend reading this deep dive article on joining between streams and joining stream with table. The important points from this article: kstream - kstream joins are windowed to control the size of data to keep in memory to search for the matching records.","title":"Design considerations"},{"location":"technology/kafka-streams/#faust-a-python-library-to-do-kafka-streaming","text":"Faust is a python library to support stream processing. It does not have its own DSL as Kafka streams in Java has, but just python functions. It uses rocksdb to support tables. For the installation, in your python environment do a pipenv run pip install faust , or pip install faust . Then use faust as a CLI. So to start an agent as worker use: faust -A nameofthepythoncode -l info Multiple instances of a Faust worker can be started independently to distribute stream processing across machines and CPU cores.","title":"Faust: a python library to do kafka streaming"},{"location":"technology/kafka-streams/#further-readings","text":"The API and product documentation . Kafka Streams concepts from Confluent Deep dive explanation for the differences between KStream and KTable from Michael Noll Our set of samples to getting started in coding kafka streams Distributed, Real-time Joins and Aggregations using Kafka Stream, from Michael Noll at Confluent Confluent Kafka Streams documentation Kafka Streams architecture article from Confluent . Andy Bryant's article on kafka stream work allocation and sub-topologies","title":"Further readings"},{"location":"technology/mq/","text":"Warning Updated 7/13/2022- Work in progress IBM MQ is the enterprise solution to exchange message over queues. As it supports loosely coupling communication between applications, via asynchronous protocol, and message exchange, it has to be part of any modern digital, responsive solutions, and so it makes sense to write about it in the context of EDA. This note is to summarize, for architects, the technology as it fits into EDA and gives pointers to important documentations, articles, and code repositories for using MQ. We already addressed the difference between event and messaging systems, and we can affirm that real production plaform needs to include both. This site includes a lot of content around Kafka as the backbone to support EDA, but EDA is not just Kafka. I will prefer to mention that EDA is about modern asynchronous microservice based solution, that need to exchange messages. Messages can be sent to MQ or Kafka or both. IBM MQ delivers different features than Kafka and it is important to assess the fit for purpose. MQ queue managers are the main component to define queues and where applications connect to. They can be organized in network to deliver messages between applications and locations. Queue Managers can be organized in cluster to increase high availability and scaling. Concepts to keep in mind \u00b6 We encourage to read the article from Richard Coppen's: 'IBM MQ fundamentals' . Queues are addressable locations to deliver messages to and store them reliably until they need to be consumed. We can have many queues and topics on one queue manager Queue managers are the MQ servers that host the queues. They can be interconnected via MQ network. Channels are the way queue managers communicate with each other and with the applications. MQ networks are loose collections of interconnected queue managers, all working together to deliver messages between applications and locations. MQ clusters are tight couplings of queue managers, enabling higher levels of scaling and availability Point to point for a single consumer. Senders produce messages to a queue, and receivers asynchronously consume messages from that queue. With multiple receivers, each message is only consumed by one receiver, distributing the workload across them all. Publish/subscribe is supported via topic and subscription, and MQ sends copies of the message to those subscribing applications Major MQ benefits in EDA \u00b6 MQ provides assured delivery of data : No data loss and no duplication, strong support of exactly once. MQ is horizontally scalable : As the workload for a single queue manager increases, it is easy to add more queue managers to share tasks and distribute the messages across them. Highly available (See section below ) with scalable architecture with different topologies Integrate well with Mainframe to propagate transaction to the eventual consistent world of cloud native distributed applications. Writing to database and MQ queue is part of the same transaction, which simplifies the injection into event backbone like Kafka, via Kafka MQ connector. Containerized to run on modern kubernetes platform. Decentralized architecture \u00b6 The figure below illustrates the different ways to organize the MQ brokers according to the applications' needs. On the top row, applications have decoupled queue managers, with independent availability / scalability. The ownership is decentralized, as each application owner also owns the broker configuration and deployment. Such cloud native application may adopt the Command Query Responsability Seggregation pattern and use queues to propagage information between the microservices. The deployment of both broker and microservices follows the same CI/CD pipeline, with a kustomize , for example, to describe the broker configuration. See the CQRS with MQ implementation , we did for the Reefer manager service in the vaccine solution. A central MQ broker can still be part of the architecture to support legacy applications integrations and federated queues. This type of deployment supports heterogenous operational procedures across technologies. High availability \u00b6 As introduced in multiple blogs and product documentation, we use the high availability definition of a capability of a system to be operational for a greater proportion of time than other available apps. In term of measurement, we are talking about 99.999% which is less than 5 minutes a year (99.99% is 1 hour/yr). Important to always revisit the requirements, in term of availability measurement for any application modernization projects. For a messaging system it is important to consider three characteristics: Redundancy so applications can connect in case of broker failure. Applications locally bound to a queue manager will limit availability. Recommended to use remote MQ client connection, and considering automatic client reconnection . Message routing : always deliver message even with failures. Message availability : not loosing messages and always readable. With IBM MQ on multiplatforms, a message is stored on exactly one queue manager. To achieve high message availability, you need to be able to recover a queue manager as quickly as possible. You can achieve service availability by having multiple instances of queue manager for client applications to use, for example by using an IBM MQ uniform cluster. A set of MQ topology can be defined to support HA: Single resilient queue manager: MQ broker runs in a VM or a single container, and if it stops the VM or pod scheduler will restart it. This is using the platform resynch capability combined with HA storage . IP Address is kept between the instances. The queue content is saved to a storage supporting HA. In the case of container, new restarted pod will connect to existing storage, and the IP gateway routes traffic to the active instance via service and app selector. Multi-instance queue manager: active - standby topology - Failover is triggered on failure of the active instance. IP Address is also kept. When using k8s, the stand-by broker is on a separate node, ready to be activated. The pods use persistence volumes with ReadWriteMany settings. Replicated data queue manager: this is an extension of the previous pattern where data saved locally is replicated to other sites. The deployed MQ broker is defined in k8s as a StatefulSet which may not restart automatically in case of node failure. So there is a time to fail over, which is not the case with the full replication mechanism of Kafka. A service will provide consistent network identity to the MQ broker. On kubernetes, MQ relies on the availability of the data on the persistent volumes. The availability of the storage providing the persistent volumes, defines IBM MQ availability. For multi-instance deployment, the shared file system must support write through to disk on flush operation, to keep transaction integrity (ensure writes have been safely committed before acknowledging the transaction), must support exclusive access to files so the queue managers write access is synchronized. Also it needs to support releasing locks in case of failure. See product documentation for testing message integrity on file systems. Active-active with uniform cluster \u00b6 With Uniform Cluster and Client Channel Definition Tables it will be possible to achieve high availability with at least three brokers and multiple application instances accessing brokers group via the CCDT. The queue managers are configured almost identically, and application interacts with the group. You can have as many application instances as there are queue managers in the cluster. Here are the main benefits of Uniform Cluster: A directory of all clustering resources, discoverable by any member in a cluster Automatic channel creation and connectivity Horizontal scaling across multiple matching queues, using message workload balancing Dynamic message routing, based on availability The brokers are communicating their states between each others, and connection rebalancing can be done behind the scene without application knowledge. In case of a Queue manager failure, the connections are rebalanced to the active ones. Those applications do not need strong ordering. With the same approach, we can add new Queue manager See this video from David Ware abour active - active with Uniform cluster to see how this rebalancing works between queue managers as part of a Uniform queue manager. See the 'MQ Uniform Cluster' related repository . Native HA \u00b6 Native HA queue managers involve an active and two replica Kubernetes Pods , which run as part of a Kubernetes StatefulSet with exactly three replicas each with their own set of Kubernetes Persistent Volumes. Native HA provides built in replication of messages and state across multiple sets of storage, removing the dependency of replication and locking from the file system. Each replica writes to its own recovery log, acknowledges the data, and then updates its own queue data from the replicated recovery log. A Kubernetes Service is used to route TCP/IP client connections to the current active instance. Set the availability in the queueManager configuration. queueManager : availability : type : NativeHA Disaster recovery \u00b6 For always on deployment we need three data center and active-active on the three data center. So how is it supported with MQ? Installation with Cloud Pak for Integration \u00b6 Starting with release 2020.2, MQ can be installed via Kubernetes Operator on Openshift platform. From the operator catalog search for MQ. See the product documentation installation guide for up to date details. You can verify your installation with the following CLI, and get the IBM catalogs accessible: oc project openshift-marketplace oc get CatalogSource NAME DISPLAY TYPE PUBLISHER AGE certified-operators Certified Operators grpc Red Hat 42d community-operators Community Operators grpc Red Hat 42d ibm-operator-catalog ibm-operator-catalog grpc IBM Content 39d opencloud-operators IBMCS Operators grpc IBM 39d redhat-marketplace Red Hat Marketplace grpc Red Hat 42d redhat-operators Red Hat Operators grpc Red Hat 42d Once everything is set up, create an operator. The IBM MQ operator can be installed scoped to a single namespace or to monitor All namespaces . Verify your environment fits the deployment. Prepare your Red Hat OpenShift Container Platform for MQ Then once the operator is installed (it could take up to a minute), go to the operator page and create a MQ Manager instance. For example be sure to have defined an ibm-entitlement-key in the project you are planning to use to deploy MQ manager. Then update the Yaml file for name, license and persistence. As an alternate, define a QueueManager manifest yaml file as: ```yaml apiVersion: mq.ibm.com/v1beta1 kind: QueueManager metadata: name: eda-mq-lab spec: version: 9.2.5.0-r3 license: accept: true license: use: NonProduction web: enabled: true queueManager: name: \"EDAQMGR1\" storage: queueManager: type: ephemeral template: pod: containers: - name: qmgr env: - name: MQSNOAUT value: \"yes\" Then create the QueueManager resource with `oc or kubectl cli`: ```shell oc apply -f mq-manager.yaml oc get queuemanager # Get the UI route oc describe queuemanager eda-mq-lab You should get the console from this URL: https://eda-mq-lab-ibm-mq-web-....containers.appdomain.cloud/ibmmq/console/#/ To access to the mqsc CLI and run configuration remote connect via oc exec -it <podname> bash . We need route to access MQ from outside of OpenShift. The connection uses TLS1.2. you can customize the environment using config map: queueManager : name : QM1 mqsc : - configMap : name : mq-mqsc-config items : - example.mqsc # ... template : pod : containers : - name : qmgr env : - name : MQSNOAUT value : 'yes' envFrom : - configMapRef : name : mq-config And apiVersion : v1 kind : ConfigMap metadata : name : mq-config data : LICENSE : accept MQ_APP_PASSWORD : passw0rd MQ_ENABLE_METRICS : \"true\" MQ_QMGR_NAME : QM1 --- apiVersion : v1 kind : ConfigMap metadata : name : mq-mqsc-config data : example.mqsc : | DEFINE QLOCAL('ITEMS') REPLACE DEFINE CHANNEL('DEV.ADMIN.SVRCONN') CHLTYPE(SVRCONN) REPLACE DEFINE QLOCAL('DEV.DEAD.LETTER.QUEUE') REPLACE ALTER QMGR DEADQ('DEV.DEAD.LETTER.QUEUE') DEFINE CHANNEL(DEV.APP.SVRCONN) CHLTYPE(SVRCONN) ALTER QMGR CHLAUTH (DISABLED) REFRESH SECURITY TYPE(CONNAUTH) Running MQ in docker \u00b6 The following recent article from Richard J. Coppen presents such deployment, and can be summarized as: # Use Docker to create a volume: docker volume create qm1data # Start queue manager: QM1 docker run --env LICENSE = accept --env MQ_QMGR_NAME = QM1 --volume qm1data:/mnt/mqm --name mq --rm --publish 1414 :1414 --publish 9443 :9443 --detach --env MQ_APP_PASSWORD = passw0rd ibmcom/mq:latest # The queue manager\u2019s listener listens on port 1414 for incoming connections and port 9443 is used by MQ console One queue is created DEV.QUEUE.1 and a channel: DEV.APP.SRVCONN. Then docker exec on the docker container and use the mqsc CLI. The ibm-messaging/mq-container github repository describes properties and different configurations. You can also run it via docker compose. We have different flavor in the real time inventory gitops repository under local-demo folder. Here is an example of such compose file: version : '3.7' services : ibmmq : image : ibmcom/mq ports : - '1414:1414' - '9443:9443' - '9157:9157' volumes : - qm1data:/mnt/mqm stdin_open : true tty : true restart : always environment : LICENSE : accept MQ_QMGR_NAME : QM1 MQ_APP_PASSWORD : passw0rd MQ_ENABLE_METRICS : \"true\" Getting access to the MQ Console \u00b6 The MQ Console is a web browser based interface for interacting with MQ objects. It comes pre-configured inside the developer version of MQ in a container. On localhost deployment the URL is https://localhost:9443/ibmmq/console/ (user admin) while on OpenShift it depends of the Route created. See this article for a very good overview for using the console. From the console we can define access and configuration: A new channel called MQ.QUICKSTART.SVRCONN. This new channel will be configured with NO MCA user. An MCA user is the identity that is used for all communication on that channel. As we are setting no MCA user this means that the identity used to connect to the queue manager will be used for MQ authorization. A channel authority record set to block no-one. We will do this so that any authority records you add in the following security step will be automatically configured to allow access the queue manager and resources below. When an application connects to a queue manager it will present an identity. That identity needs two permissions; one to connect to the queue manager and one to put/get messages from the queue. Some useful CLI \u00b6 Those commands can be run inside the docker container: oc exec -ti mq1-cp4i-ibm-mq-0 -n cp4i-mq1 bash # Display MQ version dspmqver # Display your running queue managers dspmq To access to log errors oc rsh <to-mq-broker-pod> # use you QM manager name instead of BIGGSQMGR cd /var/mqm/qmgrs/BIGGSQMGR/errors cat AMQERR01.LOG Connecting your application \u00b6 JMS should be your first choice to integrate a Java application to MQ. The mq-dev-patterns includes JMS code samples for inspiration. See also this IBM developer tutorial and our code example from the store simulator used in different MQ to Kafka labs. The article seems to have issue in links and syntax, below is the updated steps: # get the file mkdir -p com/ibm/mq/samples/jms && cd com/ibm/mq/samples/jms curl -o JmsPuGet.java https://raw.githubusercontent.com/ibm-messaging/mq-dev-samples/master/gettingStarted/jms/com/ibm/mq/samples/jms/JmsPutGet.java cd ../../../../.. # Modify the connection setting in the code # Compile javac -cp com.ibm.mq.allclient-9.2.1.0.jar:javax.jms-api-2.0.1.jar com/ibm/mq/samples/jms/JmsPutGet.java # Run it java -cp com.ibm.mq.allclient-9.2.1.0.jar:javax.jms-api-2.0.1.jar:. com.ibm.mq.samples.jms.JmsPutGet To connect to MQ server, you need to get the hostname for the Queue manager, the port number, the channel and the queue name to access. For Quarkus and Maven use the following dependencies: <dependency> <groupId> javax.jms </groupId> <artifactId> javax.jms-api </artifactId> <version> 2.0.1 </version> </dependency> <dependency> <groupId> com.ibm.mq </groupId> <artifactId> com.ibm.mq.allclient </artifactId> <version> 9.2.1.0 </version> </dependency> For automatic client reconnection, use the JMS connection factory configuration. Client Channel Definition Tables \u00b6 CCDT provides encapsulation and abstraction of connection information for applications, hiding the MQ architecture and configuration from the application. CCDT defines which real queue managers the application will connect to. Which could be a single queue manager or a group of queue managers. \"channel\" : [ { \"name\" : \"STORE.CHANNEL\" , \"clientConnection\" : { \"connection\" : [ { \"host\" : \"mq1-cp4i-ibm-mq-qm-cp4i-mq1........com\" , \"port\" : \"443\" } ], \"queueManager\" : \"BIGGSQMGR\" }, \"type\" : \"clientConnection\" } ] } Important readings MQ family page MQ 9.3 product documentation Article for developer from Richard Coppen's: 'IBM MQ fundamentals' MQ on Container Learning path: IBM MQ Developer Developer cheat sheet Code repositories \u00b6 Store simulator - JMS producer to MQ AMQP and reactive messaging","title":"MQ in EDA context"},{"location":"technology/mq/#concepts-to-keep-in-mind","text":"We encourage to read the article from Richard Coppen's: 'IBM MQ fundamentals' . Queues are addressable locations to deliver messages to and store them reliably until they need to be consumed. We can have many queues and topics on one queue manager Queue managers are the MQ servers that host the queues. They can be interconnected via MQ network. Channels are the way queue managers communicate with each other and with the applications. MQ networks are loose collections of interconnected queue managers, all working together to deliver messages between applications and locations. MQ clusters are tight couplings of queue managers, enabling higher levels of scaling and availability Point to point for a single consumer. Senders produce messages to a queue, and receivers asynchronously consume messages from that queue. With multiple receivers, each message is only consumed by one receiver, distributing the workload across them all. Publish/subscribe is supported via topic and subscription, and MQ sends copies of the message to those subscribing applications","title":"Concepts to keep in mind"},{"location":"technology/mq/#major-mq-benefits-in-eda","text":"MQ provides assured delivery of data : No data loss and no duplication, strong support of exactly once. MQ is horizontally scalable : As the workload for a single queue manager increases, it is easy to add more queue managers to share tasks and distribute the messages across them. Highly available (See section below ) with scalable architecture with different topologies Integrate well with Mainframe to propagate transaction to the eventual consistent world of cloud native distributed applications. Writing to database and MQ queue is part of the same transaction, which simplifies the injection into event backbone like Kafka, via Kafka MQ connector. Containerized to run on modern kubernetes platform.","title":"Major MQ benefits in EDA"},{"location":"technology/mq/#decentralized-architecture","text":"The figure below illustrates the different ways to organize the MQ brokers according to the applications' needs. On the top row, applications have decoupled queue managers, with independent availability / scalability. The ownership is decentralized, as each application owner also owns the broker configuration and deployment. Such cloud native application may adopt the Command Query Responsability Seggregation pattern and use queues to propagage information between the microservices. The deployment of both broker and microservices follows the same CI/CD pipeline, with a kustomize , for example, to describe the broker configuration. See the CQRS with MQ implementation , we did for the Reefer manager service in the vaccine solution. A central MQ broker can still be part of the architecture to support legacy applications integrations and federated queues. This type of deployment supports heterogenous operational procedures across technologies.","title":"Decentralized architecture"},{"location":"technology/mq/#high-availability","text":"As introduced in multiple blogs and product documentation, we use the high availability definition of a capability of a system to be operational for a greater proportion of time than other available apps. In term of measurement, we are talking about 99.999% which is less than 5 minutes a year (99.99% is 1 hour/yr). Important to always revisit the requirements, in term of availability measurement for any application modernization projects. For a messaging system it is important to consider three characteristics: Redundancy so applications can connect in case of broker failure. Applications locally bound to a queue manager will limit availability. Recommended to use remote MQ client connection, and considering automatic client reconnection . Message routing : always deliver message even with failures. Message availability : not loosing messages and always readable. With IBM MQ on multiplatforms, a message is stored on exactly one queue manager. To achieve high message availability, you need to be able to recover a queue manager as quickly as possible. You can achieve service availability by having multiple instances of queue manager for client applications to use, for example by using an IBM MQ uniform cluster. A set of MQ topology can be defined to support HA: Single resilient queue manager: MQ broker runs in a VM or a single container, and if it stops the VM or pod scheduler will restart it. This is using the platform resynch capability combined with HA storage . IP Address is kept between the instances. The queue content is saved to a storage supporting HA. In the case of container, new restarted pod will connect to existing storage, and the IP gateway routes traffic to the active instance via service and app selector. Multi-instance queue manager: active - standby topology - Failover is triggered on failure of the active instance. IP Address is also kept. When using k8s, the stand-by broker is on a separate node, ready to be activated. The pods use persistence volumes with ReadWriteMany settings. Replicated data queue manager: this is an extension of the previous pattern where data saved locally is replicated to other sites. The deployed MQ broker is defined in k8s as a StatefulSet which may not restart automatically in case of node failure. So there is a time to fail over, which is not the case with the full replication mechanism of Kafka. A service will provide consistent network identity to the MQ broker. On kubernetes, MQ relies on the availability of the data on the persistent volumes. The availability of the storage providing the persistent volumes, defines IBM MQ availability. For multi-instance deployment, the shared file system must support write through to disk on flush operation, to keep transaction integrity (ensure writes have been safely committed before acknowledging the transaction), must support exclusive access to files so the queue managers write access is synchronized. Also it needs to support releasing locks in case of failure. See product documentation for testing message integrity on file systems.","title":"High availability"},{"location":"technology/mq/#active-active-with-uniform-cluster","text":"With Uniform Cluster and Client Channel Definition Tables it will be possible to achieve high availability with at least three brokers and multiple application instances accessing brokers group via the CCDT. The queue managers are configured almost identically, and application interacts with the group. You can have as many application instances as there are queue managers in the cluster. Here are the main benefits of Uniform Cluster: A directory of all clustering resources, discoverable by any member in a cluster Automatic channel creation and connectivity Horizontal scaling across multiple matching queues, using message workload balancing Dynamic message routing, based on availability The brokers are communicating their states between each others, and connection rebalancing can be done behind the scene without application knowledge. In case of a Queue manager failure, the connections are rebalanced to the active ones. Those applications do not need strong ordering. With the same approach, we can add new Queue manager See this video from David Ware abour active - active with Uniform cluster to see how this rebalancing works between queue managers as part of a Uniform queue manager. See the 'MQ Uniform Cluster' related repository .","title":"Active-active with uniform cluster"},{"location":"technology/mq/#native-ha","text":"Native HA queue managers involve an active and two replica Kubernetes Pods , which run as part of a Kubernetes StatefulSet with exactly three replicas each with their own set of Kubernetes Persistent Volumes. Native HA provides built in replication of messages and state across multiple sets of storage, removing the dependency of replication and locking from the file system. Each replica writes to its own recovery log, acknowledges the data, and then updates its own queue data from the replicated recovery log. A Kubernetes Service is used to route TCP/IP client connections to the current active instance. Set the availability in the queueManager configuration. queueManager : availability : type : NativeHA","title":"Native HA"},{"location":"technology/mq/#disaster-recovery","text":"For always on deployment we need three data center and active-active on the three data center. So how is it supported with MQ?","title":"Disaster recovery"},{"location":"technology/mq/#installation-with-cloud-pak-for-integration","text":"Starting with release 2020.2, MQ can be installed via Kubernetes Operator on Openshift platform. From the operator catalog search for MQ. See the product documentation installation guide for up to date details. You can verify your installation with the following CLI, and get the IBM catalogs accessible: oc project openshift-marketplace oc get CatalogSource NAME DISPLAY TYPE PUBLISHER AGE certified-operators Certified Operators grpc Red Hat 42d community-operators Community Operators grpc Red Hat 42d ibm-operator-catalog ibm-operator-catalog grpc IBM Content 39d opencloud-operators IBMCS Operators grpc IBM 39d redhat-marketplace Red Hat Marketplace grpc Red Hat 42d redhat-operators Red Hat Operators grpc Red Hat 42d Once everything is set up, create an operator. The IBM MQ operator can be installed scoped to a single namespace or to monitor All namespaces . Verify your environment fits the deployment. Prepare your Red Hat OpenShift Container Platform for MQ Then once the operator is installed (it could take up to a minute), go to the operator page and create a MQ Manager instance. For example be sure to have defined an ibm-entitlement-key in the project you are planning to use to deploy MQ manager. Then update the Yaml file for name, license and persistence. As an alternate, define a QueueManager manifest yaml file as: ```yaml apiVersion: mq.ibm.com/v1beta1 kind: QueueManager metadata: name: eda-mq-lab spec: version: 9.2.5.0-r3 license: accept: true license: use: NonProduction web: enabled: true queueManager: name: \"EDAQMGR1\" storage: queueManager: type: ephemeral template: pod: containers: - name: qmgr env: - name: MQSNOAUT value: \"yes\" Then create the QueueManager resource with `oc or kubectl cli`: ```shell oc apply -f mq-manager.yaml oc get queuemanager # Get the UI route oc describe queuemanager eda-mq-lab You should get the console from this URL: https://eda-mq-lab-ibm-mq-web-....containers.appdomain.cloud/ibmmq/console/#/ To access to the mqsc CLI and run configuration remote connect via oc exec -it <podname> bash . We need route to access MQ from outside of OpenShift. The connection uses TLS1.2. you can customize the environment using config map: queueManager : name : QM1 mqsc : - configMap : name : mq-mqsc-config items : - example.mqsc # ... template : pod : containers : - name : qmgr env : - name : MQSNOAUT value : 'yes' envFrom : - configMapRef : name : mq-config And apiVersion : v1 kind : ConfigMap metadata : name : mq-config data : LICENSE : accept MQ_APP_PASSWORD : passw0rd MQ_ENABLE_METRICS : \"true\" MQ_QMGR_NAME : QM1 --- apiVersion : v1 kind : ConfigMap metadata : name : mq-mqsc-config data : example.mqsc : | DEFINE QLOCAL('ITEMS') REPLACE DEFINE CHANNEL('DEV.ADMIN.SVRCONN') CHLTYPE(SVRCONN) REPLACE DEFINE QLOCAL('DEV.DEAD.LETTER.QUEUE') REPLACE ALTER QMGR DEADQ('DEV.DEAD.LETTER.QUEUE') DEFINE CHANNEL(DEV.APP.SVRCONN) CHLTYPE(SVRCONN) ALTER QMGR CHLAUTH (DISABLED) REFRESH SECURITY TYPE(CONNAUTH)","title":"Installation with Cloud Pak for Integration"},{"location":"technology/mq/#running-mq-in-docker","text":"The following recent article from Richard J. Coppen presents such deployment, and can be summarized as: # Use Docker to create a volume: docker volume create qm1data # Start queue manager: QM1 docker run --env LICENSE = accept --env MQ_QMGR_NAME = QM1 --volume qm1data:/mnt/mqm --name mq --rm --publish 1414 :1414 --publish 9443 :9443 --detach --env MQ_APP_PASSWORD = passw0rd ibmcom/mq:latest # The queue manager\u2019s listener listens on port 1414 for incoming connections and port 9443 is used by MQ console One queue is created DEV.QUEUE.1 and a channel: DEV.APP.SRVCONN. Then docker exec on the docker container and use the mqsc CLI. The ibm-messaging/mq-container github repository describes properties and different configurations. You can also run it via docker compose. We have different flavor in the real time inventory gitops repository under local-demo folder. Here is an example of such compose file: version : '3.7' services : ibmmq : image : ibmcom/mq ports : - '1414:1414' - '9443:9443' - '9157:9157' volumes : - qm1data:/mnt/mqm stdin_open : true tty : true restart : always environment : LICENSE : accept MQ_QMGR_NAME : QM1 MQ_APP_PASSWORD : passw0rd MQ_ENABLE_METRICS : \"true\"","title":"Running MQ in docker"},{"location":"technology/mq/#getting-access-to-the-mq-console","text":"The MQ Console is a web browser based interface for interacting with MQ objects. It comes pre-configured inside the developer version of MQ in a container. On localhost deployment the URL is https://localhost:9443/ibmmq/console/ (user admin) while on OpenShift it depends of the Route created. See this article for a very good overview for using the console. From the console we can define access and configuration: A new channel called MQ.QUICKSTART.SVRCONN. This new channel will be configured with NO MCA user. An MCA user is the identity that is used for all communication on that channel. As we are setting no MCA user this means that the identity used to connect to the queue manager will be used for MQ authorization. A channel authority record set to block no-one. We will do this so that any authority records you add in the following security step will be automatically configured to allow access the queue manager and resources below. When an application connects to a queue manager it will present an identity. That identity needs two permissions; one to connect to the queue manager and one to put/get messages from the queue.","title":"Getting access to the MQ Console"},{"location":"technology/mq/#some-useful-cli","text":"Those commands can be run inside the docker container: oc exec -ti mq1-cp4i-ibm-mq-0 -n cp4i-mq1 bash # Display MQ version dspmqver # Display your running queue managers dspmq To access to log errors oc rsh <to-mq-broker-pod> # use you QM manager name instead of BIGGSQMGR cd /var/mqm/qmgrs/BIGGSQMGR/errors cat AMQERR01.LOG","title":"Some useful CLI"},{"location":"technology/mq/#connecting-your-application","text":"JMS should be your first choice to integrate a Java application to MQ. The mq-dev-patterns includes JMS code samples for inspiration. See also this IBM developer tutorial and our code example from the store simulator used in different MQ to Kafka labs. The article seems to have issue in links and syntax, below is the updated steps: # get the file mkdir -p com/ibm/mq/samples/jms && cd com/ibm/mq/samples/jms curl -o JmsPuGet.java https://raw.githubusercontent.com/ibm-messaging/mq-dev-samples/master/gettingStarted/jms/com/ibm/mq/samples/jms/JmsPutGet.java cd ../../../../.. # Modify the connection setting in the code # Compile javac -cp com.ibm.mq.allclient-9.2.1.0.jar:javax.jms-api-2.0.1.jar com/ibm/mq/samples/jms/JmsPutGet.java # Run it java -cp com.ibm.mq.allclient-9.2.1.0.jar:javax.jms-api-2.0.1.jar:. com.ibm.mq.samples.jms.JmsPutGet To connect to MQ server, you need to get the hostname for the Queue manager, the port number, the channel and the queue name to access. For Quarkus and Maven use the following dependencies: <dependency> <groupId> javax.jms </groupId> <artifactId> javax.jms-api </artifactId> <version> 2.0.1 </version> </dependency> <dependency> <groupId> com.ibm.mq </groupId> <artifactId> com.ibm.mq.allclient </artifactId> <version> 9.2.1.0 </version> </dependency> For automatic client reconnection, use the JMS connection factory configuration.","title":"Connecting your application"},{"location":"technology/mq/#client-channel-definition-tables","text":"CCDT provides encapsulation and abstraction of connection information for applications, hiding the MQ architecture and configuration from the application. CCDT defines which real queue managers the application will connect to. Which could be a single queue manager or a group of queue managers. \"channel\" : [ { \"name\" : \"STORE.CHANNEL\" , \"clientConnection\" : { \"connection\" : [ { \"host\" : \"mq1-cp4i-ibm-mq-qm-cp4i-mq1........com\" , \"port\" : \"443\" } ], \"queueManager\" : \"BIGGSQMGR\" }, \"type\" : \"clientConnection\" } ] } Important readings MQ family page MQ 9.3 product documentation Article for developer from Richard Coppen's: 'IBM MQ fundamentals' MQ on Container Learning path: IBM MQ Developer Developer cheat sheet","title":"Client Channel  Definition Tables"},{"location":"technology/mq/#code-repositories","text":"Store simulator - JMS producer to MQ AMQP and reactive messaging","title":"Code repositories"},{"location":"technology/security/","text":"Updated 04/13/2022 Review this video for a refresh on SSL and TLS certificates and keep in mind what the speaker quotes: Any message encrypted with Bob's public key can only be decrypted with Bob's private key Anyone with access to Alice's public key can verify that a message could only have been created by someone with access to Alice's private key. For a deeper dive into security administration see this confluent article and Kafka's product documentation . We also strongly recommend reading Rick Osowski's blogs Part 1 and Part 2 on Kafka security configuration. Understand the Kafka cluster listeners \u00b6 You can secure your IBM Event Streams resources by managing the access each user and application has to each resource. An Event Streams cluster can be configured to expose up to 2 internal and 1 external Kafka listeners. These listeners provide the mechanism for Kafka client applications to communicate with the Kafka brokers and these can be configured as secured listeners (which is the default for the tls and external Kafka listener you will see below). Each Kafka listener providing a connection to Event Streams can also be configured to authenticate connections with either Mutual TLS or SCRAM SHA 512 authentication mechanisms. Additionally, the Event Streams cluster can be configured to authorize operations sent via an authenticated listener using access control list defined at the user level. The following figure presents a decision tree and the actions to consider for configuring cluster and applications. In Event Streams, the following yaml snippet from an IBM Event Streams instance definition defines the following Kafka listeners\" One internal non secured kafka listener on port 9092 called plain One internal secured (TLS encrypted) Kafka listener on port 9093 called tls , which also enforces authentication throughout TLS, and One external secured (TLS encrypted) Kafka listener on port 9094 called external , which also enforces authentication throughout SCRAM credentials, that is exposed through a route. listeners : - name : plain port : 9092 type : internal tls : false - name : tls port : 9093 type : internal tls : true authentication : type : tls - name : external type : route port : 9094 tls : true authentication : type : scram-sha-512 (*) tls: true enforces traffic encryption. Default is true for Kafka listeners on ports 9093 and 9094 (**) type: internal specifies that a Kafka listener is internal. Kafka listenes on ports 9092 and 9093 default to internal. To connect to kafka using Kafka API \u00b6 The most important and essential property to connect to a Kafka broker is the bootstrap.servers property. This property tells Kafka clients what URL to use to talk to kafka cluster. bootstrap.server defines what Kafka listener your application will use to connect to Kafka. And based on that Kafka listener, you may need to provide your application with extra configuration. At the very minimum, you will need to set the security.protocol property that will tell whether you are connecting to a secured Kafka listener or not. As a result, the values for security.protocol are: PLAINTEXT - using PLAINTEXT transport layer & no authentication - default value. SSL - using SSL transport layer & certificate-based authentication or no authentication. SASL_PLAINTEXT - using PLAINTEXT transport layer & SASL-based authentication. SASL_SSL - using SSL transport layer & SASL-based authentication. Based on the above, the security protocol you will use to connect to the different Kafka listeners that IBM Event Streams deploys are: PLAINTEXT when connecting to the non secured internal plain Kafka listener on port 9092 SSL when connecting to the secured (TLS encrypted) internal tls Kafka listener on port 9093 that also enforces authentication through TLS certificates SASL_SSL when connecting to the secured (TLS encrypted) external external Kafka listener on port 9094 that also enforces authentication through SCRAM credentials. Non-secured listener \u00b6 You would only need to specify that there is no security in place for your application to connect to a non-secured kafka listener: security.protocol = PLAINTEXT Secured listener \u00b6 In order for your application to be able to connect to Kafka through the internal secured (TLS encrypted) Kafka listener, you need to set the appropriate value for security.protocol as seen above plus provide the Certificate Authority of the Kafka cluster (its public key). Depending on the technology of your application, you will need to provide the Certificate Authority of the Kafka cluster for the TLS encryption either as a PKCS12 certificate for a Java client or as a PEM certificate for anything else. PKCS12 certificates (or truststores) come in the form of a .p12 file and are secured with a password. You can inspect a PKCS12 certificate with: openssl pkcs12 -info -nodes -in truststore.p12 and providing the truststore password. An example of the output would be: MAC Iteration 100000 MAC verified OK PKCS7 Encrypted data: Certificate bag Bag Attributes friendlyName: ca.crt 2 .16.840.1.113894.746875.1.1: <Unsupported tag 6 > subject = /O = io.strimzi/CN = cluster-ca v0 issuer = /O = io.strimzi/CN = cluster-ca v0 -----BEGIN CERTIFICATE----- MIIDOzCCAiOgAwIBAgIUe0BjKXdgPF+AMpMXvPREf5XCZi8wDQYJKoZIhvcNAQEL BQAwLTETMBEGA1UECgwKaW8uc3RyaW16aTEWMBQGA1UEAwwNY2x1c3Rlci1jYSB2 MDAeFw0yMjAzMDgxMjU1MjJaFw0yMzAzMDgxMjU1MjJaMC0xEzARBgNVBAoMCmlv LnN0cmltemkxFjAUBgNVBAMMDWNsdXN0ZXItY2EgdjAwggEiMA0GCSqGSIb3DQEB AQUAA4IBDwAwggEKAoIBAQDLKGs6BfZVM1gWqZhOzbB/iqhVktBhTXC4u4V7d+kx OF4JJDPcbhZbpajn7ADABDJtE38cc6qzflJqUWlcqjIhdl7FUUSso/z9/FduSF0j dM9LUjwzII3TMq3vnqYxjbwb2u0NTtgT3n6Qi8ST/9qmlCOFJfzUvXErYx00IZ2c Bj3PG6OoZbJjb3RgkQi+2CxGL95G3xd6v/5ZmHt2YRe5MxMN7pU0z1LHOR0zZvGk H2B2d+4S8dSX6lA84XKENFbtiZiglcMEdyu9Uy5DOfznw9eXzysal6UOzEu0mInD 25gdtPJVKgrAbSMPI4eKmA9JjP8gYwhorj6r/ra0hcj7AgMBAAGjUzBRMB0GA1Ud DgQWBBT9IfWWejk2NXK8RkreFe2atXhMBDAfBgNVHSMEGDAWgBT9IfWWejk2NXK8 RkreFe2atXhMBDAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUAA4IBAQAa /aq2+Mb4g7/GLLOo+4nY7LZ2Zl7K37elymxHhafpKdhZYHEAIgj+Gda3OJytHMwq 3KqDyBJW4IptT631Z70EsMHM+J9ok/KupAbNMilCfevrzVAzXnFhSm3OCVIelLag jxlzb9da45/0ZwpTg93x2r3s8GhpLKTSUJEHL2ywsY65VZ5JSbyz9TIaYmnlnoL0 JsuP73iJlg2Nmsd7zVXekqXo/r5I/sraNet2nqc9YyLs6+pzhsfq0oTDT2nA1nZk Dl9DpLZo0fVoJF73k2z2mBk8gCjGqZk289octuOCr+MwXcGN6JTR2Iux05TBI6uf 924CQFYsZS2kdhl5GgqQ -----END CERTIFICATE----- On the other hand, PEM certificates come in the form of a .pem file and are not password protected. You can inspect them using cat . The output should be the same certificate as the one provided within the PKCS12 certificate: cat es-cert.pem -----BEGIN CERTIFICATE----- MIIDOzCCAiOgAwIBAgIUe0BjKXdgPF+AMpMXvPREf5XCZi8wDQYJKoZIhvcNAQEL BQAwLTETMBEGA1UECgwKaW8uc3RyaW16aTEWMBQGA1UEAwwNY2x1c3Rlci1jYSB2 MDAeFw0yMjAzMDgxMjU1MjJaFw0yMzAzMDgxMjU1MjJaMC0xEzARBgNVBAoMCmlv LnN0cmltemkxFjAUBgNVBAMMDWNsdXN0ZXItY2EgdjAwggEiMA0GCSqGSIb3DQEB AQUAA4IBDwAwggEKAoIBAQDLKGs6BfZVM1gWqZhOzbB/iqhVktBhTXC4u4V7d+kx OF4JJDPcbhZbpajn7ADABDJtE38cc6qzflJqUWlcqjIhdl7FUUSso/z9/FduSF0j dM9LUjwzII3TMq3vnqYxjbwb2u0NTtgT3n6Qi8ST/9qmlCOFJfzUvXErYx00IZ2c Bj3PG6OoZbJjb3RgkQi+2CxGL95G3xd6v/5ZmHt2YRe5MxMN7pU0z1LHOR0zZvGk H2B2d+4S8dSX6lA84XKENFbtiZiglcMEdyu9Uy5DOfznw9eXzysal6UOzEu0mInD 25gdtPJVKgrAbSMPI4eKmA9JjP8gYwhorj6r/ra0hcj7AgMBAAGjUzBRMB0GA1Ud DgQWBBT9IfWWejk2NXK8RkreFe2atXhMBDAfBgNVHSMEGDAWgBT9IfWWejk2NXK8 RkreFe2atXhMBDAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUAA4IBAQAa /aq2+Mb4g7/GLLOo+4nY7LZ2Zl7K37elymxHhafpKdhZYHEAIgj+Gda3OJytHMwq 3KqDyBJW4IptT631Z70EsMHM+J9ok/KupAbNMilCfevrzVAzXnFhSm3OCVIelLag jxlzb9da45/0ZwpTg93x2r3s8GhpLKTSUJEHL2ywsY65VZ5JSbyz9TIaYmnlnoL0 JsuP73iJlg2Nmsd7zVXekqXo/r5I/sraNet2nqc9YyLs6+pzhsfq0oTDT2nA1nZk Dl9DpLZo0fVoJF73k2z2mBk8gCjGqZk289octuOCr+MwXcGN6JTR2Iux05TBI6uf 924CQFYsZS2kdhl5GgqQ -----END CERTIFICATE----- You can find in the Event Streams with CP4i section how to obtain the Certificate Authority of your IBM Event Streams instance. Once you have the Certificate Authority of your Kafka cluster, you will provide its location and password in your properties file through the ssl.truststore.location and ssl.truststore.password properties. security.protocol = SSL or SASL_SSL ssl.protocol = TLSv1.2 ssl.truststore.password = <truststore.p12-password> ssl.truststore.location = truststore.p12 ssl.truststore.type = PKCS12 where security.protocol will vary between SSL or SASL_SSL based on the authentication as you will see next. Authentication \u00b6 You have seen above that your Kafka listeners can require authentication to any application or client wanting to connect to the Kafka cluster through them. It was also said that authentication could be either of type SASL-based, through SCRAM (modern Salted Challenge Response Authentication Mechanism) credentials, or certificate-based (TLS). Either way, IBM Event Streams will handle authentication through KafkaUser objects. These objects that represent Kafka users of your IBM Event Streams instance will have their authentication (and authorization through ACLs) credentials or TLS certificates associated to them stored in a secret. In order to find out how to create these KafkaUsers , which will vary depending on the authentication method, check out this section . Scram \u00b6 If you have created a KafkaUser to be used with a Kafka listener that requires SCRAM authentication, you will be able to retrieve its SCRAM credentials either from the IBM Event Streams UI at creation time or later on from the secret these are stored to: oc extract secret/<KAFKA_USER> -n <NAMESPACE> --keys = sasl.jaas.config --to = - where <KAFKA_USER> is the name of the KafkaUser object you created. <NAMESPACE> is the namespace where IBM Event Streams is deployed on. Example: oc extract secret/test-app -n tools --keys = sasl.jaas.config --to = - # sasl.jaas.config org.apache.kafka.common.security.scram.ScramLoginModule required username = \"test-app\" password = \"VgWpkjAkvxH0\" ; You can see above your SCRAM username and password. TLS \u00b6 If you have created a KafkaUser to be used with a Kafka listener that requires TLS authentication, you will be able to retrieve its TLS certificates either from the IBM Event Streams UI at creation time in a zip folder or later on from the secret these are stored to. First, describe the secret to see what certificates are stored in it: $ oc describe secret test-app-tls -n tools Name: test-app-tls Namespace: tools Labels: app.kubernetes.io/instance = test-app-tls app.kubernetes.io/managed-by = strimzi-user-operator app.kubernetes.io/name = strimzi-user-operator app.kubernetes.io/part-of = eventstreams-test-app-tls eventstreams.ibm.com/cluster = es-inst eventstreams.ibm.com/kind = KafkaUser Annotations: <none> Type: Opaque Data ==== user.key: 1704 bytes user.p12: 2384 bytes user.password: 12 bytes ca.crt: 1180 bytes user.crt: 1025 bytes You can see that the secret will store the following: user.key and user.crt - the client certificate key-pair. user.p12 - trustore that contains the user.key and user.crt . user.password - contains the user.p12 truststore password. ca.crt - CA used to sign the client certificate key-pair. Then, you can extract the appropriate certificate based on whether your application or Kafka client is Java based or not. In the case of a Java based application or Kafka client, extract the user.p12 and user.password : oc extract secret/<KAFKA_USER> -n <NAMESPACE> --keys = user.p12 oc extract secret/<KAFKA_USER> -n <NAMESPACE> --keys = user.password where <KAFKA_USER> is the name of the KafkaUser object you created. <NAMESPACE> is the namespace where IBM Event Streams is deployed on. Properties config \u00b6 Now that you know how to get the authentication credentials or certificates for a proper authentication of your application or Kafka client you need to configure the appropriate properties for that: If your Kafka listener authentication method is SCRAM: security.protocol = SASL_SSL sasl.mechanism = SCRAM-SHA-512 sasl.jaas.config = org.apache.kafka.common.security.scram.ScramLoginModule required username\\=\"<USERNAME>\" password\\=\"<PASSWORD>\"; If your Kafka listener authentication method is TLS: security.protocol = SSL ssl.keystore.location = <location_to_your_user.p12> ssl.keystore.password = <user.p12-password> ssl.keystore.type = PKCS12 Recapitulation \u00b6 Let's have a full look at how the Kafka communication properties, for a Java application or client, would look like for IBM Event Streams on RedHat OpenShift with the defaults. If you look at the IBM Event Streams instance deployment sample definitions in this GitHub repository, that is mentioned in the IBM Event Streams official documentation here , you will see that the defaults defined for the Kafka listeners for all of them (except from the light-insecure.yaml sample) are: listeners : external : type : route authentication : type : scram-sha-512 tls : authentication : type : tls This translates to Strimzi (the open source project IBM Event Streams is based on) in: listeners : - name : tls port : 9093 type : internal tls : true authentication : type : tls - name : external type : route port : 9094 tls : true authentication : type : scram-sha-512 Let's also add the plain non-secure Kafka listener to the picture so that all cases are covered in this recap section. listeners : plain : {} external : type : route authentication : type : scram-sha-512 tls : authentication : type : tls As a result, the IBM Event Streams instance deployed will count with: One internal non secured kafka listener on port 9092 called plain One internal secured (TLS encrypted) Kafka listener on port 9093 called tls , which also enforces authentication throughout TLS, and One external secured (TLS encrypted) Kafka listener on port 9094 called external , which also enforces authentication throughout SCRAM credentials, that is exposed through a route. Plain \u00b6 The Kafka properties configuration to get your application or Kafka client to properly connect and communicate through the non secured kafka listener on port 9092 called plain will be as follows: # Internal plain listener # ======================= security.protocol = PLAINTEXT bootstrap.servers = <ES_NAME>-kafka-bootstrap.<NAMESPACE>.svc\\:9092 where <ES_NAME> is the name of the IBM Event Streams instance deployed you are trying to connect to. <NAMESPACE> is the namespace the IBM Event Streams instance you are trying to connect to is deployed in. Internal tls \u00b6 The Kafka properties configuration to get your application or Kafka client to properly connect and communicate through the internal s ecured (TLS encrypted) Kafka listener on port 9093 called tls , which also enforces authentication throughout mTLS will be as follows: # Internal tls listener # ===================== bootstrap.servers = <<ES_NAME>-kafka-bootstrap.<NAMESPACE>.svc\\:9093 security.protocol = SSL ssl.protocol = TLSv1.2 ## mTLS Authentication for the client. ssl.keystore.location = <user.p12-location> ssl.keystore.password = <user.p12-password> ssl.keystore.type = PKCS12 ## Certificate Authority of your Kafka cluster ssl.truststore.password = <trustore.p12-password> ssl.truststore.location = <truststore.p12-location> ssl.truststore.type = PKCS12 where <ES_NAME> is the name of the IBM Event Streams instance deployed you are trying to connect to. <NAMESPACE> is the namespace the IBM Event Streams instance you are trying to connect to is deployed in. <user.p12-location> is the location of the user.p12 truststore containing the user.key and user.crt client certificate key-pair for the application or client mTLS authentication as explained above. <user.p12-password> is the password of the <user.p12> truststore. <truststore.p12-location> is the location of the Certificate Authority of your Kafka cluster to establish mTLS encryted communication between your IBM Event Streams instance and your application or Kafka client. <trustore.p12-password> is the password for the truststore.p12 truststore. When the application is deployed on OpenShift, certificates will be mounted to the application pod. Below is an example of a Quarkus app deployment descriptor, with environment variables: env : - name : KAFKA_SSL_TRUSTSTORE_FILE_LOCATION value : /deployments/certs/server/ca.p12 - name : KAFKA_SSL_TRUSTSTORE_TYPE value : PKCS12 - name : KAFKA_SSL_KEYSTORE_FILE_LOCATION value : /deployments/certs/user/user.p12 - name : KAFKA_SSL_KEYSTORE_TYPE value : PKCS12 - name : KAFKA_SECURITY_PROTOCOL value : SSL - name : KAFKA_USER value : tls-user - name : KAFKA_CERT_PWD valueFrom : secretKeyRef : key : ca.password name : kafka-cluster-ca-cert - name : USER_CERT_PWD valueFrom : secretKeyRef : key : user.password name : tls-user # ... volumeMounts : - mountPath : /deployments/certs/server name : kafka-cert readOnly : false subPath : \"\" - mountPath : /deployments/certs/user name : user-cert readOnly : false subPath : \"\" volumes : - name : kafka-cert secret : optional : true secretName : kafka-cluster-ca-cert - name : user-cert secret : optional : true secretName : tls-user External tls \u00b6 The Kafka properties configuration to get your application or Kafka client to properly connect and communicate through the external secured (TLS encrypted) Kafka listener on port 9094 called external , which also enforces authentication throughout SCRAM credentials, and that is exposed through a route will be as follows: # External listener SCRAM # ======================= bootstrap.servers = <ES_NAME>-kafka-bootstrap-<NAMESPACE>.<OPENSHIFT_APPS_DNS>\\:443 security.protocol = SASL_SSL ssl.protocol = TLSv1.2 ## Certificate Authority of your Kafka cluster ssl.truststore.password = <trustore.p12-password> ssl.truststore.location = <truststore.p12-location> ssl.truststore.type = PKCS12 ## Scram credentials sasl.mechanism = SCRAM-SHA-512 sasl.jaas.config = org.apache.kafka.common.security.scram.ScramLoginModule required username\\=\"<SCRAM_USERNAME>\" password\\=\"<SCRAM_PASSWORD>\"; where <ES_NAME> is the name of the IBM Event Streams instance deployed you are trying to connect to. <NAMESPACE> is the namespace the IBM Event Streams instance you are trying to connect to is deployed in. <OPENSHIFT_APPS_DNS> is your RedHat OpenShift DNS domain for application routes. <truststore.p12-location> is the location of the Certificate Authority of your Kafka cluster to establish mTLS encryted communication between your IBM Event Streams instance and your application or Kafka client. <trustore.p12-password> is the password for the truststore.p12 truststore. <SCRAM_USERNAME> and <SCRAM_PASSWORD> are your SCRAM credentials. Tips \u00b6 Remember that if the application does not run in the same namespace as the kafka cluster then you need to copy the secrets so that the application developers can access the required credentials and certificates from their own namespaces with something like if [[ -z $( oc get secret ${ TLS_USER } 2 > /dev/null ) ]] then # As the project is personal to the user, we can keep a generic name for the secret oc get secret ${ TLS_USER } -n ${ KAFKA_NS } -o json | jq -r '.metadata.name=\"tls-user\"' | jq -r '.metadata.namespace=\"' ${ YOUR_PROJECT_NAME } '\"' | oc apply -f - fi if [[ -z $( oc get secret ${ SCRAM_USER } 2 > /dev/null ) ]] then # As the project is personal to the user, we can keep a generic name for the secret oc get secret ${ SCRAM_USER } -n ${ KAFKA_NS } -o json | jq -r '.metadata.name=\"scram-user\"' | jq -r '.metadata.namespace=\"' ${ YOUR_PROJECT_NAME } '\"' | oc apply -f - fi Kafka Connect \u00b6 For Kafka connector, you need to define authentication used to connect to the Kafka Cluster: authentication : type : tls certificateAndKey : secretName : tls-user certificate : user.crt key : user.key Get TLS public cluster certificate: tls : trustedCertificates : - secretName : dev-cluster-ca-cert certificate : ca.crt Working with certificates \u00b6 To extract a PEM-based certificate from a JKS-based truststore, you can use the following command: keytool -exportcert -keypass { truststore-password } -keystore { provided-kafka-truststore.jks } -rfc -file { desired-kafka-cert-output.pem } To build a PKCS12 from a pem do openssl pkcs12 -export -in cert.pem -out cert.p12 # if you want jks keytool -importkeystore -srckeystore cert.p12 -srcstoretype pkcs12 -destkeystore cert.jks","title":"Security"},{"location":"technology/security/#understand-the-kafka-cluster-listeners","text":"You can secure your IBM Event Streams resources by managing the access each user and application has to each resource. An Event Streams cluster can be configured to expose up to 2 internal and 1 external Kafka listeners. These listeners provide the mechanism for Kafka client applications to communicate with the Kafka brokers and these can be configured as secured listeners (which is the default for the tls and external Kafka listener you will see below). Each Kafka listener providing a connection to Event Streams can also be configured to authenticate connections with either Mutual TLS or SCRAM SHA 512 authentication mechanisms. Additionally, the Event Streams cluster can be configured to authorize operations sent via an authenticated listener using access control list defined at the user level. The following figure presents a decision tree and the actions to consider for configuring cluster and applications. In Event Streams, the following yaml snippet from an IBM Event Streams instance definition defines the following Kafka listeners\" One internal non secured kafka listener on port 9092 called plain One internal secured (TLS encrypted) Kafka listener on port 9093 called tls , which also enforces authentication throughout TLS, and One external secured (TLS encrypted) Kafka listener on port 9094 called external , which also enforces authentication throughout SCRAM credentials, that is exposed through a route. listeners : - name : plain port : 9092 type : internal tls : false - name : tls port : 9093 type : internal tls : true authentication : type : tls - name : external type : route port : 9094 tls : true authentication : type : scram-sha-512 (*) tls: true enforces traffic encryption. Default is true for Kafka listeners on ports 9093 and 9094 (**) type: internal specifies that a Kafka listener is internal. Kafka listenes on ports 9092 and 9093 default to internal.","title":"Understand the Kafka cluster listeners"},{"location":"technology/security/#to-connect-to-kafka-using-kafka-api","text":"The most important and essential property to connect to a Kafka broker is the bootstrap.servers property. This property tells Kafka clients what URL to use to talk to kafka cluster. bootstrap.server defines what Kafka listener your application will use to connect to Kafka. And based on that Kafka listener, you may need to provide your application with extra configuration. At the very minimum, you will need to set the security.protocol property that will tell whether you are connecting to a secured Kafka listener or not. As a result, the values for security.protocol are: PLAINTEXT - using PLAINTEXT transport layer & no authentication - default value. SSL - using SSL transport layer & certificate-based authentication or no authentication. SASL_PLAINTEXT - using PLAINTEXT transport layer & SASL-based authentication. SASL_SSL - using SSL transport layer & SASL-based authentication. Based on the above, the security protocol you will use to connect to the different Kafka listeners that IBM Event Streams deploys are: PLAINTEXT when connecting to the non secured internal plain Kafka listener on port 9092 SSL when connecting to the secured (TLS encrypted) internal tls Kafka listener on port 9093 that also enforces authentication through TLS certificates SASL_SSL when connecting to the secured (TLS encrypted) external external Kafka listener on port 9094 that also enforces authentication through SCRAM credentials.","title":"To connect to kafka using Kafka API"},{"location":"technology/security/#non-secured-listener","text":"You would only need to specify that there is no security in place for your application to connect to a non-secured kafka listener: security.protocol = PLAINTEXT","title":"Non-secured listener"},{"location":"technology/security/#secured-listener","text":"In order for your application to be able to connect to Kafka through the internal secured (TLS encrypted) Kafka listener, you need to set the appropriate value for security.protocol as seen above plus provide the Certificate Authority of the Kafka cluster (its public key). Depending on the technology of your application, you will need to provide the Certificate Authority of the Kafka cluster for the TLS encryption either as a PKCS12 certificate for a Java client or as a PEM certificate for anything else. PKCS12 certificates (or truststores) come in the form of a .p12 file and are secured with a password. You can inspect a PKCS12 certificate with: openssl pkcs12 -info -nodes -in truststore.p12 and providing the truststore password. An example of the output would be: MAC Iteration 100000 MAC verified OK PKCS7 Encrypted data: Certificate bag Bag Attributes friendlyName: ca.crt 2 .16.840.1.113894.746875.1.1: <Unsupported tag 6 > subject = /O = io.strimzi/CN = cluster-ca v0 issuer = /O = io.strimzi/CN = cluster-ca v0 -----BEGIN CERTIFICATE----- MIIDOzCCAiOgAwIBAgIUe0BjKXdgPF+AMpMXvPREf5XCZi8wDQYJKoZIhvcNAQEL BQAwLTETMBEGA1UECgwKaW8uc3RyaW16aTEWMBQGA1UEAwwNY2x1c3Rlci1jYSB2 MDAeFw0yMjAzMDgxMjU1MjJaFw0yMzAzMDgxMjU1MjJaMC0xEzARBgNVBAoMCmlv LnN0cmltemkxFjAUBgNVBAMMDWNsdXN0ZXItY2EgdjAwggEiMA0GCSqGSIb3DQEB AQUAA4IBDwAwggEKAoIBAQDLKGs6BfZVM1gWqZhOzbB/iqhVktBhTXC4u4V7d+kx OF4JJDPcbhZbpajn7ADABDJtE38cc6qzflJqUWlcqjIhdl7FUUSso/z9/FduSF0j dM9LUjwzII3TMq3vnqYxjbwb2u0NTtgT3n6Qi8ST/9qmlCOFJfzUvXErYx00IZ2c Bj3PG6OoZbJjb3RgkQi+2CxGL95G3xd6v/5ZmHt2YRe5MxMN7pU0z1LHOR0zZvGk H2B2d+4S8dSX6lA84XKENFbtiZiglcMEdyu9Uy5DOfznw9eXzysal6UOzEu0mInD 25gdtPJVKgrAbSMPI4eKmA9JjP8gYwhorj6r/ra0hcj7AgMBAAGjUzBRMB0GA1Ud DgQWBBT9IfWWejk2NXK8RkreFe2atXhMBDAfBgNVHSMEGDAWgBT9IfWWejk2NXK8 RkreFe2atXhMBDAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUAA4IBAQAa /aq2+Mb4g7/GLLOo+4nY7LZ2Zl7K37elymxHhafpKdhZYHEAIgj+Gda3OJytHMwq 3KqDyBJW4IptT631Z70EsMHM+J9ok/KupAbNMilCfevrzVAzXnFhSm3OCVIelLag jxlzb9da45/0ZwpTg93x2r3s8GhpLKTSUJEHL2ywsY65VZ5JSbyz9TIaYmnlnoL0 JsuP73iJlg2Nmsd7zVXekqXo/r5I/sraNet2nqc9YyLs6+pzhsfq0oTDT2nA1nZk Dl9DpLZo0fVoJF73k2z2mBk8gCjGqZk289octuOCr+MwXcGN6JTR2Iux05TBI6uf 924CQFYsZS2kdhl5GgqQ -----END CERTIFICATE----- On the other hand, PEM certificates come in the form of a .pem file and are not password protected. You can inspect them using cat . The output should be the same certificate as the one provided within the PKCS12 certificate: cat es-cert.pem -----BEGIN CERTIFICATE----- MIIDOzCCAiOgAwIBAgIUe0BjKXdgPF+AMpMXvPREf5XCZi8wDQYJKoZIhvcNAQEL BQAwLTETMBEGA1UECgwKaW8uc3RyaW16aTEWMBQGA1UEAwwNY2x1c3Rlci1jYSB2 MDAeFw0yMjAzMDgxMjU1MjJaFw0yMzAzMDgxMjU1MjJaMC0xEzARBgNVBAoMCmlv LnN0cmltemkxFjAUBgNVBAMMDWNsdXN0ZXItY2EgdjAwggEiMA0GCSqGSIb3DQEB AQUAA4IBDwAwggEKAoIBAQDLKGs6BfZVM1gWqZhOzbB/iqhVktBhTXC4u4V7d+kx OF4JJDPcbhZbpajn7ADABDJtE38cc6qzflJqUWlcqjIhdl7FUUSso/z9/FduSF0j dM9LUjwzII3TMq3vnqYxjbwb2u0NTtgT3n6Qi8ST/9qmlCOFJfzUvXErYx00IZ2c Bj3PG6OoZbJjb3RgkQi+2CxGL95G3xd6v/5ZmHt2YRe5MxMN7pU0z1LHOR0zZvGk H2B2d+4S8dSX6lA84XKENFbtiZiglcMEdyu9Uy5DOfznw9eXzysal6UOzEu0mInD 25gdtPJVKgrAbSMPI4eKmA9JjP8gYwhorj6r/ra0hcj7AgMBAAGjUzBRMB0GA1Ud DgQWBBT9IfWWejk2NXK8RkreFe2atXhMBDAfBgNVHSMEGDAWgBT9IfWWejk2NXK8 RkreFe2atXhMBDAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUAA4IBAQAa /aq2+Mb4g7/GLLOo+4nY7LZ2Zl7K37elymxHhafpKdhZYHEAIgj+Gda3OJytHMwq 3KqDyBJW4IptT631Z70EsMHM+J9ok/KupAbNMilCfevrzVAzXnFhSm3OCVIelLag jxlzb9da45/0ZwpTg93x2r3s8GhpLKTSUJEHL2ywsY65VZ5JSbyz9TIaYmnlnoL0 JsuP73iJlg2Nmsd7zVXekqXo/r5I/sraNet2nqc9YyLs6+pzhsfq0oTDT2nA1nZk Dl9DpLZo0fVoJF73k2z2mBk8gCjGqZk289octuOCr+MwXcGN6JTR2Iux05TBI6uf 924CQFYsZS2kdhl5GgqQ -----END CERTIFICATE----- You can find in the Event Streams with CP4i section how to obtain the Certificate Authority of your IBM Event Streams instance. Once you have the Certificate Authority of your Kafka cluster, you will provide its location and password in your properties file through the ssl.truststore.location and ssl.truststore.password properties. security.protocol = SSL or SASL_SSL ssl.protocol = TLSv1.2 ssl.truststore.password = <truststore.p12-password> ssl.truststore.location = truststore.p12 ssl.truststore.type = PKCS12 where security.protocol will vary between SSL or SASL_SSL based on the authentication as you will see next.","title":"Secured listener"},{"location":"technology/security/#authentication","text":"You have seen above that your Kafka listeners can require authentication to any application or client wanting to connect to the Kafka cluster through them. It was also said that authentication could be either of type SASL-based, through SCRAM (modern Salted Challenge Response Authentication Mechanism) credentials, or certificate-based (TLS). Either way, IBM Event Streams will handle authentication through KafkaUser objects. These objects that represent Kafka users of your IBM Event Streams instance will have their authentication (and authorization through ACLs) credentials or TLS certificates associated to them stored in a secret. In order to find out how to create these KafkaUsers , which will vary depending on the authentication method, check out this section .","title":"Authentication"},{"location":"technology/security/#scram","text":"If you have created a KafkaUser to be used with a Kafka listener that requires SCRAM authentication, you will be able to retrieve its SCRAM credentials either from the IBM Event Streams UI at creation time or later on from the secret these are stored to: oc extract secret/<KAFKA_USER> -n <NAMESPACE> --keys = sasl.jaas.config --to = - where <KAFKA_USER> is the name of the KafkaUser object you created. <NAMESPACE> is the namespace where IBM Event Streams is deployed on. Example: oc extract secret/test-app -n tools --keys = sasl.jaas.config --to = - # sasl.jaas.config org.apache.kafka.common.security.scram.ScramLoginModule required username = \"test-app\" password = \"VgWpkjAkvxH0\" ; You can see above your SCRAM username and password.","title":"Scram"},{"location":"technology/security/#tls","text":"If you have created a KafkaUser to be used with a Kafka listener that requires TLS authentication, you will be able to retrieve its TLS certificates either from the IBM Event Streams UI at creation time in a zip folder or later on from the secret these are stored to. First, describe the secret to see what certificates are stored in it: $ oc describe secret test-app-tls -n tools Name: test-app-tls Namespace: tools Labels: app.kubernetes.io/instance = test-app-tls app.kubernetes.io/managed-by = strimzi-user-operator app.kubernetes.io/name = strimzi-user-operator app.kubernetes.io/part-of = eventstreams-test-app-tls eventstreams.ibm.com/cluster = es-inst eventstreams.ibm.com/kind = KafkaUser Annotations: <none> Type: Opaque Data ==== user.key: 1704 bytes user.p12: 2384 bytes user.password: 12 bytes ca.crt: 1180 bytes user.crt: 1025 bytes You can see that the secret will store the following: user.key and user.crt - the client certificate key-pair. user.p12 - trustore that contains the user.key and user.crt . user.password - contains the user.p12 truststore password. ca.crt - CA used to sign the client certificate key-pair. Then, you can extract the appropriate certificate based on whether your application or Kafka client is Java based or not. In the case of a Java based application or Kafka client, extract the user.p12 and user.password : oc extract secret/<KAFKA_USER> -n <NAMESPACE> --keys = user.p12 oc extract secret/<KAFKA_USER> -n <NAMESPACE> --keys = user.password where <KAFKA_USER> is the name of the KafkaUser object you created. <NAMESPACE> is the namespace where IBM Event Streams is deployed on.","title":"TLS"},{"location":"technology/security/#properties-config","text":"Now that you know how to get the authentication credentials or certificates for a proper authentication of your application or Kafka client you need to configure the appropriate properties for that: If your Kafka listener authentication method is SCRAM: security.protocol = SASL_SSL sasl.mechanism = SCRAM-SHA-512 sasl.jaas.config = org.apache.kafka.common.security.scram.ScramLoginModule required username\\=\"<USERNAME>\" password\\=\"<PASSWORD>\"; If your Kafka listener authentication method is TLS: security.protocol = SSL ssl.keystore.location = <location_to_your_user.p12> ssl.keystore.password = <user.p12-password> ssl.keystore.type = PKCS12","title":"Properties config"},{"location":"technology/security/#recapitulation","text":"Let's have a full look at how the Kafka communication properties, for a Java application or client, would look like for IBM Event Streams on RedHat OpenShift with the defaults. If you look at the IBM Event Streams instance deployment sample definitions in this GitHub repository, that is mentioned in the IBM Event Streams official documentation here , you will see that the defaults defined for the Kafka listeners for all of them (except from the light-insecure.yaml sample) are: listeners : external : type : route authentication : type : scram-sha-512 tls : authentication : type : tls This translates to Strimzi (the open source project IBM Event Streams is based on) in: listeners : - name : tls port : 9093 type : internal tls : true authentication : type : tls - name : external type : route port : 9094 tls : true authentication : type : scram-sha-512 Let's also add the plain non-secure Kafka listener to the picture so that all cases are covered in this recap section. listeners : plain : {} external : type : route authentication : type : scram-sha-512 tls : authentication : type : tls As a result, the IBM Event Streams instance deployed will count with: One internal non secured kafka listener on port 9092 called plain One internal secured (TLS encrypted) Kafka listener on port 9093 called tls , which also enforces authentication throughout TLS, and One external secured (TLS encrypted) Kafka listener on port 9094 called external , which also enforces authentication throughout SCRAM credentials, that is exposed through a route.","title":"Recapitulation"},{"location":"technology/security/#plain","text":"The Kafka properties configuration to get your application or Kafka client to properly connect and communicate through the non secured kafka listener on port 9092 called plain will be as follows: # Internal plain listener # ======================= security.protocol = PLAINTEXT bootstrap.servers = <ES_NAME>-kafka-bootstrap.<NAMESPACE>.svc\\:9092 where <ES_NAME> is the name of the IBM Event Streams instance deployed you are trying to connect to. <NAMESPACE> is the namespace the IBM Event Streams instance you are trying to connect to is deployed in.","title":"Plain"},{"location":"technology/security/#internal-tls","text":"The Kafka properties configuration to get your application or Kafka client to properly connect and communicate through the internal s ecured (TLS encrypted) Kafka listener on port 9093 called tls , which also enforces authentication throughout mTLS will be as follows: # Internal tls listener # ===================== bootstrap.servers = <<ES_NAME>-kafka-bootstrap.<NAMESPACE>.svc\\:9093 security.protocol = SSL ssl.protocol = TLSv1.2 ## mTLS Authentication for the client. ssl.keystore.location = <user.p12-location> ssl.keystore.password = <user.p12-password> ssl.keystore.type = PKCS12 ## Certificate Authority of your Kafka cluster ssl.truststore.password = <trustore.p12-password> ssl.truststore.location = <truststore.p12-location> ssl.truststore.type = PKCS12 where <ES_NAME> is the name of the IBM Event Streams instance deployed you are trying to connect to. <NAMESPACE> is the namespace the IBM Event Streams instance you are trying to connect to is deployed in. <user.p12-location> is the location of the user.p12 truststore containing the user.key and user.crt client certificate key-pair for the application or client mTLS authentication as explained above. <user.p12-password> is the password of the <user.p12> truststore. <truststore.p12-location> is the location of the Certificate Authority of your Kafka cluster to establish mTLS encryted communication between your IBM Event Streams instance and your application or Kafka client. <trustore.p12-password> is the password for the truststore.p12 truststore. When the application is deployed on OpenShift, certificates will be mounted to the application pod. Below is an example of a Quarkus app deployment descriptor, with environment variables: env : - name : KAFKA_SSL_TRUSTSTORE_FILE_LOCATION value : /deployments/certs/server/ca.p12 - name : KAFKA_SSL_TRUSTSTORE_TYPE value : PKCS12 - name : KAFKA_SSL_KEYSTORE_FILE_LOCATION value : /deployments/certs/user/user.p12 - name : KAFKA_SSL_KEYSTORE_TYPE value : PKCS12 - name : KAFKA_SECURITY_PROTOCOL value : SSL - name : KAFKA_USER value : tls-user - name : KAFKA_CERT_PWD valueFrom : secretKeyRef : key : ca.password name : kafka-cluster-ca-cert - name : USER_CERT_PWD valueFrom : secretKeyRef : key : user.password name : tls-user # ... volumeMounts : - mountPath : /deployments/certs/server name : kafka-cert readOnly : false subPath : \"\" - mountPath : /deployments/certs/user name : user-cert readOnly : false subPath : \"\" volumes : - name : kafka-cert secret : optional : true secretName : kafka-cluster-ca-cert - name : user-cert secret : optional : true secretName : tls-user","title":"Internal tls"},{"location":"technology/security/#external-tls","text":"The Kafka properties configuration to get your application or Kafka client to properly connect and communicate through the external secured (TLS encrypted) Kafka listener on port 9094 called external , which also enforces authentication throughout SCRAM credentials, and that is exposed through a route will be as follows: # External listener SCRAM # ======================= bootstrap.servers = <ES_NAME>-kafka-bootstrap-<NAMESPACE>.<OPENSHIFT_APPS_DNS>\\:443 security.protocol = SASL_SSL ssl.protocol = TLSv1.2 ## Certificate Authority of your Kafka cluster ssl.truststore.password = <trustore.p12-password> ssl.truststore.location = <truststore.p12-location> ssl.truststore.type = PKCS12 ## Scram credentials sasl.mechanism = SCRAM-SHA-512 sasl.jaas.config = org.apache.kafka.common.security.scram.ScramLoginModule required username\\=\"<SCRAM_USERNAME>\" password\\=\"<SCRAM_PASSWORD>\"; where <ES_NAME> is the name of the IBM Event Streams instance deployed you are trying to connect to. <NAMESPACE> is the namespace the IBM Event Streams instance you are trying to connect to is deployed in. <OPENSHIFT_APPS_DNS> is your RedHat OpenShift DNS domain for application routes. <truststore.p12-location> is the location of the Certificate Authority of your Kafka cluster to establish mTLS encryted communication between your IBM Event Streams instance and your application or Kafka client. <trustore.p12-password> is the password for the truststore.p12 truststore. <SCRAM_USERNAME> and <SCRAM_PASSWORD> are your SCRAM credentials.","title":"External tls"},{"location":"technology/security/#tips","text":"Remember that if the application does not run in the same namespace as the kafka cluster then you need to copy the secrets so that the application developers can access the required credentials and certificates from their own namespaces with something like if [[ -z $( oc get secret ${ TLS_USER } 2 > /dev/null ) ]] then # As the project is personal to the user, we can keep a generic name for the secret oc get secret ${ TLS_USER } -n ${ KAFKA_NS } -o json | jq -r '.metadata.name=\"tls-user\"' | jq -r '.metadata.namespace=\"' ${ YOUR_PROJECT_NAME } '\"' | oc apply -f - fi if [[ -z $( oc get secret ${ SCRAM_USER } 2 > /dev/null ) ]] then # As the project is personal to the user, we can keep a generic name for the secret oc get secret ${ SCRAM_USER } -n ${ KAFKA_NS } -o json | jq -r '.metadata.name=\"scram-user\"' | jq -r '.metadata.namespace=\"' ${ YOUR_PROJECT_NAME } '\"' | oc apply -f - fi","title":"Tips"},{"location":"technology/security/#kafka-connect","text":"For Kafka connector, you need to define authentication used to connect to the Kafka Cluster: authentication : type : tls certificateAndKey : secretName : tls-user certificate : user.crt key : user.key Get TLS public cluster certificate: tls : trustedCertificates : - secretName : dev-cluster-ca-cert certificate : ca.crt","title":"Kafka Connect"},{"location":"technology/security/#working-with-certificates","text":"To extract a PEM-based certificate from a JKS-based truststore, you can use the following command: keytool -exportcert -keypass { truststore-password } -keystore { provided-kafka-truststore.jks } -rfc -file { desired-kafka-cert-output.pem } To build a PKCS12 from a pem do openssl pkcs12 -export -in cert.pem -out cert.p12 # if you want jks keytool -importkeystore -srckeystore cert.p12 -srcstoretype pkcs12 -destkeystore cert.jks","title":"Working with certificates"},{"location":"technology/spring/","text":"Audience : Developers Spring Cloud \u00b6 Spring Cloud is based on Spring boot programming model but focusing on cloud native deployment and distributed computing. As other spring boot app it includes jetty or tomcat, health checks, metrics... It supports the following patterns: Distributed/versioned configuration : externalize config in distributed system with config server. Service registration and discovery: uses Netflix Eureka, Apache Zookeeper or Consul to keep service information. Routing: supports HTTP (Open Feign or Netflix Ribbon for load balancing) and messaging (RabbitMQ and Kafka) Service-to-service calls: Sptring Cloud Gateway and Netflix Zuul is used Load balancing Circuit Breakers: based on Netflix Hystrix: if the request fails for n time, the circuit open. Global locks Leadership election and cluster state Distributed messaging It also supports pipelines for ci/cd and contract testing for interface validation. Getting started \u00b6 Use start.spring.io to create the application starting code using Kafka, Actuator, Cloud Stream or add the Spring Cloud BOM to your maven pom.xml file. See the Adding Spring Cloud To An Existing Spring Boot Application section. As most of the microservices expose REST resource, we may need to add the starter web: <dependency> <groupId> org.springframework.boot </groupId> <artifactId> spring-boot-starter-web </artifactId> </dependency> We also need to install the Spring Cloud CLI . Then add the Spring cloud starter as dependency. When using config server, we need to add the config client. <dependency> <groupId> org.springframework.cloud </groupId> <artifactId> spring-cloud-conflig-client </artifactId> </dependency> For centralized tracing uses, starter-sleuth, and zipkin. <dependency> <groupId> org.springframework.cloud </groupId> <artifactId> spring-cloud-starter-sleuth </artifactId> </dependency> <dependency> <groupId> org.springframework.cloud </groupId> <artifactId> spring-cloud-starter-zipkin </artifactId> </dependency> For service discovery add netflix-eureka-client. <dependency> <groupId> org.springframework.cloud </groupId> <artifactId> spring-cloud-starter-eureka </artifactId> </dependency> Using the Spring Cloud CLI we can get the service registry, config server, central tracing started in one command: spring cloud eureka configserver zipkin Spring Cloud config \u00b6 Use the concept of Config Server you have a central place to manage external properties for applications across all environments. As an application moves through the deployment pipeline from dev to test and into production you can manage the configuration between those environments and be certain that applications have everything they need to run when they migrate. @Value ( \"${config.in.topic}\" ) String topicName = \"orders\" ; The value of the config.in.topic comes from local configuration or remote config server. The config server will serve content from a git. See this sample for such server. Spring Cloud Stream \u00b6 Spring Cloud Stream is a framework for building highly scalable event-driven microservices connected with shared messaging systems. It unifies lots of popular messaging platforms behind one easy to use API including RabbitMQ, Apache Kafka, Amazon Kinesis, Google PubSub, Solace PubSub+, Azure Event Hubs, and Apache RocketMQ. Spring Cloud Stream is an abstraction that uses the following important concepts to supporet middleware encapsulation: destination binders (integration with messaging systems like Kafka or RabbitMQ), destination bindings (bridge code to external systems) and message (canonical data model to communicate between producer and consumer). As other Spring boot application, it uses extrernal properties to manage most of the configuration of the binders and binding. Spring Cloud Stream Applications are standalone executable applications that communicate over messaging middleware such as Apache Kafka and RabbitMQ. The app is using uber-jars to get the minimal required library and code The following diagram illustrates those concepts for a Spring cloud app: Attention Spring Cloud Stream is not Kafka Streams or Kafka API, it is similar but it represents another abstraction. From a Kafka developer's point of view, it does not seem relevant, as why not using Kafka API and Kafka Streams API, but this is a way to encapsulate any middleware supporting pub/sub and queueing. It may be more comparable to Microprofile reactive messaging specifications and APIs, but not compatible with it. For example binding can be compared to channel of the microprofile reactive messaging constructs. So the development decision will be around middleware abstraction and the way to simplify going from one middleware to another. Now with Kafka, because of its long retention time, it means we can have any type of consumers to read the messages at any time. Those consumers may use Kafka API (Python app or nodejs apps), in this case using the Kafka API within the Spring boot application is a better approach, as the way the abstraction is used may not be fully compatible to any Kafka consumer types. With Kafka based application the best practice is also to define the message structure, using Avro or Protbuf, and use schema registry to ensure compatibility management between applications. To support that Spring Cloud Stream support using native (to the middleware) serialization, which in the case of Kafka could be any serdes APIs or avro API. We will cover that in later section . Example of Kafka binding \u00b6 The order service spring cloud template is a simple example of order service that exposes CRUD operations on the Order entity via a controller. Instead of writing to a database, this service immediately generates a message to Kafka and then the repository class consumes the message to get the data to write to the database. This is a simple way to implement 'transaction' by using the Append log of Kafka partition as a transaction log. The way to generate code from a POST or an internal processing is to use StreamBridge , which exposes a send function to send the record. @Autowired private StreamBridge streamBridge ; public Order processANewOrder ( Order order ) { order . status = OrderStatus . OPEN ; order . orderID = UUID . randomUUID (). toString (); order . creationDate = LocalDate . now (); streamBridge . send ( BINDING_NAME , order ); return order ; } As a good practice is to send a Kafka Record with a Key, which is specialy needed when sending messages to a multi partition topic: The messages with the same key will always go to the same partition. If the partition key is not present, messages will be partitioned in round-robin fashion. Spring Cloud Stream is little bit confusing as it created two concepts for partitioning: the partitionKey and the message key. The partition key is the way to support the same mechanism as Kafka is doing but for other middleware. So for Kafka we do not need to use partitionKey, but then it is important to use the message key construct. As Kafka is evolving on the partition allocation, it is recommended to do not interfere with Kafka mechanims and use the following approach: Provide the message key as a SpEL expression property for example in the header: spring.cloud.stream.bindings.<binding-name>.producer.message-key-expression : headers['messageKey'] Then in your application, when publishing the message, add a header called kafka_messagekey with the value set from the attribute to use as key. Spring Cloud Stream will use the value for this header to assign it to the Kafka record Key: Message < Order > toSend = MessageBuilder . withPayload ( order ) . setHeader ( KafkaHeaders . MESSAGE_KEY , order . customerID . getBytes ()) . setHeader ( MessageHeaders . CONTENT_TYPE , MimeTypeUtils . APPLICATION_JSON ). build (); streamBridge . send ( BINDING_NAME , toSend ); You can also build composite key with a special java bean class for that and use instance of this class as key. ```java CustomerCompanyKey cck = new CustomerCompanyKey(order.customerID,customer.company); Message<Order> toSend = MessageBuilder.withPayload(order) .setHeader(KafkaHeaders.MESSAGE_KEY, cck) .setHeader(MessageHeaders.CONTENT_TYPE, MimeTypeUtils.APPLICATION_JSON).build(); streamBridge.send(BINDING_NAME, toSend); ``` The following screen shot illustrates that all records with the same \"customerID\" are in the same partition: If you want to use the partition key as an alternate way to do partition allocation using Spring Cloud Stream strategy then use a partitionKey: spring.cloud.stream.bindings.<binding-name>.producer.partition-key-expression : headers['partitionKey'] and then in the code: Message < Order > toSend = MessageBuilder . withPayload ( order ) . setHeader ( \"partitionKey\" , order . customerID . getBytes ()) . setHeader ( MessageHeaders . CONTENT_TYPE , MimeTypeUtils . APPLICATION_JSON ). build (); streamBridge . send ( BINDING_NAME , toSend ); Consuming message \u00b6 With the last release of Spring Cloud Stream, consumers are single beans of type Function , Consumer or Supplier . Here is an example of consumer only. @Bean public Consumer < Message < Order >> consumeOrderEvent (){ return msg -> saveOrder ( msg . getPayload ()); } For thew binding configuration the name of the method gives the name of the binding: spring.cloud.stream : bindings : consumeOrderEvent-in-0 : destination : orders contentType : application/json group : orderms-grp useNativeDecoding : true kafka : bindings : consumeOrderEvent-in-0 : consumer : ackMode : MANUAL configuration : value.deserializer : ibm.eda.demo.infrastructure.events.OrderDeserializer The deserialization is declared in a specific class: package ibm.eda.demo.infrastructure.events ; import org.springframework.kafka.support.serializer.JsonDeserializer ; public class OrderDeserializer extends JsonDeserializer < Order > { } In this example above as the goal is to save to the database, we should not auto commit the offset reading. So the following settings are needed on the consumer side: spring.cloud.stream.kafka : bindings : consumeOrderEvent-in-0 : consumer : autoCommitOffset : false startOffset : latest ackMode : MANUAL And the consumer code is now looking at the acknowledge header property if present or not and perform manual acknowledge once the save operation is successful. @Bean public Consumer < Message < Order >> consumeOrderEvent (){ return msg -> { Acknowledgment acknowledgment = msg . getHeaders (). get ( KafkaHeaders . ACKNOWLEDGMENT , Acknowledgment . class ); saveOrder ( msg . getPayload ()); if ( acknowledgment != null ) { acknowledgment . acknowledge (); } }; Kafka spring cloud stream app basic \u00b6 The approach to develop such application includes the following steps: A spring boot application, with REST spring web starter Define a resource and a controller for the REST API. Define inbound and/or outbound binding to communicate to underlying middleware Add method to process incoming message, taking into account the underlying middleware and serialization. For example with Kafka, most of the consumers may not auto commit the read offset but control the commit by using manual commit. Add logic to produce message using middleware To add a consumer from a Kafka topic for example, we can add a function that will process the message, and declare it as a Bean. @Bean public Consumer < Message < CloudEvent >> consumeCloudEventEvent (){ return msg -> { Acknowledgment acknowledgment = msg . getHeaders (). get ( KafkaHeaders . ACKNOWLEDGMENT , Acknowledgment . class ); saveOrder (( Order ) msg . getPayload (). getData ()); if ( acknowledgment != null ) { System . out . println ( \"Acknowledgment provided\" ); acknowledgment . acknowledge (); } }; } This previous code is also illustrating manual offset commit. Then we add configuration to link to the binders queue or topic: consumeOrderEvent-in-0 : consumer : autoCommitOffset : false startOffset : latest ackMode : MANUAL configuration : value.deserializer : ibm.eda.demo.infrastructure.events.CloudEventDeserializer Avro serialization \u00b6 producer : useNativeEncoding : true","title":"Spring cloud"},{"location":"technology/spring/#spring-cloud","text":"Spring Cloud is based on Spring boot programming model but focusing on cloud native deployment and distributed computing. As other spring boot app it includes jetty or tomcat, health checks, metrics... It supports the following patterns: Distributed/versioned configuration : externalize config in distributed system with config server. Service registration and discovery: uses Netflix Eureka, Apache Zookeeper or Consul to keep service information. Routing: supports HTTP (Open Feign or Netflix Ribbon for load balancing) and messaging (RabbitMQ and Kafka) Service-to-service calls: Sptring Cloud Gateway and Netflix Zuul is used Load balancing Circuit Breakers: based on Netflix Hystrix: if the request fails for n time, the circuit open. Global locks Leadership election and cluster state Distributed messaging It also supports pipelines for ci/cd and contract testing for interface validation.","title":"Spring Cloud"},{"location":"technology/spring/#getting-started","text":"Use start.spring.io to create the application starting code using Kafka, Actuator, Cloud Stream or add the Spring Cloud BOM to your maven pom.xml file. See the Adding Spring Cloud To An Existing Spring Boot Application section. As most of the microservices expose REST resource, we may need to add the starter web: <dependency> <groupId> org.springframework.boot </groupId> <artifactId> spring-boot-starter-web </artifactId> </dependency> We also need to install the Spring Cloud CLI . Then add the Spring cloud starter as dependency. When using config server, we need to add the config client. <dependency> <groupId> org.springframework.cloud </groupId> <artifactId> spring-cloud-conflig-client </artifactId> </dependency> For centralized tracing uses, starter-sleuth, and zipkin. <dependency> <groupId> org.springframework.cloud </groupId> <artifactId> spring-cloud-starter-sleuth </artifactId> </dependency> <dependency> <groupId> org.springframework.cloud </groupId> <artifactId> spring-cloud-starter-zipkin </artifactId> </dependency> For service discovery add netflix-eureka-client. <dependency> <groupId> org.springframework.cloud </groupId> <artifactId> spring-cloud-starter-eureka </artifactId> </dependency> Using the Spring Cloud CLI we can get the service registry, config server, central tracing started in one command: spring cloud eureka configserver zipkin","title":"Getting started"},{"location":"technology/spring/#spring-cloud-config","text":"Use the concept of Config Server you have a central place to manage external properties for applications across all environments. As an application moves through the deployment pipeline from dev to test and into production you can manage the configuration between those environments and be certain that applications have everything they need to run when they migrate. @Value ( \"${config.in.topic}\" ) String topicName = \"orders\" ; The value of the config.in.topic comes from local configuration or remote config server. The config server will serve content from a git. See this sample for such server.","title":"Spring Cloud config"},{"location":"technology/spring/#spring-cloud-stream","text":"Spring Cloud Stream is a framework for building highly scalable event-driven microservices connected with shared messaging systems. It unifies lots of popular messaging platforms behind one easy to use API including RabbitMQ, Apache Kafka, Amazon Kinesis, Google PubSub, Solace PubSub+, Azure Event Hubs, and Apache RocketMQ. Spring Cloud Stream is an abstraction that uses the following important concepts to supporet middleware encapsulation: destination binders (integration with messaging systems like Kafka or RabbitMQ), destination bindings (bridge code to external systems) and message (canonical data model to communicate between producer and consumer). As other Spring boot application, it uses extrernal properties to manage most of the configuration of the binders and binding. Spring Cloud Stream Applications are standalone executable applications that communicate over messaging middleware such as Apache Kafka and RabbitMQ. The app is using uber-jars to get the minimal required library and code The following diagram illustrates those concepts for a Spring cloud app: Attention Spring Cloud Stream is not Kafka Streams or Kafka API, it is similar but it represents another abstraction. From a Kafka developer's point of view, it does not seem relevant, as why not using Kafka API and Kafka Streams API, but this is a way to encapsulate any middleware supporting pub/sub and queueing. It may be more comparable to Microprofile reactive messaging specifications and APIs, but not compatible with it. For example binding can be compared to channel of the microprofile reactive messaging constructs. So the development decision will be around middleware abstraction and the way to simplify going from one middleware to another. Now with Kafka, because of its long retention time, it means we can have any type of consumers to read the messages at any time. Those consumers may use Kafka API (Python app or nodejs apps), in this case using the Kafka API within the Spring boot application is a better approach, as the way the abstraction is used may not be fully compatible to any Kafka consumer types. With Kafka based application the best practice is also to define the message structure, using Avro or Protbuf, and use schema registry to ensure compatibility management between applications. To support that Spring Cloud Stream support using native (to the middleware) serialization, which in the case of Kafka could be any serdes APIs or avro API. We will cover that in later section .","title":"Spring Cloud Stream"},{"location":"technology/spring/#example-of-kafka-binding","text":"The order service spring cloud template is a simple example of order service that exposes CRUD operations on the Order entity via a controller. Instead of writing to a database, this service immediately generates a message to Kafka and then the repository class consumes the message to get the data to write to the database. This is a simple way to implement 'transaction' by using the Append log of Kafka partition as a transaction log. The way to generate code from a POST or an internal processing is to use StreamBridge , which exposes a send function to send the record. @Autowired private StreamBridge streamBridge ; public Order processANewOrder ( Order order ) { order . status = OrderStatus . OPEN ; order . orderID = UUID . randomUUID (). toString (); order . creationDate = LocalDate . now (); streamBridge . send ( BINDING_NAME , order ); return order ; } As a good practice is to send a Kafka Record with a Key, which is specialy needed when sending messages to a multi partition topic: The messages with the same key will always go to the same partition. If the partition key is not present, messages will be partitioned in round-robin fashion. Spring Cloud Stream is little bit confusing as it created two concepts for partitioning: the partitionKey and the message key. The partition key is the way to support the same mechanism as Kafka is doing but for other middleware. So for Kafka we do not need to use partitionKey, but then it is important to use the message key construct. As Kafka is evolving on the partition allocation, it is recommended to do not interfere with Kafka mechanims and use the following approach: Provide the message key as a SpEL expression property for example in the header: spring.cloud.stream.bindings.<binding-name>.producer.message-key-expression : headers['messageKey'] Then in your application, when publishing the message, add a header called kafka_messagekey with the value set from the attribute to use as key. Spring Cloud Stream will use the value for this header to assign it to the Kafka record Key: Message < Order > toSend = MessageBuilder . withPayload ( order ) . setHeader ( KafkaHeaders . MESSAGE_KEY , order . customerID . getBytes ()) . setHeader ( MessageHeaders . CONTENT_TYPE , MimeTypeUtils . APPLICATION_JSON ). build (); streamBridge . send ( BINDING_NAME , toSend ); You can also build composite key with a special java bean class for that and use instance of this class as key. ```java CustomerCompanyKey cck = new CustomerCompanyKey(order.customerID,customer.company); Message<Order> toSend = MessageBuilder.withPayload(order) .setHeader(KafkaHeaders.MESSAGE_KEY, cck) .setHeader(MessageHeaders.CONTENT_TYPE, MimeTypeUtils.APPLICATION_JSON).build(); streamBridge.send(BINDING_NAME, toSend); ``` The following screen shot illustrates that all records with the same \"customerID\" are in the same partition: If you want to use the partition key as an alternate way to do partition allocation using Spring Cloud Stream strategy then use a partitionKey: spring.cloud.stream.bindings.<binding-name>.producer.partition-key-expression : headers['partitionKey'] and then in the code: Message < Order > toSend = MessageBuilder . withPayload ( order ) . setHeader ( \"partitionKey\" , order . customerID . getBytes ()) . setHeader ( MessageHeaders . CONTENT_TYPE , MimeTypeUtils . APPLICATION_JSON ). build (); streamBridge . send ( BINDING_NAME , toSend );","title":"Example of Kafka binding"},{"location":"technology/spring/#consuming-message","text":"With the last release of Spring Cloud Stream, consumers are single beans of type Function , Consumer or Supplier . Here is an example of consumer only. @Bean public Consumer < Message < Order >> consumeOrderEvent (){ return msg -> saveOrder ( msg . getPayload ()); } For thew binding configuration the name of the method gives the name of the binding: spring.cloud.stream : bindings : consumeOrderEvent-in-0 : destination : orders contentType : application/json group : orderms-grp useNativeDecoding : true kafka : bindings : consumeOrderEvent-in-0 : consumer : ackMode : MANUAL configuration : value.deserializer : ibm.eda.demo.infrastructure.events.OrderDeserializer The deserialization is declared in a specific class: package ibm.eda.demo.infrastructure.events ; import org.springframework.kafka.support.serializer.JsonDeserializer ; public class OrderDeserializer extends JsonDeserializer < Order > { } In this example above as the goal is to save to the database, we should not auto commit the offset reading. So the following settings are needed on the consumer side: spring.cloud.stream.kafka : bindings : consumeOrderEvent-in-0 : consumer : autoCommitOffset : false startOffset : latest ackMode : MANUAL And the consumer code is now looking at the acknowledge header property if present or not and perform manual acknowledge once the save operation is successful. @Bean public Consumer < Message < Order >> consumeOrderEvent (){ return msg -> { Acknowledgment acknowledgment = msg . getHeaders (). get ( KafkaHeaders . ACKNOWLEDGMENT , Acknowledgment . class ); saveOrder ( msg . getPayload ()); if ( acknowledgment != null ) { acknowledgment . acknowledge (); } };","title":"Consuming message"},{"location":"technology/spring/#kafka-spring-cloud-stream-app-basic","text":"The approach to develop such application includes the following steps: A spring boot application, with REST spring web starter Define a resource and a controller for the REST API. Define inbound and/or outbound binding to communicate to underlying middleware Add method to process incoming message, taking into account the underlying middleware and serialization. For example with Kafka, most of the consumers may not auto commit the read offset but control the commit by using manual commit. Add logic to produce message using middleware To add a consumer from a Kafka topic for example, we can add a function that will process the message, and declare it as a Bean. @Bean public Consumer < Message < CloudEvent >> consumeCloudEventEvent (){ return msg -> { Acknowledgment acknowledgment = msg . getHeaders (). get ( KafkaHeaders . ACKNOWLEDGMENT , Acknowledgment . class ); saveOrder (( Order ) msg . getPayload (). getData ()); if ( acknowledgment != null ) { System . out . println ( \"Acknowledgment provided\" ); acknowledgment . acknowledge (); } }; } This previous code is also illustrating manual offset commit. Then we add configuration to link to the binders queue or topic: consumeOrderEvent-in-0 : consumer : autoCommitOffset : false startOffset : latest ackMode : MANUAL configuration : value.deserializer : ibm.eda.demo.infrastructure.events.CloudEventDeserializer","title":"Kafka spring cloud stream app basic"},{"location":"technology/spring/#avro-serialization","text":"producer : useNativeEncoding : true","title":"Avro serialization"},{"location":"use-cases/connect-cos/","text":"Kafka Connect to IBM COS \u00b6 Info Updated 02/22/2022 Introduction \u00b6 One of the classical use case for event driven solution based on Kafka is to keep the message for a longer time period than the retention time of the Kafka topic. This demonstration illustrates how to do so using IBM Event Streams and IBM Cloud Object Storage as a service. We have created a very simple Quarkus (a super sonic and sub-atomic Kubernetes native framework for Java) application that uses MicroProfile Reactive Messaging in order to send a stream of data to our Event Streams/Kafka topic. The Kafka Connect cluster includes a IBM Cloud Object Storage Connector to grab messages fomr the topoc and place into an IBM COS Bucket. Skill level \u00b6 The skill level of this learning path is for a beginner. Estimated time to complete \u00b6 It will take you approximately 15 minuttes to complete this entire demonstration. Scenario Prerequisites \u00b6 OpenShift Container Platform Cluster - This scenario will assume you have a 4.7+ Cluster as we will make use of Operators. Cloud Pak for Integration - Updated with 2021.0.3 release of the Cloud Pak for Integration installed on OpenShift. This story will also assume you have followed the installation instructions for Event Streams outlined in the 2021-3 product documentation or used one of our GitOps repository approach to deploy Event Streams Operator. Git - We will need to clone repositories. An IBM Cloud Account (free) - A free (Lite) IBM Cloud Object Storage trial Service account IBM Cloud Object Storage If you want to modify the code then you need: Java - Java Development Kit (JDK) v1.11+ (Java 11+) Maven - The scenario uses Maven v3.6.3 Gradle - Ideally v4.0+ (Note - the gradle shadowJar command might not work on Java versions newer to Java 8) An IDE of your choice - Visual Studio Code is used in this scenario. Use Case Guided Tour \u00b6 The use case is quite simple, but very common. Topic has configuration to keep messages in the kafka broker disks for a certain time perior or until the log reach a certain size. Which means records will be removed from the brokers over time. Some companies put retention time to a value to keep data foreever. This is possible when the amount of data in such topic is not reasonable. But most of the time we need to design solution to move data to longer persistence. S3 buckets is one of such long term persistence as it will bring elasticity, transparent replication and geolocalization. An infrastructure engineer needs to prepare the Cloud Object Storage service and organize the bucket strategy to keep data. Then on the OpenShift production cluster, Event Streams is deployed and Kafka connector cluster defined. The connector configuration will stipulate the COS connection credentials and the Event Streams connection URL and credentials. Once the connector is started all messages in the designated topics will go to the COS bucket. IBM Event Streams team has explanation of the Cloud Object Storage Sink Kafka connector configuration in this repository . Full Demo Narration \u00b6 1- Create an IBM COS Service and COS Bucket \u00b6 In this section, we are going to see how to create an IBM Cloud Obeject Storage (IBM COS) Service in your IBM Cloud account and a bucket within your IBM COS Service. We assume you already have an IBM Cloud account and, if not, you can sign up for one here at IBM Cloud . Once you are inside your IBM Cloud account, traverse to the Catalog section. In the search type in IBM Cloud Object Storage Name your IBM COS Service with something unique. Since this is a free account, we can stick with the Lite Plan . Now that the IBM Cloud Object Storage Service is created, create a new bucket. On the Create Bucket screen pick Custom Bucket . When selecting options for the bucket, name your bucket something unique. For Resiliency let's select Regional . For location select an area from the drop-down that you want. Use Standard for Storage Class . Leave everything else as-is and hit Create Bucket . 2- Create IBM COS Service Credentials \u00b6 Now that we have created our IBM Cloud Object Storage Service and bucket, we need to create the Service Credential so that Kafka connector can connect to it. Inside your IBM COS Service, select Service Credentials and then click the New Credential button. Name your credential and select Manager from the Role: drop-down menu and click Add . Expand your newly created Service Credential and write down the values for \"apikey\" and \"resource_instance_id\" . You will need this later in the Build and Apply IBM COS Sink Connector section. 3- Create a demo project \u00b6 To isolate the demonstration in the OpenShift Cluster, we will deploy the demo code, event streams cluster and Kafka Connect in one project. Clone the tutorial project git clone https://github.com/ibm-cloud-architecture/eda-quickstarts Create an OpenShift project (k8s namespace) named: eda-cos oc apply -k gitOps/env/base 4- Deploy Event Streams Cluster \u00b6 If you do not have a Event Stream cluster already deployed on OpenShift, we propose to deploy one in a demo project, using Event Streams Operator monitoring All Namespaces Using IBM entitled registry entitlement key define your a secret so deployment process can download IBM Event Streams images: KEY = <yourentitlementkey> oc create secret docker-registry ibm-entitlement-key \\ --docker-username = cp \\ --docker-password = $KEY \\ --docker-server = cp.icr.io \\ --namespace = eda-cos Deploy Event Streams Cluster. oc apply -k gitOps/services/es # --> Results eventstreams.eventstreams.ibm.com/dev created kafkatopic.eventstreams.ibm.com/edademo-orders created It will take sometime to get the cluster created. Monitor with oc get pod -w . You should get: dev-entity-operator-6d7d94f68f-6lk86 3/3 dev-ibm-es-admapi-ffd89fdf-x99lq 1/1 dev-ibm-es-ui-74bf84dc67-qx9kk 2/2 dev-kafka-0 1/1 dev-zookeeper-0 1/1 With this deployment there is no external route, only on bootstrap URL: dev-kafka-bootstrap.eda-cos.svc:9092 . The Kafka listener is using PLAINTEXT connection. So no SSL encryption and no authentication. Deploy the existing application (the image we built is in quay.io/ibmcase) using: oc apply -k gitOps/apps/eda-cos-demo/base/ Test the deployed app by accessing its route and GET API: HOST = $( oc get routes eda-cos-demo -o jsonpath = '{.spec.host}' ) curl -X GET http:// $HOST /api/v1/version 5- Deploy Kafka Connector \u00b6 We have already built a kafka connect image with the Cloud Object Storage jar and push it as image to quay.io. Deploy the connector cluster by using the KafkaConnect custom resource: oc apply -f gitOps/services/kconnect/kafka-connect.yaml # Verify cluster is ready oc get kafkaconnect 6- Deploy the COS sink connector \u00b6 Create a new file named kafka-cos-sink-connector.yaml and past the following code in it. apiVersion : eventstreams.ibm.com/v1alpha1 kind : KafkaConnector metadata : name : cos-sink-connector labels : eventstreams.ibm.com/cluster : YOUR_KAFKA_CONNECT_CLUSTER_NAME spec : class : com.ibm.eventstreams.connect.cossink.COSSinkConnector tasksMax : 1 config : key.converter : org.apache.kafka.connect.storage.StringConverter value.converter : org.apache.kafka.connect.storage.StringConverter topics : TOPIC_NAME cos.api.key : IBM_COS_API_KEY cos.bucket.location : IBM_COS_BUCKET_LOCATION cos.bucket.name : IBM_COS_BUCKET_NAME cos.bucket.resiliency : IBM_COS_RESILIENCY cos.service.crn : \"IBM_COS_CRM\" cos.object.records : 5 cos.object.deadline.seconds : 5 cos.object.interval.seconds : 5 where: YOUR_KAFKA_CONNECT_CLUSTER_NAME : is the name you gave previously to your Kakfa Connect cluster. TOPIC_NAME : is the name of the topic you created in IBM Event Streams at the beginning of this lab. IBM_COS_API_KEY : is your IBM Cloud Object Storage service credentials apikey value. Review first sections of this lab if you don't remember where and how to find this value. IBM_COS_BUCKET_LOCATION : is your IBM Cloud Object Storage bucket location. Review first sections of this lab if you don't remember where and how to find this value (it usually is in the form of something like us-east or eu-gb for example). IBM_COS_RESILIENCY : is your IBM Cloud Object Storage resiliency option. Review first sections of this lab if you don't remember where and how to find this value (it should be regional ). IBM_COS_CRM : is your IBM Cloud Object Storage CRN. Review first sections of this lab if you don't remember where and how to find this value. It usually ends with a double :: at the end of it. IMPORTANT: you might need to retain the double quotation marks here as the CRN has colons in it and may collide with yaml syntax. Apply the yaml which will create a KafkaConnnector custom resource behind the scenes and register/set up the IBM COS Sink Connector in your Kafka Connect cluster. oc apply -f kafka-cos-sink-connector.yaml The initialization of the connector can take a minute or two. You can check the status of the connector to see if everything connected succesfully. oc describe kafkaconnector cos-sink-connector When the IBM COS Sink connector is successfully up and running you should see something similar to the below. You should also see a new connector being registered if you exec into the Kafka Connect cluster pod and query for the existing connectors again: oc exec -it YOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-2-xxxxx bash bash-4.4$ curl localhost:8083/connectors [ \"cos-sink-connector\" ] 7- Test by sending some records \u00b6 Post 10 orders: sh curl -X POST 'http://$HOST/api/v1/start' -H 'accept: application/json' -H 'Content-Type: application/json' -d '10' Verify whether the messages from the IBM Event Streams topic are getting propagated to your IBM Cloud Object Storage bucket. If you go to your IBM COS bucket, you should find some files in it. The name of the file inside the bucket has starting offset and ending offset. You can download one of these object files to make sure that the value inside matches the value inside your INBOUND topic. Developer Corner \u00b6 The https://github.com/ibm-cloud-architecture/eda-quickstarts.git cos-tutorial folder includes a microprofile reactive messaging producer app basd on quarkus runtime. You have two choices for this application, reuse the existing code as-is or start by using quarkus CLI. Clone the eda-quickstarts repository. git clone https://github.com/ibm-cloud-architecture/eda-quickstarts.git cd cos-tutorial quarkus dev If you want to start on your own, then create the Quarkus project. quarkus create app cos-tutorial cd cos-tutorial quarkus ext add reactive-messaging-kafka, mutiny, openshift and get inspiration from the DemoController.java . Review the DemoController.java file. The @Channel annotation indicates that we're sending to a channel defined with reactive messaging in the application.properties. The startDemo() function generate n orders and send to the channel. The reactive messaging is defined in the application.properties . The setting are using the a kafka cluster named dev , we encourage to keep this name or you need to modify a lot of yaml files. See the COS tutorial README to run the producer application locally with docker compose or quarkus dev.","title":"Kafka Connect - COS"},{"location":"use-cases/connect-cos/#kafka-connect-to-ibm-cos","text":"Info Updated 02/22/2022","title":"Kafka Connect to IBM COS"},{"location":"use-cases/connect-cos/#introduction","text":"One of the classical use case for event driven solution based on Kafka is to keep the message for a longer time period than the retention time of the Kafka topic. This demonstration illustrates how to do so using IBM Event Streams and IBM Cloud Object Storage as a service. We have created a very simple Quarkus (a super sonic and sub-atomic Kubernetes native framework for Java) application that uses MicroProfile Reactive Messaging in order to send a stream of data to our Event Streams/Kafka topic. The Kafka Connect cluster includes a IBM Cloud Object Storage Connector to grab messages fomr the topoc and place into an IBM COS Bucket.","title":"Introduction"},{"location":"use-cases/connect-cos/#skill-level","text":"The skill level of this learning path is for a beginner.","title":"Skill level"},{"location":"use-cases/connect-cos/#estimated-time-to-complete","text":"It will take you approximately 15 minuttes to complete this entire demonstration.","title":"Estimated time to complete"},{"location":"use-cases/connect-cos/#scenario-prerequisites","text":"OpenShift Container Platform Cluster - This scenario will assume you have a 4.7+ Cluster as we will make use of Operators. Cloud Pak for Integration - Updated with 2021.0.3 release of the Cloud Pak for Integration installed on OpenShift. This story will also assume you have followed the installation instructions for Event Streams outlined in the 2021-3 product documentation or used one of our GitOps repository approach to deploy Event Streams Operator. Git - We will need to clone repositories. An IBM Cloud Account (free) - A free (Lite) IBM Cloud Object Storage trial Service account IBM Cloud Object Storage If you want to modify the code then you need: Java - Java Development Kit (JDK) v1.11+ (Java 11+) Maven - The scenario uses Maven v3.6.3 Gradle - Ideally v4.0+ (Note - the gradle shadowJar command might not work on Java versions newer to Java 8) An IDE of your choice - Visual Studio Code is used in this scenario.","title":"Scenario Prerequisites"},{"location":"use-cases/connect-cos/#use-case-guided-tour","text":"The use case is quite simple, but very common. Topic has configuration to keep messages in the kafka broker disks for a certain time perior or until the log reach a certain size. Which means records will be removed from the brokers over time. Some companies put retention time to a value to keep data foreever. This is possible when the amount of data in such topic is not reasonable. But most of the time we need to design solution to move data to longer persistence. S3 buckets is one of such long term persistence as it will bring elasticity, transparent replication and geolocalization. An infrastructure engineer needs to prepare the Cloud Object Storage service and organize the bucket strategy to keep data. Then on the OpenShift production cluster, Event Streams is deployed and Kafka connector cluster defined. The connector configuration will stipulate the COS connection credentials and the Event Streams connection URL and credentials. Once the connector is started all messages in the designated topics will go to the COS bucket. IBM Event Streams team has explanation of the Cloud Object Storage Sink Kafka connector configuration in this repository .","title":"Use Case Guided Tour"},{"location":"use-cases/connect-cos/#full-demo-narration","text":"","title":"Full Demo Narration"},{"location":"use-cases/connect-cos/#1-create-an-ibm-cos-service-and-cos-bucket","text":"In this section, we are going to see how to create an IBM Cloud Obeject Storage (IBM COS) Service in your IBM Cloud account and a bucket within your IBM COS Service. We assume you already have an IBM Cloud account and, if not, you can sign up for one here at IBM Cloud . Once you are inside your IBM Cloud account, traverse to the Catalog section. In the search type in IBM Cloud Object Storage Name your IBM COS Service with something unique. Since this is a free account, we can stick with the Lite Plan . Now that the IBM Cloud Object Storage Service is created, create a new bucket. On the Create Bucket screen pick Custom Bucket . When selecting options for the bucket, name your bucket something unique. For Resiliency let's select Regional . For location select an area from the drop-down that you want. Use Standard for Storage Class . Leave everything else as-is and hit Create Bucket .","title":"1- Create an IBM COS Service and COS Bucket"},{"location":"use-cases/connect-cos/#2-create-ibm-cos-service-credentials","text":"Now that we have created our IBM Cloud Object Storage Service and bucket, we need to create the Service Credential so that Kafka connector can connect to it. Inside your IBM COS Service, select Service Credentials and then click the New Credential button. Name your credential and select Manager from the Role: drop-down menu and click Add . Expand your newly created Service Credential and write down the values for \"apikey\" and \"resource_instance_id\" . You will need this later in the Build and Apply IBM COS Sink Connector section.","title":"2- Create IBM COS Service Credentials"},{"location":"use-cases/connect-cos/#3-create-a-demo-project","text":"To isolate the demonstration in the OpenShift Cluster, we will deploy the demo code, event streams cluster and Kafka Connect in one project. Clone the tutorial project git clone https://github.com/ibm-cloud-architecture/eda-quickstarts Create an OpenShift project (k8s namespace) named: eda-cos oc apply -k gitOps/env/base","title":"3- Create a demo project"},{"location":"use-cases/connect-cos/#4-deploy-event-streams-cluster","text":"If you do not have a Event Stream cluster already deployed on OpenShift, we propose to deploy one in a demo project, using Event Streams Operator monitoring All Namespaces Using IBM entitled registry entitlement key define your a secret so deployment process can download IBM Event Streams images: KEY = <yourentitlementkey> oc create secret docker-registry ibm-entitlement-key \\ --docker-username = cp \\ --docker-password = $KEY \\ --docker-server = cp.icr.io \\ --namespace = eda-cos Deploy Event Streams Cluster. oc apply -k gitOps/services/es # --> Results eventstreams.eventstreams.ibm.com/dev created kafkatopic.eventstreams.ibm.com/edademo-orders created It will take sometime to get the cluster created. Monitor with oc get pod -w . You should get: dev-entity-operator-6d7d94f68f-6lk86 3/3 dev-ibm-es-admapi-ffd89fdf-x99lq 1/1 dev-ibm-es-ui-74bf84dc67-qx9kk 2/2 dev-kafka-0 1/1 dev-zookeeper-0 1/1 With this deployment there is no external route, only on bootstrap URL: dev-kafka-bootstrap.eda-cos.svc:9092 . The Kafka listener is using PLAINTEXT connection. So no SSL encryption and no authentication. Deploy the existing application (the image we built is in quay.io/ibmcase) using: oc apply -k gitOps/apps/eda-cos-demo/base/ Test the deployed app by accessing its route and GET API: HOST = $( oc get routes eda-cos-demo -o jsonpath = '{.spec.host}' ) curl -X GET http:// $HOST /api/v1/version","title":"4- Deploy Event Streams Cluster"},{"location":"use-cases/connect-cos/#5-deploy-kafka-connector","text":"We have already built a kafka connect image with the Cloud Object Storage jar and push it as image to quay.io. Deploy the connector cluster by using the KafkaConnect custom resource: oc apply -f gitOps/services/kconnect/kafka-connect.yaml # Verify cluster is ready oc get kafkaconnect","title":"5- Deploy Kafka Connector"},{"location":"use-cases/connect-cos/#6-deploy-the-cos-sink-connector","text":"Create a new file named kafka-cos-sink-connector.yaml and past the following code in it. apiVersion : eventstreams.ibm.com/v1alpha1 kind : KafkaConnector metadata : name : cos-sink-connector labels : eventstreams.ibm.com/cluster : YOUR_KAFKA_CONNECT_CLUSTER_NAME spec : class : com.ibm.eventstreams.connect.cossink.COSSinkConnector tasksMax : 1 config : key.converter : org.apache.kafka.connect.storage.StringConverter value.converter : org.apache.kafka.connect.storage.StringConverter topics : TOPIC_NAME cos.api.key : IBM_COS_API_KEY cos.bucket.location : IBM_COS_BUCKET_LOCATION cos.bucket.name : IBM_COS_BUCKET_NAME cos.bucket.resiliency : IBM_COS_RESILIENCY cos.service.crn : \"IBM_COS_CRM\" cos.object.records : 5 cos.object.deadline.seconds : 5 cos.object.interval.seconds : 5 where: YOUR_KAFKA_CONNECT_CLUSTER_NAME : is the name you gave previously to your Kakfa Connect cluster. TOPIC_NAME : is the name of the topic you created in IBM Event Streams at the beginning of this lab. IBM_COS_API_KEY : is your IBM Cloud Object Storage service credentials apikey value. Review first sections of this lab if you don't remember where and how to find this value. IBM_COS_BUCKET_LOCATION : is your IBM Cloud Object Storage bucket location. Review first sections of this lab if you don't remember where and how to find this value (it usually is in the form of something like us-east or eu-gb for example). IBM_COS_RESILIENCY : is your IBM Cloud Object Storage resiliency option. Review first sections of this lab if you don't remember where and how to find this value (it should be regional ). IBM_COS_CRM : is your IBM Cloud Object Storage CRN. Review first sections of this lab if you don't remember where and how to find this value. It usually ends with a double :: at the end of it. IMPORTANT: you might need to retain the double quotation marks here as the CRN has colons in it and may collide with yaml syntax. Apply the yaml which will create a KafkaConnnector custom resource behind the scenes and register/set up the IBM COS Sink Connector in your Kafka Connect cluster. oc apply -f kafka-cos-sink-connector.yaml The initialization of the connector can take a minute or two. You can check the status of the connector to see if everything connected succesfully. oc describe kafkaconnector cos-sink-connector When the IBM COS Sink connector is successfully up and running you should see something similar to the below. You should also see a new connector being registered if you exec into the Kafka Connect cluster pod and query for the existing connectors again: oc exec -it YOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-2-xxxxx bash bash-4.4$ curl localhost:8083/connectors [ \"cos-sink-connector\" ]","title":"6- Deploy the COS sink connector"},{"location":"use-cases/connect-cos/#7-test-by-sending-some-records","text":"Post 10 orders: sh curl -X POST 'http://$HOST/api/v1/start' -H 'accept: application/json' -H 'Content-Type: application/json' -d '10' Verify whether the messages from the IBM Event Streams topic are getting propagated to your IBM Cloud Object Storage bucket. If you go to your IBM COS bucket, you should find some files in it. The name of the file inside the bucket has starting offset and ending offset. You can download one of these object files to make sure that the value inside matches the value inside your INBOUND topic.","title":"7- Test by sending some records"},{"location":"use-cases/connect-cos/#developer-corner","text":"The https://github.com/ibm-cloud-architecture/eda-quickstarts.git cos-tutorial folder includes a microprofile reactive messaging producer app basd on quarkus runtime. You have two choices for this application, reuse the existing code as-is or start by using quarkus CLI. Clone the eda-quickstarts repository. git clone https://github.com/ibm-cloud-architecture/eda-quickstarts.git cd cos-tutorial quarkus dev If you want to start on your own, then create the Quarkus project. quarkus create app cos-tutorial cd cos-tutorial quarkus ext add reactive-messaging-kafka, mutiny, openshift and get inspiration from the DemoController.java . Review the DemoController.java file. The @Channel annotation indicates that we're sending to a channel defined with reactive messaging in the application.properties. The startDemo() function generate n orders and send to the channel. The reactive messaging is defined in the application.properties . The setting are using the a kafka cluster named dev , we encourage to keep this name or you need to modify a lot of yaml files. See the COS tutorial README to run the producer application locally with docker compose or quarkus dev.","title":"Developer Corner"},{"location":"use-cases/connect-cos/older-section/","text":"Set up the Kafka Connect Cluster \u00b6 In this section, we are going to see how to deploy a Kafka Connect cluster on OpenShift which will be the engine running the source and sink connector we decide to use for our use case. IMPORTANT: We assume you have deployed your IBM Event Streams instance with an internal TLS secured listener which your Kafka Connect cluster will use to connect. For more detail about listeners, check the IBM Event Streams documentation here . If you inspect your IBM Event Streams instance by executing the following command: oc get EventStreams YOUR_IBM_EVENT_STREAMS_INSTANCE_NAME -o yaml You should see a TlS listener: Now, follow the next steps in order to get your Kafka Connect cluster deployed: Go to you IBM Event Streams dashboard, click on the Find more on the toolbox option. Click on the Set up button for the Set up a Kafka Connect environment option. Click on Download Kafka Connect ZIP button. The above downloads a zip file which contains a kafka-connect-s2i.yaml file. Open that yaml file and take note of the productID and cloudpakId values as you will need these in the following step. Instead of using the previous yaml file, create a new kafka-connect-s2i.yaml file with the following contents: apiVersion : eventstreams.ibm.com/v1beta1 kind : KafkaConnectS2I metadata : name : YOUR_KAFKA_CONNECT_CLUSTER_NAME annotations : eventstreams.ibm.com/use-connector-resources : \"true\" spec : logging : type : external name : custom-connect-log4j version : 2.6.0 replicas : 1 bootstrapServers : YOUR_INTERNAL_BOOTSTRAP_ADDRESS template : pod : imagePullSecrets : [] metadata : annotations : eventstreams.production.type : CloudPakForIntegrationNonProduction productID : YOUR_PRODUCT_ID productName : IBM Event Streams for Non Production productVersion : 10.2.0 productMetric : VIRTUAL_PROCESSOR_CORE productChargedContainers : YOUR_KAFKA_CONNECT_CLUSTER_NAME cloudpakId : YOUR_CLOUDPAK_ID cloudpakName : IBM Cloud Pak for Integration cloudpakVersion : 2020.4.1 productCloudpakRatio : \"2:1\" tls : trustedCertificates : - secretName : YOUR_CLUSTER_TLS_CERTIFICATE_SECRET certificate : ca.crt authentication : type : tls certificateAndKey : certificate : user.crt key : user.key secretName : YOUR_TLS_CREDENTIALS_SECRET config : group.id : YOUR_KAFKA_CONNECT_CLUSTER_NAME key.converter : org.apache.kafka.connect.json.JsonConverter value.converter : org.apache.kafka.connect.json.JsonConverter key.converter.schemas.enable : false value.converter.schemas.enable : false offset.storage.topic : YOUR_KAFKA_CONNECT_CLUSTER_NAME-offsets config.storage.topic : YOUR_KAFKA_CONNECT_CLUSTER_NAME-configs status.storage.topic : YOUR_KAFKA_CONNECT_CLUSTER_NAME-status config.storage.replication.factor : 1 offset.storage.replication.factor : 1 status.storage.replication.factor : 1 where you will need to replace the following placeholders with the appropriate values for you IBM Event Streams cluster and service credentials: * YOUR_KAFKA_CONNECT_CLUSTER_NAME : A name you want to provide your Kafka Connect cluster and resources with. * YOUR_INTERNAL_BOOTSTRAP_ADDRESS : This is the internal bootstrap address of your IBM Event Streams instance. You can review how to find this url here . Use the internal bootstrap address which should be in the form of YOUR_IBM_EVENT_STREAMS_INSTANCE_NAME-kafka-bootstrap.eventstreams.svc:9093 : * YOUR_TLS_CREDENTIALS_SECRET : This is the name you give to your TLS credentials for your internal IBM Event Streams listener when you click on Generate TLS credentials : * YOUR_CLUSTER_TLS_CERTIFICATE_SECRET : This is the secret name where IBM Event Streams stores the TLS certificate for establishing secure communications. This secret name is in the form of YOUR_IBM_EVENT_STREAMS_INSTANCE_NAME-cluster-ca-cert . You can always use oc get secrets to list all the secrets. * YOUR_PRODUCT_ID : This is the productID value you noted down earlier. * YOUR_CLOUDPAK_ID : This is the cloudpakID value you noted earlier. Deploy your Kafka Connect cluster by executing oc apply -f kafkaconnect-s2i.yaml If you list the pods, you should see three new pods: one for the Kafka Connect build task, another for the Kafka Connect deploy task and the actual Kafka Connect cluster pod. oc get pods NAME READY STATUS RESTARTS AGE YOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-1-build 0 /1 Completed 0 18m YOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-1-deploy 0 /1 Completed 0 17m YOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-1-xxxxx 1 /1 Running 0 17m ## Build and Inject IBM COS Sink Connector The IBM COS Sink Connector source code is availabe at this repository here . IMPORTANT: Make sure you have Java 8 installed on your workstation and that is the default Java version of your system since the IBM COS Sink Connector can only be built with that version of Java. Clone the Kafka Connect IBM COS Source Connector repository and then change your folder. git clone https://github.com/ibm-messaging/kafka-connect-ibmcos-sink.git cd kafka-connect-ibmcos-sink/ IMPORTANT Part Two: Depending on your Gradle version you have installed on your machine you will need to update the connector's gradle build file. For example Gradle v7.x the build.gradle file should look something like this as the compile() method is deprecated in newer versions. You can downgrade your Gradle version if you so choose. This is the shadowJar repository for versioning information. /* * Copyright 2019 IBM Corporation * * Licensed under the Apache License, Version 2.0 (the \"License\"); * you may not use this file except in compliance with the License. * You may obtain a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an \"AS IS\" BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. */ plugins { id 'com.github.johnrengelman.shadow' version '4.0.3' id 'java' id 'eclipse' } repositories { mavenCentral() } dependencies { implementation 'com.eclipsesource.minimal-json:minimal-json:0.9.5' implementation 'com.ibm.cos:ibm-cos-java-sdk:2.4.5' implementation 'org.apache.kafka:connect-api:2.2.1' testImplementation 'org.mockito:mockito-all:1.10.19' testImplementation 'junit:junit:4.12' testImplementation 'com.squareup.okhttp3:mockwebserver:3.9.+' } eclipse.project { natures 'org.springsource.ide.eclipse.gradle.core.nature' } shadowJar { classifier = 'all' version = '7.0.0' } Build the connect using Gradle . gradle shadowJar The newly built connector binaries are in the build/libs/ folder. Move it into a connectors folder for ease of use. mkdir connectors cp build/libs/kafka-connect-ibmcos-sink-*-all.jar connectors/ Now that we have the connector in the connectors/ folder, we somehow need embed it into our Kakfa Connect cluster. For that, we need to trigger another build for our Kafka Connect cluster but this time specifying the files we want to get embedded. What the followin command does is it builds a new image with your provided connectors/plugins and triggers a new deployment for your Kafka Connect cluster. oc start-build connect-cluster-101-connect --from-dir ./connectors/ --follow Since the last commands triggers a new build, we should now see three new pods for the build task, the deploy task and the resulting Kafka Connect cluster. Also, we should see the previous Kafka Connect cluster pod if gone. oc get pods NAME READY STATUS RESTARTS AGE YOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-1-build 0 /1 Completed 0 31m YOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-1-deploy 0 /1 Completed 0 31m YOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-2-build 0 /1 Completed 0 18m YOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-2-deploy 0 /1 Completed 0 17m YOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-2-xxxxx 1 /1 Running 0 17m Connect with mTLS \u00b6 Update the applications.properties file if you are not connecting to Event Streams via PLAINTEXT security protocol. quarkus.http.port = 8080 quarkus.log.console.enable = true quarkus.log.console.level = INFO # Event Streams Connection details mp.messaging.connector.smallrye-kafka.bootstrap.servers = REPLACE_WITH_YOUR_BOOTSTRAP_URL mp.messaging.connector.smallrye-kafka.security.protocol = SASL_SSL mp.messaging.connector.smallrye-kafka.ssl.protocol = TLSv1.2 mp.messaging.connector.smallrye-kafka.sasl.mechanism = SCRAM-SHA-512 mp.messaging.connector.smallrye-kafka.sasl.jaas.config = org.apache.kafka.common.security.scram.ScramLoginModule required \\ username=REPLACE_WITH_YOUR_SCRAM_USERNAME \\ password=REPLACE_WITH_YOUR_SCRAM_PASSWORD; mp.messaging.connector.smallrye-kafka.ssl.truststore.location = REPLACE_WITH_YOUR_PKCS12_CERTIFICATE_LOCATION mp.messaging.connector.smallrye-kafka.ssl.truststore.password = REPLACE_WITH_YOUR_PKCS12_CERTIFICATE_PASSWORD # Initial mock JSON message producer configuration mp.messaging.outgoing.INBOUND.connector = smallrye-kafka mp.messaging.outgoing.INBOUND.topic = REPLACE_WITH_YOUR_TOPIC mp.messaging.outgoing.INBOUND.value.serializer = io.quarkus.kafka.client.serialization.JsonbSerializer mp.messaging.outgoing.INBOUND.key.serializer = io.quarkus.kafka.client.serialization.JsonbSerializer * `REPLACE_WITH_YOUR_BOOTSTRAP_URL`: Your IBM Event Streams bootstrap url. * `REPLACE_WITH_YOUR_PKCS12_CERTIFICATE_LOCATION`: The location where you downloaded your PCKS12 TLS certificate to. * `REPLACE_WITH_YOUR_PKCS12_CERTIFICATE_PASSWORD`: Your PCKS12 TLS certificate password. * `REPLACE_WITH_YOUR_SCRAM_USERNAME`: Your SCRAM service credentials username. * `REPLACE_WITH_YOUR_SCRAM_PASSWORD`: Your SCRAM service credentials password. * `REPLACE_WITH_YOUR_TOPIC`: Name of the topic you created above. Review the Common pre-requisites instructions if you don't know how to find out any of the config properties above.","title":"Older section"},{"location":"use-cases/connect-cos/older-section/#set-up-the-kafka-connect-cluster","text":"In this section, we are going to see how to deploy a Kafka Connect cluster on OpenShift which will be the engine running the source and sink connector we decide to use for our use case. IMPORTANT: We assume you have deployed your IBM Event Streams instance with an internal TLS secured listener which your Kafka Connect cluster will use to connect. For more detail about listeners, check the IBM Event Streams documentation here . If you inspect your IBM Event Streams instance by executing the following command: oc get EventStreams YOUR_IBM_EVENT_STREAMS_INSTANCE_NAME -o yaml You should see a TlS listener: Now, follow the next steps in order to get your Kafka Connect cluster deployed: Go to you IBM Event Streams dashboard, click on the Find more on the toolbox option. Click on the Set up button for the Set up a Kafka Connect environment option. Click on Download Kafka Connect ZIP button. The above downloads a zip file which contains a kafka-connect-s2i.yaml file. Open that yaml file and take note of the productID and cloudpakId values as you will need these in the following step. Instead of using the previous yaml file, create a new kafka-connect-s2i.yaml file with the following contents: apiVersion : eventstreams.ibm.com/v1beta1 kind : KafkaConnectS2I metadata : name : YOUR_KAFKA_CONNECT_CLUSTER_NAME annotations : eventstreams.ibm.com/use-connector-resources : \"true\" spec : logging : type : external name : custom-connect-log4j version : 2.6.0 replicas : 1 bootstrapServers : YOUR_INTERNAL_BOOTSTRAP_ADDRESS template : pod : imagePullSecrets : [] metadata : annotations : eventstreams.production.type : CloudPakForIntegrationNonProduction productID : YOUR_PRODUCT_ID productName : IBM Event Streams for Non Production productVersion : 10.2.0 productMetric : VIRTUAL_PROCESSOR_CORE productChargedContainers : YOUR_KAFKA_CONNECT_CLUSTER_NAME cloudpakId : YOUR_CLOUDPAK_ID cloudpakName : IBM Cloud Pak for Integration cloudpakVersion : 2020.4.1 productCloudpakRatio : \"2:1\" tls : trustedCertificates : - secretName : YOUR_CLUSTER_TLS_CERTIFICATE_SECRET certificate : ca.crt authentication : type : tls certificateAndKey : certificate : user.crt key : user.key secretName : YOUR_TLS_CREDENTIALS_SECRET config : group.id : YOUR_KAFKA_CONNECT_CLUSTER_NAME key.converter : org.apache.kafka.connect.json.JsonConverter value.converter : org.apache.kafka.connect.json.JsonConverter key.converter.schemas.enable : false value.converter.schemas.enable : false offset.storage.topic : YOUR_KAFKA_CONNECT_CLUSTER_NAME-offsets config.storage.topic : YOUR_KAFKA_CONNECT_CLUSTER_NAME-configs status.storage.topic : YOUR_KAFKA_CONNECT_CLUSTER_NAME-status config.storage.replication.factor : 1 offset.storage.replication.factor : 1 status.storage.replication.factor : 1 where you will need to replace the following placeholders with the appropriate values for you IBM Event Streams cluster and service credentials: * YOUR_KAFKA_CONNECT_CLUSTER_NAME : A name you want to provide your Kafka Connect cluster and resources with. * YOUR_INTERNAL_BOOTSTRAP_ADDRESS : This is the internal bootstrap address of your IBM Event Streams instance. You can review how to find this url here . Use the internal bootstrap address which should be in the form of YOUR_IBM_EVENT_STREAMS_INSTANCE_NAME-kafka-bootstrap.eventstreams.svc:9093 : * YOUR_TLS_CREDENTIALS_SECRET : This is the name you give to your TLS credentials for your internal IBM Event Streams listener when you click on Generate TLS credentials : * YOUR_CLUSTER_TLS_CERTIFICATE_SECRET : This is the secret name where IBM Event Streams stores the TLS certificate for establishing secure communications. This secret name is in the form of YOUR_IBM_EVENT_STREAMS_INSTANCE_NAME-cluster-ca-cert . You can always use oc get secrets to list all the secrets. * YOUR_PRODUCT_ID : This is the productID value you noted down earlier. * YOUR_CLOUDPAK_ID : This is the cloudpakID value you noted earlier. Deploy your Kafka Connect cluster by executing oc apply -f kafkaconnect-s2i.yaml If you list the pods, you should see three new pods: one for the Kafka Connect build task, another for the Kafka Connect deploy task and the actual Kafka Connect cluster pod. oc get pods NAME READY STATUS RESTARTS AGE YOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-1-build 0 /1 Completed 0 18m YOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-1-deploy 0 /1 Completed 0 17m YOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-1-xxxxx 1 /1 Running 0 17m ## Build and Inject IBM COS Sink Connector The IBM COS Sink Connector source code is availabe at this repository here . IMPORTANT: Make sure you have Java 8 installed on your workstation and that is the default Java version of your system since the IBM COS Sink Connector can only be built with that version of Java. Clone the Kafka Connect IBM COS Source Connector repository and then change your folder. git clone https://github.com/ibm-messaging/kafka-connect-ibmcos-sink.git cd kafka-connect-ibmcos-sink/ IMPORTANT Part Two: Depending on your Gradle version you have installed on your machine you will need to update the connector's gradle build file. For example Gradle v7.x the build.gradle file should look something like this as the compile() method is deprecated in newer versions. You can downgrade your Gradle version if you so choose. This is the shadowJar repository for versioning information. /* * Copyright 2019 IBM Corporation * * Licensed under the Apache License, Version 2.0 (the \"License\"); * you may not use this file except in compliance with the License. * You may obtain a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an \"AS IS\" BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. */ plugins { id 'com.github.johnrengelman.shadow' version '4.0.3' id 'java' id 'eclipse' } repositories { mavenCentral() } dependencies { implementation 'com.eclipsesource.minimal-json:minimal-json:0.9.5' implementation 'com.ibm.cos:ibm-cos-java-sdk:2.4.5' implementation 'org.apache.kafka:connect-api:2.2.1' testImplementation 'org.mockito:mockito-all:1.10.19' testImplementation 'junit:junit:4.12' testImplementation 'com.squareup.okhttp3:mockwebserver:3.9.+' } eclipse.project { natures 'org.springsource.ide.eclipse.gradle.core.nature' } shadowJar { classifier = 'all' version = '7.0.0' } Build the connect using Gradle . gradle shadowJar The newly built connector binaries are in the build/libs/ folder. Move it into a connectors folder for ease of use. mkdir connectors cp build/libs/kafka-connect-ibmcos-sink-*-all.jar connectors/ Now that we have the connector in the connectors/ folder, we somehow need embed it into our Kakfa Connect cluster. For that, we need to trigger another build for our Kafka Connect cluster but this time specifying the files we want to get embedded. What the followin command does is it builds a new image with your provided connectors/plugins and triggers a new deployment for your Kafka Connect cluster. oc start-build connect-cluster-101-connect --from-dir ./connectors/ --follow Since the last commands triggers a new build, we should now see three new pods for the build task, the deploy task and the resulting Kafka Connect cluster. Also, we should see the previous Kafka Connect cluster pod if gone. oc get pods NAME READY STATUS RESTARTS AGE YOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-1-build 0 /1 Completed 0 31m YOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-1-deploy 0 /1 Completed 0 31m YOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-2-build 0 /1 Completed 0 18m YOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-2-deploy 0 /1 Completed 0 17m YOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-2-xxxxx 1 /1 Running 0 17m","title":"Set up the Kafka Connect Cluster"},{"location":"use-cases/connect-cos/older-section/#connect-with-mtls","text":"Update the applications.properties file if you are not connecting to Event Streams via PLAINTEXT security protocol. quarkus.http.port = 8080 quarkus.log.console.enable = true quarkus.log.console.level = INFO # Event Streams Connection details mp.messaging.connector.smallrye-kafka.bootstrap.servers = REPLACE_WITH_YOUR_BOOTSTRAP_URL mp.messaging.connector.smallrye-kafka.security.protocol = SASL_SSL mp.messaging.connector.smallrye-kafka.ssl.protocol = TLSv1.2 mp.messaging.connector.smallrye-kafka.sasl.mechanism = SCRAM-SHA-512 mp.messaging.connector.smallrye-kafka.sasl.jaas.config = org.apache.kafka.common.security.scram.ScramLoginModule required \\ username=REPLACE_WITH_YOUR_SCRAM_USERNAME \\ password=REPLACE_WITH_YOUR_SCRAM_PASSWORD; mp.messaging.connector.smallrye-kafka.ssl.truststore.location = REPLACE_WITH_YOUR_PKCS12_CERTIFICATE_LOCATION mp.messaging.connector.smallrye-kafka.ssl.truststore.password = REPLACE_WITH_YOUR_PKCS12_CERTIFICATE_PASSWORD # Initial mock JSON message producer configuration mp.messaging.outgoing.INBOUND.connector = smallrye-kafka mp.messaging.outgoing.INBOUND.topic = REPLACE_WITH_YOUR_TOPIC mp.messaging.outgoing.INBOUND.value.serializer = io.quarkus.kafka.client.serialization.JsonbSerializer mp.messaging.outgoing.INBOUND.key.serializer = io.quarkus.kafka.client.serialization.JsonbSerializer * `REPLACE_WITH_YOUR_BOOTSTRAP_URL`: Your IBM Event Streams bootstrap url. * `REPLACE_WITH_YOUR_PKCS12_CERTIFICATE_LOCATION`: The location where you downloaded your PCKS12 TLS certificate to. * `REPLACE_WITH_YOUR_PKCS12_CERTIFICATE_PASSWORD`: Your PCKS12 TLS certificate password. * `REPLACE_WITH_YOUR_SCRAM_USERNAME`: Your SCRAM service credentials username. * `REPLACE_WITH_YOUR_SCRAM_PASSWORD`: Your SCRAM service credentials password. * `REPLACE_WITH_YOUR_TOPIC`: Name of the topic you created above. Review the Common pre-requisites instructions if you don't know how to find out any of the config properties above.","title":"Connect with mTLS"},{"location":"use-cases/connect-jdbc/","text":"Info Updated 11/03/2020. Lab works with one small issue needs to be fixed in test or jdbc connector. This scenario is using the IBM Kafka Connect sink connector for JDBC to get data from a kafka topic and write records to the inventory table in DB2. This lab explain the definition of the connector and how to run an integration test that sends data to the inventory topic. Pre-requisites \u00b6 Pull in necessary pre-req context from Realtime Inventory Pre-reqs . As a pre-requisite you need to have a DB2 instance on cloud up and running with defined credentials. From the credentials you need the username, password and the ssljdbcurl parameter. Something like \"jdbc:db2://dashdb-tx....net:50001/BLUDB:sslConnection=true;\". Build and deploy the inventory-app . This application is a simple Java microprofile 3.3 app exposing a set of end points for CRUD operations on stores, items and inventory. It is based on Quarkus . The instructions to build, and deploy this app is in the README . At the application starts, stores and items records are uploaded to the database. Verify the stores and items records are loaded If you deploy the inventory-app from previous step, then you will have the database created and populated with some stores and items automatically. If you want to drop the data use the drop sql script and then reload them the insert sql script from src/main/resources folder. For that you can use the Run sql menu in the DB2 console: Select the database schema matching the username used as credential, and then open the SQL editor: Verify the items with select * from items; Verify the stores with select * from stores; The inventory has one record to illustrate the relationship between store, item and inventory. Run the Kafka Connector in distributed mode \u00b6 In the refarch-eda-tools repository the labs/jdbc-sink-lab folder includes a docker compose file to run the lab with kafka broker, zookeeper, the kafka connector running in distrbuted mode and an inventory app to get records from DB. # docker-compose up -d Upload the DB2 sink definition \u00b6 Update the file db2-sink-config.json with the DB2 server URL, DB2 username and password. The DB schema matches the user name, so update this setting for the table.name.format with the username. \"name\" : \"jdbc-sink-connector\" , \"config\" : { \"connector.class\" : \"com.ibm.eventstreams.connect.jdbcsink.JDBCSinkConnector\" , \"tasks.max\" : \"1\" , \"topics\" : \"inventory\" , \"connection.url\" : \"jdbc:db2://....services.dal.bluemix.net:50001/BLUDB:sslConnection=true;\" , \"connection.user\" : \"<username>\" , \"connection.password\" : \"<password>\" , \"connection.ds.pool.size\" : \"1\" , \"insert.mode.databaselevel\" : \"true\" , \"table.name.format\" : \"<username>.INVENTORY\" } Once done, you can run the ./sendJdbcSinkConfig.sh <url-kafka-connect> to upload the above definition to the Kafka connect controller. When running locally the command is ./sendJdbcSinkConfig.sh localhost:8083 . This script delete previously define connector with the same name, and then perform a POST operation on the /connectors end point. The connector trace should have something like: connector.class = com.ibm.eventstreams.connect.jdbcsink.JDBCSinkConnector errors.log.enable = false errors.log.include.messages = false errors.retry.delay.max.ms = 60000 errors.retry.timeout = 0 errors.tolerance = none header.converter = null key.converter = null name = jdbc-sink-connector tasks.max = 1 transforms = [] value.converter = null Generate some records \u00b6 The integration-tests folder includes a set of python code to load some records to the expected topic. Start a python environment with ./startPython.sh Within the bash, start python to execute the ProduceInventoryEvent.py script, and specify the number of records to send via the --size argument. python ProduceInventoryEvent.py --size 2 The trace should have something like Produce to the topic inventory ending -> { 'schema' : { 'type' : 'struct' , 'fields' : [{ 'type' : 'string' , 'optional' : False, 'field' : 'storeName' } , { 'type' : 'string' , 'optional' : False, 'field' : 'sku' } , { 'type' : 'decimal' , 'optional' : False, 'field' : 'id' } , { 'type' : 'decimal' , 'optional' : True, 'field' : 'quantity' } , { 'type' : 'decimal' , 'optional' : True, 'field' : 'price' } , { 'type' : 'string' , 'optional' : True, 'field' : 'timestamp' }] , 'optional' : False, 'name' : 'Inventory' } , 'payload' : { 'storeName' : 'Store_1' , 'sku' : 'Item_1' , 'quantity' : 16 , 'price' : 128 , 'id' : 0 , 'timestamp' : '05-Nov-2020 22:31:11' }} sending -> { 'schema' : { 'type' : 'struct' , 'fields' : [{ 'type' : 'string' , 'optional' : False, 'field' : 'storeName' } , { 'type' : 'string' , 'optional' : False, 'field' : 'sku' } , { 'type' : 'decimal' , 'optional' : False, 'field' : 'id' } , { 'type' : 'decimal' , 'optional' : True, 'field' : 'quantity' } , { 'type' : 'decimal' , 'optional' : True, 'field' : 'price' } , { 'type' : 'string' , 'optional' : True, 'field' : 'timestamp' }] , 'optional' : False, 'name' : 'Inventory' } , 'payload' : { 'storeName' : 'Store_1' , 'sku' : 'Item_8' , 'quantity' : 13 , 'price' : 38 , 'id' : 1 , 'timestamp' : '05-Nov-2020 22:31:11' }} Verify records are uploaded into the Inventory database \u00b6 You can use two approaches to get the database, by using the inventory app or using the DB2 console, use the select * from inventory; SQL query to get the last records. The swagger is visible at the address http://localhost:8080/swagger-ui and get to the URL http://localhost:8080/inventory","title":"Kafka Connect - jdbc"},{"location":"use-cases/connect-jdbc/#pre-requisites","text":"Pull in necessary pre-req context from Realtime Inventory Pre-reqs . As a pre-requisite you need to have a DB2 instance on cloud up and running with defined credentials. From the credentials you need the username, password and the ssljdbcurl parameter. Something like \"jdbc:db2://dashdb-tx....net:50001/BLUDB:sslConnection=true;\". Build and deploy the inventory-app . This application is a simple Java microprofile 3.3 app exposing a set of end points for CRUD operations on stores, items and inventory. It is based on Quarkus . The instructions to build, and deploy this app is in the README . At the application starts, stores and items records are uploaded to the database. Verify the stores and items records are loaded If you deploy the inventory-app from previous step, then you will have the database created and populated with some stores and items automatically. If you want to drop the data use the drop sql script and then reload them the insert sql script from src/main/resources folder. For that you can use the Run sql menu in the DB2 console: Select the database schema matching the username used as credential, and then open the SQL editor: Verify the items with select * from items; Verify the stores with select * from stores; The inventory has one record to illustrate the relationship between store, item and inventory.","title":"Pre-requisites"},{"location":"use-cases/connect-jdbc/#run-the-kafka-connector-in-distributed-mode","text":"In the refarch-eda-tools repository the labs/jdbc-sink-lab folder includes a docker compose file to run the lab with kafka broker, zookeeper, the kafka connector running in distrbuted mode and an inventory app to get records from DB. # docker-compose up -d","title":"Run the Kafka Connector in distributed mode"},{"location":"use-cases/connect-jdbc/#upload-the-db2-sink-definition","text":"Update the file db2-sink-config.json with the DB2 server URL, DB2 username and password. The DB schema matches the user name, so update this setting for the table.name.format with the username. \"name\" : \"jdbc-sink-connector\" , \"config\" : { \"connector.class\" : \"com.ibm.eventstreams.connect.jdbcsink.JDBCSinkConnector\" , \"tasks.max\" : \"1\" , \"topics\" : \"inventory\" , \"connection.url\" : \"jdbc:db2://....services.dal.bluemix.net:50001/BLUDB:sslConnection=true;\" , \"connection.user\" : \"<username>\" , \"connection.password\" : \"<password>\" , \"connection.ds.pool.size\" : \"1\" , \"insert.mode.databaselevel\" : \"true\" , \"table.name.format\" : \"<username>.INVENTORY\" } Once done, you can run the ./sendJdbcSinkConfig.sh <url-kafka-connect> to upload the above definition to the Kafka connect controller. When running locally the command is ./sendJdbcSinkConfig.sh localhost:8083 . This script delete previously define connector with the same name, and then perform a POST operation on the /connectors end point. The connector trace should have something like: connector.class = com.ibm.eventstreams.connect.jdbcsink.JDBCSinkConnector errors.log.enable = false errors.log.include.messages = false errors.retry.delay.max.ms = 60000 errors.retry.timeout = 0 errors.tolerance = none header.converter = null key.converter = null name = jdbc-sink-connector tasks.max = 1 transforms = [] value.converter = null","title":"Upload the DB2 sink definition"},{"location":"use-cases/connect-jdbc/#generate-some-records","text":"The integration-tests folder includes a set of python code to load some records to the expected topic. Start a python environment with ./startPython.sh Within the bash, start python to execute the ProduceInventoryEvent.py script, and specify the number of records to send via the --size argument. python ProduceInventoryEvent.py --size 2 The trace should have something like Produce to the topic inventory ending -> { 'schema' : { 'type' : 'struct' , 'fields' : [{ 'type' : 'string' , 'optional' : False, 'field' : 'storeName' } , { 'type' : 'string' , 'optional' : False, 'field' : 'sku' } , { 'type' : 'decimal' , 'optional' : False, 'field' : 'id' } , { 'type' : 'decimal' , 'optional' : True, 'field' : 'quantity' } , { 'type' : 'decimal' , 'optional' : True, 'field' : 'price' } , { 'type' : 'string' , 'optional' : True, 'field' : 'timestamp' }] , 'optional' : False, 'name' : 'Inventory' } , 'payload' : { 'storeName' : 'Store_1' , 'sku' : 'Item_1' , 'quantity' : 16 , 'price' : 128 , 'id' : 0 , 'timestamp' : '05-Nov-2020 22:31:11' }} sending -> { 'schema' : { 'type' : 'struct' , 'fields' : [{ 'type' : 'string' , 'optional' : False, 'field' : 'storeName' } , { 'type' : 'string' , 'optional' : False, 'field' : 'sku' } , { 'type' : 'decimal' , 'optional' : False, 'field' : 'id' } , { 'type' : 'decimal' , 'optional' : True, 'field' : 'quantity' } , { 'type' : 'decimal' , 'optional' : True, 'field' : 'price' } , { 'type' : 'string' , 'optional' : True, 'field' : 'timestamp' }] , 'optional' : False, 'name' : 'Inventory' } , 'payload' : { 'storeName' : 'Store_1' , 'sku' : 'Item_8' , 'quantity' : 13 , 'price' : 38 , 'id' : 1 , 'timestamp' : '05-Nov-2020 22:31:11' }}","title":"Generate some records"},{"location":"use-cases/connect-jdbc/#verify-records-are-uploaded-into-the-inventory-database","text":"You can use two approaches to get the database, by using the inventory app or using the DB2 console, use the select * from inventory; SQL query to get the last records. The swagger is visible at the address http://localhost:8080/swagger-ui and get to the URL http://localhost:8080/inventory","title":"Verify records are uploaded into the Inventory database"},{"location":"use-cases/connect-mq/","text":"This extended scenario supports different labs going from simple to more complex and addresses how to integrate IBM MQ with Event Streams Kafka as part of Cloud Pak for Integration using Kafka Connect with IBM MQ Kafka Connectors. To run the same Lab with Kafka Confluent see the ibm-cloud-architecture/eda-lab-mq-to-kafka readme file. Pre-requisites to all labs Lab 1: MQ source to Event Streams Using the Admin Console Lab 2: MQ source to Event Streams using GitOps Lab 3: MQ Sink from Kafka Lab 4: MQ Connector with Kafka Confluent Audience \u00b6 We assume readers have good knowledge of OpenShift to login, to naivgate into the Administrator console and use OC and GIT CLIs. Pre-requisites to all labs \u00b6 Access to an OpenShift Cluster and Console Login to the OpenShift Console and get the access token to use oc login . Access to git cli from your workstation Clone Lab repository git clone https://github.com/ibm-cloud-architecture/eda-lab-mq-to-kafka.git Lab 1: MQ source to Event Streams Using the Admin Console \u00b6 In this lab we assume Cloud Pak for Integration is deployed with Event Streams cluster created and at least one MQ Broker created. If not see this note to install Event Streams operator and defining one Kafka cluster using the OpenShift Console and this note to install one MQ Operator and one MQ Broker . The following figure illustrates what we will deploy: The Sell Store simulator is a separate application available in the refarch-eda-store-simulator public github and the docker image is accessible from the quay.io registry . The Event Streams cluster is named dev Get setup \u00b6 Get Cloud Pak for Integration admin user password oc get secret platform-auth-idp-credentials -o jsonpath = '{.data.admin_password}' -n ibm-common-services | base64 --decode && echo \"\" * Access to the Event Streams console oc get routes # oc get route dev-ibm-es-ui -o jsonpath = '{.spec.host}' && echo \"\" Use this URL to access to the Event Stream console: Using the Topic menu, create the items topic with 1 partition, use default retention time, and 3 replicas Get Internal Kafka bootstrap URL and generate TLS credentials with the name tls-mq-user with the Produce messages, consume messages and create topics and schemas permissions, on items topic: All consumer groups, as we may reuse this user for consumers. You can download the certificates, but in fact it is not necessary as we will use the created secrets to configure MQ Kafka connector. Get MQ Console route from the namespace where MQ brokers run oc get routes -n cp4i | grep mq-web Access to MQ Broker Console > Manage to see the Broker configured Add the local items queue Verify MQ App channels defined Create an OpenShift project named: mq-es-demo with the command: oc new-project mq-es-demo Clone the git ibm-cloud-architecture/eda-lab-mq-to-kafka repository: git clone https://github.com/ibm-cloud-architecture/eda-lab-mq-to-kafka Update the configMap named store-simulator-cm in folder kustomize/apps/store-simulator/overlays for the store simulator: Workloads > ConfigMaps > Create and use the parameters from MQ_HOST and MQ_CHANNEL. apiVersion : v1 kind : ConfigMap metadata : name : store-simulator-cm data : APP_VERSION : 0.0.6 APP_TARGET_MESSAGING : IBMMQ MQ_HOST : store-mq-ibm-mq.cp4i.svc MQ_PORT : \"1414\" MQ_QMGR : QM1 MQ_QUEUE_NAME : items MQ_CHANNEL : DEV.APP.SVRCONN Deploy Store simulator \u00b6 Deploy to the OpenShift project oc project mq-es-demo oc apply -k kustomize/apps/store-simulator/overlays Access to the application web interface: oc get route store-simulator -o jsonpath = '{.spec.host}' && echo \"\" Deploy Kafka Connect \u00b6 The Event Streams MQ Source connector product documentation describes what need to be done in details. Basically we have to do two things: Build the connector image with the needed jars for all the connectos we want to use. For example we need jar files for the MQ connector and jars for the MQ client api. Start a specific connector with the parameters to connect to the external system and to kafka. To make the MQ source connector immediatly useable for the demonstration, we have build a connector image at quay.io/ibmcase/demomqconnect and the kafkaconnect.yaml is in this file . To deploy the kafka connect framework do: oc apply -f https://raw.githubusercontent.com/ibm-cloud-architecture/eda-lab-mq-to-kafka/main/kustomize/environment/kconnect/es-mq/kafka-connect.yaml Then start the MQ source connector: oc apply -f https://raw.githubusercontent.com/ibm-cloud-architecture/eda-lab-mq-to-kafka/main/kustomize/environment/kconnect/es-mq/mq-src-connector.yaml Demonstration scripts \u00b6 The solution has multiple stores as presented in the stores menu. In the simulator menu, select the IBMMQ backend, and send some messages randomly using the left button after selecting the number of messages to send or run the predefined scenario with the right button: In MQ Broker Console go to the items queue to see the messages generated from the simulator In Event Streams console goes to the items topic to see the same messages produced by the Kafka MQ Source connector. Lab 2: MQ source to Event Streams using GitOps \u00b6 This labs uses GitOps approach with OpenShift GitOps product (ArgoCD) to deploy IBM Event Streams, MQ, the Store Simulator app, and Kafka Connect in the minimum of work. It can be used on a OpenShift Cluster witout any previously deployed Cloud Pak for Integration opersators. Basically as a SRE you will jumpstart the GitOps operator and then starts ArgoCD. Clone the store-mq-gitops repo git clone https://github.com/ibm-cloud-architecture/store-mq-gitops * Follow the instructions from the repository main Readme file. Lab 3: MQ Sink from Kafka \u00b6 A may be less used Kafka connector will be to use the MQ Sink connector to get data from Kafka to MQ. This lab addresses such integration. The target deployment looks like in the following diagram: Kafka is runnning in its own namespace, and we are using Event Streams from Cloud Pack for Integration. Setup MQ Channel \u00b6 Open a shell on the remote container or use the user interface to define the communication channel to use for the Kafka Connection. Using CLI: Change the name of the Q manager to reflect what you defined in MQ configuration. runmqsc EDAQMGR1 # Define a app channel using server connection DEFINE CHANNEL ( KAFKA.CHANNEL ) CHLTYPE ( SVRCONN ) # Set the channel authentication rules to accept connections requiring userid and password SET CHLAUTH ( KAFKA.CHANNEL ) TYPE ( BLOCKUSER ) USERLIST ( 'nobody' ) SET CHLAUTH ( '*' ) TYPE ( ADDRESSMAP ) ADDRESS ( '*' ) USERSRC ( NOACCESS ) SET CHLAUTH ( KAFKA.CHANNEL ) TYPE ( ADDRESSMAP ) ADDRESS ( '*' ) USERSRC ( CHANNEL ) CHCKCLNT ( REQUIRED ) # Set identity of the client connections ALTER AUTHINFO ( SYSTEM.DEFAULT.AUTHINFO.IDPWOS ) AUTHTYPE ( IDPWOS ) ADOPTCTX ( YES ) REFRESH SECURITY TYPE ( CONNAUTH ) # Define inventory queue DEFINE QLOCAL ( INVENTORY ) # Authorize the IBM MQ user ID to connect to and inquire the queue manager SET AUTHREC OBJTYPE ( QMGR ) PRINCIPAL ( 'admin' ) AUTHADD ( CONNECT,INQ ) # Authorize the IBM MQ user ID to use the queue: SET AUTHREC PROFILE ( INVENTORY ) OBJTYPE ( QUEUE ) PRINCIPAL ( 'admin' ) AUTHADD ( ALLMQI ) # done END As an alternate you can use the MQ administration console. TBD Setup Kafka Connect Cluster \u00b6 Connectors can be added to a Kafka Connect environment using OpenShift CLI commands and the source to image customer resource. We will use the Strimzi operator console to setup kafka connect environment. Once you downloaded the zip file, which is a yaml manifest, define the configuration for a KafkaConnectS2I instance. The major configuration settings are the server certificate settings and the authentication using Mutual TLS authentication, something like: spec : bootstrapServers : sandbox-rp-kafka-bootstrap.eventstreams.svc:9093 tls : trustedCertificates : - secretName : sandbox-rp-cluster-ca-cert certificate : ca.crt authentication : type : tls certificate : user.crt key : user.key secretName : sandbox-rp-tls-cred If you change the name of the connect cluster in the metadata, modify also the name: spec.template.pod.metadata.annotations. productChargedContainers accordingly. The secrets used above, need to be accessible from the project where the connector is deployed. The simple way to do so is to copy the source certificates from the Event streams project to your current project with the commands like: oc get secret sandbox-rp-cluster-ca-cert -n eventstreams --export -o yaml | oc apply -f - oc get secret sandbox-rp-tls-cred -n eventstreams --export -o yaml | oc apply -f - If you do not have a TLS client certificate from a TLS user, use this note to create one. Deploy the connector cluster oc apply -f kafka-connect-s2i.yaml An instance of this custom resource represents a Kafka Connect distributed worker cluster. In this mode, workload balancing is automatic, scaling is dynamic, and tasks and data are fault-tolerant. Each connector is represented by another custom resource called KafkaConnector. oc describe kafkaconnects2i eda-connect-cluster Add the mq-sink connector \u00b6 The product documentation details the available MQ connectors and the configuration process. Using the event streams console, the process is quite simple to get a connector configuration as json file. Here is an example of the final form to generate the json file: Once the json is downloaded, complete the settings { \"name\" : \"mq-sink\" , \"config\" : { \"connector.class\" : \"com.ibm.eventstreams.connect.mqsink.MQSinkConnector\" , \"tasks.max\" : \"1\" , \"topics\" : \"inventory\" , \"key.converter\" : \"org.apache.kafka.connect.storage.StringConverter\" , \"value.converter\" : \"org.apache.kafka.connect.storage.StringConverter\" , \"mq.queue.manager\" : \"EDAQMGR1\" , \"mq.connection.name.list\" : \"eda-mq-lab-ibm-mq(1414)\" , \"mq.user.name\" : \"admin\" , \"mq.password\" : \"passw0rd\" , \"mq.user.authentication.mqcsp\" : true , \"mq.channel.name\" : \"KAFKA.CHANNEL\" , \"mq.queue\" : \"INVENTORY\" , \"mq.message.builder\" : \"com.ibm.eventstreams.connect.mqsink.builders.DefaultMessageBuilder\" } } To run the connector within the cluster, we need to connector jar. You can download this jar file from the Event Stream adming console > Toolkit > Add connector to your Kafka Connect environment > Add Connector > IBM MQ Connectors , or as an alternate, we cloned the mq-sink code , so a mvn package command under kafka-connect-mq-sink folder will build the jar. Copy this jar under cp4i/my-plugins folder. * Build the connector with source to image component. With the correct credentials for IBM EventStreams and IBM MQ, Kafka Connect should connect to both services and pull data from the EventStreams topic configured to the MQ Queue configured. You will see signs of success in the container output (via oc logs, or in the UI): + curl -X POST -H Content-Type: application/json http://localhost:8083/connectors --data @/opt/kafka-connect-mq-sink/config/mq-sink.json ... { \"name\" : \"mq-sink-connector\" , \"config\" : { \"connector.class\" : \"com.ibm.eventstreams.connect.mqsink.MQSinkConnector\" , \"tasks.max\" : \"1\" , \"topics\" : \"inventory\" , \"key.converter\" : \"org.apache.kafka.connect.storage.StringConverter\" , \"value.converter\" : \"org.apache.kafka.connect.storage.StringConverter\" , \"mq.queue.manager\" : \"QM1\" , \"mq.connection.name.list\" : \"mq-service(1414)\" , \"mq.user.name\" : \"admin\" , \"mq.password\" : \"passw0rd\" , \"mq.user.authentication.mqcsp\" : \"true\" , \"mq.channel.name\" : \"KAFKA.CHANNEL\" , \"mq.queue\" : \"INVENTORY\" , \"mq.message.builder\" : \"com.ibm.eventstreams.connect.mqsink.builders.DefaultMessageBuilder\" , \"name\" : \"mq-sink-connector\" } , \"tasks\" : [{ \"connector\" : \"mq-sink-connector\" , \"task\" :0 }] , \"type\" : \"sink\" } ... [ 2020 -06-23 04 :26:26,054 ] INFO Creating task mq-sink-connector-0 ( org.apache.kafka.connect.runtime.Worker:419 ) ... [ 2020 -06-23 04 :26:26,449 ] INFO Connection to MQ established ( com.ibm.eventstreams.connect.mqsink.JMSWriter:229 ) [ 2020 -06-23 04 :26:26,449 ] INFO WorkerSinkTask { id = mq-sink-connector-0 } Sink task finished initialization and start ( org.apache.kafka.connect.runtime.WorkerSinkTask:306 ) You should now have the Kafka Connector MQ Sink running on OpenShift. MQ Sink Connector on virtual or baremetal server, MQ and Event Streams on IBM Cloud \u00b6 In this second option, we are using our own laptop for the baremetal dedployment, but the current solution will work the same on virtual server. Pre-requisites \u00b6 We assume that you have an instance of IBM Event Streams already running on IBM Cloud or on OpenShift with at least administrator credentials created. The credentials will be needed later on for configuring the Kafka Connect framework to be able to connect and work with your IBM Event Streams instance. Also, this scenario requires a topic called inventory in your IBM Event Streams instance. For gathering the credentials and creating the topic required for this scenario, please review the Common pre-requisites . IMPORTANT: if you are sharing your IBM Event Streams instance, append some unique identifier to the topic you create in IBM Event Streams. Create Local IBM MQ Instance \u00b6 In this section we are going to use Docker to create a local IBM MQ instance to simulate an IBM MQ instance somewhere in our datacenter. Create a data directory to mount onto the container. mkdir qm1data Run the IBM MQ official Docker image by execting the following command. docker run \\ --name mq \\ --detach \\ --publish 1414 :1414 \\ --publish 9443 :9443 \\ --publish 9157 :9157 \\ --volume qm1data:/mnt/mqm \\ --env LICENSE = accept \\ --env MQ_QMGR_NAME = QM1 \\ --env MQ_APP_PASSWORD = admin \\ --env MQ_ENABLE_METRICS = true \\ ibmcom/mq where we can see that out container will be called mq , it will run in detached mode (i.e. in the background), it will expose the ports IBM MQ uses for communication and the image we are actually running ibmcom/mq . You could make sure your IBM MQ Docker image is running by listing the Docker containers in your workstation. $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a7b2a115a3c6 ibmcom/mq \"runmqdevserver\" 6 minutes ago Up 6 minutes 0 .0.0.0:1414->1414/tcp, 0 .0.0.0:9157->9157/tcp, 0 .0.0.0:9443->9443/tcp mq You should also be able to log into the MQ server on port 9443 ( https://localhost:9443 ) with default user admin and password passw0rd . Now, we need to configure our local IBM MQ instance Queue Manager in order to define a server connection ( KAFKA.CHANNEL ) with authentication (user admin , password admin ) and the queue ( INVENTORY ) where our messages from Kafka will be sinked to. And we are going to so it using the IBM MQ CLI. Get into the IBM MQ Docker container we have started above by executing the following command that will give us a bash interactive terminal: docker exec -ti mq bash Now that we are in the container, start the queue manager QM1 by executing: strmqm QM1 Start the runmqsc tool to configure the queue manager by executing: runmqsc QM1 Create a server-connection channel called KAFKA.CHANNEL by executing: DEFINE CHANNEL ( KAFKA.CHANNEL ) CHLTYPE ( SVRCONN ) Set the channel authentication rules to accept connections requiring userid and password by executing: SET CHLAUTH ( KAFKA.CHANNEL ) TYPE ( BLOCKUSER ) USERLIST ( 'nobody' ) SET CHLAUTH ( '*' ) TYPE ( ADDRESSMAP ) ADDRESS ( '*' ) USERSRC ( NOACCESS ) SET CHLAUTH ( KAFKA.CHANNEL ) TYPE ( ADDRESSMAP ) ADDRESS ( '*' ) USERSRC ( CHANNEL ) CHCKCLNT ( REQUIRED ) Set the identity of the client connections based on the supplied context, the user ID by executing: ALTER AUTHINFO ( SYSTEM.DEFAULT.AUTHINFO.IDPWOS ) AUTHTYPE ( IDPWOS ) ADOPTCTX ( YES ) Refresh the connection authentication information by executing: REFRESH SECURITY TYPE ( CONNAUTH ) Create the INVENTORY queue for the connector to use by executing: DEFINE QLOCAL ( INVENTORY ) Authorize admin to connect to and inquire the queue manager by executing: SET AUTHREC OBJTYPE ( QMGR ) PRINCIPAL ( 'admin' ) AUTHADD ( CONNECT,INQ ) Finally authorize admin to use the queue by executing: SET AUTHREC PROFILE ( INVENTORY ) OBJTYPE ( QUEUE ) PRINCIPAL ( 'admin' ) AUTHADD ( ALLMQI ) End runmqsc by executing: END You should see the following output: 9 MQSC commands read. No commands have a syntax error. One valid MQSC command could not be processed. Exit the container by executing: exit If you check your IBM MQ dashboard you should see your newly created INVENTORY queue: Create MQ Kafka Connector Sink \u00b6 The MQ Connector Sink can be downloaded from this Github . The Github site includes exhaustive instructions for further detail on this connector and its usage. Clone the repository with the following command: git clone https://github.com/ibm-messaging/kafka-connect-mq-sink.git Change directory into the kafka-connect-mq-sink directory: cd kafka-connect-mq-sink Build the connector using Maven: mvn clean package Create a directory (if it does not exist yet) to contain the Kafka Connect framework configuration and cd into it. mkdir config cd config Create a configuration file called connect-distributed.properties for the Kafka Connect framework with the following properties in it: # A list of host/port pairs to use for establishing the initial connection to the Kafka cluster. bootstrap.servers = REPLACE_WITH_YOUR_BOOTSTRAP_URL ssl.enabled.protocols = TLSv1.2 ssl.protocol = TLS ssl.truststore.location = /opt/kafka/config/es-cert.p12 ssl.truststore.password = REPLACE_WITH_YOUR_PKCS12_CERTIFICATE_PASSWORD ssl.truststore.type = PKCS12 security.protocol = SASL_SSL sasl.mechanism = SCRAM-SHA-512 sasl.jaas.config = org.apache.kafka.common.security.scram.ScramLoginModule required username=\"REPLACE_WITH_YOUR_SCRAM_USERNAME\" password=\"REPLACE_WITH_YOUR_SCRAM_PASSWORD\"; # Consumer side configuration consumer.bootstrap.servers = REPLACE_WITH_YOUR_BOOTSTRAP_URL consumer.security.protocol = SASL_SSL consumer.ssl.protocol = TLSv1.2 consumer.ssl.truststore.location = /opt/kafka/config/es-cert.p12 consumer.ssl.truststore.password = REPLACE_WITH_YOUR_PKCS12_CERTIFICATE_PASSWORD consumer.ssl.truststore.type = PKCS12 consumer.sasl.mechanism = SCRAM-SHA-512 consumer.sasl.jaas.config = org.apache.kafka.common.security.scram.ScramLoginModule required username=\"REPLACE_WITH_YOUR_SCRAM_USERNAME\" password=\"REPLACE_WITH_YOUR_SCRAM_PASSWORD\"; # Producer Side producer.bootstrap.servers = REPLACE_WITH_YOUR_BOOTSTRAP_URL producer.security.protocol = SASL_SSL producer.ssl.protocol = TLSv1.2 producer.ssl.truststore.location = /opt/kafka/config/es-cert.p12 producer.ssl.truststore.password = REPLACE_WITH_YOUR_PKCS12_CERTIFICATE_PASSWORD producer.ssl.truststore.type = PKCS12 producer.sasl.mechanism = SCRAM-SHA-512 producer.sasl.jaas.config = org.apache.kafka.common.security.scram.ScramLoginModule required username=\"REPLACE_WITH_YOUR_SCRAM_USERNAME\" password=\"REPLACE_WITH_YOUR_SCRAM_PASSWORD\"; plugin.path = /opt/kafka/libs # unique name for the cluster, used in forming the Connect cluster group. Note that this must not conflict with consumer group IDs group.id = mq-sink-cluster-REPLACE_WITH_UNIQUE_IDENTIFIER # The converters specify the format of data in Kafka and how to translate it into Connect data. Every Connect user will # need to configure these based on the format they want their data in when loaded from or stored into Kafka key.converter = org.apache.kafka.connect.json.JsonConverter value.converter = org.apache.kafka.connect.json.JsonConverter # Converter-specific settings can be passed in by prefixing the Converter's setting with the converter we want to apply # it to key.converter.schemas.enable = true value.converter.schemas.enable = true # Topic to use for storing offsets. offset.storage.topic = connect-offsets-REPLACE_WITH_UNIQUE_IDENTIFIER offset.storage.replication.factor = 3 #offset.storage.partitions=25 # Topic to use for storing connector and task configurations; note that this should be a single partition, highly replicated, and compacted topic. config.storage.topic = connect-configs-REPLACE_WITH_UNIQUE_IDENTIFIER config.storage.replication.factor = 3 # Topic to use for storing statuses. This topic can have multiple partitions and should be replicated and compacted. status.storage.topic = connect-status-REPLACE_WITH_UNIQUE_IDENTIFIER status.storage.replication.factor = 3 status.storage.partitions = 5 # Flush much faster than normal, which is useful for testing/debugging offset.flush.interval.ms = 10000 IMPORTANT: You must replace all occurences of the following placeholders in the properties file above with the appropriate values for the Kafka Connect framework to work with your IBM Event Streams instance: * REPLACE_WITH_YOUR_BOOTSTRAP_URL : Your IBM Event Streams bootstrap url. * REPLACE_WITH_YOUR_PKCS12_CERTIFICATE_PASSWORD : Your PCKS12 TLS certificate password. * REPLACE_WITH_YOUR_SCRAM_USERNAME : Your SCRAM service credentials username. * REPLACE_WITH_YOUR_SCRAM_PASSWORD : Your SCRAM service credentials password. * REPLACE_WITH_UNIQUE_IDENTIFIER : A unique identifier so that the resources your kafka connect cluster will create on your IBM Event Streams instance don't collide with other users' resources (Note: there are 4 placeholders of this type. Replace them all). Review the Common pre-requisites instructions if you don't know how to find out any of the config properties above. Download the IBM Event Streams TLS certificate so that your Kafka Connect framework local instance can establish secure communication with your IBM Event Streams instance. IMPORTANT: download the PKCS12 certificate. How to get the certificate in the Common pre-requisites section. Create a log4j configuration file named connect-log4j.properties based on the template below. log4j.rootLogger = DEBUG, stdout log4j.appender.stdout = org.apache.log4j.ConsoleAppender log4j.appender.stdout.layout = org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern = [%d] %p %m (%c:%L)%n log4j.logger.org.apache.kafka = INFO Back out of the config directory to the kafka-connect-mq-sink directory: cd .. Build the Docker image for your Kafka Connect framework that will contain the IBM MQ Sink connector and all the properties files you have created and tailored earlier so that your Kafka Connect framework can work with your IBM Event Streams instance we have set up previously in this exercise ( mind the dot at the end of the command. It is necessary ): docker build -t kafkaconnect-with-mq-sink:1.3.0 . Run the Kafka Connect MQ Sink container. docker run \\ --name mq-sink \\ --detach \\ --volume $(pwd)/config:/opt/kafka/config \\ --publish 8083:8083 \\ --link mq:mq \\ kafkaconnect-with-mq-sink:1.3.0 Now that we have a Kafka Connect framework running with the configuration to connect to our IBM Event Streams instance and the IBM MQ Sink Connector jar file in it, is to create a JSON file called mq-sink.json to startup an instance of the IBM MQ Sink connector in our Kafka Connect framework with the appropriate config to work read messages from the IBM Event Streams topic we desire and sink those to the IBM MQ instance we have running locally. IMPORTANT: If you are sharing your IBM Event Streams instance and followed the instructions on this readme, you should have appended a unique identifier to the name of the topic ( inventory ) that you are meant to create in IBM Event Streams. As a result, modify the line \"topics\": \"inventory\" in the following JSON object accordignly. { \"name\" : \"mq-sink\" , \"config\" : { \"connector.class\" : \"com.ibm.eventstreams.connect.mqsink.MQSinkConnector\" , \"tasks.max\" : \"1\" , \"topics\" : \"inventory\" , \"key.converter\" : \"org.apache.kafka.connect.storage.StringConverter\" , \"value.converter\" : \"org.apache.kafka.connect.storage.StringConverter\" , \"mq.queue.manager\" : \"QM1\" , \"mq.connection.name.list\" : \"mq(1414)\" , \"mq.user.name\" : \"admin\" , \"mq.password\" : \"passw0rd\" , \"mq.user.authentication.mqcsp\" : true , \"mq.channel.name\" : \"KAFKA.CHANNEL\" , \"mq.queue\" : \"INVENTORY\" , \"mq.message.builder\" : \"com.ibm.eventstreams.connect.mqsink.builders.DefaultMessageBuilder\" } } Last piece of the puzzle is to tell our Kafka Connect framework to create and startup the IBM MQ Sink connector based on the configuration we have in the previous json file. To do so, execute the following POST request. curl -X POST -H \"Content-Type: application/json\" http://localhost:8083/connectors --data \"@./mq-sink.json\" # The response returns the metadata about the connector { \"name\" : \"mq-sink\" , \"config\" : { \"connector.class\" : \"com.ibm.eventstreams.connect.mqsink.MQSinkConnector\" , \"tasks.max\" : \"1\" , \"topics\" : \"inventory\" , \"key.converter\" : \"org.apache.kafka.connect.storage.StringConverter\" , \"value.converter\" : \"org.apache.kafka.connect.storage.StringConverter\" , \"mq.queue.manager\" : \"QM1\" , \"mq.connection.name.list\" : \"ibmmq(1414)\" , \"mq.user.name\" : \"admin\" , \"mq.password\" : \"passw0rd\" , \"mq.user.authentication.mqcsp\" : \"true\" , \"mq.channel.name\" : \"KAFKA.CHANNEL\" , \"mq.queue\" : \"INVENTORY\" , \"mq.message.builder\" : \"com.ibm.eventstreams.connect.mqsink.builders.DefaultMessageBuilder\" , \"name\" : \"mq-sink\" } , \"tasks\" : [{ \"connector\" : \"mq-sink\" , \"task\" :0 }] , \"type\" : \"sink\" } You can query your Kafka Connect framework to make sure of this. Execute the following command that should return all the connectors up and running in your Kafka Connect cluster. curl localhost:8083/connectors [ \"mq-sink\" ] You should now have a working MQ Sink connector getting messages from your topic in IBM Event Streams and sending these to your local IBM MQ instance. In order to check that out working, we first need to send few messages to that IBM Event Streams topic and then check out our INVENTORY queue in our local IBM MQ instance. In order to send messages to IBM Event Streams, we are going to use the IBM Event Streams Starter application. You can find the instruction either in the IBM Event Streams official documentation [here] or on the IBM Event Streams dashboard: Follow the instructions to run the IBM Event Streams starter application from your workstation. IMPORTANT: When generating the properties for your IBM Event Streams starter application, please choose to connect to an existing topic and select the topic you created previously as part of this exercise so that the messages we send into IBM Event Streams end up in the appropriate topic that is being monitored by your IBM MQ Sink connector runnning on the Kafka Connect framework. Once you have your application running, open it up in your web browser, click on Start producing , let the application produce a couple of messages and then click on Stop producing Check out your those messages got into the Kafka topic Check out your messages in the Kafka topic have already reached your INVENTORY queue in your local IBM MQ instance You could also inspect the logs of your Kafka Connect Docker container running on your workstation: docker logs mq-sink ... [ 2021 -01-19 19 :44:17,110 ] DEBUG Putting record for topic inventory, partition 0 and offset 0 ( com.ibm.eventstreams.connect.mqsink.MQSinkTask:89 ) [ 2021 -01-19 19 :44:17,110 ] DEBUG Value schema Schema { STRING } ( com.ibm.eventstreams.connect.mqsink.builders.DefaultMessageBuilder:60 ) [ 2021 -01-19 19 :44:18,292 ] DEBUG Flushing up to topic inventory, partition 0 and offset 1 ( com.ibm.eventstreams.connect.mqsink.MQSinkTask:110 ) [ 2021 -01-19 19 :44:18,292 ] INFO WorkerSinkTask { id = mq-sink-0 } Committing offsets asynchronously using sequence number 636 : { inventory-0 = OffsetAndMetadata { offset = 1 , leaderEpoch = null, metadata = '' }} ( org.apache.kafka.connect.runtime.WorkerSinkTask:346 ) [ 2021 -01-19 19 :44:18,711 ] DEBUG Putting record for topic inventory, partition 0 and offset 1 ( com.ibm.eventstreams.connect.mqsink.MQSinkTask:89 ) [ 2021 -01-19 19 :44:18,711 ] DEBUG Value schema Schema { STRING } ( com.ibm.eventstreams.connect.mqsink.builders.DefaultMessageBuilder:60 ) [ 2021 -01-19 19 :44:20,674 ] DEBUG Putting record for topic inventory, partition 0 and offset 2 ( com.ibm.eventstreams.connect.mqsink.MQSinkTask:89 ) [ 2021 -01-19 19 :44:20,675 ] DEBUG Value schema Schema { STRING } ( com.ibm.eventstreams.connect.mqsink.builders.DefaultMessageBuilder:60 ) [ 2021 -01-19 19 :44:28,293 ] DEBUG Flushing up to topic inventory, partition 0 and offset 3 ( com.ibm.eventstreams.connect.mqsink.MQSinkTask:110 ) [ 2021 -01-19 19 :44:28,293 ] INFO WorkerSinkTask { id = mq-sink-0 } Committing offsets asynchronously using sequence number 637 : { inventory-0 = OffsetAndMetadata { offset = 3 , leaderEpoch = null, metadata = '' }} ( org.apache.kafka.connect.runtime.WorkerSinkTask:346 ) [ 2021 -01-19 19 :44:38,296 ] DEBUG Flushing up to topic inventory, partition 0 and offset 3 ( com.ibm.eventstreams.connect.mqsink.MQSinkTask:110 ) To cleanup your environment, you can do Remove the connector from your Kafka Connect framework instance (this isn't really needed if you are going to stop and remove the Kafka Connect Docker container) curl -X DELETE http://localhost:8083/connectors/mq-sink Stop both IBM MQ and Kakfa Connect Docker containers running on your workstation docker stop mq mq-sink Remove both IBM MQ and Kakfa Connect Docker containers from your workstation docker rm mq mq-sink Lab 4: MQ Connector with Kafka Confluent \u00b6 Clone the git ibm-cloud-architecture/eda-lab-mq-to-kafka repository and follow the readme. git clone https://github.com/ibm-cloud-architecture/eda-lab-mq-to-kafka","title":"Kafka Connect - MQ"},{"location":"use-cases/connect-mq/#audience","text":"We assume readers have good knowledge of OpenShift to login, to naivgate into the Administrator console and use OC and GIT CLIs.","title":"Audience"},{"location":"use-cases/connect-mq/#pre-requisites-to-all-labs","text":"Access to an OpenShift Cluster and Console Login to the OpenShift Console and get the access token to use oc login . Access to git cli from your workstation Clone Lab repository git clone https://github.com/ibm-cloud-architecture/eda-lab-mq-to-kafka.git","title":"Pre-requisites to all labs"},{"location":"use-cases/connect-mq/#lab-1-mq-source-to-event-streams-using-the-admin-console","text":"In this lab we assume Cloud Pak for Integration is deployed with Event Streams cluster created and at least one MQ Broker created. If not see this note to install Event Streams operator and defining one Kafka cluster using the OpenShift Console and this note to install one MQ Operator and one MQ Broker . The following figure illustrates what we will deploy: The Sell Store simulator is a separate application available in the refarch-eda-store-simulator public github and the docker image is accessible from the quay.io registry . The Event Streams cluster is named dev","title":"Lab 1: MQ source to Event Streams Using the Admin Console"},{"location":"use-cases/connect-mq/#get-setup","text":"Get Cloud Pak for Integration admin user password oc get secret platform-auth-idp-credentials -o jsonpath = '{.data.admin_password}' -n ibm-common-services | base64 --decode && echo \"\" * Access to the Event Streams console oc get routes # oc get route dev-ibm-es-ui -o jsonpath = '{.spec.host}' && echo \"\" Use this URL to access to the Event Stream console: Using the Topic menu, create the items topic with 1 partition, use default retention time, and 3 replicas Get Internal Kafka bootstrap URL and generate TLS credentials with the name tls-mq-user with the Produce messages, consume messages and create topics and schemas permissions, on items topic: All consumer groups, as we may reuse this user for consumers. You can download the certificates, but in fact it is not necessary as we will use the created secrets to configure MQ Kafka connector. Get MQ Console route from the namespace where MQ brokers run oc get routes -n cp4i | grep mq-web Access to MQ Broker Console > Manage to see the Broker configured Add the local items queue Verify MQ App channels defined Create an OpenShift project named: mq-es-demo with the command: oc new-project mq-es-demo Clone the git ibm-cloud-architecture/eda-lab-mq-to-kafka repository: git clone https://github.com/ibm-cloud-architecture/eda-lab-mq-to-kafka Update the configMap named store-simulator-cm in folder kustomize/apps/store-simulator/overlays for the store simulator: Workloads > ConfigMaps > Create and use the parameters from MQ_HOST and MQ_CHANNEL. apiVersion : v1 kind : ConfigMap metadata : name : store-simulator-cm data : APP_VERSION : 0.0.6 APP_TARGET_MESSAGING : IBMMQ MQ_HOST : store-mq-ibm-mq.cp4i.svc MQ_PORT : \"1414\" MQ_QMGR : QM1 MQ_QUEUE_NAME : items MQ_CHANNEL : DEV.APP.SVRCONN","title":"Get setup"},{"location":"use-cases/connect-mq/#deploy-store-simulator","text":"Deploy to the OpenShift project oc project mq-es-demo oc apply -k kustomize/apps/store-simulator/overlays Access to the application web interface: oc get route store-simulator -o jsonpath = '{.spec.host}' && echo \"\"","title":"Deploy Store simulator"},{"location":"use-cases/connect-mq/#deploy-kafka-connect","text":"The Event Streams MQ Source connector product documentation describes what need to be done in details. Basically we have to do two things: Build the connector image with the needed jars for all the connectos we want to use. For example we need jar files for the MQ connector and jars for the MQ client api. Start a specific connector with the parameters to connect to the external system and to kafka. To make the MQ source connector immediatly useable for the demonstration, we have build a connector image at quay.io/ibmcase/demomqconnect and the kafkaconnect.yaml is in this file . To deploy the kafka connect framework do: oc apply -f https://raw.githubusercontent.com/ibm-cloud-architecture/eda-lab-mq-to-kafka/main/kustomize/environment/kconnect/es-mq/kafka-connect.yaml Then start the MQ source connector: oc apply -f https://raw.githubusercontent.com/ibm-cloud-architecture/eda-lab-mq-to-kafka/main/kustomize/environment/kconnect/es-mq/mq-src-connector.yaml","title":"Deploy Kafka Connect"},{"location":"use-cases/connect-mq/#demonstration-scripts","text":"The solution has multiple stores as presented in the stores menu. In the simulator menu, select the IBMMQ backend, and send some messages randomly using the left button after selecting the number of messages to send or run the predefined scenario with the right button: In MQ Broker Console go to the items queue to see the messages generated from the simulator In Event Streams console goes to the items topic to see the same messages produced by the Kafka MQ Source connector.","title":"Demonstration scripts"},{"location":"use-cases/connect-mq/#lab-2-mq-source-to-event-streams-using-gitops","text":"This labs uses GitOps approach with OpenShift GitOps product (ArgoCD) to deploy IBM Event Streams, MQ, the Store Simulator app, and Kafka Connect in the minimum of work. It can be used on a OpenShift Cluster witout any previously deployed Cloud Pak for Integration opersators. Basically as a SRE you will jumpstart the GitOps operator and then starts ArgoCD. Clone the store-mq-gitops repo git clone https://github.com/ibm-cloud-architecture/store-mq-gitops * Follow the instructions from the repository main Readme file.","title":"Lab 2: MQ source to Event Streams using GitOps"},{"location":"use-cases/connect-mq/#lab-3-mq-sink-from-kafka","text":"A may be less used Kafka connector will be to use the MQ Sink connector to get data from Kafka to MQ. This lab addresses such integration. The target deployment looks like in the following diagram: Kafka is runnning in its own namespace, and we are using Event Streams from Cloud Pack for Integration.","title":"Lab 3: MQ Sink from Kafka"},{"location":"use-cases/connect-mq/#setup-mq-channel","text":"Open a shell on the remote container or use the user interface to define the communication channel to use for the Kafka Connection. Using CLI: Change the name of the Q manager to reflect what you defined in MQ configuration. runmqsc EDAQMGR1 # Define a app channel using server connection DEFINE CHANNEL ( KAFKA.CHANNEL ) CHLTYPE ( SVRCONN ) # Set the channel authentication rules to accept connections requiring userid and password SET CHLAUTH ( KAFKA.CHANNEL ) TYPE ( BLOCKUSER ) USERLIST ( 'nobody' ) SET CHLAUTH ( '*' ) TYPE ( ADDRESSMAP ) ADDRESS ( '*' ) USERSRC ( NOACCESS ) SET CHLAUTH ( KAFKA.CHANNEL ) TYPE ( ADDRESSMAP ) ADDRESS ( '*' ) USERSRC ( CHANNEL ) CHCKCLNT ( REQUIRED ) # Set identity of the client connections ALTER AUTHINFO ( SYSTEM.DEFAULT.AUTHINFO.IDPWOS ) AUTHTYPE ( IDPWOS ) ADOPTCTX ( YES ) REFRESH SECURITY TYPE ( CONNAUTH ) # Define inventory queue DEFINE QLOCAL ( INVENTORY ) # Authorize the IBM MQ user ID to connect to and inquire the queue manager SET AUTHREC OBJTYPE ( QMGR ) PRINCIPAL ( 'admin' ) AUTHADD ( CONNECT,INQ ) # Authorize the IBM MQ user ID to use the queue: SET AUTHREC PROFILE ( INVENTORY ) OBJTYPE ( QUEUE ) PRINCIPAL ( 'admin' ) AUTHADD ( ALLMQI ) # done END As an alternate you can use the MQ administration console. TBD","title":"Setup MQ Channel"},{"location":"use-cases/connect-mq/#setup-kafka-connect-cluster","text":"Connectors can be added to a Kafka Connect environment using OpenShift CLI commands and the source to image customer resource. We will use the Strimzi operator console to setup kafka connect environment. Once you downloaded the zip file, which is a yaml manifest, define the configuration for a KafkaConnectS2I instance. The major configuration settings are the server certificate settings and the authentication using Mutual TLS authentication, something like: spec : bootstrapServers : sandbox-rp-kafka-bootstrap.eventstreams.svc:9093 tls : trustedCertificates : - secretName : sandbox-rp-cluster-ca-cert certificate : ca.crt authentication : type : tls certificate : user.crt key : user.key secretName : sandbox-rp-tls-cred If you change the name of the connect cluster in the metadata, modify also the name: spec.template.pod.metadata.annotations. productChargedContainers accordingly. The secrets used above, need to be accessible from the project where the connector is deployed. The simple way to do so is to copy the source certificates from the Event streams project to your current project with the commands like: oc get secret sandbox-rp-cluster-ca-cert -n eventstreams --export -o yaml | oc apply -f - oc get secret sandbox-rp-tls-cred -n eventstreams --export -o yaml | oc apply -f - If you do not have a TLS client certificate from a TLS user, use this note to create one. Deploy the connector cluster oc apply -f kafka-connect-s2i.yaml An instance of this custom resource represents a Kafka Connect distributed worker cluster. In this mode, workload balancing is automatic, scaling is dynamic, and tasks and data are fault-tolerant. Each connector is represented by another custom resource called KafkaConnector. oc describe kafkaconnects2i eda-connect-cluster","title":"Setup Kafka Connect Cluster"},{"location":"use-cases/connect-mq/#add-the-mq-sink-connector","text":"The product documentation details the available MQ connectors and the configuration process. Using the event streams console, the process is quite simple to get a connector configuration as json file. Here is an example of the final form to generate the json file: Once the json is downloaded, complete the settings { \"name\" : \"mq-sink\" , \"config\" : { \"connector.class\" : \"com.ibm.eventstreams.connect.mqsink.MQSinkConnector\" , \"tasks.max\" : \"1\" , \"topics\" : \"inventory\" , \"key.converter\" : \"org.apache.kafka.connect.storage.StringConverter\" , \"value.converter\" : \"org.apache.kafka.connect.storage.StringConverter\" , \"mq.queue.manager\" : \"EDAQMGR1\" , \"mq.connection.name.list\" : \"eda-mq-lab-ibm-mq(1414)\" , \"mq.user.name\" : \"admin\" , \"mq.password\" : \"passw0rd\" , \"mq.user.authentication.mqcsp\" : true , \"mq.channel.name\" : \"KAFKA.CHANNEL\" , \"mq.queue\" : \"INVENTORY\" , \"mq.message.builder\" : \"com.ibm.eventstreams.connect.mqsink.builders.DefaultMessageBuilder\" } } To run the connector within the cluster, we need to connector jar. You can download this jar file from the Event Stream adming console > Toolkit > Add connector to your Kafka Connect environment > Add Connector > IBM MQ Connectors , or as an alternate, we cloned the mq-sink code , so a mvn package command under kafka-connect-mq-sink folder will build the jar. Copy this jar under cp4i/my-plugins folder. * Build the connector with source to image component. With the correct credentials for IBM EventStreams and IBM MQ, Kafka Connect should connect to both services and pull data from the EventStreams topic configured to the MQ Queue configured. You will see signs of success in the container output (via oc logs, or in the UI): + curl -X POST -H Content-Type: application/json http://localhost:8083/connectors --data @/opt/kafka-connect-mq-sink/config/mq-sink.json ... { \"name\" : \"mq-sink-connector\" , \"config\" : { \"connector.class\" : \"com.ibm.eventstreams.connect.mqsink.MQSinkConnector\" , \"tasks.max\" : \"1\" , \"topics\" : \"inventory\" , \"key.converter\" : \"org.apache.kafka.connect.storage.StringConverter\" , \"value.converter\" : \"org.apache.kafka.connect.storage.StringConverter\" , \"mq.queue.manager\" : \"QM1\" , \"mq.connection.name.list\" : \"mq-service(1414)\" , \"mq.user.name\" : \"admin\" , \"mq.password\" : \"passw0rd\" , \"mq.user.authentication.mqcsp\" : \"true\" , \"mq.channel.name\" : \"KAFKA.CHANNEL\" , \"mq.queue\" : \"INVENTORY\" , \"mq.message.builder\" : \"com.ibm.eventstreams.connect.mqsink.builders.DefaultMessageBuilder\" , \"name\" : \"mq-sink-connector\" } , \"tasks\" : [{ \"connector\" : \"mq-sink-connector\" , \"task\" :0 }] , \"type\" : \"sink\" } ... [ 2020 -06-23 04 :26:26,054 ] INFO Creating task mq-sink-connector-0 ( org.apache.kafka.connect.runtime.Worker:419 ) ... [ 2020 -06-23 04 :26:26,449 ] INFO Connection to MQ established ( com.ibm.eventstreams.connect.mqsink.JMSWriter:229 ) [ 2020 -06-23 04 :26:26,449 ] INFO WorkerSinkTask { id = mq-sink-connector-0 } Sink task finished initialization and start ( org.apache.kafka.connect.runtime.WorkerSinkTask:306 ) You should now have the Kafka Connector MQ Sink running on OpenShift.","title":"Add the mq-sink connector"},{"location":"use-cases/connect-mq/#mq-sink-connector-on-virtual-or-baremetal-server-mq-and-event-streams-on-ibm-cloud","text":"In this second option, we are using our own laptop for the baremetal dedployment, but the current solution will work the same on virtual server.","title":"MQ Sink Connector on virtual or baremetal server, MQ and Event Streams on IBM Cloud"},{"location":"use-cases/connect-mq/#pre-requisites","text":"We assume that you have an instance of IBM Event Streams already running on IBM Cloud or on OpenShift with at least administrator credentials created. The credentials will be needed later on for configuring the Kafka Connect framework to be able to connect and work with your IBM Event Streams instance. Also, this scenario requires a topic called inventory in your IBM Event Streams instance. For gathering the credentials and creating the topic required for this scenario, please review the Common pre-requisites . IMPORTANT: if you are sharing your IBM Event Streams instance, append some unique identifier to the topic you create in IBM Event Streams.","title":"Pre-requisites"},{"location":"use-cases/connect-mq/#create-local-ibm-mq-instance","text":"In this section we are going to use Docker to create a local IBM MQ instance to simulate an IBM MQ instance somewhere in our datacenter. Create a data directory to mount onto the container. mkdir qm1data Run the IBM MQ official Docker image by execting the following command. docker run \\ --name mq \\ --detach \\ --publish 1414 :1414 \\ --publish 9443 :9443 \\ --publish 9157 :9157 \\ --volume qm1data:/mnt/mqm \\ --env LICENSE = accept \\ --env MQ_QMGR_NAME = QM1 \\ --env MQ_APP_PASSWORD = admin \\ --env MQ_ENABLE_METRICS = true \\ ibmcom/mq where we can see that out container will be called mq , it will run in detached mode (i.e. in the background), it will expose the ports IBM MQ uses for communication and the image we are actually running ibmcom/mq . You could make sure your IBM MQ Docker image is running by listing the Docker containers in your workstation. $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a7b2a115a3c6 ibmcom/mq \"runmqdevserver\" 6 minutes ago Up 6 minutes 0 .0.0.0:1414->1414/tcp, 0 .0.0.0:9157->9157/tcp, 0 .0.0.0:9443->9443/tcp mq You should also be able to log into the MQ server on port 9443 ( https://localhost:9443 ) with default user admin and password passw0rd . Now, we need to configure our local IBM MQ instance Queue Manager in order to define a server connection ( KAFKA.CHANNEL ) with authentication (user admin , password admin ) and the queue ( INVENTORY ) where our messages from Kafka will be sinked to. And we are going to so it using the IBM MQ CLI. Get into the IBM MQ Docker container we have started above by executing the following command that will give us a bash interactive terminal: docker exec -ti mq bash Now that we are in the container, start the queue manager QM1 by executing: strmqm QM1 Start the runmqsc tool to configure the queue manager by executing: runmqsc QM1 Create a server-connection channel called KAFKA.CHANNEL by executing: DEFINE CHANNEL ( KAFKA.CHANNEL ) CHLTYPE ( SVRCONN ) Set the channel authentication rules to accept connections requiring userid and password by executing: SET CHLAUTH ( KAFKA.CHANNEL ) TYPE ( BLOCKUSER ) USERLIST ( 'nobody' ) SET CHLAUTH ( '*' ) TYPE ( ADDRESSMAP ) ADDRESS ( '*' ) USERSRC ( NOACCESS ) SET CHLAUTH ( KAFKA.CHANNEL ) TYPE ( ADDRESSMAP ) ADDRESS ( '*' ) USERSRC ( CHANNEL ) CHCKCLNT ( REQUIRED ) Set the identity of the client connections based on the supplied context, the user ID by executing: ALTER AUTHINFO ( SYSTEM.DEFAULT.AUTHINFO.IDPWOS ) AUTHTYPE ( IDPWOS ) ADOPTCTX ( YES ) Refresh the connection authentication information by executing: REFRESH SECURITY TYPE ( CONNAUTH ) Create the INVENTORY queue for the connector to use by executing: DEFINE QLOCAL ( INVENTORY ) Authorize admin to connect to and inquire the queue manager by executing: SET AUTHREC OBJTYPE ( QMGR ) PRINCIPAL ( 'admin' ) AUTHADD ( CONNECT,INQ ) Finally authorize admin to use the queue by executing: SET AUTHREC PROFILE ( INVENTORY ) OBJTYPE ( QUEUE ) PRINCIPAL ( 'admin' ) AUTHADD ( ALLMQI ) End runmqsc by executing: END You should see the following output: 9 MQSC commands read. No commands have a syntax error. One valid MQSC command could not be processed. Exit the container by executing: exit If you check your IBM MQ dashboard you should see your newly created INVENTORY queue:","title":"Create Local IBM MQ Instance"},{"location":"use-cases/connect-mq/#create-mq-kafka-connector-sink","text":"The MQ Connector Sink can be downloaded from this Github . The Github site includes exhaustive instructions for further detail on this connector and its usage. Clone the repository with the following command: git clone https://github.com/ibm-messaging/kafka-connect-mq-sink.git Change directory into the kafka-connect-mq-sink directory: cd kafka-connect-mq-sink Build the connector using Maven: mvn clean package Create a directory (if it does not exist yet) to contain the Kafka Connect framework configuration and cd into it. mkdir config cd config Create a configuration file called connect-distributed.properties for the Kafka Connect framework with the following properties in it: # A list of host/port pairs to use for establishing the initial connection to the Kafka cluster. bootstrap.servers = REPLACE_WITH_YOUR_BOOTSTRAP_URL ssl.enabled.protocols = TLSv1.2 ssl.protocol = TLS ssl.truststore.location = /opt/kafka/config/es-cert.p12 ssl.truststore.password = REPLACE_WITH_YOUR_PKCS12_CERTIFICATE_PASSWORD ssl.truststore.type = PKCS12 security.protocol = SASL_SSL sasl.mechanism = SCRAM-SHA-512 sasl.jaas.config = org.apache.kafka.common.security.scram.ScramLoginModule required username=\"REPLACE_WITH_YOUR_SCRAM_USERNAME\" password=\"REPLACE_WITH_YOUR_SCRAM_PASSWORD\"; # Consumer side configuration consumer.bootstrap.servers = REPLACE_WITH_YOUR_BOOTSTRAP_URL consumer.security.protocol = SASL_SSL consumer.ssl.protocol = TLSv1.2 consumer.ssl.truststore.location = /opt/kafka/config/es-cert.p12 consumer.ssl.truststore.password = REPLACE_WITH_YOUR_PKCS12_CERTIFICATE_PASSWORD consumer.ssl.truststore.type = PKCS12 consumer.sasl.mechanism = SCRAM-SHA-512 consumer.sasl.jaas.config = org.apache.kafka.common.security.scram.ScramLoginModule required username=\"REPLACE_WITH_YOUR_SCRAM_USERNAME\" password=\"REPLACE_WITH_YOUR_SCRAM_PASSWORD\"; # Producer Side producer.bootstrap.servers = REPLACE_WITH_YOUR_BOOTSTRAP_URL producer.security.protocol = SASL_SSL producer.ssl.protocol = TLSv1.2 producer.ssl.truststore.location = /opt/kafka/config/es-cert.p12 producer.ssl.truststore.password = REPLACE_WITH_YOUR_PKCS12_CERTIFICATE_PASSWORD producer.ssl.truststore.type = PKCS12 producer.sasl.mechanism = SCRAM-SHA-512 producer.sasl.jaas.config = org.apache.kafka.common.security.scram.ScramLoginModule required username=\"REPLACE_WITH_YOUR_SCRAM_USERNAME\" password=\"REPLACE_WITH_YOUR_SCRAM_PASSWORD\"; plugin.path = /opt/kafka/libs # unique name for the cluster, used in forming the Connect cluster group. Note that this must not conflict with consumer group IDs group.id = mq-sink-cluster-REPLACE_WITH_UNIQUE_IDENTIFIER # The converters specify the format of data in Kafka and how to translate it into Connect data. Every Connect user will # need to configure these based on the format they want their data in when loaded from or stored into Kafka key.converter = org.apache.kafka.connect.json.JsonConverter value.converter = org.apache.kafka.connect.json.JsonConverter # Converter-specific settings can be passed in by prefixing the Converter's setting with the converter we want to apply # it to key.converter.schemas.enable = true value.converter.schemas.enable = true # Topic to use for storing offsets. offset.storage.topic = connect-offsets-REPLACE_WITH_UNIQUE_IDENTIFIER offset.storage.replication.factor = 3 #offset.storage.partitions=25 # Topic to use for storing connector and task configurations; note that this should be a single partition, highly replicated, and compacted topic. config.storage.topic = connect-configs-REPLACE_WITH_UNIQUE_IDENTIFIER config.storage.replication.factor = 3 # Topic to use for storing statuses. This topic can have multiple partitions and should be replicated and compacted. status.storage.topic = connect-status-REPLACE_WITH_UNIQUE_IDENTIFIER status.storage.replication.factor = 3 status.storage.partitions = 5 # Flush much faster than normal, which is useful for testing/debugging offset.flush.interval.ms = 10000 IMPORTANT: You must replace all occurences of the following placeholders in the properties file above with the appropriate values for the Kafka Connect framework to work with your IBM Event Streams instance: * REPLACE_WITH_YOUR_BOOTSTRAP_URL : Your IBM Event Streams bootstrap url. * REPLACE_WITH_YOUR_PKCS12_CERTIFICATE_PASSWORD : Your PCKS12 TLS certificate password. * REPLACE_WITH_YOUR_SCRAM_USERNAME : Your SCRAM service credentials username. * REPLACE_WITH_YOUR_SCRAM_PASSWORD : Your SCRAM service credentials password. * REPLACE_WITH_UNIQUE_IDENTIFIER : A unique identifier so that the resources your kafka connect cluster will create on your IBM Event Streams instance don't collide with other users' resources (Note: there are 4 placeholders of this type. Replace them all). Review the Common pre-requisites instructions if you don't know how to find out any of the config properties above. Download the IBM Event Streams TLS certificate so that your Kafka Connect framework local instance can establish secure communication with your IBM Event Streams instance. IMPORTANT: download the PKCS12 certificate. How to get the certificate in the Common pre-requisites section. Create a log4j configuration file named connect-log4j.properties based on the template below. log4j.rootLogger = DEBUG, stdout log4j.appender.stdout = org.apache.log4j.ConsoleAppender log4j.appender.stdout.layout = org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern = [%d] %p %m (%c:%L)%n log4j.logger.org.apache.kafka = INFO Back out of the config directory to the kafka-connect-mq-sink directory: cd .. Build the Docker image for your Kafka Connect framework that will contain the IBM MQ Sink connector and all the properties files you have created and tailored earlier so that your Kafka Connect framework can work with your IBM Event Streams instance we have set up previously in this exercise ( mind the dot at the end of the command. It is necessary ): docker build -t kafkaconnect-with-mq-sink:1.3.0 . Run the Kafka Connect MQ Sink container. docker run \\ --name mq-sink \\ --detach \\ --volume $(pwd)/config:/opt/kafka/config \\ --publish 8083:8083 \\ --link mq:mq \\ kafkaconnect-with-mq-sink:1.3.0 Now that we have a Kafka Connect framework running with the configuration to connect to our IBM Event Streams instance and the IBM MQ Sink Connector jar file in it, is to create a JSON file called mq-sink.json to startup an instance of the IBM MQ Sink connector in our Kafka Connect framework with the appropriate config to work read messages from the IBM Event Streams topic we desire and sink those to the IBM MQ instance we have running locally. IMPORTANT: If you are sharing your IBM Event Streams instance and followed the instructions on this readme, you should have appended a unique identifier to the name of the topic ( inventory ) that you are meant to create in IBM Event Streams. As a result, modify the line \"topics\": \"inventory\" in the following JSON object accordignly. { \"name\" : \"mq-sink\" , \"config\" : { \"connector.class\" : \"com.ibm.eventstreams.connect.mqsink.MQSinkConnector\" , \"tasks.max\" : \"1\" , \"topics\" : \"inventory\" , \"key.converter\" : \"org.apache.kafka.connect.storage.StringConverter\" , \"value.converter\" : \"org.apache.kafka.connect.storage.StringConverter\" , \"mq.queue.manager\" : \"QM1\" , \"mq.connection.name.list\" : \"mq(1414)\" , \"mq.user.name\" : \"admin\" , \"mq.password\" : \"passw0rd\" , \"mq.user.authentication.mqcsp\" : true , \"mq.channel.name\" : \"KAFKA.CHANNEL\" , \"mq.queue\" : \"INVENTORY\" , \"mq.message.builder\" : \"com.ibm.eventstreams.connect.mqsink.builders.DefaultMessageBuilder\" } } Last piece of the puzzle is to tell our Kafka Connect framework to create and startup the IBM MQ Sink connector based on the configuration we have in the previous json file. To do so, execute the following POST request. curl -X POST -H \"Content-Type: application/json\" http://localhost:8083/connectors --data \"@./mq-sink.json\" # The response returns the metadata about the connector { \"name\" : \"mq-sink\" , \"config\" : { \"connector.class\" : \"com.ibm.eventstreams.connect.mqsink.MQSinkConnector\" , \"tasks.max\" : \"1\" , \"topics\" : \"inventory\" , \"key.converter\" : \"org.apache.kafka.connect.storage.StringConverter\" , \"value.converter\" : \"org.apache.kafka.connect.storage.StringConverter\" , \"mq.queue.manager\" : \"QM1\" , \"mq.connection.name.list\" : \"ibmmq(1414)\" , \"mq.user.name\" : \"admin\" , \"mq.password\" : \"passw0rd\" , \"mq.user.authentication.mqcsp\" : \"true\" , \"mq.channel.name\" : \"KAFKA.CHANNEL\" , \"mq.queue\" : \"INVENTORY\" , \"mq.message.builder\" : \"com.ibm.eventstreams.connect.mqsink.builders.DefaultMessageBuilder\" , \"name\" : \"mq-sink\" } , \"tasks\" : [{ \"connector\" : \"mq-sink\" , \"task\" :0 }] , \"type\" : \"sink\" } You can query your Kafka Connect framework to make sure of this. Execute the following command that should return all the connectors up and running in your Kafka Connect cluster. curl localhost:8083/connectors [ \"mq-sink\" ] You should now have a working MQ Sink connector getting messages from your topic in IBM Event Streams and sending these to your local IBM MQ instance. In order to check that out working, we first need to send few messages to that IBM Event Streams topic and then check out our INVENTORY queue in our local IBM MQ instance. In order to send messages to IBM Event Streams, we are going to use the IBM Event Streams Starter application. You can find the instruction either in the IBM Event Streams official documentation [here] or on the IBM Event Streams dashboard: Follow the instructions to run the IBM Event Streams starter application from your workstation. IMPORTANT: When generating the properties for your IBM Event Streams starter application, please choose to connect to an existing topic and select the topic you created previously as part of this exercise so that the messages we send into IBM Event Streams end up in the appropriate topic that is being monitored by your IBM MQ Sink connector runnning on the Kafka Connect framework. Once you have your application running, open it up in your web browser, click on Start producing , let the application produce a couple of messages and then click on Stop producing Check out your those messages got into the Kafka topic Check out your messages in the Kafka topic have already reached your INVENTORY queue in your local IBM MQ instance You could also inspect the logs of your Kafka Connect Docker container running on your workstation: docker logs mq-sink ... [ 2021 -01-19 19 :44:17,110 ] DEBUG Putting record for topic inventory, partition 0 and offset 0 ( com.ibm.eventstreams.connect.mqsink.MQSinkTask:89 ) [ 2021 -01-19 19 :44:17,110 ] DEBUG Value schema Schema { STRING } ( com.ibm.eventstreams.connect.mqsink.builders.DefaultMessageBuilder:60 ) [ 2021 -01-19 19 :44:18,292 ] DEBUG Flushing up to topic inventory, partition 0 and offset 1 ( com.ibm.eventstreams.connect.mqsink.MQSinkTask:110 ) [ 2021 -01-19 19 :44:18,292 ] INFO WorkerSinkTask { id = mq-sink-0 } Committing offsets asynchronously using sequence number 636 : { inventory-0 = OffsetAndMetadata { offset = 1 , leaderEpoch = null, metadata = '' }} ( org.apache.kafka.connect.runtime.WorkerSinkTask:346 ) [ 2021 -01-19 19 :44:18,711 ] DEBUG Putting record for topic inventory, partition 0 and offset 1 ( com.ibm.eventstreams.connect.mqsink.MQSinkTask:89 ) [ 2021 -01-19 19 :44:18,711 ] DEBUG Value schema Schema { STRING } ( com.ibm.eventstreams.connect.mqsink.builders.DefaultMessageBuilder:60 ) [ 2021 -01-19 19 :44:20,674 ] DEBUG Putting record for topic inventory, partition 0 and offset 2 ( com.ibm.eventstreams.connect.mqsink.MQSinkTask:89 ) [ 2021 -01-19 19 :44:20,675 ] DEBUG Value schema Schema { STRING } ( com.ibm.eventstreams.connect.mqsink.builders.DefaultMessageBuilder:60 ) [ 2021 -01-19 19 :44:28,293 ] DEBUG Flushing up to topic inventory, partition 0 and offset 3 ( com.ibm.eventstreams.connect.mqsink.MQSinkTask:110 ) [ 2021 -01-19 19 :44:28,293 ] INFO WorkerSinkTask { id = mq-sink-0 } Committing offsets asynchronously using sequence number 637 : { inventory-0 = OffsetAndMetadata { offset = 3 , leaderEpoch = null, metadata = '' }} ( org.apache.kafka.connect.runtime.WorkerSinkTask:346 ) [ 2021 -01-19 19 :44:38,296 ] DEBUG Flushing up to topic inventory, partition 0 and offset 3 ( com.ibm.eventstreams.connect.mqsink.MQSinkTask:110 ) To cleanup your environment, you can do Remove the connector from your Kafka Connect framework instance (this isn't really needed if you are going to stop and remove the Kafka Connect Docker container) curl -X DELETE http://localhost:8083/connectors/mq-sink Stop both IBM MQ and Kakfa Connect Docker containers running on your workstation docker stop mq mq-sink Remove both IBM MQ and Kakfa Connect Docker containers from your workstation docker rm mq mq-sink","title":"Create MQ Kafka Connector Sink"},{"location":"use-cases/connect-mq/#lab-4-mq-connector-with-kafka-confluent","text":"Clone the git ibm-cloud-architecture/eda-lab-mq-to-kafka repository and follow the readme. git clone https://github.com/ibm-cloud-architecture/eda-lab-mq-to-kafka","title":"Lab 4: MQ Connector with  Kafka Confluent"},{"location":"use-cases/connect-rabbitmq/","text":"Kafka Connect to RabbitMQ Source Connector \u00b6 Info Updated 11/10/2020 - Lab completed on local computer. Instruction not completed for OpenShift deployment. This hands-on lab demonstrates how to use IBM RabbitMQ Kafka source connector to inject message to Event Streams On Premise or any Kafka cluster. We are using the IBM messaging github: source Kafka connector for RabbitMQ open sourced component. The configuration for this connector is also done using Json config file, with a POST to the Kafka connectors URL. The following diagram illustrates the component of this demo / lab: The configurations used in this use case are in the refarch-eda-tools repository that you should clone: git clone https://github.com/ibm-cloud-architecture/refarch-eda-tools under the labs/rabbitmq-source-lab folder. Just run it! \u00b6 For demonstration purpose, we have defined a docker compose file and a set of docker images ready to use. So it will be easier to demonstrate the scenario of sending messages to Rabbit MQ items queue, have a Kafka connector configured and a simple Kafka consumer which consumes Json doc from the items Kafka topic. To send message to RabbitMQ, we have implemented a simple store sale simulator, which sends item sale messages for a set of stores. The code is under eda-store-simulator repository and we have also uploaded the image into dockerhub ibmcase/eda-store-simulator . Start backend services \u00b6 To quickly start a local demonstration environment, the github repository for this lab includes a docker compose file under the labs/rabbitmq-source-lab folder. git clone https://github.com/ibm-cloud-architecture/refarch-eda-tools cd refarch-eda-tools/labs/rabbitmq-source-lab docker-compose up -d docker ps IMAGE NAMES PORT ibmcase/kconnect:v1.0.0 rabbitmq-source-lab_kconnect_1 0 .0.0.0:8083->8083/ strimzi/kafka:latest-kafka-2.6.0 rabbitmq-source-lab_kafka_1 0 .0.0.0:9092->9092/ ibmcase/eda-store-simulator rabbitmq-source-lab_simulator_1 0 .0.0.0:8080->8080/ rabbitmq:3-management rabbitmq-source-lab_rabbitmq_1 0 .0.0.0:5672->5672/tcp, 15671 /tcp, 15691 -15692/tcp, 25672 /tcp, 0 .0.0.0:15672->15672/tcp strimzi/kafka:latest-kafka-2.6.0 rabbitmq-source-lab_zookeeper_1 0 .0.0.0:2181->2181/ Deploy source connector \u00b6 Once the different containers are started, you can deploy the RabbitMQ source connector using the following POST # under the folder use the curl to post configuration ./sendRMQSrcConfig.sh localhost:8083 # Trace should look like { \"name\" : \"RabbitMQSourceConnector\" , \"config\" : { \"connector.class\" : \"com.ibm.eventstreams.connect.rabbitmqsource.RabbitMQSourceConnector\" , \"tasks.max\" : \"1\" , \"kafka.topic\" : \"items\" , \"rabbitmq.host\" : \"rabbitmq\" , \"rabbitmq.port\" : \"5672\" , \"rabbitmq.queue\" : \"items\" , \"rabbitmq.username\" : \"rabbit-user\" , \"rabbitmq.password\" : \"rabbit-pass\" , \"rabbitmq.prefetch.count\" : \"500\" , \"rabbitmq.automatic.recovery.enabled\" : \"true\" , \"rabbitmq.network.recovery.interval.ms\" : \"10000\" , \"rabbitmq.topology.recovery.enabled\" : \"true\" , \"name\" : \"RabbitMQSourceConnector\" } , \"tasks\" : [] , \"type\" : \"source\" } And then looking at the deployed connectors using http://localhost:8083/connectors or the RabbitMQ connector with http://localhost:8083/connectors/RabbitMQSourceConnector to get the same response as the above json. Create items topic into Kafka using the script: ./createTopics.sh Send some records using the simulator at the http://localhost:8080/#/simulator URL by first selecting RabbitMQ button and the number of message to send: Verify the RabbitMQ settings In a Web Browser go to http://localhost:15672/ using the rabbit-user/rabbit-pass login. You should reach this console: Go to the Queue tab and select the items queue: And then get 1 message from the queue: Now verify the Kafka connect-* topics are created successfully: docker run -ti --network rabbitmq-source-lab_default strimzi/kafka:latest-kafka-2.6.0 bash -c \"/opt/kafka/bin/kafka-topics.sh --bootstrap-server kafka:9092 --list\" # you should get at least __consumer_offsets connect-configs connect-offsets connect-status items Verify the message arrived to the items in kafka topic: ```shell docker run -ti --network rabbitmq-source-lab_default strimzi/kafka:latest-kafka-2.6.0 bash -c \"/opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server kafka:9092 --topic items --from-beginning\" # You may have a trace like which maps the messages sent \"{\\\"id\\\":0,\\\"price\\\":11.0,\\\"quantity\\\":6,\\\"sku\\\":\\\"Item_5\\\",\\\"storeName\\\":\\\"Store_3\\\",\\\"timestamp\\\":\\\"2020-10-23T19:11:26.395325\\\",\\\"type\\\":\\\"SALE\\\"}\" \"{\\\"id\\\":1,\\\"price\\\":68.5,\\\"quantity\\\":0,\\\"sku\\\":\\\"Item_5\\\",\\\"storeName\\\":\\\"Store_1\\\",\\\"timestamp\\\":\\\"2020-10-23T19:11:26.395447\\\",\\\"type\\\":\\\"SALE\\\"}\" ``` You validated the solution works end to end. Now we can try to deploy those components to OpenShift. Deploy on OpenShift \u00b6 In this section, you will deploy the Kafka connector and Rabbit MQ on OpenShift using Event Streams on Premise as Kafka cluster. Pre-requisites \u00b6 To run the demonstration be sure to have the event store simulator deployed: see instructions here. Pull in necessary pre-req from the Realtime Inventory Pre-reqs , like for example the deployment of the store simulator. Deploy RabbitMQ using operators \u00b6 See the installation instructions to get a RabbitMQ operator installed, or use our manifests from the refarch-eda-tools repository, which will set user 1000 and 999 for OpenShift deployment. The RabbitMQ cluster operator runs as user ID 1000 . The RabbitMQ pod runs the RabbitMQ container as user ID 999 and an init container as user ID 0 . Create a dedicated project to run RabbitMQ operator to observe any projects, and install CRD, service account, RBAC role, deployment... oc new-project rabbitmq-system # under the labs/rabbitmq-source-lab folder do kubectl apply -f cluster-operator.yml kubectl get customresourcedefinitions.apiextensions.k8s.io Create a second project for RabbitMQ cluster. It is recommended to have separate project for RabbitMQ cluster. Modify the namespace to be able to deploy RabbitMQ cluster into it. So we need to get user 999 authorized. For that we specify a security constraint named: rabbitmq-cluster #Define a Security Context Constraint oc apply -f rmq-scc.yaml Update the name space to get openshift scc userid and groupid range. You can do that in two ways, by directly editing the namespace definition: oc edit namespace rabbitmq-cluster # Then add the following annotations: annotations: openshift.io/sa.scc.supplemental-groups: 999 /1 openshift.io/sa.scc.uid-range: 0 -999/1 Or by using our custom cluster definition: oc apply -f rmq-cluster-ns.yaml For every RabbitMQ cluster assign the previously created security context constraint to the cluster's service account: oc adm policy add-scc-to-user rabbitmq-cluster -z rmqcluster-rabbitmq-server Finally create a cluster instance: oc apply -f rmq-cluster.yaml . Define a route on the client service on port 15672 Get the admin user and password as defined in the secrets oc describe secret rmqcluster-rabbitmq-default-user # Get user name oc get secret rmqcluster-rabbitmq-default-user -o jsonpath = '{.data.username}' | base64 --decode # Get password oc get secret rmqcluster-rabbitmq-default-user -o jsonpath = '{.data.password}' | base64 --decode To validate the RabbitMQ cluster runs correctly you can use the RabbitMQ console, and add the items queue. Configure the kafka connector for Rabbitmq source \u00b6 The rabbitmq-source.json define the connector and the RabbitMQ connection parameters: { \"name\" : \"RabbitMQSourceConnector\" , \"config\" : { \"connector.class\" : \"com.ibm.eventstreams.connect.rabbitmqsource.RabbitMQSourceConnector\" , \"tasks.max\" : \"1\" , \"kafka.topic\" : \"items\" , \"rabbitmq.host\" : \"rabbitmq\" , \"rabbitmq.port\" : 5672 , \"rabbitmq.queue\" : \"items\" , \"rabbitmq.prefetch.count\" : \"500\" , \"rabbitmq.automatic.recovery.enabled\" : \"true\" , \"rabbitmq.network.recovery.interval.ms\" : \"10000\" , \"rabbitmq.topology.recovery.enabled\" : \"true\" } } This file is uploaded to Kafka Connect via a PORT operation: curl -X POST -H \"Content-Type: application/json\" http://localhost:8083/connectors --data \"@./rabbitmq-source.json\" To verify use: curl -X GET http://localhost:8083/connectors . In Kafka connect trace you should see: [ Worker clientId = connect-1, groupId = eda-kconnect ] Connector RabbitMQSourceConnector config updated ... Starting connector RabbitMQSourceConnector ... Starting task RabbitMQSourceConnector-0 And Rabbitmq that get the connection from Kafka Connect. rabbitmq_1 [ info ] < 0 .1766.0> accepting AMQP connection < 0 .1766.0> ( 172 .19.0.3:33040 -> 172 .19.0.2:5672 ) kconnect_1 INFO Creating Channel ( com.ibm.eventstreams.connect.rabbitmqsource.RabbitMQSourceTask:61 ) rabbitmq_1 connection < 0 .1766.0> ( 172 .19.0.3:33040 -> 172 .19.0.2:5672 ) : user 'rabbit-user' authenticated and granted access to vhost '/'","title":"Kafka Connect - Rabbitmq"},{"location":"use-cases/connect-rabbitmq/#kafka-connect-to-rabbitmq-source-connector","text":"Info Updated 11/10/2020 - Lab completed on local computer. Instruction not completed for OpenShift deployment. This hands-on lab demonstrates how to use IBM RabbitMQ Kafka source connector to inject message to Event Streams On Premise or any Kafka cluster. We are using the IBM messaging github: source Kafka connector for RabbitMQ open sourced component. The configuration for this connector is also done using Json config file, with a POST to the Kafka connectors URL. The following diagram illustrates the component of this demo / lab: The configurations used in this use case are in the refarch-eda-tools repository that you should clone: git clone https://github.com/ibm-cloud-architecture/refarch-eda-tools under the labs/rabbitmq-source-lab folder.","title":"Kafka Connect to RabbitMQ Source Connector"},{"location":"use-cases/connect-rabbitmq/#just-run-it","text":"For demonstration purpose, we have defined a docker compose file and a set of docker images ready to use. So it will be easier to demonstrate the scenario of sending messages to Rabbit MQ items queue, have a Kafka connector configured and a simple Kafka consumer which consumes Json doc from the items Kafka topic. To send message to RabbitMQ, we have implemented a simple store sale simulator, which sends item sale messages for a set of stores. The code is under eda-store-simulator repository and we have also uploaded the image into dockerhub ibmcase/eda-store-simulator .","title":"Just run it!"},{"location":"use-cases/connect-rabbitmq/#start-backend-services","text":"To quickly start a local demonstration environment, the github repository for this lab includes a docker compose file under the labs/rabbitmq-source-lab folder. git clone https://github.com/ibm-cloud-architecture/refarch-eda-tools cd refarch-eda-tools/labs/rabbitmq-source-lab docker-compose up -d docker ps IMAGE NAMES PORT ibmcase/kconnect:v1.0.0 rabbitmq-source-lab_kconnect_1 0 .0.0.0:8083->8083/ strimzi/kafka:latest-kafka-2.6.0 rabbitmq-source-lab_kafka_1 0 .0.0.0:9092->9092/ ibmcase/eda-store-simulator rabbitmq-source-lab_simulator_1 0 .0.0.0:8080->8080/ rabbitmq:3-management rabbitmq-source-lab_rabbitmq_1 0 .0.0.0:5672->5672/tcp, 15671 /tcp, 15691 -15692/tcp, 25672 /tcp, 0 .0.0.0:15672->15672/tcp strimzi/kafka:latest-kafka-2.6.0 rabbitmq-source-lab_zookeeper_1 0 .0.0.0:2181->2181/","title":"Start backend services"},{"location":"use-cases/connect-rabbitmq/#deploy-source-connector","text":"Once the different containers are started, you can deploy the RabbitMQ source connector using the following POST # under the folder use the curl to post configuration ./sendRMQSrcConfig.sh localhost:8083 # Trace should look like { \"name\" : \"RabbitMQSourceConnector\" , \"config\" : { \"connector.class\" : \"com.ibm.eventstreams.connect.rabbitmqsource.RabbitMQSourceConnector\" , \"tasks.max\" : \"1\" , \"kafka.topic\" : \"items\" , \"rabbitmq.host\" : \"rabbitmq\" , \"rabbitmq.port\" : \"5672\" , \"rabbitmq.queue\" : \"items\" , \"rabbitmq.username\" : \"rabbit-user\" , \"rabbitmq.password\" : \"rabbit-pass\" , \"rabbitmq.prefetch.count\" : \"500\" , \"rabbitmq.automatic.recovery.enabled\" : \"true\" , \"rabbitmq.network.recovery.interval.ms\" : \"10000\" , \"rabbitmq.topology.recovery.enabled\" : \"true\" , \"name\" : \"RabbitMQSourceConnector\" } , \"tasks\" : [] , \"type\" : \"source\" } And then looking at the deployed connectors using http://localhost:8083/connectors or the RabbitMQ connector with http://localhost:8083/connectors/RabbitMQSourceConnector to get the same response as the above json. Create items topic into Kafka using the script: ./createTopics.sh Send some records using the simulator at the http://localhost:8080/#/simulator URL by first selecting RabbitMQ button and the number of message to send: Verify the RabbitMQ settings In a Web Browser go to http://localhost:15672/ using the rabbit-user/rabbit-pass login. You should reach this console: Go to the Queue tab and select the items queue: And then get 1 message from the queue: Now verify the Kafka connect-* topics are created successfully: docker run -ti --network rabbitmq-source-lab_default strimzi/kafka:latest-kafka-2.6.0 bash -c \"/opt/kafka/bin/kafka-topics.sh --bootstrap-server kafka:9092 --list\" # you should get at least __consumer_offsets connect-configs connect-offsets connect-status items Verify the message arrived to the items in kafka topic: ```shell docker run -ti --network rabbitmq-source-lab_default strimzi/kafka:latest-kafka-2.6.0 bash -c \"/opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server kafka:9092 --topic items --from-beginning\" # You may have a trace like which maps the messages sent \"{\\\"id\\\":0,\\\"price\\\":11.0,\\\"quantity\\\":6,\\\"sku\\\":\\\"Item_5\\\",\\\"storeName\\\":\\\"Store_3\\\",\\\"timestamp\\\":\\\"2020-10-23T19:11:26.395325\\\",\\\"type\\\":\\\"SALE\\\"}\" \"{\\\"id\\\":1,\\\"price\\\":68.5,\\\"quantity\\\":0,\\\"sku\\\":\\\"Item_5\\\",\\\"storeName\\\":\\\"Store_1\\\",\\\"timestamp\\\":\\\"2020-10-23T19:11:26.395447\\\",\\\"type\\\":\\\"SALE\\\"}\" ``` You validated the solution works end to end. Now we can try to deploy those components to OpenShift.","title":"Deploy source connector"},{"location":"use-cases/connect-rabbitmq/#deploy-on-openshift","text":"In this section, you will deploy the Kafka connector and Rabbit MQ on OpenShift using Event Streams on Premise as Kafka cluster.","title":"Deploy on OpenShift"},{"location":"use-cases/connect-rabbitmq/#pre-requisites","text":"To run the demonstration be sure to have the event store simulator deployed: see instructions here. Pull in necessary pre-req from the Realtime Inventory Pre-reqs , like for example the deployment of the store simulator.","title":"Pre-requisites"},{"location":"use-cases/connect-rabbitmq/#deploy-rabbitmq-using-operators","text":"See the installation instructions to get a RabbitMQ operator installed, or use our manifests from the refarch-eda-tools repository, which will set user 1000 and 999 for OpenShift deployment. The RabbitMQ cluster operator runs as user ID 1000 . The RabbitMQ pod runs the RabbitMQ container as user ID 999 and an init container as user ID 0 . Create a dedicated project to run RabbitMQ operator to observe any projects, and install CRD, service account, RBAC role, deployment... oc new-project rabbitmq-system # under the labs/rabbitmq-source-lab folder do kubectl apply -f cluster-operator.yml kubectl get customresourcedefinitions.apiextensions.k8s.io Create a second project for RabbitMQ cluster. It is recommended to have separate project for RabbitMQ cluster. Modify the namespace to be able to deploy RabbitMQ cluster into it. So we need to get user 999 authorized. For that we specify a security constraint named: rabbitmq-cluster #Define a Security Context Constraint oc apply -f rmq-scc.yaml Update the name space to get openshift scc userid and groupid range. You can do that in two ways, by directly editing the namespace definition: oc edit namespace rabbitmq-cluster # Then add the following annotations: annotations: openshift.io/sa.scc.supplemental-groups: 999 /1 openshift.io/sa.scc.uid-range: 0 -999/1 Or by using our custom cluster definition: oc apply -f rmq-cluster-ns.yaml For every RabbitMQ cluster assign the previously created security context constraint to the cluster's service account: oc adm policy add-scc-to-user rabbitmq-cluster -z rmqcluster-rabbitmq-server Finally create a cluster instance: oc apply -f rmq-cluster.yaml . Define a route on the client service on port 15672 Get the admin user and password as defined in the secrets oc describe secret rmqcluster-rabbitmq-default-user # Get user name oc get secret rmqcluster-rabbitmq-default-user -o jsonpath = '{.data.username}' | base64 --decode # Get password oc get secret rmqcluster-rabbitmq-default-user -o jsonpath = '{.data.password}' | base64 --decode To validate the RabbitMQ cluster runs correctly you can use the RabbitMQ console, and add the items queue.","title":"Deploy RabbitMQ using operators"},{"location":"use-cases/connect-rabbitmq/#configure-the-kafka-connector-for-rabbitmq-source","text":"The rabbitmq-source.json define the connector and the RabbitMQ connection parameters: { \"name\" : \"RabbitMQSourceConnector\" , \"config\" : { \"connector.class\" : \"com.ibm.eventstreams.connect.rabbitmqsource.RabbitMQSourceConnector\" , \"tasks.max\" : \"1\" , \"kafka.topic\" : \"items\" , \"rabbitmq.host\" : \"rabbitmq\" , \"rabbitmq.port\" : 5672 , \"rabbitmq.queue\" : \"items\" , \"rabbitmq.prefetch.count\" : \"500\" , \"rabbitmq.automatic.recovery.enabled\" : \"true\" , \"rabbitmq.network.recovery.interval.ms\" : \"10000\" , \"rabbitmq.topology.recovery.enabled\" : \"true\" } } This file is uploaded to Kafka Connect via a PORT operation: curl -X POST -H \"Content-Type: application/json\" http://localhost:8083/connectors --data \"@./rabbitmq-source.json\" To verify use: curl -X GET http://localhost:8083/connectors . In Kafka connect trace you should see: [ Worker clientId = connect-1, groupId = eda-kconnect ] Connector RabbitMQSourceConnector config updated ... Starting connector RabbitMQSourceConnector ... Starting task RabbitMQSourceConnector-0 And Rabbitmq that get the connection from Kafka Connect. rabbitmq_1 [ info ] < 0 .1766.0> accepting AMQP connection < 0 .1766.0> ( 172 .19.0.3:33040 -> 172 .19.0.2:5672 ) kconnect_1 INFO Creating Channel ( com.ibm.eventstreams.connect.rabbitmqsource.RabbitMQSourceTask:61 ) rabbitmq_1 connection < 0 .1766.0> ( 172 .19.0.3:33040 -> 172 .19.0.2:5672 ) : user 'rabbit-user' authenticated and granted access to vhost '/'","title":"Configure the kafka connector for Rabbitmq source"},{"location":"use-cases/connect-s3/","text":"Overview \u00b6 This scenario walkthrough will cover the usage of IBM Event Streams as a Kafka provider and Amazon S3 as an object storage service as systems to integrate with the Kafka Connect framework . Through the use of the Apache Camel opensource project , we are able to use the Apache Camel Kafka Connector in both a source and a sink capacity to provide bidirectional communication between IBM Event Streams and AWS S3 . As different use cases will require different configuration details to accommodate different situational requirements, the Kafka to S3 Source and Sink capabilities described here can be used to move data between S3 buckets with a Kafka topic being the middle-man or move data between Kafka topics with an S3 Bucket being the middle-man. However, take care to ensure that you do not create an infinite processing loop by writing to the same Kafka topics and the same S3 buckets with both a Source and Sink connector deployed at the same time. Scenario prereqs \u00b6 OpenShift Container Platform This deployment scenario was developed for use on the OpenShift Container Platform, with a minimum version of 4.2 . Strimzi - This deployment scenario will make use of the Strimzi Operator for Kafka deployments and the custom resources it manages. - A minimum version of 0.17.0 is required for this scenario. This scenario has been explicitly validated with version 0.17.0 . - The simplest scenario is to deploy the Strimzi Operator to watch all namespaces for relevant custom resource creation and management. - This can be done in the OpenShift console via the Operators > Operator Hub page. Amazon Web Services account - As this scenario will make use of AWS S3 , an active Amazon Web Services account is required. - Using the configuration described in this walkthrough, an additional IAM user can be created for further separation of permission, roles, and responsibilities. - This new IAM user should contain: - The AmazonS3FullAccess policy attached to it (as it will need both read and write access to S3) , - An S3 Bucket Policy set on the Bucket to allow the IAM user to perform CRUD actions on the bucket and its objects. - Create a file named aws-credentials.properties with the following format: aws_access_key_id=AKIA123456EXAMPLE aws_secret_access_key=strWrz/bb8%c3po/r2d2EXAMPLEKEY Create a Kubernetes Secret from this file to inject into the Kafka Connect cluster at runtime: kubectl create secret generic aws-credentials --from-file = aws-credentials.properties - Additional work is underway to enable configuration of the components to make use of IAM Roles instead. IBM Event Streams API Key - This scenario is written with IBM Event Streams as the provider of the Kafka endpoints. - API Keys are required for connectivity to the Kafka brokers and interaction with Kafka topics. - An API key should be created with (at minimum) read and write access to the source and target Kafka topics the connectors will interact with. - A Kubernetes Secret must be created with the Event Streams API to inject into the Kafka Connect cluster at runtime: kubectl create secret generic eventstreams-apikey --from-literal = password = <eventstreams_api_key> IBM Event Streams Certificates on IBM Cloud Pak for Integration - If you are using an IBM Event Streams instance deployed via the IBM Cloud Pak for Integration, you must also download the generated truststore file to provide TLS communication between the connectors and the Kafka brokers. - This file, along with its password, can be found on the Connect to this cluster dialog in the Event Streams UI. - Once downloaded, it must be configured to work with the Kafka Connect certificate deployment pattern: keytool -importkeystore -srckeystore es-cert.jks -destkeystore es-cert.p12 -deststoretype PKCS12 openssl pkcs12 -in es-cert.p12 -nokeys -out es-cert.crt kubectl create secret generic eventstreams-truststore-cert --from-file = es-cert.crt IBM Event Streams Certificates on IBM Cloud - If you are using an IBM Event Streams instance deployed on IBM Cloud, you need to provide the root CA certificate to connect correctly from the Kafka Connect instance. - This file can be downloaded as eventstreams.cloud.ibm.com.cer from the endpoint defined in your service credentials via the kafka_http_url property. - Once downloaded, it must be configured to work with the Kafka Connect certificate deployment pattern: openssl x509 -inform DER -in eventstreams.cloud.ibm.com.cer -out es-cert.crt kubectl create secret generic eventstreams-truststore-cert --from-file = es-cert.crt Kafka Connect Cluster \u00b6 We will take advantage of some of the developer experience improvements that OpenShift and the Strimi Operator brings to the Kafka Connect framework. The Strimzi Operator provides a KafkaConnect custom resource which will manage a Kafka Connect cluster for us with minimal system interaction. The only work we need to do is to update the container image that the Kafka Connect deployment will use with the necessary Camel Kafka Connector binaries, which OpenShift can help us with through the use of its Build capabilities. (Optional) Create ConfigMap for log4j configuration \u00b6 Due to the robust nature of Apache Camel, the default logging settings for the Apache Kafka Connect classes will send potentially sensitive information to the logs during Apache Camel context configuration. To avoid this, we can provide an updated logging configuration to the log4j configuration that is used by our deployments. Save the properties file below and name it log4j.properties . Then create a ConfigMap via kubectl create configmap custom-connect-log4j --from-file=log4j.properties . This ConfigMap will then be used in our KafkaConnect cluster creation to filter out any logging output containing accesskey or secretkey permutations. # Do not change this generated file. Logging can be configured in the corresponding kubernetes/openshift resource. log4j.appender.CONSOLE = org.apache.log4j.ConsoleAppender log4j.appender.CONSOLE.layout = org.apache.log4j.PatternLayout log4j.appender.CONSOLE.layout.ConversionPattern = %d{ISO8601} %p %m (%c) [%t]%n connect.root.logger.level = INFO log4j.rootLogger = ${connect.root.logger.level}, CONSOLE log4j.logger.org.apache.zookeeper = ERROR log4j.logger.org.I0Itec.zkclient = ERROR log4j.logger.org.reflections = ERROR # Due to back-leveled version of log4j that is included in Kafka Connect, # we can use multiple StringMatchFilters to remove all the permutations # of the AWS accessKey and secretKey values that may get dumped to stdout # and thus into any connected logging system. log4j.appender.CONSOLE.filter.a = org.apache.log4j.varia.StringMatchFilter log4j.appender.CONSOLE.filter.a.StringToMatch = accesskey log4j.appender.CONSOLE.filter.a.AcceptOnMatch = false log4j.appender.CONSOLE.filter.b = org.apache.log4j.varia.StringMatchFilter log4j.appender.CONSOLE.filter.b.StringToMatch = accessKey log4j.appender.CONSOLE.filter.b.AcceptOnMatch = false log4j.appender.CONSOLE.filter.c = org.apache.log4j.varia.StringMatchFilter log4j.appender.CONSOLE.filter.c.StringToMatch = AccessKey log4j.appender.CONSOLE.filter.c.AcceptOnMatch = false log4j.appender.CONSOLE.filter.d = org.apache.log4j.varia.StringMatchFilter log4j.appender.CONSOLE.filter.d.StringToMatch = ACCESSKEY log4j.appender.CONSOLE.filter.d.AcceptOnMatch = false log4j.appender.CONSOLE.filter.e = org.apache.log4j.varia.StringMatchFilter log4j.appender.CONSOLE.filter.e.StringToMatch = secretkey log4j.appender.CONSOLE.filter.e.AcceptOnMatch = false log4j.appender.CONSOLE.filter.f = org.apache.log4j.varia.StringMatchFilter log4j.appender.CONSOLE.filter.f.StringToMatch = secretKey log4j.appender.CONSOLE.filter.f.AcceptOnMatch = false log4j.appender.CONSOLE.filter.g = org.apache.log4j.varia.StringMatchFilter log4j.appender.CONSOLE.filter.g.StringToMatch = SecretKey log4j.appender.CONSOLE.filter.g.AcceptOnMatch = false log4j.appender.CONSOLE.filter.h = org.apache.log4j.varia.StringMatchFilter log4j.appender.CONSOLE.filter.h.StringToMatch = SECRETKEY log4j.appender.CONSOLE.filter.h.AcceptOnMatch = false Deploy the baseline Kafka Connect Cluster \u00b6 Review the YAML description for our KafkaConnectS2I custom resource below, named connect-cluster-101 . Pay close attention to (using YAML notation) : - spec.logging.name should point to the name of the ConfigMap created in the previous step to configure custom log4j logging filters (optional) - spec.bootstrapServers should be updated with your local Event Streams endpoints - spec.tls.trustedCertificates[0].secretName should match the Kubernetes Secret containing the IBM Event Streams certificate - spec.authentication.passwordSecret.secretName should match the Kubernetes Secret containing the IBM Event Streams API key - spec.externalConfiguration.volumes[0].secret.secretName should match the Kubernetes Secret containing your AWS credentials - spec.config['group.id'] should be a unique name for this Kafka Connect cluster across all Kafka Connect instances that will be communicating with the same set of Kafka brokers. - spec.config['*.storage.topic'] should be updated to provide unique topics for this Kafka Connect cluster inside your Kafka deployment. Distinct Kafka Connect clusters should not share metadata topics. apiVersion : kafka.strimzi.io/v1alpha1 kind : KafkaConnectS2I metadata : name : connect-cluster-101 annotations : strimzi.io/use-connector-resources : \"true\" spec : #logging: # type: external # name: custom-connect-log4j replicas : 1 bootstrapServers : es-1-ibm-es-proxy-route-bootstrap-eventstreams.apps.cluster.local:443 tls : trustedCertificates : - certificate : es-cert.crt secretName : eventstreams-truststore-cert authentication : passwordSecret : secretName : eventstreams-apikey password : password username : token type : plain externalConfiguration : volumes : - name : aws-credentials secret : secretName : aws-credentials config : group.id : connect-cluster-101 config.providers : file config.providers.file.class : org.apache.kafka.common.config.provider.FileConfigProvider key.converter : org.apache.kafka.connect.json.JsonConverter value.converter : org.apache.kafka.connect.json.JsonConverter key.converter.schemas.enable : false value.converter.schemas.enable : false offset.storage.topic : connect-cluster-101-offsets config.storage.topic : connect-cluster-101-configs status.storage.topic : connect-cluster-101-status Save the YAML above into a file named kafka-connect.yaml . If you created the ConfigMap in the previous step to filter out accesskey and secretkey values from the logs, uncomment the spec.logging lines to allow for the custom logging filters to be enabled during Kafka Connect cluster creation. Then this resource can be created via kubectl apply -f kafka-connect.yaml . You can then tail the output of the connect-cluster-101 pods for updates on the connector status. Build the Camel Kafka Connector \u00b6 The next step is to build the Camel Kafka Connector binaries so that they can be loaded into the just-deployed Kafka Connect cluster's container images. Clone the repository https://github.com/osowski/camel-kafka-connector to your local machine: git clone https://github.com/osowski/camel-kafka-connector.git Check out the camel-kafka-connector-0.1.0-branch : git checkout camel-kafka-connector-0.1.0-branch From the root directory of the repository, build the project components: mvn clean package Go get a coffee and take a walk... as this build will take around 30 minutes on a normal developer workstation. To reduce the overall build scope of the project, you can comment out the undesired modules from the <modules> element of the connectors/pom.xml using <!-- --> notation. Copy the generated S3 artifacts to the core package build artifacts: cp connectors/camel-aws-s3-kafka-connector/target/camel-aws-s3-kafka-connector-0.1.0.jar core/target/camel-kafka-connector-0.1.0-package/share/java/camel-kafka-connector/ Some items to note: The repository used here (https://github.com/osowski/camel-kafka-connector) is a fork of the official repository (https://github.com/apache/camel-kafka-connector) with a minor update applied to allow for dynamic endpoints to be specified via configuration, which is critical for our Kafka to S3 Sink Connector scenario. This step (and the next step) will eventually be eliminated by providing an existing container image with the necessary Camel Kafka Connector binaries as part of a build system. Deploy the Camel Kafka Connector binaries \u00b6 Now that the Camel Kafka Connector binaries have been built, we need to include them on the classpath inside of the container image which our Kafka Connect clusters are using. From the root directory of the repository, start a new OpenShift Build, using the generated build artifacts: oc start-build connect-cluster-101-connect --from-dir = ./core/target/camel-kafka-connector-0.1.0-package/share/java --follow Watch the Kubernetes pods as they are updated with the new build and rollout of the Kafka Connect Cluster using the updated container image (which now includes the Camel Kafka Connector binaries) : kubectl get pods -w Once the connect-cluster-101-connect-2-[random-suffix] pod is in a Running state, you can proceed. Kafka to S3 Sink Connector \u00b6 Now that you have a Kafka Connect cluster up and running, you will need to configure a connector to actually begin the transmission of data from one system to the other. This will be done by taking advantage of Strimzi and using the KafkaConnector custom resource the Strimzi Operator manages for us. Review the YAML description for our KafkaConnector custom resource below, named s3-sink-connector . Pay close attention to: - The strimzi.io/cluster label must match the deployed Kafka Connect cluster you previously deployed (or else Strimzi will not connect the KafkaConnector to your KafkaConnect cluster) - The topics parameter (named my-source-topic here) - The S3 Bucket parameter of the camel.sink.url configuration option (named my-s3-bucket here) apiVersion : kafka.strimzi.io/v1alpha1 kind : KafkaConnector metadata : name : s3-sink-connector labels : strimzi.io/cluster : connect-cluster-101 spec : class : org.apache.camel.kafkaconnector.CamelSinkConnector tasksMax : 1 config : key.converter : org.apache.kafka.connect.storage.StringConverter value.converter : org.apache.kafka.connect.storage.StringConverter topics : my-source-topic camel.sink.url : aws-s3://my-s3-bucket?keyName=${date:now:yyyyMMdd-HHmmssSSS}-${exchangeId} camel.sink.maxPollDuration : 10000 camel.component.aws-s3.configuration.autocloseBody : false camel.component.aws-s3.accessKey : ${file:/opt/kafka/external-configuration/aws-credentials/aws-credentials.properties:aws_access_key_id} camel.component.aws-s3.secretKey : ${file:/opt/kafka/external-configuration/aws-credentials/aws-credentials.properties:aws_secret_access_key} camel.component.aws-s3.region : US_EAST_1 Once you have updated the YAML and saved it in a file named kafka-sink-connector.yaml , this resource can be created via kubectl apply -f kafka-sink-connector.yaml . You can then tail the output of the connect-cluster-101 pods for updates on the connector status. NOTE: If you require objects in S3 to reside in a sub-folder of the bucket root, you can place a folder name prefix in the keyName query parameter of the camel.sink.url configuration option above. For example, camel.sink.url: aws-s3://my-s3-bucket?keyName=myfoldername/${date:now:yyyyMMdd-HHmmssSSS}-${exchangeId} . S3 to Kafka Source Connector \u00b6 Similar to the Kafka to S3 Sink Connector scenario, this scenario will make use of the Strimzi KafkaConnector custom resource to configure the specific connector instance. Review the YAML description for our KafkaConnector custom resource below, named s3-source-connector . Pay close attention to: - The strimzi.io/cluster label must match the deployed Kafka Connect cluster you previously deployed (or else Strimzi will not connect the KafkaConnector to your KafkaConnect cluster) - The topics and camel.source.kafka.topic parameters (named my-target-topic here) - The S3 Bucket parameter of the camel.sink.url configuration option (named my-s3-bucket here) Please note that it is an explicit intention that the topics used in the Kafka to S3 Sink Connector configuration and the S3 to Kafka Source Connector configuration are different. If these configurations were to use the same Kafka topic and the same S3 Bucket , we would create an infinite processing loop of the same information being endlessly recycled through the system. In our example deployments here, we are deploying to different topics but the same S3 Bucket. apiVersion : kafka.strimzi.io/v1alpha1 kind : KafkaConnector metadata : name : s3-source-connector labels : strimzi.io/cluster : connect-cluster-101 spec : class : org.apache.camel.kafkaconnector.CamelSourceConnector tasksMax : 1 config : key.converter : org.apache.kafka.connect.storage.StringConverter value.converter : org.apache.camel.kafkaconnector.awss3.converters.S3ObjectConverter topics : my-target-topic camel.source.kafka.topic : my-target-topic camel.source.url : aws-s3://my-s3-bucket?autocloseBody=false camel.source.maxPollDuration : 10000 camel.component.aws-s3.accessKey : ${file:/opt/kafka/external-configuration/aws-credentials/aws-credentials.properties:aws_access_key_id} camel.component.aws-s3.secretKey : ${file:/opt/kafka/external-configuration/aws-credentials/aws-credentials.properties:aws_secret_access_key} camel.component.aws-s3.region : US_EAST_1 Once you have updated the YAML and saved it in a file named kafka-source-connector.yaml , this resource can be created via kubectl apply -f kafka-source-connector.yaml . You can then tail the output of the connect-cluster-101 pods for updates on the connector status. NOTE: If you require the connector to only read objects from a subdirecotry of the S3 bucket root, you can set the camel.component.aws-s3.configuration.prefix configuration option with the value of the subdirectory name. For example, camel.component.aws-s3.configuration.prefix: myfoldername . Event Streams v10 Addendum \u00b6 The steps up to now assumed usage of an OpenShift Container Platform v4.3.x cluster, IBM Cloud Pak for Integration v2020.1.1 and IBM Event Streams v2019.4.2 With the release of Event Streams v10 (which is built on top of the Strimzi Kafka Operator) and the new CP4I there are some things need that need to be adjusted in the yamls to reflect that. You can edit the previous kafka-connect.yaml or create a new file. kafka-connect-esv10.yaml apiVersion : eventstreams.ibm.com/v1beta1 kind : KafkaConnectS2I metadata : name : connect-cluster-101 annotations : eventstreams.ibm.com/use-connector-resources : \"true\" spec : #logging: # type: external # name: custom-connect-log4j version : 2.5.0 replicas : 1 bootstrapServers : { internal-bootstrap-server-address } template : pod : imagePullSecrets : [] metadata : annotations : eventstreams.production.type : CloudPakForIntegrationNonProduction productID : { product-id } productName : IBM Event Streams for Non Production productVersion : 10.0.0 productMetric : VIRTUAL_PROCESSOR_CORE productChargedContainers : { connect-cluster-101 } -connect cloudpakId : { cloudpak-id } cloudpakName : IBM Cloud Pak for Integration cloudpakVersion : 2020.2.1 productCloudpakRatio : \"2:1\" tls : trustedCertificates : - secretName : { your-es-instance-name } -cluster-ca-cert certificate : ca.crt authentication : type : tls certificateAndKey : certificate : user.crt key : user.key secretName : { generated-secret-from-ui } externalConfiguration : volumes : - name : aws-credentials secret : secretName : aws-credentials config : group.id : connect-cluster-101 config.providers : file config.providers.file.class : org.apache.kafka.common.config.provider.FileConfigProvider key.converter : org.apache.kafka.connect.json.JsonConverter value.converter : org.apache.kafka.connect.json.JsonConverter key.converter.schemas.enable : false value.converter.schemas.enable : false offset.storage.topic : connect-cluster-101-offsets config.storage.topic : connect-cluster-101-configs status.storage.topic : connect-cluster-101-status There are a couple changes of note here. apiVersion: eventstreams.ibm.com/v1beta1 instead of the previous Strimzi one. metadata.annotations.eventstreams.ibm.com/use-connector-resources: \"true\" instead of the previous Strimzi one as well. spec.bootstrapservers: Previously in Event Streams v2019.4.2 and prior there was only a single external facing route for this bootstrap server address. In Event Streams v10 there is an External and Internal one now. Replace {internal-bootstrap-server-address} with the Internal bootstrap server address from the Event Streams v10 UI. spec.template.pod.metadata.annotations.* This entire section is new and represents new metadata that the Event Streams instance needs. spec.template.pod.metadata.annotations.productID You can find this value when trying to deploy an Event Streams Custom Resource from the Installed Event Streams Operator YAML deployment or when applying this YAML through the command-line, OCP will provide you the proper values if they're wrong. spec.template.pod.metadata.annotations.cloudpakId Same as productID. spec.tls.trustedCertificates This is different from the previous one in that this certificate was automatically generated on creation of your Event Streams Kafka Cluster. If your Event Streams instance is named eventstreams-dev then this value should be eventstreams-dev-cluster-ca-cert . spec.authentication.* This section is mostly different from the previous section. Prior we used a generated API Key, but on Event Streams v10 we will need to generate TLS credentials. In the Internal connection details there will be a Generate TLS Credentials button. Here you can name your secret and provide the proper access to it. Note 1 - This is automatically created by the Event Streams Operator into the same namespace as both a KafkaUser Custom Resource as well as a secret. If your secret is named internal-secret then there will be an automatically generated Kubernetes/OpenShift secret named that as well as a KafkaUser . user.crt and user.key are certificates automatically generated within said secret. Note 2 - Deletion of that secret will replicate itself. You will need to delete the associated and created KafkaUser . kafka-sink-connector.yaml-esv10.yaml apiVersion : eventstreams.ibm.com/v1alpha1 kind : KafkaConnector metadata : name : s3-sink-connector labels : eventstreams.ibm.com/cluster : connect-cluster-101 spec : class : org.apache.camel.kafkaconnector.CamelSinkConnector tasksMax : 1 config : key.converter : org.apache.kafka.connect.storage.StringConverter value.converter : org.apache.kafka.connect.storage.StringConverter topics : my-topic camel.sink.url : aws-s3://my-s3-bucket?keyName=${date:now:yyyyMMdd-HHmmssSSS}-${exchangeId} camel.sink.maxPollDuration : 10000 camel.component.aws-s3.configuration.autocloseBody : false camel.component.aws-s3.accessKey : ${file:/opt/kafka/external-configuration/aws-credentials/aws-credentials.properties:aws_access_key_id} camel.component.aws-s3.secretKey : ${file:/opt/kafka/external-configuration/aws-credentials/aws-credentials.properties:aws_secret_access_key} camel.component.aws-s3.region : my-s3-bucket-region - For the most part identical besides two minor changes. - apiVersion: eventstreams.ibm.com/v1alpha1 instead of Strimzi. - metadata.labels.eventstreams.ibm.com/cluster: connect-cluster-101 again instead of Strimzi. kafka-source-connector.yaml-esv10.yaml apiVersion : eventstreams.ibm.com/v1alpha1 kind : KafkaConnector metadata : name : s3-source-connector labels : eventstreams.ibm.com/cluster : connect-cluster-101 spec : class : org.apache.camel.kafkaconnector.CamelSourceConnector tasksMax : 1 config : key.converter : org.apache.kafka.connect.storage.StringConverter value.converter : org.apache.camel.kafkaconnector.awss3.converters.S3ObjectConverter topics : my-topic camel.source.kafka.topic : my-topic camel.source.url : aws-s3://my-s3-bucket?autocloseBody=false camel.source.maxPollDuration : 10000 camel.component.aws-s3.accessKey : ${file:/opt/kafka/external-configuration/aws-credentials/aws-credentials.properties:aws_access_key_id} camel.component.aws-s3.secretKey : ${file:/opt/kafka/external-configuration/aws-credentials/aws-credentials.properties:aws_secret_access_key} camel.component.aws-s3.region : my-s3-bucket-region - Similar to the S3 sink connector yaml. - apiVersion: eventstreams.ibm.com/v1alpha1 instead of Strimzi. - metadata.labels.eventstreams.ibm.com/cluster: connect-cluster-101 again instead of Strimzi. Troubleshooting \u00b6 The log output from the Kafka Connector instances will be available in the KafkaConnectS2I pods, named in the pattern of {connect-cluster-name}-connect-{latest-build-id}-{random-suffix} . This pattern will be referred to as {connect-pod} throughout the rest of the Troubleshooting section. To increase the logging output of the deployed connector instances: Follow the steps for \"Create ConfigMap for log4j configuration\" in Kafka Connect Cluster above Update the log4j.properties ConfigMap to include the line log4j.logger.org.apache.camel.kafkaconnector=DEBUG Reapply the YAML configuration of the KafkaConnectS2I cluster To check the deployment status of applied Kafka Connect configuration via Kafka Connect's REST API, run the following commands: oc exec {connect-pod} bash -- -c \"curl --silent http://localhost:8083/connectors/\" oc exec {connect-pod} bash -- -c \"curl --silent http://localhost:8083/connectors/{deployed-connector-name}/status\" Next steps \u00b6 Enable use of IAM Credentials in the Connector configuration (as the default Java code currently outputs aws_access_key_id and aws_secret_access_key to container runtime logs due to their existence as configuration properties) Optionally configure individual connector instances to startup with offset value of -1 (to enable to run from beginning of available messages) Implement a build system to produce container images with the Camel Kafka Connector binaries already present References \u00b6 Apache Camel Kafka Connector - Try it out on OpenShift with Strimzi Apache Camel - Available pattern elements for use in the keyName parameter of the camel.sink.url property. Apache Camel - Dynamic Endpoint reference Red Hat Developer Blog - Using secrets in Kafka Connect configuration Apache Kafka - Connect Overview","title":"Kafka Connect - S3"},{"location":"use-cases/connect-s3/#overview","text":"This scenario walkthrough will cover the usage of IBM Event Streams as a Kafka provider and Amazon S3 as an object storage service as systems to integrate with the Kafka Connect framework . Through the use of the Apache Camel opensource project , we are able to use the Apache Camel Kafka Connector in both a source and a sink capacity to provide bidirectional communication between IBM Event Streams and AWS S3 . As different use cases will require different configuration details to accommodate different situational requirements, the Kafka to S3 Source and Sink capabilities described here can be used to move data between S3 buckets with a Kafka topic being the middle-man or move data between Kafka topics with an S3 Bucket being the middle-man. However, take care to ensure that you do not create an infinite processing loop by writing to the same Kafka topics and the same S3 buckets with both a Source and Sink connector deployed at the same time.","title":"Overview"},{"location":"use-cases/connect-s3/#scenario-prereqs","text":"OpenShift Container Platform This deployment scenario was developed for use on the OpenShift Container Platform, with a minimum version of 4.2 . Strimzi - This deployment scenario will make use of the Strimzi Operator for Kafka deployments and the custom resources it manages. - A minimum version of 0.17.0 is required for this scenario. This scenario has been explicitly validated with version 0.17.0 . - The simplest scenario is to deploy the Strimzi Operator to watch all namespaces for relevant custom resource creation and management. - This can be done in the OpenShift console via the Operators > Operator Hub page. Amazon Web Services account - As this scenario will make use of AWS S3 , an active Amazon Web Services account is required. - Using the configuration described in this walkthrough, an additional IAM user can be created for further separation of permission, roles, and responsibilities. - This new IAM user should contain: - The AmazonS3FullAccess policy attached to it (as it will need both read and write access to S3) , - An S3 Bucket Policy set on the Bucket to allow the IAM user to perform CRUD actions on the bucket and its objects. - Create a file named aws-credentials.properties with the following format: aws_access_key_id=AKIA123456EXAMPLE aws_secret_access_key=strWrz/bb8%c3po/r2d2EXAMPLEKEY Create a Kubernetes Secret from this file to inject into the Kafka Connect cluster at runtime: kubectl create secret generic aws-credentials --from-file = aws-credentials.properties - Additional work is underway to enable configuration of the components to make use of IAM Roles instead. IBM Event Streams API Key - This scenario is written with IBM Event Streams as the provider of the Kafka endpoints. - API Keys are required for connectivity to the Kafka brokers and interaction with Kafka topics. - An API key should be created with (at minimum) read and write access to the source and target Kafka topics the connectors will interact with. - A Kubernetes Secret must be created with the Event Streams API to inject into the Kafka Connect cluster at runtime: kubectl create secret generic eventstreams-apikey --from-literal = password = <eventstreams_api_key> IBM Event Streams Certificates on IBM Cloud Pak for Integration - If you are using an IBM Event Streams instance deployed via the IBM Cloud Pak for Integration, you must also download the generated truststore file to provide TLS communication between the connectors and the Kafka brokers. - This file, along with its password, can be found on the Connect to this cluster dialog in the Event Streams UI. - Once downloaded, it must be configured to work with the Kafka Connect certificate deployment pattern: keytool -importkeystore -srckeystore es-cert.jks -destkeystore es-cert.p12 -deststoretype PKCS12 openssl pkcs12 -in es-cert.p12 -nokeys -out es-cert.crt kubectl create secret generic eventstreams-truststore-cert --from-file = es-cert.crt IBM Event Streams Certificates on IBM Cloud - If you are using an IBM Event Streams instance deployed on IBM Cloud, you need to provide the root CA certificate to connect correctly from the Kafka Connect instance. - This file can be downloaded as eventstreams.cloud.ibm.com.cer from the endpoint defined in your service credentials via the kafka_http_url property. - Once downloaded, it must be configured to work with the Kafka Connect certificate deployment pattern: openssl x509 -inform DER -in eventstreams.cloud.ibm.com.cer -out es-cert.crt kubectl create secret generic eventstreams-truststore-cert --from-file = es-cert.crt","title":"Scenario prereqs"},{"location":"use-cases/connect-s3/#kafka-connect-cluster","text":"We will take advantage of some of the developer experience improvements that OpenShift and the Strimi Operator brings to the Kafka Connect framework. The Strimzi Operator provides a KafkaConnect custom resource which will manage a Kafka Connect cluster for us with minimal system interaction. The only work we need to do is to update the container image that the Kafka Connect deployment will use with the necessary Camel Kafka Connector binaries, which OpenShift can help us with through the use of its Build capabilities.","title":"Kafka Connect Cluster"},{"location":"use-cases/connect-s3/#optional-create-configmap-for-log4j-configuration","text":"Due to the robust nature of Apache Camel, the default logging settings for the Apache Kafka Connect classes will send potentially sensitive information to the logs during Apache Camel context configuration. To avoid this, we can provide an updated logging configuration to the log4j configuration that is used by our deployments. Save the properties file below and name it log4j.properties . Then create a ConfigMap via kubectl create configmap custom-connect-log4j --from-file=log4j.properties . This ConfigMap will then be used in our KafkaConnect cluster creation to filter out any logging output containing accesskey or secretkey permutations. # Do not change this generated file. Logging can be configured in the corresponding kubernetes/openshift resource. log4j.appender.CONSOLE = org.apache.log4j.ConsoleAppender log4j.appender.CONSOLE.layout = org.apache.log4j.PatternLayout log4j.appender.CONSOLE.layout.ConversionPattern = %d{ISO8601} %p %m (%c) [%t]%n connect.root.logger.level = INFO log4j.rootLogger = ${connect.root.logger.level}, CONSOLE log4j.logger.org.apache.zookeeper = ERROR log4j.logger.org.I0Itec.zkclient = ERROR log4j.logger.org.reflections = ERROR # Due to back-leveled version of log4j that is included in Kafka Connect, # we can use multiple StringMatchFilters to remove all the permutations # of the AWS accessKey and secretKey values that may get dumped to stdout # and thus into any connected logging system. log4j.appender.CONSOLE.filter.a = org.apache.log4j.varia.StringMatchFilter log4j.appender.CONSOLE.filter.a.StringToMatch = accesskey log4j.appender.CONSOLE.filter.a.AcceptOnMatch = false log4j.appender.CONSOLE.filter.b = org.apache.log4j.varia.StringMatchFilter log4j.appender.CONSOLE.filter.b.StringToMatch = accessKey log4j.appender.CONSOLE.filter.b.AcceptOnMatch = false log4j.appender.CONSOLE.filter.c = org.apache.log4j.varia.StringMatchFilter log4j.appender.CONSOLE.filter.c.StringToMatch = AccessKey log4j.appender.CONSOLE.filter.c.AcceptOnMatch = false log4j.appender.CONSOLE.filter.d = org.apache.log4j.varia.StringMatchFilter log4j.appender.CONSOLE.filter.d.StringToMatch = ACCESSKEY log4j.appender.CONSOLE.filter.d.AcceptOnMatch = false log4j.appender.CONSOLE.filter.e = org.apache.log4j.varia.StringMatchFilter log4j.appender.CONSOLE.filter.e.StringToMatch = secretkey log4j.appender.CONSOLE.filter.e.AcceptOnMatch = false log4j.appender.CONSOLE.filter.f = org.apache.log4j.varia.StringMatchFilter log4j.appender.CONSOLE.filter.f.StringToMatch = secretKey log4j.appender.CONSOLE.filter.f.AcceptOnMatch = false log4j.appender.CONSOLE.filter.g = org.apache.log4j.varia.StringMatchFilter log4j.appender.CONSOLE.filter.g.StringToMatch = SecretKey log4j.appender.CONSOLE.filter.g.AcceptOnMatch = false log4j.appender.CONSOLE.filter.h = org.apache.log4j.varia.StringMatchFilter log4j.appender.CONSOLE.filter.h.StringToMatch = SECRETKEY log4j.appender.CONSOLE.filter.h.AcceptOnMatch = false","title":"(Optional) Create ConfigMap for log4j configuration"},{"location":"use-cases/connect-s3/#deploy-the-baseline-kafka-connect-cluster","text":"Review the YAML description for our KafkaConnectS2I custom resource below, named connect-cluster-101 . Pay close attention to (using YAML notation) : - spec.logging.name should point to the name of the ConfigMap created in the previous step to configure custom log4j logging filters (optional) - spec.bootstrapServers should be updated with your local Event Streams endpoints - spec.tls.trustedCertificates[0].secretName should match the Kubernetes Secret containing the IBM Event Streams certificate - spec.authentication.passwordSecret.secretName should match the Kubernetes Secret containing the IBM Event Streams API key - spec.externalConfiguration.volumes[0].secret.secretName should match the Kubernetes Secret containing your AWS credentials - spec.config['group.id'] should be a unique name for this Kafka Connect cluster across all Kafka Connect instances that will be communicating with the same set of Kafka brokers. - spec.config['*.storage.topic'] should be updated to provide unique topics for this Kafka Connect cluster inside your Kafka deployment. Distinct Kafka Connect clusters should not share metadata topics. apiVersion : kafka.strimzi.io/v1alpha1 kind : KafkaConnectS2I metadata : name : connect-cluster-101 annotations : strimzi.io/use-connector-resources : \"true\" spec : #logging: # type: external # name: custom-connect-log4j replicas : 1 bootstrapServers : es-1-ibm-es-proxy-route-bootstrap-eventstreams.apps.cluster.local:443 tls : trustedCertificates : - certificate : es-cert.crt secretName : eventstreams-truststore-cert authentication : passwordSecret : secretName : eventstreams-apikey password : password username : token type : plain externalConfiguration : volumes : - name : aws-credentials secret : secretName : aws-credentials config : group.id : connect-cluster-101 config.providers : file config.providers.file.class : org.apache.kafka.common.config.provider.FileConfigProvider key.converter : org.apache.kafka.connect.json.JsonConverter value.converter : org.apache.kafka.connect.json.JsonConverter key.converter.schemas.enable : false value.converter.schemas.enable : false offset.storage.topic : connect-cluster-101-offsets config.storage.topic : connect-cluster-101-configs status.storage.topic : connect-cluster-101-status Save the YAML above into a file named kafka-connect.yaml . If you created the ConfigMap in the previous step to filter out accesskey and secretkey values from the logs, uncomment the spec.logging lines to allow for the custom logging filters to be enabled during Kafka Connect cluster creation. Then this resource can be created via kubectl apply -f kafka-connect.yaml . You can then tail the output of the connect-cluster-101 pods for updates on the connector status.","title":"Deploy the baseline Kafka Connect Cluster"},{"location":"use-cases/connect-s3/#build-the-camel-kafka-connector","text":"The next step is to build the Camel Kafka Connector binaries so that they can be loaded into the just-deployed Kafka Connect cluster's container images. Clone the repository https://github.com/osowski/camel-kafka-connector to your local machine: git clone https://github.com/osowski/camel-kafka-connector.git Check out the camel-kafka-connector-0.1.0-branch : git checkout camel-kafka-connector-0.1.0-branch From the root directory of the repository, build the project components: mvn clean package Go get a coffee and take a walk... as this build will take around 30 minutes on a normal developer workstation. To reduce the overall build scope of the project, you can comment out the undesired modules from the <modules> element of the connectors/pom.xml using <!-- --> notation. Copy the generated S3 artifacts to the core package build artifacts: cp connectors/camel-aws-s3-kafka-connector/target/camel-aws-s3-kafka-connector-0.1.0.jar core/target/camel-kafka-connector-0.1.0-package/share/java/camel-kafka-connector/ Some items to note: The repository used here (https://github.com/osowski/camel-kafka-connector) is a fork of the official repository (https://github.com/apache/camel-kafka-connector) with a minor update applied to allow for dynamic endpoints to be specified via configuration, which is critical for our Kafka to S3 Sink Connector scenario. This step (and the next step) will eventually be eliminated by providing an existing container image with the necessary Camel Kafka Connector binaries as part of a build system.","title":"Build the Camel Kafka Connector"},{"location":"use-cases/connect-s3/#deploy-the-camel-kafka-connector-binaries","text":"Now that the Camel Kafka Connector binaries have been built, we need to include them on the classpath inside of the container image which our Kafka Connect clusters are using. From the root directory of the repository, start a new OpenShift Build, using the generated build artifacts: oc start-build connect-cluster-101-connect --from-dir = ./core/target/camel-kafka-connector-0.1.0-package/share/java --follow Watch the Kubernetes pods as they are updated with the new build and rollout of the Kafka Connect Cluster using the updated container image (which now includes the Camel Kafka Connector binaries) : kubectl get pods -w Once the connect-cluster-101-connect-2-[random-suffix] pod is in a Running state, you can proceed.","title":"Deploy the Camel Kafka Connector binaries"},{"location":"use-cases/connect-s3/#kafka-to-s3-sink-connector","text":"Now that you have a Kafka Connect cluster up and running, you will need to configure a connector to actually begin the transmission of data from one system to the other. This will be done by taking advantage of Strimzi and using the KafkaConnector custom resource the Strimzi Operator manages for us. Review the YAML description for our KafkaConnector custom resource below, named s3-sink-connector . Pay close attention to: - The strimzi.io/cluster label must match the deployed Kafka Connect cluster you previously deployed (or else Strimzi will not connect the KafkaConnector to your KafkaConnect cluster) - The topics parameter (named my-source-topic here) - The S3 Bucket parameter of the camel.sink.url configuration option (named my-s3-bucket here) apiVersion : kafka.strimzi.io/v1alpha1 kind : KafkaConnector metadata : name : s3-sink-connector labels : strimzi.io/cluster : connect-cluster-101 spec : class : org.apache.camel.kafkaconnector.CamelSinkConnector tasksMax : 1 config : key.converter : org.apache.kafka.connect.storage.StringConverter value.converter : org.apache.kafka.connect.storage.StringConverter topics : my-source-topic camel.sink.url : aws-s3://my-s3-bucket?keyName=${date:now:yyyyMMdd-HHmmssSSS}-${exchangeId} camel.sink.maxPollDuration : 10000 camel.component.aws-s3.configuration.autocloseBody : false camel.component.aws-s3.accessKey : ${file:/opt/kafka/external-configuration/aws-credentials/aws-credentials.properties:aws_access_key_id} camel.component.aws-s3.secretKey : ${file:/opt/kafka/external-configuration/aws-credentials/aws-credentials.properties:aws_secret_access_key} camel.component.aws-s3.region : US_EAST_1 Once you have updated the YAML and saved it in a file named kafka-sink-connector.yaml , this resource can be created via kubectl apply -f kafka-sink-connector.yaml . You can then tail the output of the connect-cluster-101 pods for updates on the connector status. NOTE: If you require objects in S3 to reside in a sub-folder of the bucket root, you can place a folder name prefix in the keyName query parameter of the camel.sink.url configuration option above. For example, camel.sink.url: aws-s3://my-s3-bucket?keyName=myfoldername/${date:now:yyyyMMdd-HHmmssSSS}-${exchangeId} .","title":"Kafka to S3 Sink Connector"},{"location":"use-cases/connect-s3/#s3-to-kafka-source-connector","text":"Similar to the Kafka to S3 Sink Connector scenario, this scenario will make use of the Strimzi KafkaConnector custom resource to configure the specific connector instance. Review the YAML description for our KafkaConnector custom resource below, named s3-source-connector . Pay close attention to: - The strimzi.io/cluster label must match the deployed Kafka Connect cluster you previously deployed (or else Strimzi will not connect the KafkaConnector to your KafkaConnect cluster) - The topics and camel.source.kafka.topic parameters (named my-target-topic here) - The S3 Bucket parameter of the camel.sink.url configuration option (named my-s3-bucket here) Please note that it is an explicit intention that the topics used in the Kafka to S3 Sink Connector configuration and the S3 to Kafka Source Connector configuration are different. If these configurations were to use the same Kafka topic and the same S3 Bucket , we would create an infinite processing loop of the same information being endlessly recycled through the system. In our example deployments here, we are deploying to different topics but the same S3 Bucket. apiVersion : kafka.strimzi.io/v1alpha1 kind : KafkaConnector metadata : name : s3-source-connector labels : strimzi.io/cluster : connect-cluster-101 spec : class : org.apache.camel.kafkaconnector.CamelSourceConnector tasksMax : 1 config : key.converter : org.apache.kafka.connect.storage.StringConverter value.converter : org.apache.camel.kafkaconnector.awss3.converters.S3ObjectConverter topics : my-target-topic camel.source.kafka.topic : my-target-topic camel.source.url : aws-s3://my-s3-bucket?autocloseBody=false camel.source.maxPollDuration : 10000 camel.component.aws-s3.accessKey : ${file:/opt/kafka/external-configuration/aws-credentials/aws-credentials.properties:aws_access_key_id} camel.component.aws-s3.secretKey : ${file:/opt/kafka/external-configuration/aws-credentials/aws-credentials.properties:aws_secret_access_key} camel.component.aws-s3.region : US_EAST_1 Once you have updated the YAML and saved it in a file named kafka-source-connector.yaml , this resource can be created via kubectl apply -f kafka-source-connector.yaml . You can then tail the output of the connect-cluster-101 pods for updates on the connector status. NOTE: If you require the connector to only read objects from a subdirecotry of the S3 bucket root, you can set the camel.component.aws-s3.configuration.prefix configuration option with the value of the subdirectory name. For example, camel.component.aws-s3.configuration.prefix: myfoldername .","title":"S3 to Kafka Source Connector"},{"location":"use-cases/connect-s3/#event-streams-v10-addendum","text":"The steps up to now assumed usage of an OpenShift Container Platform v4.3.x cluster, IBM Cloud Pak for Integration v2020.1.1 and IBM Event Streams v2019.4.2 With the release of Event Streams v10 (which is built on top of the Strimzi Kafka Operator) and the new CP4I there are some things need that need to be adjusted in the yamls to reflect that. You can edit the previous kafka-connect.yaml or create a new file. kafka-connect-esv10.yaml apiVersion : eventstreams.ibm.com/v1beta1 kind : KafkaConnectS2I metadata : name : connect-cluster-101 annotations : eventstreams.ibm.com/use-connector-resources : \"true\" spec : #logging: # type: external # name: custom-connect-log4j version : 2.5.0 replicas : 1 bootstrapServers : { internal-bootstrap-server-address } template : pod : imagePullSecrets : [] metadata : annotations : eventstreams.production.type : CloudPakForIntegrationNonProduction productID : { product-id } productName : IBM Event Streams for Non Production productVersion : 10.0.0 productMetric : VIRTUAL_PROCESSOR_CORE productChargedContainers : { connect-cluster-101 } -connect cloudpakId : { cloudpak-id } cloudpakName : IBM Cloud Pak for Integration cloudpakVersion : 2020.2.1 productCloudpakRatio : \"2:1\" tls : trustedCertificates : - secretName : { your-es-instance-name } -cluster-ca-cert certificate : ca.crt authentication : type : tls certificateAndKey : certificate : user.crt key : user.key secretName : { generated-secret-from-ui } externalConfiguration : volumes : - name : aws-credentials secret : secretName : aws-credentials config : group.id : connect-cluster-101 config.providers : file config.providers.file.class : org.apache.kafka.common.config.provider.FileConfigProvider key.converter : org.apache.kafka.connect.json.JsonConverter value.converter : org.apache.kafka.connect.json.JsonConverter key.converter.schemas.enable : false value.converter.schemas.enable : false offset.storage.topic : connect-cluster-101-offsets config.storage.topic : connect-cluster-101-configs status.storage.topic : connect-cluster-101-status There are a couple changes of note here. apiVersion: eventstreams.ibm.com/v1beta1 instead of the previous Strimzi one. metadata.annotations.eventstreams.ibm.com/use-connector-resources: \"true\" instead of the previous Strimzi one as well. spec.bootstrapservers: Previously in Event Streams v2019.4.2 and prior there was only a single external facing route for this bootstrap server address. In Event Streams v10 there is an External and Internal one now. Replace {internal-bootstrap-server-address} with the Internal bootstrap server address from the Event Streams v10 UI. spec.template.pod.metadata.annotations.* This entire section is new and represents new metadata that the Event Streams instance needs. spec.template.pod.metadata.annotations.productID You can find this value when trying to deploy an Event Streams Custom Resource from the Installed Event Streams Operator YAML deployment or when applying this YAML through the command-line, OCP will provide you the proper values if they're wrong. spec.template.pod.metadata.annotations.cloudpakId Same as productID. spec.tls.trustedCertificates This is different from the previous one in that this certificate was automatically generated on creation of your Event Streams Kafka Cluster. If your Event Streams instance is named eventstreams-dev then this value should be eventstreams-dev-cluster-ca-cert . spec.authentication.* This section is mostly different from the previous section. Prior we used a generated API Key, but on Event Streams v10 we will need to generate TLS credentials. In the Internal connection details there will be a Generate TLS Credentials button. Here you can name your secret and provide the proper access to it. Note 1 - This is automatically created by the Event Streams Operator into the same namespace as both a KafkaUser Custom Resource as well as a secret. If your secret is named internal-secret then there will be an automatically generated Kubernetes/OpenShift secret named that as well as a KafkaUser . user.crt and user.key are certificates automatically generated within said secret. Note 2 - Deletion of that secret will replicate itself. You will need to delete the associated and created KafkaUser . kafka-sink-connector.yaml-esv10.yaml apiVersion : eventstreams.ibm.com/v1alpha1 kind : KafkaConnector metadata : name : s3-sink-connector labels : eventstreams.ibm.com/cluster : connect-cluster-101 spec : class : org.apache.camel.kafkaconnector.CamelSinkConnector tasksMax : 1 config : key.converter : org.apache.kafka.connect.storage.StringConverter value.converter : org.apache.kafka.connect.storage.StringConverter topics : my-topic camel.sink.url : aws-s3://my-s3-bucket?keyName=${date:now:yyyyMMdd-HHmmssSSS}-${exchangeId} camel.sink.maxPollDuration : 10000 camel.component.aws-s3.configuration.autocloseBody : false camel.component.aws-s3.accessKey : ${file:/opt/kafka/external-configuration/aws-credentials/aws-credentials.properties:aws_access_key_id} camel.component.aws-s3.secretKey : ${file:/opt/kafka/external-configuration/aws-credentials/aws-credentials.properties:aws_secret_access_key} camel.component.aws-s3.region : my-s3-bucket-region - For the most part identical besides two minor changes. - apiVersion: eventstreams.ibm.com/v1alpha1 instead of Strimzi. - metadata.labels.eventstreams.ibm.com/cluster: connect-cluster-101 again instead of Strimzi. kafka-source-connector.yaml-esv10.yaml apiVersion : eventstreams.ibm.com/v1alpha1 kind : KafkaConnector metadata : name : s3-source-connector labels : eventstreams.ibm.com/cluster : connect-cluster-101 spec : class : org.apache.camel.kafkaconnector.CamelSourceConnector tasksMax : 1 config : key.converter : org.apache.kafka.connect.storage.StringConverter value.converter : org.apache.camel.kafkaconnector.awss3.converters.S3ObjectConverter topics : my-topic camel.source.kafka.topic : my-topic camel.source.url : aws-s3://my-s3-bucket?autocloseBody=false camel.source.maxPollDuration : 10000 camel.component.aws-s3.accessKey : ${file:/opt/kafka/external-configuration/aws-credentials/aws-credentials.properties:aws_access_key_id} camel.component.aws-s3.secretKey : ${file:/opt/kafka/external-configuration/aws-credentials/aws-credentials.properties:aws_secret_access_key} camel.component.aws-s3.region : my-s3-bucket-region - Similar to the S3 sink connector yaml. - apiVersion: eventstreams.ibm.com/v1alpha1 instead of Strimzi. - metadata.labels.eventstreams.ibm.com/cluster: connect-cluster-101 again instead of Strimzi.","title":"Event Streams v10 Addendum"},{"location":"use-cases/connect-s3/#troubleshooting","text":"The log output from the Kafka Connector instances will be available in the KafkaConnectS2I pods, named in the pattern of {connect-cluster-name}-connect-{latest-build-id}-{random-suffix} . This pattern will be referred to as {connect-pod} throughout the rest of the Troubleshooting section. To increase the logging output of the deployed connector instances: Follow the steps for \"Create ConfigMap for log4j configuration\" in Kafka Connect Cluster above Update the log4j.properties ConfigMap to include the line log4j.logger.org.apache.camel.kafkaconnector=DEBUG Reapply the YAML configuration of the KafkaConnectS2I cluster To check the deployment status of applied Kafka Connect configuration via Kafka Connect's REST API, run the following commands: oc exec {connect-pod} bash -- -c \"curl --silent http://localhost:8083/connectors/\" oc exec {connect-pod} bash -- -c \"curl --silent http://localhost:8083/connectors/{deployed-connector-name}/status\"","title":"Troubleshooting"},{"location":"use-cases/connect-s3/#next-steps","text":"Enable use of IAM Credentials in the Connector configuration (as the default Java code currently outputs aws_access_key_id and aws_secret_access_key to container runtime logs due to their existence as configuration properties) Optionally configure individual connector instances to startup with offset value of -1 (to enable to run from beginning of available messages) Implement a build system to produce container images with the Camel Kafka Connector binaries already present","title":"Next steps"},{"location":"use-cases/connect-s3/#references","text":"Apache Camel Kafka Connector - Try it out on OpenShift with Strimzi Apache Camel - Available pattern elements for use in the keyName parameter of the camel.sink.url property. Apache Camel - Dynamic Endpoint reference Red Hat Developer Blog - Using secrets in Kafka Connect configuration Apache Kafka - Connect Overview","title":"References"},{"location":"use-cases/db2-debezium/","text":"This lab goes over how to implement a change data capture on order events table created using the outbox pattern with the Debezium open source project. What you will learn is: DB2 settings for change data capture Configuring Debezium DB2 connector to publish OrderEvents to Kafka topic Validate the Kafka topic content. We expect existing knowledge of Kafka, and Kafka connector. Updated 11/13/2020 Ready for validation when running local. Need to be completed for OpenShift Deployment. Quick summary of Debezium \u00b6 Debezium is an open source project, led by Red Hat, to support capturing changes to a database and generate those changes to Kafka. It runs in Kafka Connect so support High availability and horizontal scaling. To get started we recommend going into the tutorial , review the product documentation and for deeper dive you can leverage the Debezium examples . In an data pipeline architecture, Change Data Capture, helps to inject existing data from existing Database to Kafka and the event-driven microservice. It is important to note that the data generated will be close to what is in the data base, it is possible to do some data transformation to generate some 'business event' from the database updates. Or use raw data and add a Kafka Streams processing to do the data transformation. Debezium supports DB2 as data source as introduced by this project . As part of the Debezium tutorial in the Debezium examples , you can find a docker compose to start DB2 and Debezium. For most of development effort, we are using Docker Compose to run a basic infrastructure with Kafka and Kafka Connect. Once DB server and Kafka Connect are started, the approach is to register the DB connector using a json file like below. CDC uses a specific schema to keep source table update. We will detail that in next section. { \"name\" : \"order-connector\" , \"config\" : { \"connector.class\" : \"io.debezium.connector.db2.Db2Connector\" , \"tasks.max\" : \"1\" , \"database.server.name\" : \"vaccine_lot_db\" , \"database.hostname\" : \"db2\" , \"database.port\" : \"50000\" , \"database.user\" : \"db2inst1\" , \"database.password\" : \"db2inst1\" , \"database.dbname\" : \"TESTDB\" , \"database.cdcschema\" : \"ASNCDC\" , \"database.history.kafka.bootstrap.servers\" : \"kafka:9092\" , \"database.history.kafka.topic\" : \"db_history_vaccine_orders\" , \"topic.creation.default.replication.factor\" : 1 , \"topic.creation.default.partitions\" : 1 , \"topic.creation.default.cleanup.policy\" : \"compact\" , \"table.include.list\" : \"DB2INST1.ORDEREVENTS\" , \"tombstones.on.delete\" : \"false\" } } DB2 connector \u00b6 The project documentation presents in detail this connector, but below is a quick summary of the features: Tables to monitor are in capture mode, so they have associated chage data table. The Db2 connector reads change events from change-data tables and emits the events to Kafka topics. The Debezium Db2 connector is based on the ASN Capture/Apply agents. A capture agent: Generates change-data tables for tables that are in capture mode. Monitors tables in capture mode and stores change events for updates to those tables in their corresponding change-data tables. A user defined function is needed to start or stop the ADN agent, put expected tables in capture mode, create the ASN schema abd change data tables. The connector emits a change event for each row-level insert, update, and delete operation to a Kafka topic that has the same name as the changed table. When the Db2 connector first connects to a particular Db2 database, it starts by performing a consistent snapshot of each table that is in capture mode The connector keeps the log sequence number (LSN) of the change data table entry. Database schema is also replicated so it supports schema updates Each event contains the structure of its key and the payload. Or a reference for a schema registry entry. Use Case overview \u00b6 The use case is part of a larger scenario about order vaccines management. Vaccine orders are managed by an order microservice and using the outbox pattern order created and order updated events are produced to a specific table which is captured by the Debezium connector. The implementation of the outbox is done using Quarkus Debezium outbox extension and explained in this separate note . In this lab, you will get the component running on you computer or on OpenShift. Run locally \u00b6 Clone the order management service: git clone https://github.com/ibm-cloud-architecture/vaccine-order-mgr And then start the five processes (one Kafka broker, one ZooKeeper, one DB2 container for the persistence, one Kafka Connect with the Debezium code and DB2 JDBC driver and one vaccine-order-service) with Docker Compose. cd environment # with the option to build the db2, and debezium cdc container images docker-compose -f strimzi-docker-compose.yaml up -d --build # or with pre-existing images coming from dockerhub docker-compose -f strimzi-docker-compose.yaml up -d Create the needed topics # Under environment folder ./createTopic.sh # validate topics created ./listTopics.sh __consumer_offsets db_history_vaccine_orders src_connect_configs src_connect_offsets src_connect_statuses vaccine_shipmentplan The db_history_vaccine_orders is the topic used to include database schema change on the vaccine orders table. Verify starting states \u00b6 To validate the DB2 settings, you can do one of the following troubleshooting commands: # connect to DB2 server docker exec -ti db2 bash # Access the database db2 connect to TESTDB USER DB2INST1 # use db2inst1 as password # list existing schemas db2 \"select * from syscat.schemata\" # list tables db2 list tables # this is the outcomes if the order services was started Table/View Schema Type Creation time ------------------------------- --------------- ----- -------------------------- ORDERCREATEDEVENT DB2INST1 T 2020 -11-12-01.50.10.400490 ORDEREVENTS DB2INST1 T 2020 -11-12-01.50.10.650172 ORDERUPDATEDEVENT DB2INST1 T 2020 -11-12-01.50.10.796566 VACCINEORDERENTITY DB2INST1 T 2020 -11-12-01.50.10.874172 # Verify the content of the current orders db2 \"select * from vaccineorderentity\" # List the table for the change data capture db2 list tables for schema asncdc The DB2 container was built to define ASNCDC schema to support table capture. The setup is described in this note and supported by this script . When reusing this asset, the only thing you need to configure is the startup-cdc.sql to specify the table(s) you want to capture. VALUES ASNCDC.ASNCDCSERVICES ( 'status' , 'asncdc' ) ; CALL ASNCDC.ADDTABLE ( 'DB2INST1' , 'ORDEREVENTS' ) ; VALUES ASNCDC.ASNCDCSERVICES ( 'reinit' , 'asncdc' ) ; and tune the content of the register-db2.json file to configure the Kafka Connector (see next section). The application may have some issue to start as DB2 may take some time to configure, so it is important to verify the containers running (if you know how to add healthcheck on DB2 container... open a PR): docker ps CONTAINER ID IMAGES NAMES 5ddd45b5856e ibmcase/vaccineorderms vaccineorderms 5bce18c820fc ibmcase/cdc-connector cdc-connector 7fd6951972df strimzi/kafka:latest-kafka-2.6.0 kafka b0f9127c874e strimzi/kafka:latest-kafka-2.6.0 zookeeper 7f356633ea2f ibmcase/db2orders db2 # Get come logs using the container name docker logs vaccineorderms If for any reason the vaccineorderms , doing a docker-compose up -d will restart the container. You can use the User interface to get the current order loaded. At the starting time there should be only one record. http://localhost:8080/#/Orders . Define the CDC connector \u00b6 Deploy and start the Debezium DB2 connector. The connector definition is in register-db2.json . The important elements of this file are below: # t he na mespace f or t he server t ha t will be used i n t he t opic crea te d \"database.server.name\" : \"vaccine_lot_db\" , # da ta base crede nt ials # The lis t o f ta ble t o cap ture \"table.include.list\" : \"DB2INST1.ORDEREVENTS\" , # na me f or t he t opic t o keep tra ck o f t he da ta base schema cha n ges. \"database.history.kafka.topic\" : \"db_history_vaccine_orders\" , To deploy to the Kafka Connector instance, perform a POST with the previous configuration as: # under environment/cdc curl -i -X POST -H \"Accept:application/json\" -H \"Content-Type:application/json\" http://localhost:8083/connectors/ -d @cdc/register-db2.json Get the status of the Kafka connector at: http://localhost:8083/connectors/orderdb-connector/ Verify the newly created topics: ./listTopics.sh vaccine_lot_db vaccine_lot_db.DB2INST1.ORDEREVENTS The newly created vaccine_lot_db topic includes definition of the database and the connector. It does not aim to be used by application. The one to be used to get business events is vaccine_lot_db.DB2INST1.ORDEREVENTS . The connector is doing a snapshot of the DB2INST1.ORDEREVENTS table to send existing records to the topic. Start consumer \u00b6 Start a Kafka consumer, using the console consumer tool: docker-compose exec kafka /opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server kafka:9092 --from-beginning --property print.key = true --topic db2server.DB2INST1.ORDERS Create an order \u00b6 You can use the user interface to add a new order, or use the swagger operation at http://localhost:8080/swagger-ui/#/default/post_api_v1_orders Use the following JSON: { \"deliveryDate\" : \"2021-07-25\" , \"deliveryLocation\" : \"Milano\" , \"askingOrganization\" : \"Italy gov\" , \"priority\" : 1 , \"quantity\" : 100 , \"type\" : \"COVID-19\" } The expected result in the topic consumer should have the following records in the Kafka topic: { \"ID\" : \"lvz4gYs/Q+aSqKmWjVGMXg==\" } { \"before\" : null , \"after\" :{ \"ID\" : \"lvz4gYs/Q+aSqKmWjVGMXg==\" , \"AGGREGATETYPE\" : \"VaccineOrderEntity\" , \"AGGREGATEID\" : \"21\" , \"TYPE\" : \"OrderCreated\" , \"TIMESTAMP\" : 1605304440331350 , \"PAYLOAD\" : \"{\\\"orderID\\\":21,\\\"deliveryLocation\\\":\\\"London\\\",\\\"quantity\\\":150,\\\"priority\\\":2,\\\"deliveryDate\\\":\\\"2020-12-25\\\",\\\"askingOrganization\\\":\\\"UK Governement\\\",\\\"vaccineType\\\":\\\"COVID-19\\\",\\\"status\\\":\\\"OPEN\\\",\\\"creationDate\\\":\\\"13-Nov-2020 21:54:00\\\"}\" }, \"source\" :{ \"version\" : \"1.3.0.Final\" , \"connector\" : \"db2\" , \"name\" : \"vaccine_lot_db\" , \"ts_ms\" : 1605304806596 , \"snapshot\" : \"last\" , \"db\" : \"TESTDB\" , \"schema\" : \"DB2INST1\" , \"table\" : \"ORDEREVENTS\" , \"change_lsn\" : null , \"commit_lsn\" : \"00000000:0000150f:0000000000048fca\" }, \"op\" : \"r\" , \"ts_ms\" : 1605304806600 , \"transaction\" : null } References \u00b6 Outbox pattern for quarkus Blog on the outbox pattern Db2 Debezium connector","title":"DB2 - CDC Debezium - Outbox"},{"location":"use-cases/db2-debezium/#quick-summary-of-debezium","text":"Debezium is an open source project, led by Red Hat, to support capturing changes to a database and generate those changes to Kafka. It runs in Kafka Connect so support High availability and horizontal scaling. To get started we recommend going into the tutorial , review the product documentation and for deeper dive you can leverage the Debezium examples . In an data pipeline architecture, Change Data Capture, helps to inject existing data from existing Database to Kafka and the event-driven microservice. It is important to note that the data generated will be close to what is in the data base, it is possible to do some data transformation to generate some 'business event' from the database updates. Or use raw data and add a Kafka Streams processing to do the data transformation. Debezium supports DB2 as data source as introduced by this project . As part of the Debezium tutorial in the Debezium examples , you can find a docker compose to start DB2 and Debezium. For most of development effort, we are using Docker Compose to run a basic infrastructure with Kafka and Kafka Connect. Once DB server and Kafka Connect are started, the approach is to register the DB connector using a json file like below. CDC uses a specific schema to keep source table update. We will detail that in next section. { \"name\" : \"order-connector\" , \"config\" : { \"connector.class\" : \"io.debezium.connector.db2.Db2Connector\" , \"tasks.max\" : \"1\" , \"database.server.name\" : \"vaccine_lot_db\" , \"database.hostname\" : \"db2\" , \"database.port\" : \"50000\" , \"database.user\" : \"db2inst1\" , \"database.password\" : \"db2inst1\" , \"database.dbname\" : \"TESTDB\" , \"database.cdcschema\" : \"ASNCDC\" , \"database.history.kafka.bootstrap.servers\" : \"kafka:9092\" , \"database.history.kafka.topic\" : \"db_history_vaccine_orders\" , \"topic.creation.default.replication.factor\" : 1 , \"topic.creation.default.partitions\" : 1 , \"topic.creation.default.cleanup.policy\" : \"compact\" , \"table.include.list\" : \"DB2INST1.ORDEREVENTS\" , \"tombstones.on.delete\" : \"false\" } }","title":"Quick summary of Debezium"},{"location":"use-cases/db2-debezium/#db2-connector","text":"The project documentation presents in detail this connector, but below is a quick summary of the features: Tables to monitor are in capture mode, so they have associated chage data table. The Db2 connector reads change events from change-data tables and emits the events to Kafka topics. The Debezium Db2 connector is based on the ASN Capture/Apply agents. A capture agent: Generates change-data tables for tables that are in capture mode. Monitors tables in capture mode and stores change events for updates to those tables in their corresponding change-data tables. A user defined function is needed to start or stop the ADN agent, put expected tables in capture mode, create the ASN schema abd change data tables. The connector emits a change event for each row-level insert, update, and delete operation to a Kafka topic that has the same name as the changed table. When the Db2 connector first connects to a particular Db2 database, it starts by performing a consistent snapshot of each table that is in capture mode The connector keeps the log sequence number (LSN) of the change data table entry. Database schema is also replicated so it supports schema updates Each event contains the structure of its key and the payload. Or a reference for a schema registry entry.","title":"DB2 connector"},{"location":"use-cases/db2-debezium/#use-case-overview","text":"The use case is part of a larger scenario about order vaccines management. Vaccine orders are managed by an order microservice and using the outbox pattern order created and order updated events are produced to a specific table which is captured by the Debezium connector. The implementation of the outbox is done using Quarkus Debezium outbox extension and explained in this separate note . In this lab, you will get the component running on you computer or on OpenShift.","title":"Use Case overview"},{"location":"use-cases/db2-debezium/#run-locally","text":"Clone the order management service: git clone https://github.com/ibm-cloud-architecture/vaccine-order-mgr And then start the five processes (one Kafka broker, one ZooKeeper, one DB2 container for the persistence, one Kafka Connect with the Debezium code and DB2 JDBC driver and one vaccine-order-service) with Docker Compose. cd environment # with the option to build the db2, and debezium cdc container images docker-compose -f strimzi-docker-compose.yaml up -d --build # or with pre-existing images coming from dockerhub docker-compose -f strimzi-docker-compose.yaml up -d Create the needed topics # Under environment folder ./createTopic.sh # validate topics created ./listTopics.sh __consumer_offsets db_history_vaccine_orders src_connect_configs src_connect_offsets src_connect_statuses vaccine_shipmentplan The db_history_vaccine_orders is the topic used to include database schema change on the vaccine orders table.","title":"Run locally"},{"location":"use-cases/db2-debezium/#verify-starting-states","text":"To validate the DB2 settings, you can do one of the following troubleshooting commands: # connect to DB2 server docker exec -ti db2 bash # Access the database db2 connect to TESTDB USER DB2INST1 # use db2inst1 as password # list existing schemas db2 \"select * from syscat.schemata\" # list tables db2 list tables # this is the outcomes if the order services was started Table/View Schema Type Creation time ------------------------------- --------------- ----- -------------------------- ORDERCREATEDEVENT DB2INST1 T 2020 -11-12-01.50.10.400490 ORDEREVENTS DB2INST1 T 2020 -11-12-01.50.10.650172 ORDERUPDATEDEVENT DB2INST1 T 2020 -11-12-01.50.10.796566 VACCINEORDERENTITY DB2INST1 T 2020 -11-12-01.50.10.874172 # Verify the content of the current orders db2 \"select * from vaccineorderentity\" # List the table for the change data capture db2 list tables for schema asncdc The DB2 container was built to define ASNCDC schema to support table capture. The setup is described in this note and supported by this script . When reusing this asset, the only thing you need to configure is the startup-cdc.sql to specify the table(s) you want to capture. VALUES ASNCDC.ASNCDCSERVICES ( 'status' , 'asncdc' ) ; CALL ASNCDC.ADDTABLE ( 'DB2INST1' , 'ORDEREVENTS' ) ; VALUES ASNCDC.ASNCDCSERVICES ( 'reinit' , 'asncdc' ) ; and tune the content of the register-db2.json file to configure the Kafka Connector (see next section). The application may have some issue to start as DB2 may take some time to configure, so it is important to verify the containers running (if you know how to add healthcheck on DB2 container... open a PR): docker ps CONTAINER ID IMAGES NAMES 5ddd45b5856e ibmcase/vaccineorderms vaccineorderms 5bce18c820fc ibmcase/cdc-connector cdc-connector 7fd6951972df strimzi/kafka:latest-kafka-2.6.0 kafka b0f9127c874e strimzi/kafka:latest-kafka-2.6.0 zookeeper 7f356633ea2f ibmcase/db2orders db2 # Get come logs using the container name docker logs vaccineorderms If for any reason the vaccineorderms , doing a docker-compose up -d will restart the container. You can use the User interface to get the current order loaded. At the starting time there should be only one record. http://localhost:8080/#/Orders .","title":"Verify starting states"},{"location":"use-cases/db2-debezium/#define-the-cdc-connector","text":"Deploy and start the Debezium DB2 connector. The connector definition is in register-db2.json . The important elements of this file are below: # t he na mespace f or t he server t ha t will be used i n t he t opic crea te d \"database.server.name\" : \"vaccine_lot_db\" , # da ta base crede nt ials # The lis t o f ta ble t o cap ture \"table.include.list\" : \"DB2INST1.ORDEREVENTS\" , # na me f or t he t opic t o keep tra ck o f t he da ta base schema cha n ges. \"database.history.kafka.topic\" : \"db_history_vaccine_orders\" , To deploy to the Kafka Connector instance, perform a POST with the previous configuration as: # under environment/cdc curl -i -X POST -H \"Accept:application/json\" -H \"Content-Type:application/json\" http://localhost:8083/connectors/ -d @cdc/register-db2.json Get the status of the Kafka connector at: http://localhost:8083/connectors/orderdb-connector/ Verify the newly created topics: ./listTopics.sh vaccine_lot_db vaccine_lot_db.DB2INST1.ORDEREVENTS The newly created vaccine_lot_db topic includes definition of the database and the connector. It does not aim to be used by application. The one to be used to get business events is vaccine_lot_db.DB2INST1.ORDEREVENTS . The connector is doing a snapshot of the DB2INST1.ORDEREVENTS table to send existing records to the topic.","title":"Define the CDC connector"},{"location":"use-cases/db2-debezium/#start-consumer","text":"Start a Kafka consumer, using the console consumer tool: docker-compose exec kafka /opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server kafka:9092 --from-beginning --property print.key = true --topic db2server.DB2INST1.ORDERS","title":"Start consumer"},{"location":"use-cases/db2-debezium/#create-an-order","text":"You can use the user interface to add a new order, or use the swagger operation at http://localhost:8080/swagger-ui/#/default/post_api_v1_orders Use the following JSON: { \"deliveryDate\" : \"2021-07-25\" , \"deliveryLocation\" : \"Milano\" , \"askingOrganization\" : \"Italy gov\" , \"priority\" : 1 , \"quantity\" : 100 , \"type\" : \"COVID-19\" } The expected result in the topic consumer should have the following records in the Kafka topic: { \"ID\" : \"lvz4gYs/Q+aSqKmWjVGMXg==\" } { \"before\" : null , \"after\" :{ \"ID\" : \"lvz4gYs/Q+aSqKmWjVGMXg==\" , \"AGGREGATETYPE\" : \"VaccineOrderEntity\" , \"AGGREGATEID\" : \"21\" , \"TYPE\" : \"OrderCreated\" , \"TIMESTAMP\" : 1605304440331350 , \"PAYLOAD\" : \"{\\\"orderID\\\":21,\\\"deliveryLocation\\\":\\\"London\\\",\\\"quantity\\\":150,\\\"priority\\\":2,\\\"deliveryDate\\\":\\\"2020-12-25\\\",\\\"askingOrganization\\\":\\\"UK Governement\\\",\\\"vaccineType\\\":\\\"COVID-19\\\",\\\"status\\\":\\\"OPEN\\\",\\\"creationDate\\\":\\\"13-Nov-2020 21:54:00\\\"}\" }, \"source\" :{ \"version\" : \"1.3.0.Final\" , \"connector\" : \"db2\" , \"name\" : \"vaccine_lot_db\" , \"ts_ms\" : 1605304806596 , \"snapshot\" : \"last\" , \"db\" : \"TESTDB\" , \"schema\" : \"DB2INST1\" , \"table\" : \"ORDEREVENTS\" , \"change_lsn\" : null , \"commit_lsn\" : \"00000000:0000150f:0000000000048fca\" }, \"op\" : \"r\" , \"ts_ms\" : 1605304806600 , \"transaction\" : null }","title":"Create an order"},{"location":"use-cases/db2-debezium/#references","text":"Outbox pattern for quarkus Blog on the outbox pattern Db2 Debezium connector","title":"References"},{"location":"use-cases/gitops/","text":"Info Updated 2/15/2022 Audience : Architects, Application developers, Site reliability engineers, Administrators Overview \u00b6 The purpose of the tutorial is to teach architects, developers and operations staff how to deploy a production-ready event-driven solution OpenShift Container Platform . It makes extensive use of the IBM Cloud Pak for Integration (CP4I) and other cloud native technologies such as Tekton, Kustomize, ArgoCD, Prometheus, Grafana and Kibana. GitOps is a declarative way to implement continuous deployment for cloud-native applications. The Red Hat\u00ae OpenShift\u00ae Container Platform offers the OpenShift GitOps operator , which manages the entire lifecycle for Argo CD and its components. Argo applications are added to the Argo CD server. An application defines the source of the Kubernetes resources and the target cluster where those resources should be deployed. The Argo CD server \"installs\" a Cloud Pak by synchronizing the applications representing the Cloud Pak into the target cluster. System context \u00b6 A system context diagram helps us understand how our system interacts with its different users and other systems. For a generic event-driven solution the diagram looks like We can see the different entities that interact with a typical event-driven solution deployment. These include users as well as applications and messaging as a service infrastructure which includes event backbone, queueing systems, schema registry, API management, governance platform and monitoring components. We'll be developing the Event-driven solution deployment at the centre of the diagram. We can see that it connects applications to systems and infrastructure. Its users are at least developers, SREs, Kubernetes administrators, architects... Components for GitOps \u00b6 The following diagram shows the technical components used in a typical event-driven solution production deployment. ( the source code of this diagram is a `.drawio format and is in the diagrams folder . ) The diagram organizes the components according to when they are introduced in system development (earlier or later) and whether they are a relatively high level application-oriented component, or a relatively low level system- oriented component. For example, GitHub is a system component that is fundamental to how we structure the event-driven solution deployment. In contrast, streaming or event-driven applications are higher level components, and requires other components to be deployed prior to them. The color coding illustrates that blue components are part of the solution, red are part of the GitOps on OpenShift and green components are externals to OpenShit cluster, most likely event if they could be. Kustomize represents way to define deployment, and Sealed secret is a service to manage secrets. As part of the later components to deploy, we have addressed everything to monitor the solution and the infrastructure. Here is a brief introduction of those components: Event-driven applications \u00b6 Those applications are supporting business logic, microservice based, and using Reactive messaging, MQ or Kafka APIs. Those applications provide OpenAPIs to the mobile or web applications but also AsyncAPI when they produce events to Kafka or messages to MQ. OpenAPI and AsyncAPI definitions are managed by API manager and event end-point manager. Schema definitions are managed by a Schema Registry . Event-streaming applications \u00b6 Those applications are also supporting business logic, but more with stateful processing using Kafka Stream APIs or different product such as Apache Flink . Queue manager \u00b6 A queue manager provides queueing services via one of the many MQ APIs. A queue manager hosts the queues that store the messages produced and consumed by connected applications and systems. Queue managers can be connected together via network channels to allow messages to flow between disparate systems and applications on different platforms including on-premise and cloud systems. OpenShift GitOps or ArgoCD \u00b6 OpenShift GitOps (ArgoCD) is used for the continuous deployment of software components to the Kubernetes cluster. OpenShift GitOps watches a Git repository for new or changed Kubernetes resource definitions, and applies them to a cluster. In this way, OpenShift GitOps ensures that the component configuration stored in GitHub always reflects the state of the cluster. OpenShift GitOps also has the added benefit of being able to monitor resources that it has deployed to ensure that if they drift from their desired values, they will be automatically restored to those values by OpenShift GitOps. OpenShift Pipelines or Tekton \u00b6 OpenShift Pipelines (Tekton) is used to automate manual tasks using the concept of a pipeline. A pipeline comprises a set of tasks that are executed in a specified order in order to accomplish a specific objective. We use pipelines as part of the continuous integration process to build, test and deliver event-driven applications ready for deployment by OpenShift GitOps. Queue manager \u00b6 A queue manager provides queueing services via one of the many MQ APIs. A queue manager hosts the queues that store the messages produced and consumed by connected applications and systems. Queue managers can be connected together via network channels to allow messages to flow between disparate systems and applications on different platforms including on-premise and cloud systems. Sealed secrets \u00b6 Very often a component has a Kubernetes secret associated with it. Inside the secret might be a private key to access the IBM entitled container registry, for example. For obvious reasons, we don't want to store the secrets in GitHub with the rest of the configuration. A sealed secret solves this problem by introducing a new kind of Kubernetes resource. A sealed secret is created from a regular secret, and can be safely stored in a Git repository. A deployment time, the sealed secret controller will recreate the secret in its original form so that it can be access by components with the appropriate RBAC authority. Image Registry \u00b6 OpenShift contains a registry for storing container images. Images are built and stored by OpenShift Pipelines as part of the CICD process. Tekton pipelines and ArgoCD also retrieve the latest best images from the image registry to ensure that what's being tested or deployed in higher environments is the same as what's tested in development environments. We often refer to uploading images as pushing and downloading images as pulling . Cert manager \u00b6 Managing certificates is a difficult process; certificate creation requires a Certificate Authority (CA), certificates expire after a period of time, and private keys can sometimes be compromised -- requiring a certificate to be revoked and a new one issued. Cert manager makes all these processes relatively straightforward by introducing new Kubernetes resources for certificate issuers and certificates. These resource types radically simplify the management of certificates: their creation, expiry and revocation. Moreover, Cert manager makes it feasible to adopt mutual TLS (mTLS) as an authorization strategy in Kafka based solution. Prometheus \u00b6 Prometheus is used in conjunction with Grafana. It stores the different component's metrics as a set of tuples in a time-series , which allows it to be subsequently used to create Grafana views to assist with monitoring Kafka brokers, MQ queue managers, schema registry.... Kustomize \u00b6 Kubernetes resources have their operational properties defined using YAMLs. As these resources move through environments such as dev, stage and prod, Kustomize provides a natural way to adapt ( customize !) these YAMLs to these environments. For example, we might want to change the CPU or memory available to a service in a production environment compared to a development environment. Because Kustomize is built into the kubectl and oc commands via the -k option, it makes configuration management both easy and natural. GitHub \u00b6 This popular version control system is based on git and stores the event-driven application source code and configuration as well as the other Kubernetes resources. By keeping our event-driven applications and configurations in Git, and using that to build, test and deploy our applications to the Kubernetes cluster, we have a single source of truth -- what's in Git is running in the cluster. Moreover, by using Git operations such as pull , push and merge to make changes, we can exploit the extensive governance and change control provided by Git when managing our event-driven solution estate. OpenShift (Kubernetes) Cluster \u00b6 This is the \"operating system\" used to orchestrate our applications and related component containers. Kubernetes is portable across on-premise and cloud systems and allows us to easily scale our workloads across these environments as required. High-level architecture view \u00b6 Using a GitOps approach we can design a high-level architecture view for the deployment of all the previously listed components. It is important to recall that most of the RedHat and IBM products used in event-driven solution are using Operators and Custom resources manifests to deploy operands. Operator is a long running process that performs products (Operands) deployment and Day 2 operations, like upgrades, failover, or scaling. Operator is constantly watching your cluster\u2019s desired state for the software installed. Helm and Operators represent two different phases in managing complex application workloads on Kubernetes. Helm\u2019s primary focus is on the day-1 operation of deploying Kubernetes artifacts in a cluster. The \u2018domain\u2019 that it understands is that of Kubernetes YAMLs that are composed of available Kubernetes Resources / APIs. Operators, on the other hand, are primarily focused on addressing day-2 management tasks of stateful / complex workloads such as Postgres, Cassandra, Spark, Kafka, SSL Cert Mgmt etc. on Kubernetes. Operator Lifecycle Manager (OLM) helps you to deploy, and update, and generally manage the lifecycle of all of the Operators (and their associated services) running across your clusters The Operators deployment is part of a bootstraping step of the GitOps process. We are using a special Git repository to manage a catalog of operator definitions/ subscriptions. This is the goal of the eda-gitops-catalog repository. A solution will have a specific gitops repository that manages services (operands) and application specifics deployment manifests. With this base, the following figure illustrates a potential architecture: ( the source code of this diagram is a `.drawio format and is in the diagrams folder . ) Here are the assumptions we define for any event-driven solution: Single admin team for OCP cluster and production projects within the cluster. Developers manages staging and dev environment. This is a functional team developing the solution For the solution one gitops will define all environments and apps/services of the solution. Developers will not have access to OpenShift cluster administration Cloud Pak for integration operators are installed in all namespaces, and there is only one instance of each operator. Only one Platform Navigator installed per cluster (in all namespaces) and it displays instances of capabilities from the whole cluster. ibm-common-services is unique to the cluster. For real production deployment, the production OpenShift cluster will be separated from dev and staging, running in different infrastructure, but using the same github source. The top-right cluster is for dev and staging, and each of those environmentd will be in different namespace. To enforce isolation and clear separation of concern, each of those dev or staging namespace, may have Event Streams, MQ brokers, schema registry deployed. The openshift-operators is used to deploy Operators that manage multiple namespaces. The openshift-gitops is for the ArgoCD server and the ArgoCD apps. GitOps Model \u00b6 Gitops is a way of implementing Continuous Deployment for containerized applications. The core idea of GitOps is having a Git repository that always contains declarative descriptions of the infrastructure currently desired in the production environment and an automated process to make the production environment matches the described state in the repository. From the diagram above, we can see two main components that are essentials to a production-ready event-driven solution deployment: A Kubernetes cluster containing: event-driven applications per namespace / project to facilitate isolation Event Streams, API mangement, MQ operators per namespace / project Kafka brokers, Kafka connect, mirror maker, schema registry, End point event gateway, API manager, MQ queue managers per namespace / project OpenShit GitOps, Pipelines,... GitHub as a source of truth for the cluster runtime containing: application source with schema definitions application configuration Kafka Cluster configuration Topic configuration OpenAPI and AsyncAPI documents Kafka Connnector configuration Mirror maker configuration Queue manager configuration GitHub repositories \u00b6 We propose to use one GitOps Catalog to centralize the Operator subscription definitions with Kustomize overlays to control operator versioning. An example of such catalog is the eda-gitops-catalog . Each operator is defined with the subscription manifest and then overlays to change the product version. Here is an example for Event Streams: \u251c\u2500\u2500 event-streams \u2502 \u2502 \u251c\u2500\u2500 README.md \u2502 \u2502 \u2514\u2500\u2500 operator \u2502 \u2502 \u251c\u2500\u2500 base \u2502 \u2502 \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2502 \u2502 \u2514\u2500\u2500 subscription.yaml \u2502 \u2502 \u2514\u2500\u2500 overlays \u2502 \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2502 \u251c\u2500\u2500 v2.3 \u2502 \u2502 \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2502 \u2502 \u2514\u2500\u2500 patch-channel.yaml \u2502 \u2502 \u251c\u2500\u2500 v2.4 \u2502 \u2502 \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2502 \u2502 \u2514\u2500\u2500 patch-channel.yaml \u2502 \u2502 \u2514\u2500\u2500 v2.5 \u2502 \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2502 \u2514\u2500\u2500 patch-channel.yaml The subscription.yam is classical operator definition: apiVersion : operators.coreos.com/v1alpha1 kind : Subscription metadata : name : ibm-eventstreams namespace : openshift-operators spec : channel : stable name : ibm-eventstreams installPlanApproval : Automatic source : ibm-operator-catalog sourceNamespace : openshift-marketplace And then each overlays use this manifest as a base resource and apply patch for channel and version: # kustomization.yaml under an overlays bases : - ../../base patchesJson6902 : - path : patch-channel.yaml target : kind : Subscription name : ibm-eventstreams # patch-channel.yaml - op : replace path : /metadata/namespace value : cp4i - op : replace path : /spec/channel value : v2.5 - op : replace path : /spec/startingCSV value : ibm-eventstreams.v2.5.1 The second major GitOps will be for the solution itself. We use KAM CLI to bootstrap its creation. KAM's goal is to help creating a GitOps project for an existing application as day 1 operations and then add more services as part of day 2 operation . Examples of solution GitOps \u00b6 The following solution GitOps repositories are illustrating the proposed approach: refarch-kc-gitops : For the shipping fresh food overseas solution we have defined. It includes the SAGA choreography pattern implemented with Kafka eda-kc-gitops : For the shipping fresh food overseas solution we have defined. It includes the SAGA orchestration pattern implemented with MQ eda-rt-inventory-gitops to deploy the demo of real-time inventory As a lab example, you may want to clone the real-time inventory demo and bootstrap the GitOps apps to deploy services and apps: git clone https://github.com/ibm-cloud-architecture/eda-rt-inventory-gitops See the main readme for up to date instructions. All the images are in the public image registry: quay.io/ibmcase","title":"Event-driven solution GitOps"},{"location":"use-cases/gitops/#overview","text":"The purpose of the tutorial is to teach architects, developers and operations staff how to deploy a production-ready event-driven solution OpenShift Container Platform . It makes extensive use of the IBM Cloud Pak for Integration (CP4I) and other cloud native technologies such as Tekton, Kustomize, ArgoCD, Prometheus, Grafana and Kibana. GitOps is a declarative way to implement continuous deployment for cloud-native applications. The Red Hat\u00ae OpenShift\u00ae Container Platform offers the OpenShift GitOps operator , which manages the entire lifecycle for Argo CD and its components. Argo applications are added to the Argo CD server. An application defines the source of the Kubernetes resources and the target cluster where those resources should be deployed. The Argo CD server \"installs\" a Cloud Pak by synchronizing the applications representing the Cloud Pak into the target cluster.","title":"Overview"},{"location":"use-cases/gitops/#system-context","text":"A system context diagram helps us understand how our system interacts with its different users and other systems. For a generic event-driven solution the diagram looks like We can see the different entities that interact with a typical event-driven solution deployment. These include users as well as applications and messaging as a service infrastructure which includes event backbone, queueing systems, schema registry, API management, governance platform and monitoring components. We'll be developing the Event-driven solution deployment at the centre of the diagram. We can see that it connects applications to systems and infrastructure. Its users are at least developers, SREs, Kubernetes administrators, architects...","title":"System context"},{"location":"use-cases/gitops/#components-for-gitops","text":"The following diagram shows the technical components used in a typical event-driven solution production deployment. ( the source code of this diagram is a `.drawio format and is in the diagrams folder . ) The diagram organizes the components according to when they are introduced in system development (earlier or later) and whether they are a relatively high level application-oriented component, or a relatively low level system- oriented component. For example, GitHub is a system component that is fundamental to how we structure the event-driven solution deployment. In contrast, streaming or event-driven applications are higher level components, and requires other components to be deployed prior to them. The color coding illustrates that blue components are part of the solution, red are part of the GitOps on OpenShift and green components are externals to OpenShit cluster, most likely event if they could be. Kustomize represents way to define deployment, and Sealed secret is a service to manage secrets. As part of the later components to deploy, we have addressed everything to monitor the solution and the infrastructure. Here is a brief introduction of those components:","title":"Components for GitOps"},{"location":"use-cases/gitops/#event-driven-applications","text":"Those applications are supporting business logic, microservice based, and using Reactive messaging, MQ or Kafka APIs. Those applications provide OpenAPIs to the mobile or web applications but also AsyncAPI when they produce events to Kafka or messages to MQ. OpenAPI and AsyncAPI definitions are managed by API manager and event end-point manager. Schema definitions are managed by a Schema Registry .","title":"Event-driven applications"},{"location":"use-cases/gitops/#event-streaming-applications","text":"Those applications are also supporting business logic, but more with stateful processing using Kafka Stream APIs or different product such as Apache Flink .","title":"Event-streaming applications"},{"location":"use-cases/gitops/#queue-manager","text":"A queue manager provides queueing services via one of the many MQ APIs. A queue manager hosts the queues that store the messages produced and consumed by connected applications and systems. Queue managers can be connected together via network channels to allow messages to flow between disparate systems and applications on different platforms including on-premise and cloud systems.","title":"Queue manager"},{"location":"use-cases/gitops/#openshift-gitops-or-argocd","text":"OpenShift GitOps (ArgoCD) is used for the continuous deployment of software components to the Kubernetes cluster. OpenShift GitOps watches a Git repository for new or changed Kubernetes resource definitions, and applies them to a cluster. In this way, OpenShift GitOps ensures that the component configuration stored in GitHub always reflects the state of the cluster. OpenShift GitOps also has the added benefit of being able to monitor resources that it has deployed to ensure that if they drift from their desired values, they will be automatically restored to those values by OpenShift GitOps.","title":"OpenShift GitOps or ArgoCD"},{"location":"use-cases/gitops/#openshift-pipelines-or-tekton","text":"OpenShift Pipelines (Tekton) is used to automate manual tasks using the concept of a pipeline. A pipeline comprises a set of tasks that are executed in a specified order in order to accomplish a specific objective. We use pipelines as part of the continuous integration process to build, test and deliver event-driven applications ready for deployment by OpenShift GitOps.","title":"OpenShift Pipelines or Tekton"},{"location":"use-cases/gitops/#queue-manager_1","text":"A queue manager provides queueing services via one of the many MQ APIs. A queue manager hosts the queues that store the messages produced and consumed by connected applications and systems. Queue managers can be connected together via network channels to allow messages to flow between disparate systems and applications on different platforms including on-premise and cloud systems.","title":"Queue manager"},{"location":"use-cases/gitops/#sealed-secrets","text":"Very often a component has a Kubernetes secret associated with it. Inside the secret might be a private key to access the IBM entitled container registry, for example. For obvious reasons, we don't want to store the secrets in GitHub with the rest of the configuration. A sealed secret solves this problem by introducing a new kind of Kubernetes resource. A sealed secret is created from a regular secret, and can be safely stored in a Git repository. A deployment time, the sealed secret controller will recreate the secret in its original form so that it can be access by components with the appropriate RBAC authority.","title":"Sealed secrets"},{"location":"use-cases/gitops/#image-registry","text":"OpenShift contains a registry for storing container images. Images are built and stored by OpenShift Pipelines as part of the CICD process. Tekton pipelines and ArgoCD also retrieve the latest best images from the image registry to ensure that what's being tested or deployed in higher environments is the same as what's tested in development environments. We often refer to uploading images as pushing and downloading images as pulling .","title":"Image Registry"},{"location":"use-cases/gitops/#cert-manager","text":"Managing certificates is a difficult process; certificate creation requires a Certificate Authority (CA), certificates expire after a period of time, and private keys can sometimes be compromised -- requiring a certificate to be revoked and a new one issued. Cert manager makes all these processes relatively straightforward by introducing new Kubernetes resources for certificate issuers and certificates. These resource types radically simplify the management of certificates: their creation, expiry and revocation. Moreover, Cert manager makes it feasible to adopt mutual TLS (mTLS) as an authorization strategy in Kafka based solution.","title":"Cert manager"},{"location":"use-cases/gitops/#prometheus","text":"Prometheus is used in conjunction with Grafana. It stores the different component's metrics as a set of tuples in a time-series , which allows it to be subsequently used to create Grafana views to assist with monitoring Kafka brokers, MQ queue managers, schema registry....","title":"Prometheus"},{"location":"use-cases/gitops/#kustomize","text":"Kubernetes resources have their operational properties defined using YAMLs. As these resources move through environments such as dev, stage and prod, Kustomize provides a natural way to adapt ( customize !) these YAMLs to these environments. For example, we might want to change the CPU or memory available to a service in a production environment compared to a development environment. Because Kustomize is built into the kubectl and oc commands via the -k option, it makes configuration management both easy and natural.","title":"Kustomize"},{"location":"use-cases/gitops/#github","text":"This popular version control system is based on git and stores the event-driven application source code and configuration as well as the other Kubernetes resources. By keeping our event-driven applications and configurations in Git, and using that to build, test and deploy our applications to the Kubernetes cluster, we have a single source of truth -- what's in Git is running in the cluster. Moreover, by using Git operations such as pull , push and merge to make changes, we can exploit the extensive governance and change control provided by Git when managing our event-driven solution estate.","title":"GitHub"},{"location":"use-cases/gitops/#openshift-kubernetes-cluster","text":"This is the \"operating system\" used to orchestrate our applications and related component containers. Kubernetes is portable across on-premise and cloud systems and allows us to easily scale our workloads across these environments as required.","title":"OpenShift (Kubernetes) Cluster"},{"location":"use-cases/gitops/#high-level-architecture-view","text":"Using a GitOps approach we can design a high-level architecture view for the deployment of all the previously listed components. It is important to recall that most of the RedHat and IBM products used in event-driven solution are using Operators and Custom resources manifests to deploy operands. Operator is a long running process that performs products (Operands) deployment and Day 2 operations, like upgrades, failover, or scaling. Operator is constantly watching your cluster\u2019s desired state for the software installed. Helm and Operators represent two different phases in managing complex application workloads on Kubernetes. Helm\u2019s primary focus is on the day-1 operation of deploying Kubernetes artifacts in a cluster. The \u2018domain\u2019 that it understands is that of Kubernetes YAMLs that are composed of available Kubernetes Resources / APIs. Operators, on the other hand, are primarily focused on addressing day-2 management tasks of stateful / complex workloads such as Postgres, Cassandra, Spark, Kafka, SSL Cert Mgmt etc. on Kubernetes. Operator Lifecycle Manager (OLM) helps you to deploy, and update, and generally manage the lifecycle of all of the Operators (and their associated services) running across your clusters The Operators deployment is part of a bootstraping step of the GitOps process. We are using a special Git repository to manage a catalog of operator definitions/ subscriptions. This is the goal of the eda-gitops-catalog repository. A solution will have a specific gitops repository that manages services (operands) and application specifics deployment manifests. With this base, the following figure illustrates a potential architecture: ( the source code of this diagram is a `.drawio format and is in the diagrams folder . ) Here are the assumptions we define for any event-driven solution: Single admin team for OCP cluster and production projects within the cluster. Developers manages staging and dev environment. This is a functional team developing the solution For the solution one gitops will define all environments and apps/services of the solution. Developers will not have access to OpenShift cluster administration Cloud Pak for integration operators are installed in all namespaces, and there is only one instance of each operator. Only one Platform Navigator installed per cluster (in all namespaces) and it displays instances of capabilities from the whole cluster. ibm-common-services is unique to the cluster. For real production deployment, the production OpenShift cluster will be separated from dev and staging, running in different infrastructure, but using the same github source. The top-right cluster is for dev and staging, and each of those environmentd will be in different namespace. To enforce isolation and clear separation of concern, each of those dev or staging namespace, may have Event Streams, MQ brokers, schema registry deployed. The openshift-operators is used to deploy Operators that manage multiple namespaces. The openshift-gitops is for the ArgoCD server and the ArgoCD apps.","title":"High-level architecture view"},{"location":"use-cases/gitops/#gitops-model","text":"Gitops is a way of implementing Continuous Deployment for containerized applications. The core idea of GitOps is having a Git repository that always contains declarative descriptions of the infrastructure currently desired in the production environment and an automated process to make the production environment matches the described state in the repository. From the diagram above, we can see two main components that are essentials to a production-ready event-driven solution deployment: A Kubernetes cluster containing: event-driven applications per namespace / project to facilitate isolation Event Streams, API mangement, MQ operators per namespace / project Kafka brokers, Kafka connect, mirror maker, schema registry, End point event gateway, API manager, MQ queue managers per namespace / project OpenShit GitOps, Pipelines,... GitHub as a source of truth for the cluster runtime containing: application source with schema definitions application configuration Kafka Cluster configuration Topic configuration OpenAPI and AsyncAPI documents Kafka Connnector configuration Mirror maker configuration Queue manager configuration","title":"GitOps Model"},{"location":"use-cases/gitops/#github-repositories","text":"We propose to use one GitOps Catalog to centralize the Operator subscription definitions with Kustomize overlays to control operator versioning. An example of such catalog is the eda-gitops-catalog . Each operator is defined with the subscription manifest and then overlays to change the product version. Here is an example for Event Streams: \u251c\u2500\u2500 event-streams \u2502 \u2502 \u251c\u2500\u2500 README.md \u2502 \u2502 \u2514\u2500\u2500 operator \u2502 \u2502 \u251c\u2500\u2500 base \u2502 \u2502 \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2502 \u2502 \u2514\u2500\u2500 subscription.yaml \u2502 \u2502 \u2514\u2500\u2500 overlays \u2502 \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2502 \u251c\u2500\u2500 v2.3 \u2502 \u2502 \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2502 \u2502 \u2514\u2500\u2500 patch-channel.yaml \u2502 \u2502 \u251c\u2500\u2500 v2.4 \u2502 \u2502 \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2502 \u2502 \u2514\u2500\u2500 patch-channel.yaml \u2502 \u2502 \u2514\u2500\u2500 v2.5 \u2502 \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2502 \u2514\u2500\u2500 patch-channel.yaml The subscription.yam is classical operator definition: apiVersion : operators.coreos.com/v1alpha1 kind : Subscription metadata : name : ibm-eventstreams namespace : openshift-operators spec : channel : stable name : ibm-eventstreams installPlanApproval : Automatic source : ibm-operator-catalog sourceNamespace : openshift-marketplace And then each overlays use this manifest as a base resource and apply patch for channel and version: # kustomization.yaml under an overlays bases : - ../../base patchesJson6902 : - path : patch-channel.yaml target : kind : Subscription name : ibm-eventstreams # patch-channel.yaml - op : replace path : /metadata/namespace value : cp4i - op : replace path : /spec/channel value : v2.5 - op : replace path : /spec/startingCSV value : ibm-eventstreams.v2.5.1 The second major GitOps will be for the solution itself. We use KAM CLI to bootstrap its creation. KAM's goal is to help creating a GitOps project for an existing application as day 1 operations and then add more services as part of day 2 operation .","title":"GitHub repositories"},{"location":"use-cases/gitops/#examples-of-solution-gitops","text":"The following solution GitOps repositories are illustrating the proposed approach: refarch-kc-gitops : For the shipping fresh food overseas solution we have defined. It includes the SAGA choreography pattern implemented with Kafka eda-kc-gitops : For the shipping fresh food overseas solution we have defined. It includes the SAGA orchestration pattern implemented with MQ eda-rt-inventory-gitops to deploy the demo of real-time inventory As a lab example, you may want to clone the real-time inventory demo and bootstrap the GitOps apps to deploy services and apps: git clone https://github.com/ibm-cloud-architecture/eda-rt-inventory-gitops See the main readme for up to date instructions. All the images are in the public image registry: quay.io/ibmcase","title":"Examples of solution GitOps"},{"location":"use-cases/kafka-mm2/","text":"Info The last up to date mirror maker 2 enablement is in the tech academy content . Pre-requisites \u00b6 The following steps need to be done to get the configurations for the different scenario and the docker-compose file to start a local cluster. Clone the lab repository git clone https://github.com/ibm-cloud-architecture/refarch-eda-tools 1. Get Docker desktop and docker compose on your local computer. Kafka Mirror Maker 2 Labs \u00b6 These hands-on labs walk you through building and testing Kafka Mirror Maker 2 replication. Scenario Description Link Lab 1 Replicate from Event Streams on Cloud to local Kafka. Kafka Mirror Maker 2 - Lab 1 Lab 2 Replicate from Event Streams CP4I to local Kafka. Kafka Mirror Maker 2 - Lab 2 Lab 3 Using Mirror Maker 2 Active Passive mirroring Kafka Mirror Maker 2 - Lab 3","title":"Mirror maker 2 labs"},{"location":"use-cases/kafka-mm2/#pre-requisites","text":"The following steps need to be done to get the configurations for the different scenario and the docker-compose file to start a local cluster. Clone the lab repository git clone https://github.com/ibm-cloud-architecture/refarch-eda-tools 1. Get Docker desktop and docker compose on your local computer.","title":"Pre-requisites"},{"location":"use-cases/kafka-mm2/#kafka-mirror-maker-2-labs","text":"These hands-on labs walk you through building and testing Kafka Mirror Maker 2 replication. Scenario Description Link Lab 1 Replicate from Event Streams on Cloud to local Kafka. Kafka Mirror Maker 2 - Lab 1 Lab 2 Replicate from Event Streams CP4I to local Kafka. Kafka Mirror Maker 2 - Lab 2 Lab 3 Using Mirror Maker 2 Active Passive mirroring Kafka Mirror Maker 2 - Lab 3","title":"Kafka Mirror Maker 2 Labs"},{"location":"use-cases/kafka-mm2/lab-1/","text":"Updated 01/08/2021 Overview \u00b6 For this scenario the source cluster is an Event Streams on IBM Cloud as managed service, and the target is a local Kafka cluster running with docker compose: it simulates an on-premise deployment. The figure below presents the components used: Mirror maker 2 runs in standalone mode on local server A python producer write to products topic defined on Evenstreams on IBM Cloud A consumer using Kafka console consumer tool to validate the replicated records from the 'products' topic As a pre-requisite you need to run your local cluster by using the docker compose as introduced in this note . Start the local Kafka cluster \u00b6 In the refarch-eda-tools/labs/mirror-maker2/es-ic-to-local folder there is a docker compose file to start a local three brokers cluster with one Zookeeper node. In one Terminal window, start the local cluster using the command: docker-compose up -d The data are persisted on the local disk within the folder named kafka-data . Your local environment is up and running. Start Mirror Maker 2 \u00b6 Rename the .env-tmpl file to .env From Event Streams on Cloud > Service Credentials, get the brokers address and the APIKEY. If needed read this note . If not done already, create a products topic (with one partition) in the EventStreams on Cloud cluster using the management console. See this note if needed, to see how to do it. Modify this .env file to set environment variables for Source Event Streams cluster brokers address and APIKEY. ES_IC_BROKERS = broker-0-q.....cloud.ibm.com:9093 ES_IC_USER = token ES_IC_PASSWORD = \"<replace with apikey from event streams service credentials>\" ES_IC_SASL_MECHANISM = PLAIN ES_IC_LOGIN_MODULE = org.apache.kafka.common.security.plain.PlainLoginModule 1. To configure Mirror Maker 2 in standalone mode, we need to define a mm2.properties file. We have define a template file which will be used by the script that launch Mirror Maker 2. The template looks like the following declaration: ```properties clusters=es-ic, target es-ic.bootstrap.servers=KAFKA_SOURCE_BROKERS target.bootstrap.servers=KAFKA_TARGET_BROKERS es-ic.security.protocol=SASL_SSL es-ic.ssl.protocol=TLSv1.2 es-ic.ssl.endpoint.identification.algorithm=https es-ic.sasl.mechanism=SOURCE_KAFKA_SASL_MECHANISM es-ic.sasl.jaas.config=SOURCE_LOGIN_MODULE required username=KAFKA_SOURCE_USER password=KAFKA_SOURCE_PASSWORD; sync.topic.acls.enabled=false replication.factor=1 internal.topic.replication.factor=1 es-ic.offset.storage.topic=mm2-cluster-offsets es-ic.configs.storage.topic=mm2-cluster-configs es-ic.status.storage.topic=mm2-cluster-status # enable and configure individual replication flows es-ic->target.enabled=true es-ic->target.topics=products ``` A lot of those properties are for the security settings. The clusters property defines the alias name for the source to target, and then the es-ic->target.* properties define the topic to replicate... Start Mirror Maker2 using the launch script: # In the es-ic-to-local folder ./launchMM2.sh This script updates the properties file from the environment variables defined in the .env file and starts a Kafka container with a command very similar as: docker run -ti --network es-ic-to-local_default -v $( pwd ) :/home -v $( pwd ) /mirror-maker-2/logs:/opt/kafka/logs strimzi/kafka:latest-kafka-2.6.0 /bin/bash -c \"/opt/kafka/bin/connect-mirror-maker.sh /home/mirror-maker-2/es-to-local/mm2.properties\" The mm2.properties file is mounted in the /home within the container. The network argument is important to get the host names resolved and the connector to connect to Kafka Brokers Verify the MM2 topics are created: docker exec -ti kafka1 /bin/bash -c \"/opt/kafka/bin/kafka-topics.sh --bootstrap-server kafka1:9091 --list\" __consumer_offsets es-ic.checkpoints.internal es-ic.products es-ic.source.heartbeats heartbeats mm2-configs.es-ic.internal mm2-offsets.es-ic.internal mm2-status.es-ic.internal Understanding MirrorMaker 2 trace \u00b6 A lot of configuration validation at the beginning of the trace to get the connection to both clusters. Any configuration issue to define the connection is generally well reported. URL, TLS certificate, secrets are the common cause of connection issues. If some messages happen with NO_LEADER for one of the topics, this means MM2 is not able to create the topic on the target cluster and so it is needed to create the topic with command or User Interface. Then we can observe the following: * It creates a producer to the target cluster for the offsets topics: [Producer clientId=producer-1] Cluster ID: Bj7Ui3UPQaKtJx7HOkWxPw * It creates consumer for the 25 offset topic partitions: [Consumer clientId=consumer-mirrormaker2-cluster-1, groupId=mirrormaker2-cluster] Subscribed to partition(s): mirrormaker2-cluster-offsets-0,.... * One Kafka connect worker is started: Worker started ... Starting KafkaBasedLog with topic mirrormaker2-cluster-status * Create a producer and consumers for the mirrormaker2-cluster-status topic for 5 partitions * Create another producer and consumer for the mirrormaker2-cluster-config topic Create WorkerSourceTask{id=es-1->es-ic.MirrorHeartbeatConnector-0} is the connector - task for the internal HeartBeat. WorkerSourceTask{id=es-1->es-ic.MirrorSourceConnector-0} for the topic to replicate Start consumer from target cluster \u00b6 Use Apache Kafka tool like Console consumer to trace the message received on a topic docker exec -ti kafka2 bash -c \"/opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server kafka1:9091 --topic es-ic.products --from-beginning\" Start Producer to source cluster \u00b6 We are reusing a python environment as defined in the integration tests for the 'kcontainer' solution. https://hub.docker.com/r/ibmcase/kcontainer-python . This time the script is producing products data. Here are the steps to send 5 records. # in the es-ic-to-local folder ./sendProductRecords.sh The traces should look like: [KafkaProducer] - {'bootstrap.servers': 'broker-0-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-3-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093', 'group.id': 'ProductsProducer', 'delivery.timeout.ms': 15000, 'request.timeout.ms': 15000, 'security.protocol': 'SASL_SSL', 'sasl.mechanisms': 'PLAIN', 'sasl.username': 'token', 'sasl.password': '..hidden...'} {'product_id': 'P01', 'description': 'Carrots', 'target_temperature': 4, 'target_humidity_level': 0.4, 'content_type': 1} {'product_id': 'P02', 'description': 'Banana', 'target_temperature': 6, 'target_humidity_level': 0.6, 'content_type': 2} {'product_id': 'P03', 'description': 'Salad', 'target_temperature': 4, 'target_humidity_level': 0.4, 'content_type': 1} {'product_id': 'P04', 'description': 'Avocado', 'target_temperature': 6, 'target_humidity_level': 0.4, 'content_type': 1} {'product_id': 'P05', 'description': 'Tomato', 'target_temperature': 4, 'target_humidity_level': 0.4, 'content_type': 2} [KafkaProducer] - Message delivered to products [0] [KafkaProducer] - Message delivered to products [0] [KafkaProducer] - Message delivered to products [0] [KafkaProducer] - Message delivered to products [0] [KafkaProducer] - Message delivered to products [0] Validate the replication is done from the consumer terminal { \"product_id\" : \"P01\" , \"description\" : \"Carrots\" , \"target_temperature\" : 4 , \"target_humidity_level\" : 0 .4, \"content_type\" : 1 } { \"product_id\" : \"P02\" , \"description\" : \"Banana\" , \"target_temperature\" : 6 , \"target_humidity_level\" : 0 .6, \"content_type\" : 2 } { \"product_id\" : \"P03\" , \"description\" : \"Salad\" , \"target_temperature\" : 4 , \"target_humidity_level\" : 0 .4, \"content_type\" : 1 } Stop producer with Ctrl C. Clean up \u00b6 You are done with the lab, to stop everything: # stop mirror maker 2 docker stop mm2 # stop local kafka cluster docker-compose down rm -r logs rm -r kafka-data/ Or run cleanLab.sh","title":"Mirror Maker 2 - Event Streams Service to Local Kafka - Lab 1"},{"location":"use-cases/kafka-mm2/lab-1/#overview","text":"For this scenario the source cluster is an Event Streams on IBM Cloud as managed service, and the target is a local Kafka cluster running with docker compose: it simulates an on-premise deployment. The figure below presents the components used: Mirror maker 2 runs in standalone mode on local server A python producer write to products topic defined on Evenstreams on IBM Cloud A consumer using Kafka console consumer tool to validate the replicated records from the 'products' topic As a pre-requisite you need to run your local cluster by using the docker compose as introduced in this note .","title":"Overview"},{"location":"use-cases/kafka-mm2/lab-1/#start-the-local-kafka-cluster","text":"In the refarch-eda-tools/labs/mirror-maker2/es-ic-to-local folder there is a docker compose file to start a local three brokers cluster with one Zookeeper node. In one Terminal window, start the local cluster using the command: docker-compose up -d The data are persisted on the local disk within the folder named kafka-data . Your local environment is up and running.","title":"Start the local Kafka cluster"},{"location":"use-cases/kafka-mm2/lab-1/#start-mirror-maker-2","text":"Rename the .env-tmpl file to .env From Event Streams on Cloud > Service Credentials, get the brokers address and the APIKEY. If needed read this note . If not done already, create a products topic (with one partition) in the EventStreams on Cloud cluster using the management console. See this note if needed, to see how to do it. Modify this .env file to set environment variables for Source Event Streams cluster brokers address and APIKEY. ES_IC_BROKERS = broker-0-q.....cloud.ibm.com:9093 ES_IC_USER = token ES_IC_PASSWORD = \"<replace with apikey from event streams service credentials>\" ES_IC_SASL_MECHANISM = PLAIN ES_IC_LOGIN_MODULE = org.apache.kafka.common.security.plain.PlainLoginModule 1. To configure Mirror Maker 2 in standalone mode, we need to define a mm2.properties file. We have define a template file which will be used by the script that launch Mirror Maker 2. The template looks like the following declaration: ```properties clusters=es-ic, target es-ic.bootstrap.servers=KAFKA_SOURCE_BROKERS target.bootstrap.servers=KAFKA_TARGET_BROKERS es-ic.security.protocol=SASL_SSL es-ic.ssl.protocol=TLSv1.2 es-ic.ssl.endpoint.identification.algorithm=https es-ic.sasl.mechanism=SOURCE_KAFKA_SASL_MECHANISM es-ic.sasl.jaas.config=SOURCE_LOGIN_MODULE required username=KAFKA_SOURCE_USER password=KAFKA_SOURCE_PASSWORD; sync.topic.acls.enabled=false replication.factor=1 internal.topic.replication.factor=1 es-ic.offset.storage.topic=mm2-cluster-offsets es-ic.configs.storage.topic=mm2-cluster-configs es-ic.status.storage.topic=mm2-cluster-status # enable and configure individual replication flows es-ic->target.enabled=true es-ic->target.topics=products ``` A lot of those properties are for the security settings. The clusters property defines the alias name for the source to target, and then the es-ic->target.* properties define the topic to replicate... Start Mirror Maker2 using the launch script: # In the es-ic-to-local folder ./launchMM2.sh This script updates the properties file from the environment variables defined in the .env file and starts a Kafka container with a command very similar as: docker run -ti --network es-ic-to-local_default -v $( pwd ) :/home -v $( pwd ) /mirror-maker-2/logs:/opt/kafka/logs strimzi/kafka:latest-kafka-2.6.0 /bin/bash -c \"/opt/kafka/bin/connect-mirror-maker.sh /home/mirror-maker-2/es-to-local/mm2.properties\" The mm2.properties file is mounted in the /home within the container. The network argument is important to get the host names resolved and the connector to connect to Kafka Brokers Verify the MM2 topics are created: docker exec -ti kafka1 /bin/bash -c \"/opt/kafka/bin/kafka-topics.sh --bootstrap-server kafka1:9091 --list\" __consumer_offsets es-ic.checkpoints.internal es-ic.products es-ic.source.heartbeats heartbeats mm2-configs.es-ic.internal mm2-offsets.es-ic.internal mm2-status.es-ic.internal","title":"Start Mirror Maker 2"},{"location":"use-cases/kafka-mm2/lab-1/#understanding-mirrormaker-2-trace","text":"A lot of configuration validation at the beginning of the trace to get the connection to both clusters. Any configuration issue to define the connection is generally well reported. URL, TLS certificate, secrets are the common cause of connection issues. If some messages happen with NO_LEADER for one of the topics, this means MM2 is not able to create the topic on the target cluster and so it is needed to create the topic with command or User Interface. Then we can observe the following: * It creates a producer to the target cluster for the offsets topics: [Producer clientId=producer-1] Cluster ID: Bj7Ui3UPQaKtJx7HOkWxPw * It creates consumer for the 25 offset topic partitions: [Consumer clientId=consumer-mirrormaker2-cluster-1, groupId=mirrormaker2-cluster] Subscribed to partition(s): mirrormaker2-cluster-offsets-0,.... * One Kafka connect worker is started: Worker started ... Starting KafkaBasedLog with topic mirrormaker2-cluster-status * Create a producer and consumers for the mirrormaker2-cluster-status topic for 5 partitions * Create another producer and consumer for the mirrormaker2-cluster-config topic Create WorkerSourceTask{id=es-1->es-ic.MirrorHeartbeatConnector-0} is the connector - task for the internal HeartBeat. WorkerSourceTask{id=es-1->es-ic.MirrorSourceConnector-0} for the topic to replicate","title":"Understanding MirrorMaker 2 trace"},{"location":"use-cases/kafka-mm2/lab-1/#start-consumer-from-target-cluster","text":"Use Apache Kafka tool like Console consumer to trace the message received on a topic docker exec -ti kafka2 bash -c \"/opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server kafka1:9091 --topic es-ic.products --from-beginning\"","title":"Start consumer from target cluster"},{"location":"use-cases/kafka-mm2/lab-1/#start-producer-to-source-cluster","text":"We are reusing a python environment as defined in the integration tests for the 'kcontainer' solution. https://hub.docker.com/r/ibmcase/kcontainer-python . This time the script is producing products data. Here are the steps to send 5 records. # in the es-ic-to-local folder ./sendProductRecords.sh The traces should look like: [KafkaProducer] - {'bootstrap.servers': 'broker-0-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-3-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093', 'group.id': 'ProductsProducer', 'delivery.timeout.ms': 15000, 'request.timeout.ms': 15000, 'security.protocol': 'SASL_SSL', 'sasl.mechanisms': 'PLAIN', 'sasl.username': 'token', 'sasl.password': '..hidden...'} {'product_id': 'P01', 'description': 'Carrots', 'target_temperature': 4, 'target_humidity_level': 0.4, 'content_type': 1} {'product_id': 'P02', 'description': 'Banana', 'target_temperature': 6, 'target_humidity_level': 0.6, 'content_type': 2} {'product_id': 'P03', 'description': 'Salad', 'target_temperature': 4, 'target_humidity_level': 0.4, 'content_type': 1} {'product_id': 'P04', 'description': 'Avocado', 'target_temperature': 6, 'target_humidity_level': 0.4, 'content_type': 1} {'product_id': 'P05', 'description': 'Tomato', 'target_temperature': 4, 'target_humidity_level': 0.4, 'content_type': 2} [KafkaProducer] - Message delivered to products [0] [KafkaProducer] - Message delivered to products [0] [KafkaProducer] - Message delivered to products [0] [KafkaProducer] - Message delivered to products [0] [KafkaProducer] - Message delivered to products [0] Validate the replication is done from the consumer terminal { \"product_id\" : \"P01\" , \"description\" : \"Carrots\" , \"target_temperature\" : 4 , \"target_humidity_level\" : 0 .4, \"content_type\" : 1 } { \"product_id\" : \"P02\" , \"description\" : \"Banana\" , \"target_temperature\" : 6 , \"target_humidity_level\" : 0 .6, \"content_type\" : 2 } { \"product_id\" : \"P03\" , \"description\" : \"Salad\" , \"target_temperature\" : 4 , \"target_humidity_level\" : 0 .4, \"content_type\" : 1 } Stop producer with Ctrl C.","title":"Start Producer to source cluster"},{"location":"use-cases/kafka-mm2/lab-1/#clean-up","text":"You are done with the lab, to stop everything: # stop mirror maker 2 docker stop mm2 # stop local kafka cluster docker-compose down rm -r logs rm -r kafka-data/ Or run cleanLab.sh","title":"Clean up"},{"location":"use-cases/kafka-mm2/lab-2/","text":"Scenario Prerequisites Overview Start Strimzi Kafka Cluster Produce messages to source cluster Start Mirror Maker 2 Start Consumer from target cluster Clean up Updated 01/22/2021 Overview \u00b6 For this scenario, the source cluster will be an IBM Event Streams instance on OpenShift and the target cluster will be another Kafka cluster (using Strimzi ) running locally on your workstation. Mirror Maker 2 will also run locally on your workstation. This lab is similar to the previous Lab 1 , but instead it uses IBM Event Streams within the Cloud Pak for Integration as illustrated in the figure below: Mirror Maker 2 runs locally on your workstation. A producer to send records to the products topic that also runs locally although it could be deployed on OpenShift as a job as well. A Kafka cluster running locally on your workstation that will contain the replicated topic and a Kafka console consumer to see the replicated messages. Scenario Prerequisites \u00b6 An IBM Event Streams instance running on OpenShift. See here for more detail about installing IBM Event Streams. Docker Compose Git CLI Complete the following steps in order to get ready for executing this lab scenario Create the products topic in your IBM Event Streams instance running on OpenShift. IMPORTANT: Create the topic with just 1 partition . To do so, please review the instructions in the Common pre-requisites of this website here . IMPORTANT: If you are sharing the IBM Event Streams instance, append a unique identifier to the products topic name so that you don't collide with anyone else. If you did not complete Lab 1 , clone the following GitHub repository to your local workstation to get the Mirror Maker 2 configuration files for this lab: git clone https://github.com/ibm-cloud-architecture/refarch-eda-tools Change directory into refarch-eda-tools/labs/mirror-maker2/es-cp4i-to-local cd refarch-eda-tools/labs/mirror-maker2/es-cp4i-to-local Rename the .env-tmpl properties file to .env mv .env-tmpl .env Download the IBM Event Streams TLS certificate so that your Kafka Connect framework local instance can establish secure communication with your IBM Event Streams instance. IMPORTANT: download the PKCS12 certificate. How to get the certificate in the Common pre-requisites section. The .env properties file will contain the properties needed for Mirror Maker 2 to be able to connect with your IBM Event Streams instance running on Openshift. Therefore, replace the following placeholder in the properties file: REPLACE_WITH_YOUR_BOOTSTRAP_URL : Your IBM Event Streams bootstrap url. REPLACE_WITH_YOUR_PKCS12_CERTIFICATE_PASSWORD : Your PCKS12 TLS certificate password. REPLACE_WITH_YOUR_SCRAM_USERNAME : Your SCRAM service credentials username. REPLACE_WITH_YOUR_SCRAM_PASSWORD : Your SCRAM service credentials password. REPLACE_WITH_YOUR_TOPIC : Name of the topic you created above. Review the Common pre-requisites instructions if you don't know how to find out any of the config properties above. Start Strimzi Kafka Cluster \u00b6 In this section, we are going to deploy and start a local Strimzi Kafka cluster which will act as your target cluster for Mirror Maker 2 to mirror the messages getting into the products topic in your IBM Event Streams instance to. In order to deploy this local Strimzi Kafka cluster, we are providing a Docker Compose file that will coordinate the startup of all the components in this Strimzi Kafka cluster. Make sure you are in refarch-eda-tools/labs/mirror-maker2/es-cp4i-to-local . Execute the following command docker-compose up -d The above command should start all the components in detached mode ( -d ) and you should see the following output: Creating zookeeper1 ... done Creating kafka1 ... done Creating kafka2 ... done Creating kafka3 ... done You should see the following Docker containers running on your workstation at the moment docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 1981f1913ab6 strimzi/kafka:latest-kafka-2.6.0 \"sh -c 'bin/kafka-se\u2026\" 2 minutes ago Up 2 minutes 0 .0.0.0:9093->9093/tcp, 0 .0.0.0:29093->29093/tcp kafka3 5f8fd3e80406 strimzi/kafka:latest-kafka-2.6.0 \"sh -c 'bin/kafka-se\u2026\" 2 minutes ago Up 2 minutes 0 .0.0.0:9091->9091/tcp, 0 .0.0.0:29091->29091/tcp kafka1 b19a05bd74dd strimzi/kafka:latest-kafka-2.6.0 \"sh -c 'bin/kafka-se\u2026\" 2 minutes ago Up 2 minutes 0 .0.0.0:9092->9092/tcp, 0 .0.0.0:29092->29092/tcp kafka2 93f500c8517a strimzi/kafka:latest-kafka-2.6.0 \"sh -c 'bin/zookeepe\u2026\" 2 minutes ago Up 2 minutes 0 .0.0.0:2181->2181/tcp zookeeper1 Produce messages to source cluster \u00b6 In this section, we are going to finally send events to the products topic in your IBM Event streams instance, which is your source cluster, and then verify those messages get mirrored by Mirror Maker 2 into your local Strimzi Kafka cluster, which is your target cluster. We are going to use a shell script which, in turn, will run a Python application that will send the messages to the source cluster. Since the application sending the messages to the source cluster is not a Java application, we will first need to download the PEM TLS certificate to allow the secure connection from the python application sending the messages to IBM Event Streams. Make sure you are in the refarch-eda-tools/labs/mirror-maker2/es-cp4i-to-local directory. Download the PEM TLS certificate there. Review the Common pre-requisites instructions if you don't remember how to download the certificate. Now we are going to send five records. In a new terminal window, make sure you are in the refarch-eda-tools/labs/mirror-maker2/es-cp4i-to-local directory and execute the following bash script. ./sendProductRecords.sh You should see the following output indicating your messages have been delivered to the source cluster topic --- This is the configuration for the producer: --- [ KafkaProducer ] - { 'bootstrap.servers' : 'kafka-bootstrap-integration.apps.net:443' , 'group.id' : 'ProductsProducer' , 'delivery.timeout.ms' : 15000 , 'request.timeout.ms' : 15000 , 'security.protocol' : 'SASL_SSL' , 'sasl.mechanisms' : 'SCRAM-SHA-512' , 'sasl.username' : 'test_user' , 'sasl.password' : '******' , 'ssl.ca.location' : '/home/es-cp4i-to-local/es-cert.pem' } --------------------------------------------------- { 'product_id' : 'P01' , 'description' : 'Carrots' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } { 'product_id' : 'P02' , 'description' : 'Banana' , 'target_temperature' : 6 , 'target_humidity_level' : 0 .6, 'content_type' : 2 } { 'product_id' : 'P03' , 'description' : 'Salad' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } { 'product_id' : 'P04' , 'description' : 'Avocado' , 'target_temperature' : 6 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } { 'product_id' : 'P05' , 'description' : 'Tomato' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 2 } [ KafkaProducer ] - Message delivered to products [ 0 ] [ KafkaProducer ] - Message delivered to products [ 0 ] [ KafkaProducer ] - Message delivered to products [ 0 ] [ KafkaProducer ] - Message delivered to products [ 0 ] [ KafkaProducer ] - Message delivered to products [ 0 ] If you go to the IBM Event Streams console, you should also see those messages in the topic Start Mirror Maker 2 \u00b6 In this section, we are going to go through the steps to get Mirror Maker 2 running locally on your workstation and configure it so that it replicates the messages from the products topic in your IBM Event Streams instance running on OpenShift to the local Strimzi Kafka cluster you deployed in the previous section as the target cluster for those messages. Make sure you are in refarch-eda-tools/labs/mirror-maker2/es-cp4i-to-local and you have done all steps in the Scenario Prerequisites section. Start your local Mirror Maker 2 instance by executing the following bash script. ./launchMM2.sh After quite some long output on your screen, you should see the following messages with the name of your topic. Don't worry if you dont find these as there is a lot of ouput. You will make sure the messages are replicated in the next section. INFO [ Consumer clientId = consumer-null-14, groupId = null ] Subscribed to partition ( s ) : products-0 ( org.apache.kafka.clients.consumer.KafkaConsumer:1120 ) INFO Starting with 1 previously uncommitted partitions. ( org.apache.kafka.connect.mirror.MirrorSourceTask:94 ) INFO [ Consumer clientId = consumer-null-14, groupId = null ] Seeking to offset 0 for partition products-0 ( org.apache.kafka.clients.consumer.KafkaConsumer:1596 ) INFO [ Consumer clientId = consumer-null-15, groupId = null ] Subscribed to partition ( s ) : heartbeats-0 ( org.apache.kafka.clients.consumer.KafkaConsumer:1120 ) INFO task-thread-MirrorSourceConnector-0 replicating 1 topic-partitions es-cp4i->target: [ products-0 ] . ( org.apache.kafka.connect.mirror.MirrorSourceTask:98 ) INFO WorkerSourceTask { id = MirrorSourceConnector-0 } Source task finished initialization and start ( org.apache.kafka.connect.runtime.WorkerSourceTask:233 ) INFO Starting with 1 previously uncommitted partitions. ( org.apache.kafka.connect.mirror.MirrorSourceTask:94 ) INFO [ Consumer clientId = consumer-null-15, groupId = null ] Seeking to offset 0 for partition heartbeats-0 ( org.apache.kafka.clients.consumer.KafkaConsumer:1596 ) INFO task-thread-MirrorSourceConnector-1 replicating 1 topic-partitions es-cp4i->target: [ heartbeats-0 ] . ( org.apache.kafka.connect.mirror.MirrorSourceTask:98 ) INFO WorkerSourceTask { id = MirrorSourceConnector-1 } Source task finished initialization and start ( org.apache.kafka.connect.runtime.WorkerSourceTask:233 ) Start consumer from target cluster \u00b6 In this section, we are going to start a consumer to consume messages from the target cluster (your local Strimzi Kafka cluster) to make sure we receive mirrored messages from your source cluster (your IBM Event Streams instance running on OpenShift). We are going to use a couple of Apache Kafka tools comming with the open source Strimzi Kafka Docker image you already have running. Make sure your target mirrored topic has been created executing the following command on a new terminal window. docker exec kafka2 bash -c \"/opt/kafka/bin/kafka-topics.sh --list --bootstrap-server kafka1:9091\" __consumer_offsets es-cp4i.checkpoints.internal es-cp4i.heartbeats es-cp4i.products heartbeats mm2-configs.es-cp4i.internal mm2-offsets.es-cp4i.internal mm2-status.es-cp4i.internal You should see a topic called es-cp4i.YOUR_TOPIC where YOUR_TOPIC should be the name of the topic you created before in the Scenario Prerequisites section. Now, execute the following command replacing the TOPIC_NAME placeholder with the name of the topic you verified above ( ex-cp4i.YOUR_TOPIC ) docker exec -ti kafka2 bash -c \"/opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server kafka1:9091 --topic TOPIC_NAME --from-beginning\" You should see the mirrored messages now in your replicated topic in your target local Strimzi Kafka cluster { \"product_id\" : \"P01\" , \"description\" : \"Carrots\" , \"target_temperature\" : 4 , \"target_humidity_level\" : 0 .4, \"content_type\" : 1 } { \"product_id\" : \"P02\" , \"description\" : \"Banana\" , \"target_temperature\" : 6 , \"target_humidity_level\" : 0 .6, \"content_type\" : 2 } { \"product_id\" : \"P03\" , \"description\" : \"Salad\" , \"target_temperature\" : 4 , \"target_humidity_level\" : 0 .4, \"content_type\" : 1 } { \"product_id\" : \"P04\" , \"description\" : \"Avocado\" , \"target_temperature\" : 6 , \"target_humidity_level\" : 0 .4, \"content_type\" : 1 } { \"product_id\" : \"P05\" , \"description\" : \"Tomato\" , \"target_temperature\" : 4 , \"target_humidity_level\" : 0 .4, \"content_type\" : 2 } Clean up \u00b6 You have now successfully finished the lab. You can stop the consumer and the Mirror Maker 2 console output pressing ctrl+c in their respective terminals. You can also stop and remove the Docker containers for both the Mirror Maker 2 and Strimzi Kafka clusters running on your workstation by executing the following script: ./cleanLab.sh","title":"Mirror Maker 2 ES on RHOS to local cluster"},{"location":"use-cases/kafka-mm2/lab-2/#overview","text":"For this scenario, the source cluster will be an IBM Event Streams instance on OpenShift and the target cluster will be another Kafka cluster (using Strimzi ) running locally on your workstation. Mirror Maker 2 will also run locally on your workstation. This lab is similar to the previous Lab 1 , but instead it uses IBM Event Streams within the Cloud Pak for Integration as illustrated in the figure below: Mirror Maker 2 runs locally on your workstation. A producer to send records to the products topic that also runs locally although it could be deployed on OpenShift as a job as well. A Kafka cluster running locally on your workstation that will contain the replicated topic and a Kafka console consumer to see the replicated messages.","title":"Overview"},{"location":"use-cases/kafka-mm2/lab-2/#scenario-prerequisites","text":"An IBM Event Streams instance running on OpenShift. See here for more detail about installing IBM Event Streams. Docker Compose Git CLI Complete the following steps in order to get ready for executing this lab scenario Create the products topic in your IBM Event Streams instance running on OpenShift. IMPORTANT: Create the topic with just 1 partition . To do so, please review the instructions in the Common pre-requisites of this website here . IMPORTANT: If you are sharing the IBM Event Streams instance, append a unique identifier to the products topic name so that you don't collide with anyone else. If you did not complete Lab 1 , clone the following GitHub repository to your local workstation to get the Mirror Maker 2 configuration files for this lab: git clone https://github.com/ibm-cloud-architecture/refarch-eda-tools Change directory into refarch-eda-tools/labs/mirror-maker2/es-cp4i-to-local cd refarch-eda-tools/labs/mirror-maker2/es-cp4i-to-local Rename the .env-tmpl properties file to .env mv .env-tmpl .env Download the IBM Event Streams TLS certificate so that your Kafka Connect framework local instance can establish secure communication with your IBM Event Streams instance. IMPORTANT: download the PKCS12 certificate. How to get the certificate in the Common pre-requisites section. The .env properties file will contain the properties needed for Mirror Maker 2 to be able to connect with your IBM Event Streams instance running on Openshift. Therefore, replace the following placeholder in the properties file: REPLACE_WITH_YOUR_BOOTSTRAP_URL : Your IBM Event Streams bootstrap url. REPLACE_WITH_YOUR_PKCS12_CERTIFICATE_PASSWORD : Your PCKS12 TLS certificate password. REPLACE_WITH_YOUR_SCRAM_USERNAME : Your SCRAM service credentials username. REPLACE_WITH_YOUR_SCRAM_PASSWORD : Your SCRAM service credentials password. REPLACE_WITH_YOUR_TOPIC : Name of the topic you created above. Review the Common pre-requisites instructions if you don't know how to find out any of the config properties above.","title":"Scenario Prerequisites"},{"location":"use-cases/kafka-mm2/lab-2/#start-strimzi-kafka-cluster","text":"In this section, we are going to deploy and start a local Strimzi Kafka cluster which will act as your target cluster for Mirror Maker 2 to mirror the messages getting into the products topic in your IBM Event Streams instance to. In order to deploy this local Strimzi Kafka cluster, we are providing a Docker Compose file that will coordinate the startup of all the components in this Strimzi Kafka cluster. Make sure you are in refarch-eda-tools/labs/mirror-maker2/es-cp4i-to-local . Execute the following command docker-compose up -d The above command should start all the components in detached mode ( -d ) and you should see the following output: Creating zookeeper1 ... done Creating kafka1 ... done Creating kafka2 ... done Creating kafka3 ... done You should see the following Docker containers running on your workstation at the moment docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 1981f1913ab6 strimzi/kafka:latest-kafka-2.6.0 \"sh -c 'bin/kafka-se\u2026\" 2 minutes ago Up 2 minutes 0 .0.0.0:9093->9093/tcp, 0 .0.0.0:29093->29093/tcp kafka3 5f8fd3e80406 strimzi/kafka:latest-kafka-2.6.0 \"sh -c 'bin/kafka-se\u2026\" 2 minutes ago Up 2 minutes 0 .0.0.0:9091->9091/tcp, 0 .0.0.0:29091->29091/tcp kafka1 b19a05bd74dd strimzi/kafka:latest-kafka-2.6.0 \"sh -c 'bin/kafka-se\u2026\" 2 minutes ago Up 2 minutes 0 .0.0.0:9092->9092/tcp, 0 .0.0.0:29092->29092/tcp kafka2 93f500c8517a strimzi/kafka:latest-kafka-2.6.0 \"sh -c 'bin/zookeepe\u2026\" 2 minutes ago Up 2 minutes 0 .0.0.0:2181->2181/tcp zookeeper1","title":"Start Strimzi Kafka Cluster"},{"location":"use-cases/kafka-mm2/lab-2/#produce-messages-to-source-cluster","text":"In this section, we are going to finally send events to the products topic in your IBM Event streams instance, which is your source cluster, and then verify those messages get mirrored by Mirror Maker 2 into your local Strimzi Kafka cluster, which is your target cluster. We are going to use a shell script which, in turn, will run a Python application that will send the messages to the source cluster. Since the application sending the messages to the source cluster is not a Java application, we will first need to download the PEM TLS certificate to allow the secure connection from the python application sending the messages to IBM Event Streams. Make sure you are in the refarch-eda-tools/labs/mirror-maker2/es-cp4i-to-local directory. Download the PEM TLS certificate there. Review the Common pre-requisites instructions if you don't remember how to download the certificate. Now we are going to send five records. In a new terminal window, make sure you are in the refarch-eda-tools/labs/mirror-maker2/es-cp4i-to-local directory and execute the following bash script. ./sendProductRecords.sh You should see the following output indicating your messages have been delivered to the source cluster topic --- This is the configuration for the producer: --- [ KafkaProducer ] - { 'bootstrap.servers' : 'kafka-bootstrap-integration.apps.net:443' , 'group.id' : 'ProductsProducer' , 'delivery.timeout.ms' : 15000 , 'request.timeout.ms' : 15000 , 'security.protocol' : 'SASL_SSL' , 'sasl.mechanisms' : 'SCRAM-SHA-512' , 'sasl.username' : 'test_user' , 'sasl.password' : '******' , 'ssl.ca.location' : '/home/es-cp4i-to-local/es-cert.pem' } --------------------------------------------------- { 'product_id' : 'P01' , 'description' : 'Carrots' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } { 'product_id' : 'P02' , 'description' : 'Banana' , 'target_temperature' : 6 , 'target_humidity_level' : 0 .6, 'content_type' : 2 } { 'product_id' : 'P03' , 'description' : 'Salad' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } { 'product_id' : 'P04' , 'description' : 'Avocado' , 'target_temperature' : 6 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } { 'product_id' : 'P05' , 'description' : 'Tomato' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 2 } [ KafkaProducer ] - Message delivered to products [ 0 ] [ KafkaProducer ] - Message delivered to products [ 0 ] [ KafkaProducer ] - Message delivered to products [ 0 ] [ KafkaProducer ] - Message delivered to products [ 0 ] [ KafkaProducer ] - Message delivered to products [ 0 ] If you go to the IBM Event Streams console, you should also see those messages in the topic","title":"Produce messages to source cluster"},{"location":"use-cases/kafka-mm2/lab-2/#start-mirror-maker-2","text":"In this section, we are going to go through the steps to get Mirror Maker 2 running locally on your workstation and configure it so that it replicates the messages from the products topic in your IBM Event Streams instance running on OpenShift to the local Strimzi Kafka cluster you deployed in the previous section as the target cluster for those messages. Make sure you are in refarch-eda-tools/labs/mirror-maker2/es-cp4i-to-local and you have done all steps in the Scenario Prerequisites section. Start your local Mirror Maker 2 instance by executing the following bash script. ./launchMM2.sh After quite some long output on your screen, you should see the following messages with the name of your topic. Don't worry if you dont find these as there is a lot of ouput. You will make sure the messages are replicated in the next section. INFO [ Consumer clientId = consumer-null-14, groupId = null ] Subscribed to partition ( s ) : products-0 ( org.apache.kafka.clients.consumer.KafkaConsumer:1120 ) INFO Starting with 1 previously uncommitted partitions. ( org.apache.kafka.connect.mirror.MirrorSourceTask:94 ) INFO [ Consumer clientId = consumer-null-14, groupId = null ] Seeking to offset 0 for partition products-0 ( org.apache.kafka.clients.consumer.KafkaConsumer:1596 ) INFO [ Consumer clientId = consumer-null-15, groupId = null ] Subscribed to partition ( s ) : heartbeats-0 ( org.apache.kafka.clients.consumer.KafkaConsumer:1120 ) INFO task-thread-MirrorSourceConnector-0 replicating 1 topic-partitions es-cp4i->target: [ products-0 ] . ( org.apache.kafka.connect.mirror.MirrorSourceTask:98 ) INFO WorkerSourceTask { id = MirrorSourceConnector-0 } Source task finished initialization and start ( org.apache.kafka.connect.runtime.WorkerSourceTask:233 ) INFO Starting with 1 previously uncommitted partitions. ( org.apache.kafka.connect.mirror.MirrorSourceTask:94 ) INFO [ Consumer clientId = consumer-null-15, groupId = null ] Seeking to offset 0 for partition heartbeats-0 ( org.apache.kafka.clients.consumer.KafkaConsumer:1596 ) INFO task-thread-MirrorSourceConnector-1 replicating 1 topic-partitions es-cp4i->target: [ heartbeats-0 ] . ( org.apache.kafka.connect.mirror.MirrorSourceTask:98 ) INFO WorkerSourceTask { id = MirrorSourceConnector-1 } Source task finished initialization and start ( org.apache.kafka.connect.runtime.WorkerSourceTask:233 )","title":"Start Mirror Maker 2"},{"location":"use-cases/kafka-mm2/lab-2/#start-consumer-from-target-cluster","text":"In this section, we are going to start a consumer to consume messages from the target cluster (your local Strimzi Kafka cluster) to make sure we receive mirrored messages from your source cluster (your IBM Event Streams instance running on OpenShift). We are going to use a couple of Apache Kafka tools comming with the open source Strimzi Kafka Docker image you already have running. Make sure your target mirrored topic has been created executing the following command on a new terminal window. docker exec kafka2 bash -c \"/opt/kafka/bin/kafka-topics.sh --list --bootstrap-server kafka1:9091\" __consumer_offsets es-cp4i.checkpoints.internal es-cp4i.heartbeats es-cp4i.products heartbeats mm2-configs.es-cp4i.internal mm2-offsets.es-cp4i.internal mm2-status.es-cp4i.internal You should see a topic called es-cp4i.YOUR_TOPIC where YOUR_TOPIC should be the name of the topic you created before in the Scenario Prerequisites section. Now, execute the following command replacing the TOPIC_NAME placeholder with the name of the topic you verified above ( ex-cp4i.YOUR_TOPIC ) docker exec -ti kafka2 bash -c \"/opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server kafka1:9091 --topic TOPIC_NAME --from-beginning\" You should see the mirrored messages now in your replicated topic in your target local Strimzi Kafka cluster { \"product_id\" : \"P01\" , \"description\" : \"Carrots\" , \"target_temperature\" : 4 , \"target_humidity_level\" : 0 .4, \"content_type\" : 1 } { \"product_id\" : \"P02\" , \"description\" : \"Banana\" , \"target_temperature\" : 6 , \"target_humidity_level\" : 0 .6, \"content_type\" : 2 } { \"product_id\" : \"P03\" , \"description\" : \"Salad\" , \"target_temperature\" : 4 , \"target_humidity_level\" : 0 .4, \"content_type\" : 1 } { \"product_id\" : \"P04\" , \"description\" : \"Avocado\" , \"target_temperature\" : 6 , \"target_humidity_level\" : 0 .4, \"content_type\" : 1 } { \"product_id\" : \"P05\" , \"description\" : \"Tomato\" , \"target_temperature\" : 4 , \"target_humidity_level\" : 0 .4, \"content_type\" : 2 }","title":"Start consumer from target cluster"},{"location":"use-cases/kafka-mm2/lab-2/#clean-up","text":"You have now successfully finished the lab. You can stop the consumer and the Mirror Maker 2 console output pressing ctrl+c in their respective terminals. You can also stop and remove the Docker containers for both the Mirror Maker 2 and Strimzi Kafka clusters running on your workstation by executing the following script: ./cleanLab.sh","title":"Clean up"},{"location":"use-cases/kafka-mm2/lab-3/","text":"Overview Pre-requisites Start Mirror Maker 2 Start Producer to source cluster Consuming records on source Failover to target Updated 01/08/2021 Overview \u00b6 This lab presents how to leverage Mirror Maker 2 between two on-premise Kafka clusters running on OpenShift, one having no consumer and producer connected to it: it is in passive mode. The cluster is still getting replicated data. The lab goes up to the failover and reconnect consumers to the newly promoted active cluster. Mirror Maker 2 runs on OpenShift as pod in the same namespace as Event Streams on the target cluster A producer in python to send records to products topic, will run locally or could be deployed on OpenShift as a job A consumer, also in python is consuming n records, in auto commit, so there will be a consumer lag before the failover. For the failover, we will stop the producer. We could stop event streams, but the most important is that there is no more records coming from the source cluster to the target cluster via mirroring. The goal not is to connect the consumer to the target cluster and then continue from where the consumer on the source cluster has stopped. If those consumer was writing to a database then the DB in the passive environment will receive the new records. If the database is in a 3nd environment like a managed service in the cloud with HA, then new records will be added to the original one. If the database server was also doing replication between active and passive environments then it may be possible to get a gap in the data, depending of the DB replication settings. In the figure above, the offset numbering does not have to match with source. This is where mirror maker 2 is keeping offset metadata on its own topics. The commit offsets for each consumer groups is also saved, so consumers restarting on the target cluster will continue for the matching offset corresponding to the last read committed offset. Pre-requisites \u00b6 We assume, you have access to two Kafka Clusters deployed on OpenShift. We use two Event Streams instances on the same OpensShift cluster for this lab. Login to the OpenShift cluster using the console and get the API token oc login --token = L0.... --server = https://api.eda-solutions.gse-ocp.net:6443 If not done from lab 1, clone the github to get access to the Mirror Maker 2 manifests we are using: git clone https://github.com/ibm-cloud-architecture/refarch-eda-tools * Create the products topic on the source cluster. * Under the labs/MirrorMaker2/active-passive folder, rename the .env-tmpl file to .env . * Get the source and target bootstrap server external URLs for the producer and consumer code using oc get routes | grep bootstrap . We will use to demonstrate the offset management and consumer reconnection then modify the addresses in the .env file ES_SRC_BROKERS = light-es-kafka-bootstrap-eventstreams.gse-eda-2020-10-3-0143c5dd31acd8e030a1d6e0ab1380e3-0000.us-east.containers.appdomain.cloud:443 ES_TGT_BROKERS = gse-eda-dev-kafka-bootstrap-eventstreams.gse-eda-2020-10-3-0143c5dd31acd8e030a1d6e0ab1380e3-0000.us-east.containers.appdomain.cloud:443 * Get SCRAM users for both cluster and set their names in the .env file ES_SRC_USER = starter ES_TGT_USER = user2 * Get pem certificates from both clusters using each admin console web apps, or the CLI: cloudctl es init # select one of the cluster, then... cloudctl es certificates --format pem # rename the es-cert.pem to mv es-cert.pem es-src-cert.pem # Get for the second cluster cloudctl es init cloudctl es certificates --format pem mv es-cert.pem es-tgt-cert.pem Verify the Event Streams on OpenShift service end point URLs. Those URLs will be used to configure Mirror Maker 2. # Use the bootstrap internal URL oc get svc | grep bootstrap # Get both internal URLs light-es-kafka-bootstrap.evenstreams.svc:9092 gse-eda-dev-kafka-bootstrap.evenstreams.svc:9092 Start Mirror Maker 2 \u00b6 In this lab, Mirror Maker 2 will run on the same cluster as Event Streams within the same namespace (e.g. eventstreams). Define source and target cluster properties in a Mirror Maker 2 es-to-es.yml descriptor file. We strongly recommend to study the schema definition of this custom resource from this page . Here are some important parameters you need to consider: The namespace needs to match the event streams project, and the annotations the product version and ID. The connectCluster needs to match the alias of the target cluster. The alias es-tgt represents the kafka cluster Mirror Maker 2 needs to connect to: apiVersion : eventstreams.ibm.com/v1alpha1 kind : KafkaMirrorMaker2 metadata : name : mm2 namespace : eventstreams spec : template : pod : metadata : annotations : eventstreams.production.type : CloudPakForIntegrationNonProduction productCloudpakRatio : \"2:1\" productChargedContainers : mm2-mirrormaker2 productVersion : 10.1.0 productID : 2a79e49111f44ec3acd89608e56138f5 cloudpakName : IBM Cloud Pak for Integration cloudpakId : c8b82d189e7545f0892db9ef2731b90d productName : IBM Event Streams for Non Production cloudpakVersion : 2020.3.1 productMetric : VIRTUAL_PROCESSOR_CORE version : 2.6.0 replicas : 1 connectCluster : \"es-tgt\" The version matches the Kafka version we use. The number of replicas can be set to 1 to start with or use the default of 3. The eventstreams.production.type is needed for Event Streams. Then the yaml defines the connection configuration for each clusters: clusters : - alias : \"es-src\" bootstrapServers : config : config.storage.replication.factor : 3 offset.storage.replication.factor : 3 status.storage.replication.factor : 3 tls : {} For Event Streams on premise running within OpenShift, the connection uses TLS, certificates and SCRAM credentials. As we run in a separate namespace the URL is the 'external' one. - alias : \"es-src\" bootstrapServers : light-es-kafka-bootstrap.integration.svc:9093 config : ssl.endpoint.identification.algorithm : https tls : trustedCertificates : - secretName : light-es-cluster-ca-cert certificate : ca.crt authentication : type : tls certificateAndKey : certificate : user.crt key : user.key secretName : es-tls-user Finally the connectCluster attribute defines the cluster alias used by MirrorMaker2 to define its hidden topics, it must match the target cluster of the replication in the list at spec.clusters . # under active-passive folder oc apply -f es-to-es.yml Verify the characteristics of the Mirror Maker 2 instance using the CLI oc describe kafkamirrormaker2 mm2 * See the logs: oc get pods | grep mm2 oc logs mm2-mirrormaker2-... Start Producer to source cluster \u00b6 As seen in lab 1, we will use the same python script to create products records. This time the script is producing product records to the products topic. Now send 100 records: ./sendProductRecords.sh --random 100 The trace looks like: ``` --- This is the configuration for the producer: --- [KafkaProducer] - {'bootstrap.servers': 'light-es-kafka-bootstrap-eventstreams.gse-eda-2020-10-3-0143c5dd31acd8e030a1d6e0ab1380e3-0000.us-east.containers.appdomain.cloud:443', 'group.id': 'ProductsProducer', 'delivery.timeout.ms': 15000, 'request.timeout.ms': 15000, 'security.protocol': 'SASL_SSL', 'sasl.mechanisms': 'SCRAM-SHA-512', 'sasl.username': 'starter', 'sasl.password': 'd8zsUzhK9qUZ', 'ssl.ca.location': '/home/active-passive/es-cert.pem'} {'product_id': 'T1', 'description': 'Product 1', 'target_temperature': 6.321923853806639, 'target_humidity_level': 0.4, 'content_type': 1} {'product_id': 'T2', 'description': 'Product 2', 'target_temperature': 4.991504310455889, 'target_humidity_level': 0.4, 'content_type': 1} {'product_id': 'T3', 'description': 'Product 3', 'target_temperature': 4.491634291119919, 'target_humidity_level': 0.4, 'content_type': 1} {'product_id': 'T4', 'description': 'Product 4', 'target_temperature': 2.4855241432802613, 'target_humidity_level': 0.4, 'content_type': 1} {'product_id': 'T5', 'description': 'Product 5', 'target_temperature': 4.286428275499635, 'target_humidity_level': 0.4, 'content_type': 1} {'product_id': 'T6', 'description': 'Product 6', 'target_temperature': 1.6025770613167736, 'target_humidity_level': 0.4, 'content_type': 1} ``` Going to the Event Streams Console we can see the produced messages in the products topic. Start the Consumer to source cluster \u00b6 To simulate the offset mapping between source and target, we will use a python consumer and read only n records. shell ./receiveProductSrc.sh 20 The trace may look like: ``` --------- Start Consuming products -------------- [KafkaConsumer] - This is the configuration for the consumer: [KafkaConsumer] - ------------------------------------------- [KafkaConsumer] - Bootstrap Server: light-es-kafka-bootstrap-eventstreams.gse-eda-2020-10-3-0143c5dd31acd8e030a1d6e0ab1380e3-0000.us-east.containers.appdomain.cloud:443 [KafkaConsumer] - Topic: products [KafkaConsumer] - Topic timeout: 10 [KafkaConsumer] - Security Protocol: SASL_SSL [KafkaConsumer] - SASL Mechanism: SCRAM-SHA-512 [KafkaConsumer] - SASL Username: starter [KafkaConsumer] - SASL Password: d*****Z [KafkaConsumer] - SSL CA Location: /home/active-passive/es-cert.pem [KafkaConsumer] - Offset Reset: earliest [KafkaConsumer] - Autocommit: True [KafkaConsumer] - ------------------------------------------- [KafkaConsumer] - Next Message consumed from products partition: [0] at offset: 0 key: P01 value: {\"product_id\": \"P01\", \"description\": \"Carrots\", \"target_temperature\": 4, \"target_humidity_level\": 0.4, \"content_type\": 1} [KafkaConsumer] - Next Message consumed from products partition: [0] at offset: 1 key: P02 value: {\"product_id\": \"P02\", \"description\": \"Banana\", \"target_temperature\": 6, \"target_humidity_level\": 0.6, \"content_type\": 2} [KafkaConsumer] - Next Message consumed from products partition: [0] at offset: 2 key: P03 value: {\"product_id\": \"P03\", \"description\": \"Salad\", \"target_temperature\": 4, \"target_humidity_level\": 0.4, \"content_type\": 1} [KafkaConsumer] - Next Message consumed from products partition: [0] at offset: 3 key: P04 value: {\"product_id\": \"P04\", \"description\": \"Avocado\", \"target_temperature\": 6, \"target_humidity_level\": 0.4, \"content_type\": 1} [KafkaConsumer] - Next Message consumed from products partition: [0] at offset: 4 key: P05 value: {\"product_id\": \"P05\", \"description\": \"Tomato\", \"target_temperature\": 4, \"target_humidity_level\": 0.4, \"content_type\": 2} [KafkaConsumer] - Next Message consumed from products partition: [0] at offset: 5 key: T1 value: {\"product_id\": \"T1\", \"description\": \"Product 1\", \"target_temperature\": 6.321923853806639, \"target_humidity_level\": 0.4, \"content_type\": 1} [KafkaConsumer] - Next Message consumed from products partition: [0] at offset: 6 ``` The python code is ProductConsumer.py . If we go to Event Streams consumer group monitoring user interface we can see the consumer only got 20 messages and so there is an offset lag, as illustrated in figure below: Failover to target \u00b6 At this stage, the producer code is not running anymore, Mirror Maker 2 has replicated the data to the target topic named: es-src.products , the consumer has not read all the messages from source cluster. This simulate a crash on source cluster. So let now connect the consumer to the target cluster and continue to process the records. For that the consumer needs to get the offset mapping using the RemoteClusterUtils class to translate the consumer group offset from the source cluster to the corresponding offset for the target cluster.","title":"Mirror Maker 2 Active Passive"},{"location":"use-cases/kafka-mm2/lab-3/#overview","text":"This lab presents how to leverage Mirror Maker 2 between two on-premise Kafka clusters running on OpenShift, one having no consumer and producer connected to it: it is in passive mode. The cluster is still getting replicated data. The lab goes up to the failover and reconnect consumers to the newly promoted active cluster. Mirror Maker 2 runs on OpenShift as pod in the same namespace as Event Streams on the target cluster A producer in python to send records to products topic, will run locally or could be deployed on OpenShift as a job A consumer, also in python is consuming n records, in auto commit, so there will be a consumer lag before the failover. For the failover, we will stop the producer. We could stop event streams, but the most important is that there is no more records coming from the source cluster to the target cluster via mirroring. The goal not is to connect the consumer to the target cluster and then continue from where the consumer on the source cluster has stopped. If those consumer was writing to a database then the DB in the passive environment will receive the new records. If the database is in a 3nd environment like a managed service in the cloud with HA, then new records will be added to the original one. If the database server was also doing replication between active and passive environments then it may be possible to get a gap in the data, depending of the DB replication settings. In the figure above, the offset numbering does not have to match with source. This is where mirror maker 2 is keeping offset metadata on its own topics. The commit offsets for each consumer groups is also saved, so consumers restarting on the target cluster will continue for the matching offset corresponding to the last read committed offset.","title":"Overview"},{"location":"use-cases/kafka-mm2/lab-3/#pre-requisites","text":"We assume, you have access to two Kafka Clusters deployed on OpenShift. We use two Event Streams instances on the same OpensShift cluster for this lab. Login to the OpenShift cluster using the console and get the API token oc login --token = L0.... --server = https://api.eda-solutions.gse-ocp.net:6443 If not done from lab 1, clone the github to get access to the Mirror Maker 2 manifests we are using: git clone https://github.com/ibm-cloud-architecture/refarch-eda-tools * Create the products topic on the source cluster. * Under the labs/MirrorMaker2/active-passive folder, rename the .env-tmpl file to .env . * Get the source and target bootstrap server external URLs for the producer and consumer code using oc get routes | grep bootstrap . We will use to demonstrate the offset management and consumer reconnection then modify the addresses in the .env file ES_SRC_BROKERS = light-es-kafka-bootstrap-eventstreams.gse-eda-2020-10-3-0143c5dd31acd8e030a1d6e0ab1380e3-0000.us-east.containers.appdomain.cloud:443 ES_TGT_BROKERS = gse-eda-dev-kafka-bootstrap-eventstreams.gse-eda-2020-10-3-0143c5dd31acd8e030a1d6e0ab1380e3-0000.us-east.containers.appdomain.cloud:443 * Get SCRAM users for both cluster and set their names in the .env file ES_SRC_USER = starter ES_TGT_USER = user2 * Get pem certificates from both clusters using each admin console web apps, or the CLI: cloudctl es init # select one of the cluster, then... cloudctl es certificates --format pem # rename the es-cert.pem to mv es-cert.pem es-src-cert.pem # Get for the second cluster cloudctl es init cloudctl es certificates --format pem mv es-cert.pem es-tgt-cert.pem Verify the Event Streams on OpenShift service end point URLs. Those URLs will be used to configure Mirror Maker 2. # Use the bootstrap internal URL oc get svc | grep bootstrap # Get both internal URLs light-es-kafka-bootstrap.evenstreams.svc:9092 gse-eda-dev-kafka-bootstrap.evenstreams.svc:9092","title":"Pre-requisites"},{"location":"use-cases/kafka-mm2/lab-3/#start-mirror-maker-2","text":"In this lab, Mirror Maker 2 will run on the same cluster as Event Streams within the same namespace (e.g. eventstreams). Define source and target cluster properties in a Mirror Maker 2 es-to-es.yml descriptor file. We strongly recommend to study the schema definition of this custom resource from this page . Here are some important parameters you need to consider: The namespace needs to match the event streams project, and the annotations the product version and ID. The connectCluster needs to match the alias of the target cluster. The alias es-tgt represents the kafka cluster Mirror Maker 2 needs to connect to: apiVersion : eventstreams.ibm.com/v1alpha1 kind : KafkaMirrorMaker2 metadata : name : mm2 namespace : eventstreams spec : template : pod : metadata : annotations : eventstreams.production.type : CloudPakForIntegrationNonProduction productCloudpakRatio : \"2:1\" productChargedContainers : mm2-mirrormaker2 productVersion : 10.1.0 productID : 2a79e49111f44ec3acd89608e56138f5 cloudpakName : IBM Cloud Pak for Integration cloudpakId : c8b82d189e7545f0892db9ef2731b90d productName : IBM Event Streams for Non Production cloudpakVersion : 2020.3.1 productMetric : VIRTUAL_PROCESSOR_CORE version : 2.6.0 replicas : 1 connectCluster : \"es-tgt\" The version matches the Kafka version we use. The number of replicas can be set to 1 to start with or use the default of 3. The eventstreams.production.type is needed for Event Streams. Then the yaml defines the connection configuration for each clusters: clusters : - alias : \"es-src\" bootstrapServers : config : config.storage.replication.factor : 3 offset.storage.replication.factor : 3 status.storage.replication.factor : 3 tls : {} For Event Streams on premise running within OpenShift, the connection uses TLS, certificates and SCRAM credentials. As we run in a separate namespace the URL is the 'external' one. - alias : \"es-src\" bootstrapServers : light-es-kafka-bootstrap.integration.svc:9093 config : ssl.endpoint.identification.algorithm : https tls : trustedCertificates : - secretName : light-es-cluster-ca-cert certificate : ca.crt authentication : type : tls certificateAndKey : certificate : user.crt key : user.key secretName : es-tls-user Finally the connectCluster attribute defines the cluster alias used by MirrorMaker2 to define its hidden topics, it must match the target cluster of the replication in the list at spec.clusters . # under active-passive folder oc apply -f es-to-es.yml Verify the characteristics of the Mirror Maker 2 instance using the CLI oc describe kafkamirrormaker2 mm2 * See the logs: oc get pods | grep mm2 oc logs mm2-mirrormaker2-...","title":"Start Mirror Maker 2"},{"location":"use-cases/kafka-mm2/lab-3/#start-producer-to-source-cluster","text":"As seen in lab 1, we will use the same python script to create products records. This time the script is producing product records to the products topic. Now send 100 records: ./sendProductRecords.sh --random 100 The trace looks like: ``` --- This is the configuration for the producer: --- [KafkaProducer] - {'bootstrap.servers': 'light-es-kafka-bootstrap-eventstreams.gse-eda-2020-10-3-0143c5dd31acd8e030a1d6e0ab1380e3-0000.us-east.containers.appdomain.cloud:443', 'group.id': 'ProductsProducer', 'delivery.timeout.ms': 15000, 'request.timeout.ms': 15000, 'security.protocol': 'SASL_SSL', 'sasl.mechanisms': 'SCRAM-SHA-512', 'sasl.username': 'starter', 'sasl.password': 'd8zsUzhK9qUZ', 'ssl.ca.location': '/home/active-passive/es-cert.pem'} {'product_id': 'T1', 'description': 'Product 1', 'target_temperature': 6.321923853806639, 'target_humidity_level': 0.4, 'content_type': 1} {'product_id': 'T2', 'description': 'Product 2', 'target_temperature': 4.991504310455889, 'target_humidity_level': 0.4, 'content_type': 1} {'product_id': 'T3', 'description': 'Product 3', 'target_temperature': 4.491634291119919, 'target_humidity_level': 0.4, 'content_type': 1} {'product_id': 'T4', 'description': 'Product 4', 'target_temperature': 2.4855241432802613, 'target_humidity_level': 0.4, 'content_type': 1} {'product_id': 'T5', 'description': 'Product 5', 'target_temperature': 4.286428275499635, 'target_humidity_level': 0.4, 'content_type': 1} {'product_id': 'T6', 'description': 'Product 6', 'target_temperature': 1.6025770613167736, 'target_humidity_level': 0.4, 'content_type': 1} ``` Going to the Event Streams Console we can see the produced messages in the products topic.","title":"Start Producer to source cluster"},{"location":"use-cases/kafka-mm2/lab-3/#start-the-consumer-to-source-cluster","text":"To simulate the offset mapping between source and target, we will use a python consumer and read only n records. shell ./receiveProductSrc.sh 20 The trace may look like: ``` --------- Start Consuming products -------------- [KafkaConsumer] - This is the configuration for the consumer: [KafkaConsumer] - ------------------------------------------- [KafkaConsumer] - Bootstrap Server: light-es-kafka-bootstrap-eventstreams.gse-eda-2020-10-3-0143c5dd31acd8e030a1d6e0ab1380e3-0000.us-east.containers.appdomain.cloud:443 [KafkaConsumer] - Topic: products [KafkaConsumer] - Topic timeout: 10 [KafkaConsumer] - Security Protocol: SASL_SSL [KafkaConsumer] - SASL Mechanism: SCRAM-SHA-512 [KafkaConsumer] - SASL Username: starter [KafkaConsumer] - SASL Password: d*****Z [KafkaConsumer] - SSL CA Location: /home/active-passive/es-cert.pem [KafkaConsumer] - Offset Reset: earliest [KafkaConsumer] - Autocommit: True [KafkaConsumer] - ------------------------------------------- [KafkaConsumer] - Next Message consumed from products partition: [0] at offset: 0 key: P01 value: {\"product_id\": \"P01\", \"description\": \"Carrots\", \"target_temperature\": 4, \"target_humidity_level\": 0.4, \"content_type\": 1} [KafkaConsumer] - Next Message consumed from products partition: [0] at offset: 1 key: P02 value: {\"product_id\": \"P02\", \"description\": \"Banana\", \"target_temperature\": 6, \"target_humidity_level\": 0.6, \"content_type\": 2} [KafkaConsumer] - Next Message consumed from products partition: [0] at offset: 2 key: P03 value: {\"product_id\": \"P03\", \"description\": \"Salad\", \"target_temperature\": 4, \"target_humidity_level\": 0.4, \"content_type\": 1} [KafkaConsumer] - Next Message consumed from products partition: [0] at offset: 3 key: P04 value: {\"product_id\": \"P04\", \"description\": \"Avocado\", \"target_temperature\": 6, \"target_humidity_level\": 0.4, \"content_type\": 1} [KafkaConsumer] - Next Message consumed from products partition: [0] at offset: 4 key: P05 value: {\"product_id\": \"P05\", \"description\": \"Tomato\", \"target_temperature\": 4, \"target_humidity_level\": 0.4, \"content_type\": 2} [KafkaConsumer] - Next Message consumed from products partition: [0] at offset: 5 key: T1 value: {\"product_id\": \"T1\", \"description\": \"Product 1\", \"target_temperature\": 6.321923853806639, \"target_humidity_level\": 0.4, \"content_type\": 1} [KafkaConsumer] - Next Message consumed from products partition: [0] at offset: 6 ``` The python code is ProductConsumer.py . If we go to Event Streams consumer group monitoring user interface we can see the consumer only got 20 messages and so there is an offset lag, as illustrated in figure below:","title":"Start the Consumer to source cluster"},{"location":"use-cases/kafka-mm2/lab-3/#failover-to-target","text":"At this stage, the producer code is not running anymore, Mirror Maker 2 has replicated the data to the target topic named: es-src.products , the consumer has not read all the messages from source cluster. This simulate a crash on source cluster. So let now connect the consumer to the target cluster and continue to process the records. For that the consumer needs to get the offset mapping using the RemoteClusterUtils class to translate the consumer group offset from the source cluster to the corresponding offset for the target cluster.","title":"Failover to target"},{"location":"use-cases/kafka-streams/","text":"Kafka Streams Labs \u00b6 The following hands-on labs walk users through building and testing Kafka Streams-based applications developed with Kafka API and Quarkus. Scenario Description Link Lab 0 Introduction to Kafka Streams application code and test capabilities Kafka Streams - Lab 0 Lab 1 Advanced Kafka Streams test cases and utilizing state stores Kafka Streams - Lab 1 Lab 2 Advanced Kafka Streams test cases and connecting Kafka Streams to IBM Event Streams instances Kafka Streams - Lab 2 Lab 3 Inventory management with Kafka Streams with IBM Event Streams on OpenShift Kafka Streams - Lab 3","title":"Kafka Streams labs"},{"location":"use-cases/kafka-streams/#kafka-streams-labs","text":"The following hands-on labs walk users through building and testing Kafka Streams-based applications developed with Kafka API and Quarkus. Scenario Description Link Lab 0 Introduction to Kafka Streams application code and test capabilities Kafka Streams - Lab 0 Lab 1 Advanced Kafka Streams test cases and utilizing state stores Kafka Streams - Lab 1 Lab 2 Advanced Kafka Streams test cases and connecting Kafka Streams to IBM Event Streams instances Kafka Streams - Lab 2 Lab 3 Inventory management with Kafka Streams with IBM Event Streams on OpenShift Kafka Streams - Lab 3","title":"Kafka Streams Labs"},{"location":"use-cases/kafka-streams/lab-0/","text":"Kafka Streams Test Lab 0 \u00b6 An introduction to using test Kafka Streams Test Suite to test Kafka Streams Topologies. Info Updated 03/10/2022 Overview \u00b6 We are testing a Kafka Streams topology using Apache Kafka Streams TestDriver. The topology While using the TestDriver we will perform basic stateless operations and understand the testing infrastructure. The code for this lab is in this repository eda-kstreams-labs folder kstream-lab0 Scenario Prerequisites \u00b6 Java For the purposes of this lab we suggest Java 8+ Quarkus CLI Maven Maven will be needed for bootstrapping our application from the command-line and running our application. An IDE of your choice Ideally an IDE that supports Quarkus (such as Visual Studio Code) Setting up the Quarkus Application \u00b6 We will bootstrap the Quarkus application with the following command quarkus create kstream-lab0 Since we will be using the Kafka Streams testing functionality, we will need to edit the pom.xml to add the dependency to our project. Open pom.xml and add the following: <dependency> <groupId> org.apache.kafka </groupId> <artifactId> kafka-streams </artifactId> <version> 3.1.0 </version> </dependency> <dependency> <groupId> org.apache.kafka </groupId> <artifactId> kafka-streams-test-utils </artifactId> <version> 3.1.0 </version> <scope> test </scope> </dependency> <dependency> <groupId> org.hamcrest </groupId> <artifactId> hamcrest </artifactId> <version> 2.2 </version> </dependency> The last dependency is for the hamcrest Domain Specific Language for test assertion. Creating your first Test Class \u00b6 Now let's create our first Test Class. Create the directory structure you will need for your Java file. ( NOTE: If you are working in an IDE, this may be done for you when you create your package and classes.) mkdir -p src/test/java/ibm/eda/kstreams/lab0 Create a new file named src/test/java/eda/kstreams/lab0/FirstKafkaStreamsTest.java . Paste the following content into the FirstKafkaStreamsTest class: package eda.kafka.streams ; import static org.hamcrest.CoreMatchers.equalTo ; import static org.hamcrest.CoreMatchers.is ; import static org.hamcrest.MatcherAssert.assertThat ; import java.util.Properties ; import org.apache.kafka.common.serialization.Serdes ; import org.apache.kafka.common.serialization.StringDeserializer ; import org.apache.kafka.common.serialization.StringSerializer ; import org.apache.kafka.streams.StreamsBuilder ; import org.apache.kafka.streams.StreamsConfig ; import org.apache.kafka.streams.TestInputTopic ; import org.apache.kafka.streams.TestOutputTopic ; import org.apache.kafka.streams.Topology ; import org.apache.kafka.streams.TopologyTestDriver ; import org.apache.kafka.streams.kstream.Consumed ; import org.apache.kafka.streams.kstream.KStream ; import org.junit.jupiter.api.AfterEach ; import org.junit.jupiter.api.BeforeEach ; import org.junit.jupiter.api.Test ; import io.quarkus.test.junit.QuarkusTest ; @QuarkusTest public class FirstKafkaStreamsTest { private static TopologyTestDriver testDriver ; private static String inTopicName = \"my-input-topic\" ; private static String outTopicName = \"my-output-topic\" ; private static TestInputTopic < String , String > inTopic ; private static TestOutputTopic < String , String > outTopic ; @BeforeEach public void buildTopology () { final Properties props = new Properties (); props . put ( StreamsConfig . APPLICATION_ID_CONFIG , \"kstream-lab0\" ); props . put ( StreamsConfig . BOOTSTRAP_SERVERS_CONFIG , \"dummmy:2345\" ); props . put ( StreamsConfig . DEFAULT_KEY_SERDE_CLASS_CONFIG , Serdes . String (). getClass ()); props . put ( StreamsConfig . DEFAULT_VALUE_SERDE_CLASS_CONFIG , Serdes . String (). getClass ()); final StreamsBuilder builder = new StreamsBuilder (); KStream < String , String > basicColors = builder . stream ( inTopicName , Consumed . with ( Serdes . String (), Serdes . String ())); basicColors . peek (( key , value ) -> System . out . println ( \"PRE-FILTER: key=\" + key + \", value=\" + value )) . filter (( key , value ) -> ( \"BLUE\" . equalsIgnoreCase ( value ))) . peek (( key , value ) -> System . out . println ( \"POST-FILTER: key=\" + key + \", value=\" + value )) . to ( outTopicName ); Topology topology = builder . build (); testDriver = new TopologyTestDriver ( topology , props ); inTopic = testDriver . createInputTopic ( inTopicName , new StringSerializer (), new StringSerializer ()); outTopic = testDriver . createOutputTopic ( outTopicName , new StringDeserializer (), new StringDeserializer ()); } @AfterEach public void teardown () { testDriver . close (); } } The above code does a lot in a few lines, so we'll walk through some of that here. The @BeforeEach annotation on the buildTopology method means that it will be run each time before each test is executed, while the @AfterEach annotation on the teardown method ensures that it will be run each time after each test execution. This allows us to spin up and tear down all the necessary components to test in isolation with each test case. The buildTopology method utilizes the StreamsBuilder class to construct a simple topology, reading from the input Kafka topic defined by the inTopicName String. The topology, we build here, utilizes three of the stateless processors the Kafka Streams API: peek allows us to look at the key and the value of the record passing through the stream and continue processing it unaffected (so we leverage this before and after the next processor used to see what is making its way through the topology) filter allows us to drop records that do not meet the criteria specified (either for the key or the value) . In this test class, we are filtering on any value that does not match the word \"BLUE\" (using a case-insensitive search) to is the final processor used and to write the contents of the topology at that point to an output Kafka topic The Kafka Streams Test infrastructure provides us the capability to leverage driver classes that function as their own input and output topics, removing the need from connecting directly to a live Kafka instance. The inTopic and outTopic instantiation at the bottom of the buildTopology method hooks into this test infrastructure, so that our test methods can use them to write to and read from the topology. The teardown method cleans up the topology and all the data that has been sent through it for any given test run, allowing us to reset and rerun test cases as needed. Build the application by running the following: ./mvnw clean verify You should see output similar to the following: ... [INFO] [INFO] ------------------------------------------------------- [INFO] T E S T S [INFO] ------------------------------------------------------- [INFO] [INFO] Results: [INFO] [INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0 [INFO] ... [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 29.470 s [INFO] Finished at: 2020-09-17T09:34:26-05:00 [INFO] ------------------------------------------------------------------------ The build compiled and the test topology was successfully created. But no tests were run, because no tests were written! Add your first Tests \u00b6 Open src/test/java/eda/kafka/streams/FirstKafkaStreamsTest.java and add the following tests to the bottom of the FirstKafkaStreamsTest class: @Test public void isEmpty () { assertThat ( outTopic . isEmpty (), is ( true )); } @Test public void isNotEmpty () { assertThat ( outTopic . isEmpty (), is ( true )); inTopic . pipeInput ( \"C01\" , \"blue\" ); assertThat ( outTopic . getQueueSize (), equalTo ( 1L ) ); assertThat ( outTopic . readValue (), equalTo ( \"blue\" )); assertThat ( outTopic . getQueueSize (), equalTo ( 0 L ) ); } @Test public void selectBlues () { assertThat ( outTopic . isEmpty (), is ( true )); inTopic . pipeInput ( \"C01\" , \"blue\" ); inTopic . pipeInput ( \"C02\" , \"red\" ); inTopic . pipeInput ( \"C03\" , \"green\" ); inTopic . pipeInput ( \"C04\" , \"Blue\" ); assertThat ( outTopic . getQueueSize (), equalTo ( 2L ) ); assertThat ( outTopic . isEmpty (), is ( false )); assertThat ( outTopic . readValue (), equalTo ( \"blue\" )); assertThat ( outTopic . readValue (), equalTo ( \"Blue\" )); assertThat ( outTopic . getQueueSize (), equalTo ( 0 L ) ); } These are three simple tests: The isEmpty test method checks to make sure the output topic is empty when nothing is sent through the topology The isNotEmpty test method checks to make sure the output topic is not empty when an item matching our filters is sent through the topology The selectBlues test method checks to make sure that our topology is filtering correctly when we send multiple items through the topology and the output topic empties correctly when the testing infrastructure reads from it. You should see the tests pass with the following output: [ INFO ] [ INFO ] ------------------------------------------------------- [ INFO ] T E S T S [ INFO ] ------------------------------------------------------- [ INFO ] Running eda.kafka.streams.FirstKafkaStreamsTest 2020 -09-17 09 :44:33,247 INFO [ io.sma.rea.mes.provider ] ( main ) SRMSG00208: Deployment done ... start processing 2020 -09-17 09 :44:33,250 INFO [ io.sma.rea.mes.provider ] ( main ) SRMSG00226: Found incoming connectors: [ smallrye-kafka ] 2020 -09-17 09 :44:33,251 INFO [ io.sma.rea.mes.provider ] ( main ) SRMSG00227: Found outgoing connectors: [ smallrye-kafka ] 2020 -09-17 09 :44:33,252 INFO [ io.sma.rea.mes.provider ] ( main ) SRMSG00229: Channel manager initializing... 2020 -09-17 09 :44:33,254 INFO [ io.sma.rea.mes.provider ] ( main ) SRMSG00209: Initializing mediators 2020 -09-17 09 :44:33,255 INFO [ io.sma.rea.mes.provider ] ( main ) SRMSG00215: Connecting mediators 2020 -09-17 09 :44:33,382 INFO [ io.quarkus ] ( main ) Quarkus 1 .8.0.Final on JVM started in 2 .029s. Listening on: http://0.0.0.0:8081 2020 -09-17 09 :44:33,382 INFO [ io.quarkus ] ( main ) Profile test activated. 2020 -09-17 09 :44:33,382 INFO [ io.quarkus ] ( main ) Installed features: [ cdi, kafka-streams, mutiny, resteasy-jsonb, smallrye-context-propagation, smallrye-reactive-messaging, smallrye-reactive-messaging-kafka, vertx ] PRE-FILTER: key = C01, value = blue POST-FILTER: key = C01, value = blue PRE-FILTER: key = C02, value = red PRE-FILTER: key = C03, value = green PRE-FILTER: key = C04, value = Blue POST-FILTER: key = C04, value = Blue PRE-FILTER: key = C01, value = blue POST-FILTER: key = C01, value = blue [ INFO ] Tests run: 3 , Failures: 0 , Errors: 0 , Skipped: 0 , Time elapsed: 4 .722 s - in eda.kafka.streams.FirstKafkaStreamsTest 2020 -09-17 09 :44:34,026 INFO [ io.sma.rea.mes.provider ] ( main ) SRMSG00207: Cancel subscriptions 2020 -09-17 09 :44:34,038 INFO [ io.quarkus ] ( main ) Quarkus stopped in 0 .024s [ INFO ] [ INFO ] Results: [ INFO ] [ INFO ] Tests run: 3 , Failures: 0 , Errors: 0 , Skipped: 0 [ INFO ] [ INFO ] Next Steps \u00b6 Now that you have finished the foundational Kafka Streams testing lab, you can proceed to Lab 1 for a deeper dive into more robust real-world Kafka Streams testing use cases!","title":"Kafka Streams Test Lab 0"},{"location":"use-cases/kafka-streams/lab-0/#kafka-streams-test-lab-0","text":"An introduction to using test Kafka Streams Test Suite to test Kafka Streams Topologies. Info Updated 03/10/2022","title":"Kafka Streams Test Lab 0"},{"location":"use-cases/kafka-streams/lab-0/#overview","text":"We are testing a Kafka Streams topology using Apache Kafka Streams TestDriver. The topology While using the TestDriver we will perform basic stateless operations and understand the testing infrastructure. The code for this lab is in this repository eda-kstreams-labs folder kstream-lab0","title":"Overview"},{"location":"use-cases/kafka-streams/lab-0/#scenario-prerequisites","text":"Java For the purposes of this lab we suggest Java 8+ Quarkus CLI Maven Maven will be needed for bootstrapping our application from the command-line and running our application. An IDE of your choice Ideally an IDE that supports Quarkus (such as Visual Studio Code)","title":"Scenario Prerequisites"},{"location":"use-cases/kafka-streams/lab-0/#setting-up-the-quarkus-application","text":"We will bootstrap the Quarkus application with the following command quarkus create kstream-lab0 Since we will be using the Kafka Streams testing functionality, we will need to edit the pom.xml to add the dependency to our project. Open pom.xml and add the following: <dependency> <groupId> org.apache.kafka </groupId> <artifactId> kafka-streams </artifactId> <version> 3.1.0 </version> </dependency> <dependency> <groupId> org.apache.kafka </groupId> <artifactId> kafka-streams-test-utils </artifactId> <version> 3.1.0 </version> <scope> test </scope> </dependency> <dependency> <groupId> org.hamcrest </groupId> <artifactId> hamcrest </artifactId> <version> 2.2 </version> </dependency> The last dependency is for the hamcrest Domain Specific Language for test assertion.","title":"Setting up the Quarkus Application"},{"location":"use-cases/kafka-streams/lab-0/#creating-your-first-test-class","text":"Now let's create our first Test Class. Create the directory structure you will need for your Java file. ( NOTE: If you are working in an IDE, this may be done for you when you create your package and classes.) mkdir -p src/test/java/ibm/eda/kstreams/lab0 Create a new file named src/test/java/eda/kstreams/lab0/FirstKafkaStreamsTest.java . Paste the following content into the FirstKafkaStreamsTest class: package eda.kafka.streams ; import static org.hamcrest.CoreMatchers.equalTo ; import static org.hamcrest.CoreMatchers.is ; import static org.hamcrest.MatcherAssert.assertThat ; import java.util.Properties ; import org.apache.kafka.common.serialization.Serdes ; import org.apache.kafka.common.serialization.StringDeserializer ; import org.apache.kafka.common.serialization.StringSerializer ; import org.apache.kafka.streams.StreamsBuilder ; import org.apache.kafka.streams.StreamsConfig ; import org.apache.kafka.streams.TestInputTopic ; import org.apache.kafka.streams.TestOutputTopic ; import org.apache.kafka.streams.Topology ; import org.apache.kafka.streams.TopologyTestDriver ; import org.apache.kafka.streams.kstream.Consumed ; import org.apache.kafka.streams.kstream.KStream ; import org.junit.jupiter.api.AfterEach ; import org.junit.jupiter.api.BeforeEach ; import org.junit.jupiter.api.Test ; import io.quarkus.test.junit.QuarkusTest ; @QuarkusTest public class FirstKafkaStreamsTest { private static TopologyTestDriver testDriver ; private static String inTopicName = \"my-input-topic\" ; private static String outTopicName = \"my-output-topic\" ; private static TestInputTopic < String , String > inTopic ; private static TestOutputTopic < String , String > outTopic ; @BeforeEach public void buildTopology () { final Properties props = new Properties (); props . put ( StreamsConfig . APPLICATION_ID_CONFIG , \"kstream-lab0\" ); props . put ( StreamsConfig . BOOTSTRAP_SERVERS_CONFIG , \"dummmy:2345\" ); props . put ( StreamsConfig . DEFAULT_KEY_SERDE_CLASS_CONFIG , Serdes . String (). getClass ()); props . put ( StreamsConfig . DEFAULT_VALUE_SERDE_CLASS_CONFIG , Serdes . String (). getClass ()); final StreamsBuilder builder = new StreamsBuilder (); KStream < String , String > basicColors = builder . stream ( inTopicName , Consumed . with ( Serdes . String (), Serdes . String ())); basicColors . peek (( key , value ) -> System . out . println ( \"PRE-FILTER: key=\" + key + \", value=\" + value )) . filter (( key , value ) -> ( \"BLUE\" . equalsIgnoreCase ( value ))) . peek (( key , value ) -> System . out . println ( \"POST-FILTER: key=\" + key + \", value=\" + value )) . to ( outTopicName ); Topology topology = builder . build (); testDriver = new TopologyTestDriver ( topology , props ); inTopic = testDriver . createInputTopic ( inTopicName , new StringSerializer (), new StringSerializer ()); outTopic = testDriver . createOutputTopic ( outTopicName , new StringDeserializer (), new StringDeserializer ()); } @AfterEach public void teardown () { testDriver . close (); } } The above code does a lot in a few lines, so we'll walk through some of that here. The @BeforeEach annotation on the buildTopology method means that it will be run each time before each test is executed, while the @AfterEach annotation on the teardown method ensures that it will be run each time after each test execution. This allows us to spin up and tear down all the necessary components to test in isolation with each test case. The buildTopology method utilizes the StreamsBuilder class to construct a simple topology, reading from the input Kafka topic defined by the inTopicName String. The topology, we build here, utilizes three of the stateless processors the Kafka Streams API: peek allows us to look at the key and the value of the record passing through the stream and continue processing it unaffected (so we leverage this before and after the next processor used to see what is making its way through the topology) filter allows us to drop records that do not meet the criteria specified (either for the key or the value) . In this test class, we are filtering on any value that does not match the word \"BLUE\" (using a case-insensitive search) to is the final processor used and to write the contents of the topology at that point to an output Kafka topic The Kafka Streams Test infrastructure provides us the capability to leverage driver classes that function as their own input and output topics, removing the need from connecting directly to a live Kafka instance. The inTopic and outTopic instantiation at the bottom of the buildTopology method hooks into this test infrastructure, so that our test methods can use them to write to and read from the topology. The teardown method cleans up the topology and all the data that has been sent through it for any given test run, allowing us to reset and rerun test cases as needed. Build the application by running the following: ./mvnw clean verify You should see output similar to the following: ... [INFO] [INFO] ------------------------------------------------------- [INFO] T E S T S [INFO] ------------------------------------------------------- [INFO] [INFO] Results: [INFO] [INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0 [INFO] ... [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 29.470 s [INFO] Finished at: 2020-09-17T09:34:26-05:00 [INFO] ------------------------------------------------------------------------ The build compiled and the test topology was successfully created. But no tests were run, because no tests were written!","title":"Creating your first Test Class"},{"location":"use-cases/kafka-streams/lab-0/#add-your-first-tests","text":"Open src/test/java/eda/kafka/streams/FirstKafkaStreamsTest.java and add the following tests to the bottom of the FirstKafkaStreamsTest class: @Test public void isEmpty () { assertThat ( outTopic . isEmpty (), is ( true )); } @Test public void isNotEmpty () { assertThat ( outTopic . isEmpty (), is ( true )); inTopic . pipeInput ( \"C01\" , \"blue\" ); assertThat ( outTopic . getQueueSize (), equalTo ( 1L ) ); assertThat ( outTopic . readValue (), equalTo ( \"blue\" )); assertThat ( outTopic . getQueueSize (), equalTo ( 0 L ) ); } @Test public void selectBlues () { assertThat ( outTopic . isEmpty (), is ( true )); inTopic . pipeInput ( \"C01\" , \"blue\" ); inTopic . pipeInput ( \"C02\" , \"red\" ); inTopic . pipeInput ( \"C03\" , \"green\" ); inTopic . pipeInput ( \"C04\" , \"Blue\" ); assertThat ( outTopic . getQueueSize (), equalTo ( 2L ) ); assertThat ( outTopic . isEmpty (), is ( false )); assertThat ( outTopic . readValue (), equalTo ( \"blue\" )); assertThat ( outTopic . readValue (), equalTo ( \"Blue\" )); assertThat ( outTopic . getQueueSize (), equalTo ( 0 L ) ); } These are three simple tests: The isEmpty test method checks to make sure the output topic is empty when nothing is sent through the topology The isNotEmpty test method checks to make sure the output topic is not empty when an item matching our filters is sent through the topology The selectBlues test method checks to make sure that our topology is filtering correctly when we send multiple items through the topology and the output topic empties correctly when the testing infrastructure reads from it. You should see the tests pass with the following output: [ INFO ] [ INFO ] ------------------------------------------------------- [ INFO ] T E S T S [ INFO ] ------------------------------------------------------- [ INFO ] Running eda.kafka.streams.FirstKafkaStreamsTest 2020 -09-17 09 :44:33,247 INFO [ io.sma.rea.mes.provider ] ( main ) SRMSG00208: Deployment done ... start processing 2020 -09-17 09 :44:33,250 INFO [ io.sma.rea.mes.provider ] ( main ) SRMSG00226: Found incoming connectors: [ smallrye-kafka ] 2020 -09-17 09 :44:33,251 INFO [ io.sma.rea.mes.provider ] ( main ) SRMSG00227: Found outgoing connectors: [ smallrye-kafka ] 2020 -09-17 09 :44:33,252 INFO [ io.sma.rea.mes.provider ] ( main ) SRMSG00229: Channel manager initializing... 2020 -09-17 09 :44:33,254 INFO [ io.sma.rea.mes.provider ] ( main ) SRMSG00209: Initializing mediators 2020 -09-17 09 :44:33,255 INFO [ io.sma.rea.mes.provider ] ( main ) SRMSG00215: Connecting mediators 2020 -09-17 09 :44:33,382 INFO [ io.quarkus ] ( main ) Quarkus 1 .8.0.Final on JVM started in 2 .029s. Listening on: http://0.0.0.0:8081 2020 -09-17 09 :44:33,382 INFO [ io.quarkus ] ( main ) Profile test activated. 2020 -09-17 09 :44:33,382 INFO [ io.quarkus ] ( main ) Installed features: [ cdi, kafka-streams, mutiny, resteasy-jsonb, smallrye-context-propagation, smallrye-reactive-messaging, smallrye-reactive-messaging-kafka, vertx ] PRE-FILTER: key = C01, value = blue POST-FILTER: key = C01, value = blue PRE-FILTER: key = C02, value = red PRE-FILTER: key = C03, value = green PRE-FILTER: key = C04, value = Blue POST-FILTER: key = C04, value = Blue PRE-FILTER: key = C01, value = blue POST-FILTER: key = C01, value = blue [ INFO ] Tests run: 3 , Failures: 0 , Errors: 0 , Skipped: 0 , Time elapsed: 4 .722 s - in eda.kafka.streams.FirstKafkaStreamsTest 2020 -09-17 09 :44:34,026 INFO [ io.sma.rea.mes.provider ] ( main ) SRMSG00207: Cancel subscriptions 2020 -09-17 09 :44:34,038 INFO [ io.quarkus ] ( main ) Quarkus stopped in 0 .024s [ INFO ] [ INFO ] Results: [ INFO ] [ INFO ] Tests run: 3 , Failures: 0 , Errors: 0 , Skipped: 0 [ INFO ] [ INFO ]","title":"Add your first Tests"},{"location":"use-cases/kafka-streams/lab-0/#next-steps","text":"Now that you have finished the foundational Kafka Streams testing lab, you can proceed to Lab 1 for a deeper dive into more robust real-world Kafka Streams testing use cases!","title":"Next Steps"},{"location":"use-cases/kafka-streams/lab-1/","text":"Kafka Streams Test Lab 1 \u00b6 Updated 03/10/2022 Overview \u00b6 In this lab scenario we are still using Apache Kafka Streams TestDriver to test a Topology, a Stream and Table. While using the TestDriver we will perform operations such as groupBy, join with another Stream or Kafka Table. The code for this lab is in this repository eda-kstreams-labs folder LoadKtableFromTopic Scenario Prerequisites \u00b6 Java For the purposes of this lab we suggest Java 11+ Quarkus CLI Maven Maven will be needed for bootstrapping our application from the command-line and running our application. An IDE of your choice Ideally an IDE that supports Quarkus (such as Visual Studio Code) Setting up the Quarkus Application \u00b6 We will bootstrap the Quarkus application with the following Maven command quarkus create LoadKtableFromTopic You can replace the fields within {} as you like. Since we will be using the Kafka Streams testing functionality we will need to edit the pom.xml to add the dependency to our project. Open pom.xml and add the following. <dependency> <groupId> org.apache.kafka </groupId> <artifactId> kafka-streams </artifactId> <version> 3.1.0 </version> </dependency> <dependency> <groupId> org.apache.kafka </groupId> <artifactId> kafka-streams-test-utils </artifactId> <version> 3.1.0 </version> <scope> test </scope> </dependency> <dependency> <groupId> org.hamcrest </groupId> <artifactId> hamcrest </artifactId> <version> 2.2 </version> </dependency> Creating your Test Class \u00b6 Open the TestLoadKtableFromTopic.java file and paste the following content. package ibm.eda.kstreams.lab1 ; import java.util.Properties ; import org.apache.kafka.common.serialization.Serdes ; import org.apache.kafka.common.serialization.StringSerializer ; import org.apache.kafka.streams.KeyValue ; import org.apache.kafka.streams.StreamsBuilder ; import org.apache.kafka.streams.StreamsConfig ; import org.apache.kafka.streams.TestInputTopic ; import org.apache.kafka.streams.TestOutputTopic ; import org.apache.kafka.streams.TopologyTestDriver ; import org.apache.kafka.streams.kstream.Consumed ; import org.apache.kafka.streams.kstream.KTable ; import org.apache.kafka.streams.kstream.Materialized ; import org.apache.kafka.streams.processor.StateStore ; import org.apache.kafka.streams.state.KeyValueBytesStoreSupplier ; import org.apache.kafka.streams.state.KeyValueIterator ; import org.apache.kafka.streams.state.KeyValueStore ; import org.apache.kafka.streams.state.Stores ; import org.apache.kafka.streams.state.ValueAndTimestamp ; import org.junit.jupiter.api.AfterAll ; import org.junit.jupiter.api.Assertions ; import org.junit.jupiter.api.BeforeAll ; import org.junit.jupiter.api.Test ; import io.quarkus.test.junit.QuarkusTest ; /** * This is a simple example of loading some reference data from stream into a Ktable for * lookup. It uses a persistent state store. */ @QuarkusTest public class TestLoadKtableFromTopic { private static TopologyTestDriver testDriver ; private static String companySectorsTopic = \"sector-types\" ; private static String storeName = \"sector-types-store\" ; private static TestInputTopic < String , String > inTopic ; private static TestOutputTopic < String , Long > outTopic ; private static TestOutputTopic < String , String > errorTopic ; public static Properties getStreamsConfig () { final Properties props = new Properties (); props . put ( StreamsConfig . APPLICATION_ID_CONFIG , \"kstream-lab1\" ); props . put ( StreamsConfig . BOOTSTRAP_SERVERS_CONFIG , \"dummmy:1234\" ); return props ; } @BeforeAll public static void buildTopology (){ final StreamsBuilder builder = new StreamsBuilder (); // Adding a state store is a simple matter of creating a StoreSupplier // instance with one of the static factory methods on the Stores class. // all persistent StateStore instances provide local storage using RocksDB KeyValueBytesStoreSupplier storeSupplier = Stores . persistentKeyValueStore ( storeName ); // A KTable is created from the companySectorsTopic, with key and value deserialized. // With Materialized.as() causing the Table to force a state store materialization (storeSupplier). KTable < String , String > sectorTypeTable = builder . table ( companySectorsTopic , Consumed . with ( Serdes . String (), Serdes . String ()), Materialized . as ( storeSupplier )); testDriver = new TopologyTestDriver ( builder . build (), getStreamsConfig ()); inTopic = testDriver . createInputTopic ( companySectorsTopic , new StringSerializer (), new StringSerializer ()); } @AfterAll public static void close (){ testDriver . close (); } @Test public void shouldHaveSixSectorTypes (){ inTopic . pipeInput ( \"C01\" , \"Health Care\" ); inTopic . pipeInput ( \"C02\" , \"Finance\" ); inTopic . pipeInput ( \"C03\" , \"Consumer Services\" ); inTopic . pipeInput ( \"C04\" , \"Transportation\" ); inTopic . pipeInput ( \"C05\" , \"Capital Goods\" ); inTopic . pipeInput ( \"C06\" , \"Public Utilities\" ); KeyValueStore < String , ValueAndTimestamp < String >> store = testDriver . getTimestampedKeyValueStore ( storeName ); Assertions . assertNotNull ( store ); ValueAndTimestamp < String > sector = store . get ( \"C02\" ); Assertions . assertNotNull ( sector ); Assertions . assertEquals ( \"Finance\" , sector . value ()); Assertions . assertEquals ( 6 , store . approximateNumEntries ()); // demonstrate how to get all the values from the table: KeyValueIterator < String , ValueAndTimestamp < String >> sectors = store . all (); while ( sectors . hasNext ()) { KeyValue < String , ValueAndTimestamp < String >> s = sectors . next (); System . out . println ( s . key + \":\" + s . value . value ()); } for ( StateStore s : testDriver . getAllStateStores (). values ()) { System . out . println ( s . name ()); } } } The above code uses TopologyTestDriver to mimic a Topology. A Topology is basically a graph of stream processors (nodes) and the edges between these nodes are the streams. In the first section we instantiate our TopologyTestDriver named testDriver , as well as the topic name and store name. Test the application by running the following: ./mvnw clean verify You should see the tests pass with the following output: [ INFO ] ------------------------------------------------------- [ INFO ] T E S T S [ INFO ] ------------------------------------------------------- [ INFO ] Running com.ibm.GreetingResourceTest 2021 -01-16 14 :20:26,488 INFO [ io.quarkus ] ( main ) Quarkus 1 .10.5.Final on JVM started in 2 .089s. Listening on: http://localhost:8081 2021 -01-16 14 :20:26,490 INFO [ io.quarkus ] ( main ) Profile test activated. 2021 -01-16 14 :20:26,490 INFO [ io.quarkus ] ( main ) Installed features: [ cdi, kafka-streams, resteasy, resteasy-jsonb ] [ INFO ] Tests run: 1 , Failures: 0 , Errors: 0 , Skipped: 0 , Time elapsed: 6 .096 s - in com.ibm.GreetingResourceTest [ INFO ] Running com.ibm.garage.cpat.lab.TestLoadKtableFromTopic C01:Health Care C02:Finance C03:Consumer Services C04:Transportation C05:Capital Goods C06:Public Utilities sector-types-store 2021 -01-16 14 :20:28,253 WARN [ org.apa.kaf.str.sta.int.RocksDBStore ] ( main ) Closing 1 open iterators for store sector-types-store 2021 -01-16 14 :20:28,256 INFO [ org.apa.kaf.str.pro.int.StateDirectory ] ( main ) stream-thread [ main ] Deleting state directory 0_0 for task 0_0 as user calling cleanup. [ INFO ] Tests run: 1 , Failures: 0 , Errors: 0 , Skipped: 0 , Time elapsed: 0 .222 s - in com.ibm.garage.cpat.lab.TestLoadKtableFromTopic 2021 -01-16 14 :20:28,292 INFO [ io.quarkus ] ( main ) Quarkus stopped in 0 .028s [ INFO ] [ INFO ] Results: [ INFO ] [ INFO ] Tests run: 2 , Failures: 0 , Errors: 0 , Skipped: 0 How this test topology creation flow works: A StreamsBuilder object (builder) from the Kafka Streams DSL API is created. A KeyValueBytesStoreSupplier (storeSupplier) is configured with String variable (storeName). A KTable is created reading from the topic (companySectorsTopic), deserialized and materialized as the previously create (storeSupplier). A TopologyTestDriver (testDriver) is built from the provided config properties and the KTable within the builder topology. Lastly test input topic (inTopic) is created from the testDriver topology. When inTopic.pipeInput(\"C01\",\"Health Care\"); is invoked, it populates the topic, which then populates the KTable which ultimately persists in a KeyValue State Store. You should see the tests pass. These are three simple tests. The first of which checks that the value fetched from the Kafka Table is not null,the second makes sure that value retrieved from key C02 is equal to Finance and lastly we make sure that the our state store (which was piped by ways of the Kafka Topic) indeed has six key-value pairs. More Robust Kafka Streams Testing \u00b6 add jsonb Serdes using Quarkus kafka client library: quarkus ext add kafka-client jsonb Now that we have tested some simple functionality by using the Kafka Streams API let's check out some other operators that we can use. Let's create a new class for our Plain Old Java Object (POJO) named FinancialMessage and copy and paste the following content into the newly created file. package ibm.eda.kstreams.lab.domain ; import io.quarkus.runtime.annotations.RegisterForReflection ; @RegisterForReflection public class FinancialMessage implements JSONSerdeCompatible { public String userId ; public String stockSymbol ; public String exchangeId ; public int quantity ; public double stockPrice ; public double totalCost ; public int institutionId ; public int countryId ; public boolean technicalValidation ; public FinancialMessage () { } public FinancialMessage ( String userId , String stockSymbol , String exchangeId , int quantity , double stockPrice , double totalCost , int institutionId , int countryId , boolean technicalValidation ) { this . userId = userId ; this . stockSymbol = stockSymbol ; this . exchangeId = exchangeId ; this . quantity = quantity ; this . stockPrice = stockPrice ; this . totalCost = totalCost ; this . institutionId = institutionId ; this . countryId = countryId ; this . technicalValidation = technicalValidation ; } } Note: We have not provided any accessors (getters) or mutators (setters) for simplicity. You can set those at your own discretion. Add the following interface for JSon Serde /** * An interface for registering types that can be de/serialized with {@link JSONSerde}. */ @SuppressWarnings ( \"DefaultAnnotationParam\" ) // being explicit for the example @JsonTypeInfo ( use = JsonTypeInfo . Id . NAME , include = JsonTypeInfo . As . PROPERTY , property = \"_t\" ) @JsonSubTypes ({ @JsonSubTypes.Type ( value = FinancialMessage . class , name = \"fm\" ) }) public interface JSONSerdeCompatible { } And the Generic JSONSerde from this class Now that we have our new Java class, let's create a new and separate Java Test class: src/test/java/ibm/eda/kstreams/lab1/TestFinancialMessage.java . Copy the contents below: package ibm.eda.kstreams.lab1 ; import java.util.Properties ; import org.apache.kafka.common.serialization.LongDeserializer ; import org.apache.kafka.common.serialization.Serde ; import org.apache.kafka.common.serialization.Serdes ; import org.apache.kafka.common.serialization.StringDeserializer ; import org.apache.kafka.common.serialization.StringSerializer ; import org.apache.kafka.streams.KeyValue ; import org.apache.kafka.streams.StreamsBuilder ; import org.apache.kafka.streams.StreamsConfig ; import org.apache.kafka.streams.TestInputTopic ; import org.apache.kafka.streams.TestOutputTopic ; import org.apache.kafka.streams.TopologyTestDriver ; import org.apache.kafka.streams.kstream.Consumed ; import org.apache.kafka.streams.kstream.KStream ; import org.apache.kafka.streams.kstream.Materialized ; import org.apache.kafka.streams.kstream.Produced ; import org.apache.kafka.streams.kstream.Windowed ; import org.apache.kafka.streams.kstream.WindowedSerdes ; import org.apache.kafka.streams.state.KeyValueBytesStoreSupplier ; import org.apache.kafka.streams.state.KeyValueStore ; import org.apache.kafka.streams.state.Stores ; import org.apache.kafka.streams.state.ValueAndTimestamp ; import org.junit.jupiter.api.AfterAll ; import org.junit.jupiter.api.Assertions ; import org.junit.jupiter.api.BeforeAll ; import org.junit.jupiter.api.Test ; import io.quarkus.kafka.client.serialization.JsonbSerde ; import io.quarkus.kafka.client.serialization.JsonbSerializer ; import io.quarkus.test.junit.QuarkusTest ; import com.ibm.garage.cpat.domain.* ; @QuarkusTest public class TestFinancialMessage { private static TopologyTestDriver testDriver ; private static String inTopicName = \"transactions\" ; private static String outTopicName = \"output\" ; private static String errorTopicName = \"errors\" ; private static String storeName = \"transactionCount\" ; private static TestInputTopic < String , FinancialMessage > inTopic ; private static TestOutputTopic < String , Long > outTopic ; private static TestOutputTopic < String , String > errorTopic ; public static Properties getStreamsConfig () { final Properties props = new Properties (); props . put ( StreamsConfig . APPLICATION_ID_CONFIG , \"kstream-lab2\" ); props . put ( StreamsConfig . BOOTSTRAP_SERVERS_CONFIG , \"dummmy:2345\" ); props . put ( StreamsConfig . DEFAULT_KEY_SERDE_CLASS_CONFIG , Serdes . String (). getClass ()); props . put ( StreamsConfig . DEFAULT_VALUE_SERDE_CLASS_CONFIG , JSONSerde . class ); return props ; } @BeforeAll public static void buildTopology () { final StreamsBuilder builder = new StreamsBuilder (); KeyValueBytesStoreSupplier storeSupplier = Stores . persistentKeyValueStore ( storeName ); KStream < String , FinancialMessage > transactionStream = builder . stream ( inTopicName , Consumed . with ( Serdes . String (), financialMessageSerde ) ); // First verify user id is present, if not route to error Map < String , KStream < String , FinancialMessage >> branches = transactionStream . split ( Named . as ( \"tx-\" )) . branch (( key , value ) -> value . userId == null , Branched . as ( \"no-userid\" )) . defaultBranch ( Branched . as ( \"non-null\" )); // Handle error by sending to the errors topic. branches . get ( \"tx-no-userid\" ). map ( ( key , value ) -> { return KeyValue . pair ( key , \"No customer id provided\" ); }) . to ( errorTopicName , Produced . with ( Serdes . String (), Serdes . String ())); // use groupBy to swap the key, then count by customer id, branches . get ( \"tx-non-null\" ). groupBy ( ( key , value ) -> value . userId ) . count ( Materialized . as ( storeSupplier ) ) . toStream () . to ( outTopicName , Produced . with ( Serdes . String (), Serdes . Long ()) ); testDriver = new TopologyTestDriver ( builder . build (), getStreamsConfig ()); inTopic = testDriver . createInputTopic ( inTopicName , new StringSerializer (), new JSONSerde < FinancialMessage > ()); outTopic = testDriver . createOutputTopic ( outTopicName , new StringDeserializer (), new LongDeserializer ()); errorTopic = testDriver . createOutputTopic ( errorTopicName , new StringDeserializer (), new StringDeserializer ()); } @AfterAll public static void close (){ testDriver . close (); } } We have the setup for the TestTopology. Now, we can add a test that will insert two events into the topic. Add the following code to your test class: @Test public void shouldHaveOneTransaction () { // A FinancialMessage is mocked and set to the input topic. Within the Topology, // this gets sent to the outTopic because a userId exists for the incoming message. FinancialMessage mock = new FinancialMessage ( \"1\" , \"MET\" , \"SWISS\" , 12 , 1822.38 , 21868.55 , 94 , 7 , true ); FinancialMessage mock2 = new FinancialMessage ( \"2\" , \"ASDF\" , \"HELLO\" , 5 , 1000.22 , 4444.12 , 38 , 6 , true ); inTopic . pipeInput ( \"T01\" , mock ); inTopic . pipeInput ( \"T02\" , mock2 ); Assertions . assertFalse ( outTopic . isEmpty ()); Assertions . assertEquals ( 1 , outTopic . readKeyValue (). value ); KeyValueStore < String , ValueAndTimestamp < FinancialMessage >> store = testDriver . getTimestampedKeyValueStore ( storeName ); Assertions . assertEquals ( 1 , store . approximateNumEntries ()); } Test the application by running the following: ./mvnw clean verify You should see the following output: [ INFO ] ------------------------------------------------------- [ INFO ] T E S T S [ INFO ] ------------------------------------------------------- [ INFO ] Running com.ibm.GreetingResourceTest 2021 -01-16 17 :21:37,836 INFO [ io.quarkus ] ( main ) Quarkus 1 .10.5.Final on JVM started in 1 .996s. Listening on: http://localhost:8081 2021 -01-16 17 :21:37,837 INFO [ io.quarkus ] ( main ) Profile test activated. 2021 -01-16 17 :21:37,838 INFO [ io.quarkus ] ( main ) Installed features: [ cdi, kafka-streams, resteasy, resteasy-jsonb ] [ INFO ] Tests run: 1 , Failures: 0 , Errors: 0 , Skipped: 0 , Time elapsed: 5 .234 s - in com.ibm.GreetingResourceTest [ INFO ] Running com.ibm.garage.cpat.lab.TestFinancialMessage 2021 -01-16 17 :21:39,460 INFO [ org.apa.kaf.str.pro.int.StateDirectory ] ( main ) stream-thread [ main ] Deleting state directory 0_0 for task 0_0 as user calling cleanup. [ ERROR ] Tests run: 1 , Failures: 1 , Errors: 0 , Skipped: 0 , Time elapsed: 0 .273 s <<< FAILURE! - in com.ibm.garage.cpat.lab.TestFinancialMessage [ ERROR ] shouldHaveOneTransaction Time elapsed: 0 .09 s <<< FAILURE! org.opentest4j.AssertionFailedError: expected: < 1 > but was: < 2 > at com.ibm.garage.cpat.lab.TestFinancialMessage.shouldHaveOneTransaction ( TestFinancialMessage.java:132 ) [ INFO ] Running com.ibm.garage.cpat.lab.TestLoadKtableFromTopic C01:Health Care C02:Finance C03:Consumer Services C04:Transportation C05:Capital Goods C06:Public Utilities sector-types-store 2021 -01-16 17 :21:39,505 WARN [ org.apa.kaf.str.sta.int.RocksDBStore ] ( main ) Closing 1 open iterators for store sector-types-store 2021 -01-16 17 :21:39,507 INFO [ org.apa.kaf.str.pro.int.StateDirectory ] ( main ) stream-thread [ main ] Deleting state directory 0_0 for task 0_0 as user calling cleanup. [ INFO ] Tests run: 1 , Failures: 0 , Errors: 0 , Skipped: 0 , Time elapsed: 0 .039 s - in com.ibm.garage.cpat.lab.TestLoadKtableFromTopic 2021 -01-16 17 :21:39,541 INFO [ io.quarkus ] ( main ) Quarkus stopped in 0 .028s [ INFO ] [ INFO ] Results: [ INFO ] [ ERROR ] Failures: [ ERROR ] TestFinancialMessage.shouldHaveOneTransaction:132 expected: < 1 > but was: < 2 > [ INFO ] [ ERROR ] Tests run: 3 , Failures: 1 , Errors: 0 , Skipped: 0 We see that our recently added test failed . And this is expected due to the fact that we inserted two records but our test expects one. To remedy this test we must change Assertions.assertEquals(1, store.approximateNumEntries()); Set to 2 the comparisson. Next let's add another very simple test. Copy the following code to your Java test class: @Test public void testErrorTopicIsNotEmpty () { FinancialMessage mock = new FinancialMessage ( null , \"MET\" , \"SWISS\" , 12 , 1822.38 , 21868.55 , 94 , 7 , true ); inTopic . pipeInput ( \"T03\" , mock ); Assertions . assertFalse ( errorTopic . isEmpty ()); } Test the application by running the following: ./mvnw clean verify You should see the following output: [ INFO ] ------------------------------------------------------- [ INFO ] T E S T S [ INFO ] ------------------------------------------------------- [ INFO ] Running com.ibm.GreetingResourceTest 2021 -01-16 17 :29:34,258 INFO [ io.quarkus ] ( main ) Quarkus 1 .10.5.Final on JVM started in 2 .470s. Listening on: http://localhost:8081 2021 -01-16 17 :29:34,260 INFO [ io.quarkus ] ( main ) Profile test activated. 2021 -01-16 17 :29:34,260 INFO [ io.quarkus ] ( main ) Installed features: [ cdi, kafka-streams, resteasy, resteasy-jsonb ] [ INFO ] Tests run: 1 , Failures: 0 , Errors: 0 , Skipped: 0 , Time elapsed: 5 .694 s - in com.ibm.GreetingResourceTest [ INFO ] Running com.ibm.garage.cpat.lab.TestFinancialMessage 2021 -01-16 17 :29:36,001 INFO [ org.apa.kaf.str.pro.int.StateDirectory ] ( main ) stream-thread [ main ] Deleting state directory 0_0 for task 0_0 as user calling cleanup. [ INFO ] Tests run: 2 , Failures: 0 , Errors: 0 , Skipped: 0 , Time elapsed: 0 .309 s - in com.ibm.garage.cpat.lab.TestFinancialMessage [ INFO ] Running com.ibm.garage.cpat.lab.TestLoadKtableFromTopic C01:Health Care C02:Finance C03:Consumer Services C04:Transportation C05:Capital Goods C06:Public Utilities sector-types-store 2021 -01-16 17 :29:36,057 WARN [ org.apa.kaf.str.sta.int.RocksDBStore ] ( main ) Closing 1 open iterators for store sector-types-store 2021 -01-16 17 :29:36,059 INFO [ org.apa.kaf.str.pro.int.StateDirectory ] ( main ) stream-thread [ main ] Deleting state directory 0_0 for task 0_0 as user calling cleanup. [ INFO ] Tests run: 1 , Failures: 0 , Errors: 0 , Skipped: 0 , Time elapsed: 0 .049 s - in com.ibm.garage.cpat.lab.TestLoadKtableFromTopic 2021 -01-16 17 :29:36,099 INFO [ io.quarkus ] ( main ) Quarkus stopped in 0 .031s [ INFO ] [ INFO ] Results: [ INFO ] [ INFO ] Tests run: 4 , Failures: 0 , Errors: 0 , Skipped: 0 As you can see here, our message payload is created with null for the userId field which means this message will branch out to the errorTopic . The purpose of the test is to check if our errorTopic is empty, which should not be. Since our errorTopic.isEmpty() resolves to false and our assertion is asserting that it is false as well, thus the test passes. Now that we have two simple tests, let's update our first branch to allow us to filter the stream on a condition that we want. Let's edit our branches[1] statement so that it will filter out and retain only the records where the totalCost is greater than 5000. branches [ 1 ] . filter ( ( key , value ) -> ( value . totalCost > 5000 ) ) . groupBy ( ( key , value ) -> value . userId ) . count ( Materialized . as ( storeSupplier ) ) . toStream () . to ( outTopicName , Produced . with ( Serdes . String (), Serdes . Long ()) ); Test the application by running the following: ./mvnw clean verify You should see the following output: [ INFO ] ------------------------------------------------------- [ INFO ] T E S T S [ INFO ] ------------------------------------------------------- [ INFO ] Running com.ibm.GreetingResourceTest 2021 -01-16 17 :40:50,765 INFO [ io.quarkus ] ( main ) Quarkus 1 .10.5.Final on JVM started in 2 .102s. Listening on: http://localhost:8081 2021 -01-16 17 :40:50,766 INFO [ io.quarkus ] ( main ) Profile test activated. 2021 -01-16 17 :40:50,766 INFO [ io.quarkus ] ( main ) Installed features: [ cdi, kafka-streams, resteasy, resteasy-jsonb ] [ INFO ] Tests run: 1 , Failures: 0 , Errors: 0 , Skipped: 0 , Time elapsed: 5 .474 s - in com.ibm.GreetingResourceTest [ INFO ] Running com.ibm.garage.cpat.lab.TestFinancialMessage 2021 -01-16 17 :40:52,393 INFO [ org.apa.kaf.str.pro.int.StateDirectory ] ( main ) stream-thread [ main ] Deleting state directory 0_0 for task 0_0 as user calling cleanup. [ ERROR ] Tests run: 2 , Failures: 1 , Errors: 0 , Skipped: 0 , Time elapsed: 0 .307 s <<< FAILURE! - in com.ibm.garage.cpat.lab.TestFinancialMessage [ ERROR ] shouldHaveOneTransaction Time elapsed: 0 .022 s <<< FAILURE! org.opentest4j.AssertionFailedError: expected: < 2 > but was: < 1 > at com.ibm.garage.cpat.lab.TestFinancialMessage.shouldHaveOneTransaction ( TestFinancialMessage.java:135 ) [ INFO ] Running com.ibm.garage.cpat.lab.TestLoadKtableFromTopic C01:Health Care C02:Finance C03:Consumer Services C04:Transportation C05:Capital Goods C06:Public Utilities sector-types-store 2021 -01-16 17 :40:52,445 WARN [ org.apa.kaf.str.sta.int.RocksDBStore ] ( main ) Closing 1 open iterators for store sector-types-store 2021 -01-16 17 :40:52,447 INFO [ org.apa.kaf.str.pro.int.StateDirectory ] ( main ) stream-thread [ main ] Deleting state directory 0_0 for task 0_0 as user calling cleanup. [ INFO ] Tests run: 1 , Failures: 0 , Errors: 0 , Skipped: 0 , Time elapsed: 0 .046 s - in com.ibm.garage.cpat.lab.TestLoadKtableFromTopic 2021 -01-16 17 :40:52,487 INFO [ io.quarkus ] ( main ) Quarkus stopped in 0 .031s [ INFO ] [ INFO ] Results: [ INFO ] [ ERROR ] Failures: [ ERROR ] TestFinancialMessage.shouldHaveOneTransaction:135 expected: < 2 > but was: < 1 > [ INFO ] [ ERROR ] Tests run: 4 , Failures: 1 , Errors: 0 , Skipped: 0 We see that our first tests is now failing again . And this is expected because we are changing the logic of how branches[1] works to filter out those transactions less than 5000 . This makes the second record we send in to get filtered out. In order to fix our test again, we either decrease the assertion for the expected entries in our store back to 1 or we modify the amount of the second transaction to be greater than 5000 . Once we do that, if we test the application again, we should get all tests passing. Next Steps \u00b6 Now that you have finished this initial part of Lab 1 you can optionally proceed to Lab 2","title":"Kafka Streams Test Lab 1"},{"location":"use-cases/kafka-streams/lab-1/#kafka-streams-test-lab-1","text":"Updated 03/10/2022","title":"Kafka Streams Test Lab 1"},{"location":"use-cases/kafka-streams/lab-1/#overview","text":"In this lab scenario we are still using Apache Kafka Streams TestDriver to test a Topology, a Stream and Table. While using the TestDriver we will perform operations such as groupBy, join with another Stream or Kafka Table. The code for this lab is in this repository eda-kstreams-labs folder LoadKtableFromTopic","title":"Overview"},{"location":"use-cases/kafka-streams/lab-1/#scenario-prerequisites","text":"Java For the purposes of this lab we suggest Java 11+ Quarkus CLI Maven Maven will be needed for bootstrapping our application from the command-line and running our application. An IDE of your choice Ideally an IDE that supports Quarkus (such as Visual Studio Code)","title":"Scenario Prerequisites"},{"location":"use-cases/kafka-streams/lab-1/#setting-up-the-quarkus-application","text":"We will bootstrap the Quarkus application with the following Maven command quarkus create LoadKtableFromTopic You can replace the fields within {} as you like. Since we will be using the Kafka Streams testing functionality we will need to edit the pom.xml to add the dependency to our project. Open pom.xml and add the following. <dependency> <groupId> org.apache.kafka </groupId> <artifactId> kafka-streams </artifactId> <version> 3.1.0 </version> </dependency> <dependency> <groupId> org.apache.kafka </groupId> <artifactId> kafka-streams-test-utils </artifactId> <version> 3.1.0 </version> <scope> test </scope> </dependency> <dependency> <groupId> org.hamcrest </groupId> <artifactId> hamcrest </artifactId> <version> 2.2 </version> </dependency>","title":"Setting up the Quarkus Application"},{"location":"use-cases/kafka-streams/lab-1/#creating-your-test-class","text":"Open the TestLoadKtableFromTopic.java file and paste the following content. package ibm.eda.kstreams.lab1 ; import java.util.Properties ; import org.apache.kafka.common.serialization.Serdes ; import org.apache.kafka.common.serialization.StringSerializer ; import org.apache.kafka.streams.KeyValue ; import org.apache.kafka.streams.StreamsBuilder ; import org.apache.kafka.streams.StreamsConfig ; import org.apache.kafka.streams.TestInputTopic ; import org.apache.kafka.streams.TestOutputTopic ; import org.apache.kafka.streams.TopologyTestDriver ; import org.apache.kafka.streams.kstream.Consumed ; import org.apache.kafka.streams.kstream.KTable ; import org.apache.kafka.streams.kstream.Materialized ; import org.apache.kafka.streams.processor.StateStore ; import org.apache.kafka.streams.state.KeyValueBytesStoreSupplier ; import org.apache.kafka.streams.state.KeyValueIterator ; import org.apache.kafka.streams.state.KeyValueStore ; import org.apache.kafka.streams.state.Stores ; import org.apache.kafka.streams.state.ValueAndTimestamp ; import org.junit.jupiter.api.AfterAll ; import org.junit.jupiter.api.Assertions ; import org.junit.jupiter.api.BeforeAll ; import org.junit.jupiter.api.Test ; import io.quarkus.test.junit.QuarkusTest ; /** * This is a simple example of loading some reference data from stream into a Ktable for * lookup. It uses a persistent state store. */ @QuarkusTest public class TestLoadKtableFromTopic { private static TopologyTestDriver testDriver ; private static String companySectorsTopic = \"sector-types\" ; private static String storeName = \"sector-types-store\" ; private static TestInputTopic < String , String > inTopic ; private static TestOutputTopic < String , Long > outTopic ; private static TestOutputTopic < String , String > errorTopic ; public static Properties getStreamsConfig () { final Properties props = new Properties (); props . put ( StreamsConfig . APPLICATION_ID_CONFIG , \"kstream-lab1\" ); props . put ( StreamsConfig . BOOTSTRAP_SERVERS_CONFIG , \"dummmy:1234\" ); return props ; } @BeforeAll public static void buildTopology (){ final StreamsBuilder builder = new StreamsBuilder (); // Adding a state store is a simple matter of creating a StoreSupplier // instance with one of the static factory methods on the Stores class. // all persistent StateStore instances provide local storage using RocksDB KeyValueBytesStoreSupplier storeSupplier = Stores . persistentKeyValueStore ( storeName ); // A KTable is created from the companySectorsTopic, with key and value deserialized. // With Materialized.as() causing the Table to force a state store materialization (storeSupplier). KTable < String , String > sectorTypeTable = builder . table ( companySectorsTopic , Consumed . with ( Serdes . String (), Serdes . String ()), Materialized . as ( storeSupplier )); testDriver = new TopologyTestDriver ( builder . build (), getStreamsConfig ()); inTopic = testDriver . createInputTopic ( companySectorsTopic , new StringSerializer (), new StringSerializer ()); } @AfterAll public static void close (){ testDriver . close (); } @Test public void shouldHaveSixSectorTypes (){ inTopic . pipeInput ( \"C01\" , \"Health Care\" ); inTopic . pipeInput ( \"C02\" , \"Finance\" ); inTopic . pipeInput ( \"C03\" , \"Consumer Services\" ); inTopic . pipeInput ( \"C04\" , \"Transportation\" ); inTopic . pipeInput ( \"C05\" , \"Capital Goods\" ); inTopic . pipeInput ( \"C06\" , \"Public Utilities\" ); KeyValueStore < String , ValueAndTimestamp < String >> store = testDriver . getTimestampedKeyValueStore ( storeName ); Assertions . assertNotNull ( store ); ValueAndTimestamp < String > sector = store . get ( \"C02\" ); Assertions . assertNotNull ( sector ); Assertions . assertEquals ( \"Finance\" , sector . value ()); Assertions . assertEquals ( 6 , store . approximateNumEntries ()); // demonstrate how to get all the values from the table: KeyValueIterator < String , ValueAndTimestamp < String >> sectors = store . all (); while ( sectors . hasNext ()) { KeyValue < String , ValueAndTimestamp < String >> s = sectors . next (); System . out . println ( s . key + \":\" + s . value . value ()); } for ( StateStore s : testDriver . getAllStateStores (). values ()) { System . out . println ( s . name ()); } } } The above code uses TopologyTestDriver to mimic a Topology. A Topology is basically a graph of stream processors (nodes) and the edges between these nodes are the streams. In the first section we instantiate our TopologyTestDriver named testDriver , as well as the topic name and store name. Test the application by running the following: ./mvnw clean verify You should see the tests pass with the following output: [ INFO ] ------------------------------------------------------- [ INFO ] T E S T S [ INFO ] ------------------------------------------------------- [ INFO ] Running com.ibm.GreetingResourceTest 2021 -01-16 14 :20:26,488 INFO [ io.quarkus ] ( main ) Quarkus 1 .10.5.Final on JVM started in 2 .089s. Listening on: http://localhost:8081 2021 -01-16 14 :20:26,490 INFO [ io.quarkus ] ( main ) Profile test activated. 2021 -01-16 14 :20:26,490 INFO [ io.quarkus ] ( main ) Installed features: [ cdi, kafka-streams, resteasy, resteasy-jsonb ] [ INFO ] Tests run: 1 , Failures: 0 , Errors: 0 , Skipped: 0 , Time elapsed: 6 .096 s - in com.ibm.GreetingResourceTest [ INFO ] Running com.ibm.garage.cpat.lab.TestLoadKtableFromTopic C01:Health Care C02:Finance C03:Consumer Services C04:Transportation C05:Capital Goods C06:Public Utilities sector-types-store 2021 -01-16 14 :20:28,253 WARN [ org.apa.kaf.str.sta.int.RocksDBStore ] ( main ) Closing 1 open iterators for store sector-types-store 2021 -01-16 14 :20:28,256 INFO [ org.apa.kaf.str.pro.int.StateDirectory ] ( main ) stream-thread [ main ] Deleting state directory 0_0 for task 0_0 as user calling cleanup. [ INFO ] Tests run: 1 , Failures: 0 , Errors: 0 , Skipped: 0 , Time elapsed: 0 .222 s - in com.ibm.garage.cpat.lab.TestLoadKtableFromTopic 2021 -01-16 14 :20:28,292 INFO [ io.quarkus ] ( main ) Quarkus stopped in 0 .028s [ INFO ] [ INFO ] Results: [ INFO ] [ INFO ] Tests run: 2 , Failures: 0 , Errors: 0 , Skipped: 0 How this test topology creation flow works: A StreamsBuilder object (builder) from the Kafka Streams DSL API is created. A KeyValueBytesStoreSupplier (storeSupplier) is configured with String variable (storeName). A KTable is created reading from the topic (companySectorsTopic), deserialized and materialized as the previously create (storeSupplier). A TopologyTestDriver (testDriver) is built from the provided config properties and the KTable within the builder topology. Lastly test input topic (inTopic) is created from the testDriver topology. When inTopic.pipeInput(\"C01\",\"Health Care\"); is invoked, it populates the topic, which then populates the KTable which ultimately persists in a KeyValue State Store. You should see the tests pass. These are three simple tests. The first of which checks that the value fetched from the Kafka Table is not null,the second makes sure that value retrieved from key C02 is equal to Finance and lastly we make sure that the our state store (which was piped by ways of the Kafka Topic) indeed has six key-value pairs.","title":"Creating your Test Class"},{"location":"use-cases/kafka-streams/lab-1/#more-robust-kafka-streams-testing","text":"add jsonb Serdes using Quarkus kafka client library: quarkus ext add kafka-client jsonb Now that we have tested some simple functionality by using the Kafka Streams API let's check out some other operators that we can use. Let's create a new class for our Plain Old Java Object (POJO) named FinancialMessage and copy and paste the following content into the newly created file. package ibm.eda.kstreams.lab.domain ; import io.quarkus.runtime.annotations.RegisterForReflection ; @RegisterForReflection public class FinancialMessage implements JSONSerdeCompatible { public String userId ; public String stockSymbol ; public String exchangeId ; public int quantity ; public double stockPrice ; public double totalCost ; public int institutionId ; public int countryId ; public boolean technicalValidation ; public FinancialMessage () { } public FinancialMessage ( String userId , String stockSymbol , String exchangeId , int quantity , double stockPrice , double totalCost , int institutionId , int countryId , boolean technicalValidation ) { this . userId = userId ; this . stockSymbol = stockSymbol ; this . exchangeId = exchangeId ; this . quantity = quantity ; this . stockPrice = stockPrice ; this . totalCost = totalCost ; this . institutionId = institutionId ; this . countryId = countryId ; this . technicalValidation = technicalValidation ; } } Note: We have not provided any accessors (getters) or mutators (setters) for simplicity. You can set those at your own discretion. Add the following interface for JSon Serde /** * An interface for registering types that can be de/serialized with {@link JSONSerde}. */ @SuppressWarnings ( \"DefaultAnnotationParam\" ) // being explicit for the example @JsonTypeInfo ( use = JsonTypeInfo . Id . NAME , include = JsonTypeInfo . As . PROPERTY , property = \"_t\" ) @JsonSubTypes ({ @JsonSubTypes.Type ( value = FinancialMessage . class , name = \"fm\" ) }) public interface JSONSerdeCompatible { } And the Generic JSONSerde from this class Now that we have our new Java class, let's create a new and separate Java Test class: src/test/java/ibm/eda/kstreams/lab1/TestFinancialMessage.java . Copy the contents below: package ibm.eda.kstreams.lab1 ; import java.util.Properties ; import org.apache.kafka.common.serialization.LongDeserializer ; import org.apache.kafka.common.serialization.Serde ; import org.apache.kafka.common.serialization.Serdes ; import org.apache.kafka.common.serialization.StringDeserializer ; import org.apache.kafka.common.serialization.StringSerializer ; import org.apache.kafka.streams.KeyValue ; import org.apache.kafka.streams.StreamsBuilder ; import org.apache.kafka.streams.StreamsConfig ; import org.apache.kafka.streams.TestInputTopic ; import org.apache.kafka.streams.TestOutputTopic ; import org.apache.kafka.streams.TopologyTestDriver ; import org.apache.kafka.streams.kstream.Consumed ; import org.apache.kafka.streams.kstream.KStream ; import org.apache.kafka.streams.kstream.Materialized ; import org.apache.kafka.streams.kstream.Produced ; import org.apache.kafka.streams.kstream.Windowed ; import org.apache.kafka.streams.kstream.WindowedSerdes ; import org.apache.kafka.streams.state.KeyValueBytesStoreSupplier ; import org.apache.kafka.streams.state.KeyValueStore ; import org.apache.kafka.streams.state.Stores ; import org.apache.kafka.streams.state.ValueAndTimestamp ; import org.junit.jupiter.api.AfterAll ; import org.junit.jupiter.api.Assertions ; import org.junit.jupiter.api.BeforeAll ; import org.junit.jupiter.api.Test ; import io.quarkus.kafka.client.serialization.JsonbSerde ; import io.quarkus.kafka.client.serialization.JsonbSerializer ; import io.quarkus.test.junit.QuarkusTest ; import com.ibm.garage.cpat.domain.* ; @QuarkusTest public class TestFinancialMessage { private static TopologyTestDriver testDriver ; private static String inTopicName = \"transactions\" ; private static String outTopicName = \"output\" ; private static String errorTopicName = \"errors\" ; private static String storeName = \"transactionCount\" ; private static TestInputTopic < String , FinancialMessage > inTopic ; private static TestOutputTopic < String , Long > outTopic ; private static TestOutputTopic < String , String > errorTopic ; public static Properties getStreamsConfig () { final Properties props = new Properties (); props . put ( StreamsConfig . APPLICATION_ID_CONFIG , \"kstream-lab2\" ); props . put ( StreamsConfig . BOOTSTRAP_SERVERS_CONFIG , \"dummmy:2345\" ); props . put ( StreamsConfig . DEFAULT_KEY_SERDE_CLASS_CONFIG , Serdes . String (). getClass ()); props . put ( StreamsConfig . DEFAULT_VALUE_SERDE_CLASS_CONFIG , JSONSerde . class ); return props ; } @BeforeAll public static void buildTopology () { final StreamsBuilder builder = new StreamsBuilder (); KeyValueBytesStoreSupplier storeSupplier = Stores . persistentKeyValueStore ( storeName ); KStream < String , FinancialMessage > transactionStream = builder . stream ( inTopicName , Consumed . with ( Serdes . String (), financialMessageSerde ) ); // First verify user id is present, if not route to error Map < String , KStream < String , FinancialMessage >> branches = transactionStream . split ( Named . as ( \"tx-\" )) . branch (( key , value ) -> value . userId == null , Branched . as ( \"no-userid\" )) . defaultBranch ( Branched . as ( \"non-null\" )); // Handle error by sending to the errors topic. branches . get ( \"tx-no-userid\" ). map ( ( key , value ) -> { return KeyValue . pair ( key , \"No customer id provided\" ); }) . to ( errorTopicName , Produced . with ( Serdes . String (), Serdes . String ())); // use groupBy to swap the key, then count by customer id, branches . get ( \"tx-non-null\" ). groupBy ( ( key , value ) -> value . userId ) . count ( Materialized . as ( storeSupplier ) ) . toStream () . to ( outTopicName , Produced . with ( Serdes . String (), Serdes . Long ()) ); testDriver = new TopologyTestDriver ( builder . build (), getStreamsConfig ()); inTopic = testDriver . createInputTopic ( inTopicName , new StringSerializer (), new JSONSerde < FinancialMessage > ()); outTopic = testDriver . createOutputTopic ( outTopicName , new StringDeserializer (), new LongDeserializer ()); errorTopic = testDriver . createOutputTopic ( errorTopicName , new StringDeserializer (), new StringDeserializer ()); } @AfterAll public static void close (){ testDriver . close (); } } We have the setup for the TestTopology. Now, we can add a test that will insert two events into the topic. Add the following code to your test class: @Test public void shouldHaveOneTransaction () { // A FinancialMessage is mocked and set to the input topic. Within the Topology, // this gets sent to the outTopic because a userId exists for the incoming message. FinancialMessage mock = new FinancialMessage ( \"1\" , \"MET\" , \"SWISS\" , 12 , 1822.38 , 21868.55 , 94 , 7 , true ); FinancialMessage mock2 = new FinancialMessage ( \"2\" , \"ASDF\" , \"HELLO\" , 5 , 1000.22 , 4444.12 , 38 , 6 , true ); inTopic . pipeInput ( \"T01\" , mock ); inTopic . pipeInput ( \"T02\" , mock2 ); Assertions . assertFalse ( outTopic . isEmpty ()); Assertions . assertEquals ( 1 , outTopic . readKeyValue (). value ); KeyValueStore < String , ValueAndTimestamp < FinancialMessage >> store = testDriver . getTimestampedKeyValueStore ( storeName ); Assertions . assertEquals ( 1 , store . approximateNumEntries ()); } Test the application by running the following: ./mvnw clean verify You should see the following output: [ INFO ] ------------------------------------------------------- [ INFO ] T E S T S [ INFO ] ------------------------------------------------------- [ INFO ] Running com.ibm.GreetingResourceTest 2021 -01-16 17 :21:37,836 INFO [ io.quarkus ] ( main ) Quarkus 1 .10.5.Final on JVM started in 1 .996s. Listening on: http://localhost:8081 2021 -01-16 17 :21:37,837 INFO [ io.quarkus ] ( main ) Profile test activated. 2021 -01-16 17 :21:37,838 INFO [ io.quarkus ] ( main ) Installed features: [ cdi, kafka-streams, resteasy, resteasy-jsonb ] [ INFO ] Tests run: 1 , Failures: 0 , Errors: 0 , Skipped: 0 , Time elapsed: 5 .234 s - in com.ibm.GreetingResourceTest [ INFO ] Running com.ibm.garage.cpat.lab.TestFinancialMessage 2021 -01-16 17 :21:39,460 INFO [ org.apa.kaf.str.pro.int.StateDirectory ] ( main ) stream-thread [ main ] Deleting state directory 0_0 for task 0_0 as user calling cleanup. [ ERROR ] Tests run: 1 , Failures: 1 , Errors: 0 , Skipped: 0 , Time elapsed: 0 .273 s <<< FAILURE! - in com.ibm.garage.cpat.lab.TestFinancialMessage [ ERROR ] shouldHaveOneTransaction Time elapsed: 0 .09 s <<< FAILURE! org.opentest4j.AssertionFailedError: expected: < 1 > but was: < 2 > at com.ibm.garage.cpat.lab.TestFinancialMessage.shouldHaveOneTransaction ( TestFinancialMessage.java:132 ) [ INFO ] Running com.ibm.garage.cpat.lab.TestLoadKtableFromTopic C01:Health Care C02:Finance C03:Consumer Services C04:Transportation C05:Capital Goods C06:Public Utilities sector-types-store 2021 -01-16 17 :21:39,505 WARN [ org.apa.kaf.str.sta.int.RocksDBStore ] ( main ) Closing 1 open iterators for store sector-types-store 2021 -01-16 17 :21:39,507 INFO [ org.apa.kaf.str.pro.int.StateDirectory ] ( main ) stream-thread [ main ] Deleting state directory 0_0 for task 0_0 as user calling cleanup. [ INFO ] Tests run: 1 , Failures: 0 , Errors: 0 , Skipped: 0 , Time elapsed: 0 .039 s - in com.ibm.garage.cpat.lab.TestLoadKtableFromTopic 2021 -01-16 17 :21:39,541 INFO [ io.quarkus ] ( main ) Quarkus stopped in 0 .028s [ INFO ] [ INFO ] Results: [ INFO ] [ ERROR ] Failures: [ ERROR ] TestFinancialMessage.shouldHaveOneTransaction:132 expected: < 1 > but was: < 2 > [ INFO ] [ ERROR ] Tests run: 3 , Failures: 1 , Errors: 0 , Skipped: 0 We see that our recently added test failed . And this is expected due to the fact that we inserted two records but our test expects one. To remedy this test we must change Assertions.assertEquals(1, store.approximateNumEntries()); Set to 2 the comparisson. Next let's add another very simple test. Copy the following code to your Java test class: @Test public void testErrorTopicIsNotEmpty () { FinancialMessage mock = new FinancialMessage ( null , \"MET\" , \"SWISS\" , 12 , 1822.38 , 21868.55 , 94 , 7 , true ); inTopic . pipeInput ( \"T03\" , mock ); Assertions . assertFalse ( errorTopic . isEmpty ()); } Test the application by running the following: ./mvnw clean verify You should see the following output: [ INFO ] ------------------------------------------------------- [ INFO ] T E S T S [ INFO ] ------------------------------------------------------- [ INFO ] Running com.ibm.GreetingResourceTest 2021 -01-16 17 :29:34,258 INFO [ io.quarkus ] ( main ) Quarkus 1 .10.5.Final on JVM started in 2 .470s. Listening on: http://localhost:8081 2021 -01-16 17 :29:34,260 INFO [ io.quarkus ] ( main ) Profile test activated. 2021 -01-16 17 :29:34,260 INFO [ io.quarkus ] ( main ) Installed features: [ cdi, kafka-streams, resteasy, resteasy-jsonb ] [ INFO ] Tests run: 1 , Failures: 0 , Errors: 0 , Skipped: 0 , Time elapsed: 5 .694 s - in com.ibm.GreetingResourceTest [ INFO ] Running com.ibm.garage.cpat.lab.TestFinancialMessage 2021 -01-16 17 :29:36,001 INFO [ org.apa.kaf.str.pro.int.StateDirectory ] ( main ) stream-thread [ main ] Deleting state directory 0_0 for task 0_0 as user calling cleanup. [ INFO ] Tests run: 2 , Failures: 0 , Errors: 0 , Skipped: 0 , Time elapsed: 0 .309 s - in com.ibm.garage.cpat.lab.TestFinancialMessage [ INFO ] Running com.ibm.garage.cpat.lab.TestLoadKtableFromTopic C01:Health Care C02:Finance C03:Consumer Services C04:Transportation C05:Capital Goods C06:Public Utilities sector-types-store 2021 -01-16 17 :29:36,057 WARN [ org.apa.kaf.str.sta.int.RocksDBStore ] ( main ) Closing 1 open iterators for store sector-types-store 2021 -01-16 17 :29:36,059 INFO [ org.apa.kaf.str.pro.int.StateDirectory ] ( main ) stream-thread [ main ] Deleting state directory 0_0 for task 0_0 as user calling cleanup. [ INFO ] Tests run: 1 , Failures: 0 , Errors: 0 , Skipped: 0 , Time elapsed: 0 .049 s - in com.ibm.garage.cpat.lab.TestLoadKtableFromTopic 2021 -01-16 17 :29:36,099 INFO [ io.quarkus ] ( main ) Quarkus stopped in 0 .031s [ INFO ] [ INFO ] Results: [ INFO ] [ INFO ] Tests run: 4 , Failures: 0 , Errors: 0 , Skipped: 0 As you can see here, our message payload is created with null for the userId field which means this message will branch out to the errorTopic . The purpose of the test is to check if our errorTopic is empty, which should not be. Since our errorTopic.isEmpty() resolves to false and our assertion is asserting that it is false as well, thus the test passes. Now that we have two simple tests, let's update our first branch to allow us to filter the stream on a condition that we want. Let's edit our branches[1] statement so that it will filter out and retain only the records where the totalCost is greater than 5000. branches [ 1 ] . filter ( ( key , value ) -> ( value . totalCost > 5000 ) ) . groupBy ( ( key , value ) -> value . userId ) . count ( Materialized . as ( storeSupplier ) ) . toStream () . to ( outTopicName , Produced . with ( Serdes . String (), Serdes . Long ()) ); Test the application by running the following: ./mvnw clean verify You should see the following output: [ INFO ] ------------------------------------------------------- [ INFO ] T E S T S [ INFO ] ------------------------------------------------------- [ INFO ] Running com.ibm.GreetingResourceTest 2021 -01-16 17 :40:50,765 INFO [ io.quarkus ] ( main ) Quarkus 1 .10.5.Final on JVM started in 2 .102s. Listening on: http://localhost:8081 2021 -01-16 17 :40:50,766 INFO [ io.quarkus ] ( main ) Profile test activated. 2021 -01-16 17 :40:50,766 INFO [ io.quarkus ] ( main ) Installed features: [ cdi, kafka-streams, resteasy, resteasy-jsonb ] [ INFO ] Tests run: 1 , Failures: 0 , Errors: 0 , Skipped: 0 , Time elapsed: 5 .474 s - in com.ibm.GreetingResourceTest [ INFO ] Running com.ibm.garage.cpat.lab.TestFinancialMessage 2021 -01-16 17 :40:52,393 INFO [ org.apa.kaf.str.pro.int.StateDirectory ] ( main ) stream-thread [ main ] Deleting state directory 0_0 for task 0_0 as user calling cleanup. [ ERROR ] Tests run: 2 , Failures: 1 , Errors: 0 , Skipped: 0 , Time elapsed: 0 .307 s <<< FAILURE! - in com.ibm.garage.cpat.lab.TestFinancialMessage [ ERROR ] shouldHaveOneTransaction Time elapsed: 0 .022 s <<< FAILURE! org.opentest4j.AssertionFailedError: expected: < 2 > but was: < 1 > at com.ibm.garage.cpat.lab.TestFinancialMessage.shouldHaveOneTransaction ( TestFinancialMessage.java:135 ) [ INFO ] Running com.ibm.garage.cpat.lab.TestLoadKtableFromTopic C01:Health Care C02:Finance C03:Consumer Services C04:Transportation C05:Capital Goods C06:Public Utilities sector-types-store 2021 -01-16 17 :40:52,445 WARN [ org.apa.kaf.str.sta.int.RocksDBStore ] ( main ) Closing 1 open iterators for store sector-types-store 2021 -01-16 17 :40:52,447 INFO [ org.apa.kaf.str.pro.int.StateDirectory ] ( main ) stream-thread [ main ] Deleting state directory 0_0 for task 0_0 as user calling cleanup. [ INFO ] Tests run: 1 , Failures: 0 , Errors: 0 , Skipped: 0 , Time elapsed: 0 .046 s - in com.ibm.garage.cpat.lab.TestLoadKtableFromTopic 2021 -01-16 17 :40:52,487 INFO [ io.quarkus ] ( main ) Quarkus stopped in 0 .031s [ INFO ] [ INFO ] Results: [ INFO ] [ ERROR ] Failures: [ ERROR ] TestFinancialMessage.shouldHaveOneTransaction:135 expected: < 2 > but was: < 1 > [ INFO ] [ ERROR ] Tests run: 4 , Failures: 1 , Errors: 0 , Skipped: 0 We see that our first tests is now failing again . And this is expected because we are changing the logic of how branches[1] works to filter out those transactions less than 5000 . This makes the second record we send in to get filtered out. In order to fix our test again, we either decrease the assertion for the expected entries in our store back to 1 or we modify the amount of the second transaction to be greater than 5000 . Once we do that, if we test the application again, we should get all tests passing.","title":"More Robust Kafka Streams Testing"},{"location":"use-cases/kafka-streams/lab-1/#next-steps","text":"Now that you have finished this initial part of Lab 1 you can optionally proceed to Lab 2","title":"Next Steps"},{"location":"use-cases/kafka-streams/lab-2/","text":"Kafka Streams Test Lab 2 \u00b6 Overview \u00b6 This is a continuation of the previous Lab 1 . You should complete Lab 1 first before you get started here. There's a few more pre-reqs (if you so choose to use them) outlined below. Scenario Prerequisites \u00b6 Java For the purposes of this lab we suggest Java 8+ Maven Maven will be needed for bootstrapping our application from the command-line and running our application. An IDE of your choice Ideally an IDE that supports Quarkus (such as Visual Studio Code) OpenShift Container Platform, IBM Cloud Pak for Integration and IBM Event Streams This is an optional portion of the lab for those who have access to an OCP Cluster where IBM Cloud Pak for Integration has been installed on top and an IBM Event Streams instance deployed. The following are optional OpenShift Container Platform v4.4.x IBM Cloud Pak for Integration CP4I2022.2 IBM Event Streams : IBM Event Streams v10 or later preferrably. If you are using a previous version of IBM Event Streams, there are some differences as to how you would configure application.properties to establish the connection to IBM Event Streams. Adding in more Kafka Streams operators \u00b6 In this section we are going to to add more functionality to our previous test class in order to see, work with and understand more Kafka Streams operators. Add the following definitions to the TestFinancialMessage.java Java class: private static String tradingTable = \"tradingTable\" ; private static String tradingStoreName = \"tradingStore\" ; private static TestInputTopic < String , String > tradingTableTopic ; Add the following Store and KTable definitions inside the buildTopology() function to support the trading fuctionality we are adding to our application: KeyValueBytesStoreSupplier tradingStoreSupplier = Stores . persistentKeyValueStore ( tradingStoreName ); KTable < String , String > stockTradingStore = builder . table ( tradingTable , Consumed . with ( Serdes . String (), Serdes . String ()), Materialized . as ( tradingStoreSupplier )); Add the following import to your Java class so that you can use objects of type KTable: import org.apache.kafka.streams.kstream.KTable ; Edit the branch[1] logic again to create new KeyValue pairs of userId and stockSymbol branches [ 1 ] . filter ( ( key , value ) -> ( value . totalCost > 5000 ) ) . map ( ( key , value ) -> KeyValue . pair ( value . userId , value . stockSymbol ) ) . to ( tradingTable , Produced . with ( Serdes . String (), Serdes . String ()) ); Notice that, previously, we wrote straight to outTopic . However, we are now writing to a KTable which we can query in our tests by using the State Store it is materialised as. Before we create a test for the new functionality, remove or comment out the previous existing tests cases as these do no longer apply. Create a new test with the code below: @Test public void filterAndMapNewPair () { FinancialMessage mock = new FinancialMessage ( \"1\" , \"MET\" , \"SWISS\" , 12 , 1822.38 , 21868.55 , 94 , 7 , true ); inTopic . pipeInput ( \"1\" , mock ); KeyValueStore < String , ValueAndTimestamp < String >> tableStore = testDriver . getTimestampedKeyValueStore ( tradingStoreName ); Assertions . assertEquals ( 1 , tableStore . approximateNumEntries ()); Assertions . assertEquals ( \"MET\" , tableStore . get ( \"1\" ). value ()); } The first assertion checks whether the store has a record. The second assertion checks that the mock record that we inserted has the correct value as our map function created new KeyValue pairs of type <userId, stockSymbol> . Test the application by running the following: ./mvnw clean verify You should see the tests pass with the following output: [ INFO ] ------------------------------------------------------- [ INFO ] T E S T S [ INFO ] ------------------------------------------------------- [ INFO ] Running com.ibm.GreetingResourceTest 2021 -01-16 20 :47:06,478 INFO [ io.quarkus ] ( main ) Quarkus 1 .10.5.Final on JVM started in 2 .086s. Listening on: http://localhost:8081 2021 -01-16 20 :47:06,479 INFO [ io.quarkus ] ( main ) Profile test activated. 2021 -01-16 20 :47:06,479 INFO [ io.quarkus ] ( main ) Installed features: [ cdi, kafka-streams, resteasy, resteasy-jsonb ] [ INFO ] Tests run: 1 , Failures: 0 , Errors: 0 , Skipped: 0 , Time elapsed: 5 .611 s - in com.ibm.GreetingResourceTest [ INFO ] Running com.ibm.garage.cpat.lab.TestFinancialMessage 2021 -01-16 20 :47:08,248 INFO [ org.apa.kaf.str.pro.int.StateDirectory ] ( main ) stream-thread [ main ] Deleting state directory 0_0 for task 0_0 as user calling cleanup. [ INFO ] Tests run: 1 , Failures: 0 , Errors: 0 , Skipped: 0 , Time elapsed: 0 .276 s - in com.ibm.garage.cpat.lab.TestFinancialMessage [ INFO ] Running com.ibm.garage.cpat.lab.TestLoadKtableFromTopic C01:Health Care C02:Finance C03:Consumer Services C04:Transportation C05:Capital Goods C06:Public Utilities sector-types-store 2021 -01-16 20 :47:08,290 WARN [ org.apa.kaf.str.sta.int.RocksDBStore ] ( main ) Closing 1 open iterators for store sector-types-store 2021 -01-16 20 :47:08,292 INFO [ org.apa.kaf.str.pro.int.StateDirectory ] ( main ) stream-thread [ main ] Deleting state directory 0_0 for task 0_0 as user calling cleanup. [ INFO ] Tests run: 1 , Failures: 0 , Errors: 0 , Skipped: 0 , Time elapsed: 0 .035 s - in com.ibm.garage.cpat.lab.TestLoadKtableFromTopic 2021 -01-16 20 :47:08,325 INFO [ io.quarkus ] ( main ) Quarkus stopped in 0 .026s [ INFO ] [ INFO ] Results: [ INFO ] [ INFO ] Tests run: 3 , Failures: 0 , Errors: 0 , Skipped: 0 Now we are going to do something a little bit more advanced. We are going to join a KStream with a KTable. The Streams API has an inner join, left join, and an outer join. KStream-KTable joins are non-windowed and asymmetric. By asymmetric we mean that a join only gets triggered if the left input KStream gets a new record while the right KTable holds the latest input records materialized. Add the following new attributes: private static String joinedTopicName = \"joinedTopic\" ; private static TestOutputTopic < String , String > joinedTopic ; private static String joinedStoreName = \"joinedStore\" ; Replace the buildTopology() function for the following new one: public static void buildTopology () { final StreamsBuilder builder = new StreamsBuilder (); KeyValueBytesStoreSupplier storeSupplier = Stores . persistentKeyValueStore ( storeName ); KeyValueBytesStoreSupplier tradingStoreSupplier = Stores . persistentKeyValueStore ( tradingStoreName ); KeyValueBytesStoreSupplier joinedStoreSupplier = Stores . persistentKeyValueStore ( joinedStoreName ); KStream < String , FinancialMessage > transactionStream = builder . stream ( inTopicName , Consumed . with ( Serdes . String (), financialMessageSerde ) ); KTable < String , String > stockTradingStore = builder . table ( tradingTable , Consumed . with ( Serdes . String (), Serdes . String ()), Materialized . as ( tradingStoreSupplier )); KTable < String , String > joinedMessageStore = builder . table ( joinedTopicName , Consumed . with ( Serdes . String (), Serdes . String ()), Materialized . as ( joinedStoreSupplier )); KStream < String , String > joinedStream = transactionStream . join ( stockTradingStore , ( financialMessage , companyName ) -> \"userId = \" + financialMessage . userId + \" companyName = \" + companyName ); joinedStream . to ( joinedTopicName , Produced . with ( Serdes . String (), Serdes . String ())); testDriver = new TopologyTestDriver ( builder . build (), getStreamsConfig ()); inTopic = testDriver . createInputTopic ( inTopicName , new StringSerializer (), new JsonbSerializer < FinancialMessage > ()); tradingTableTopic = testDriver . createInputTopic ( tradingTable , new StringSerializer (), new StringSerializer ()); joinedTopic = testDriver . createOutputTopic ( joinedTopicName , new StringDeserializer (), new StringDeserializer ()); } We can see that our buildTopology() function still contains the transactionsStream KStream that will contain the stream of FinancialMessages being received through the inTopicName . Then, We can see a KTable called stockTradingStore , which will get materialized as a State Store, that will contain the messages comming in through the input topic called tradingTable . This KTable will hold data about the tradding companies that will serve to enhance the incoming FinancialMessages with. The join between the KStream and the KTable is being done below and is called joinedStream . The result of this join is being outputed into a topic called joinedTopicName . Finally, this output topic is being store in a KTable called joinedMessageStore , and materialized in its respective State Store, in order to be able to query it later on in our tests. The inner join is performed on matching keys between the KStream and the KTable and the matched records produce a new <String, String> pair with the value of userId and companyName . In order to test the above new functionality of the buildTopology() function, remove or comment out the existing test and create the following new one: @Test public void checkStreamAndTableJoinHasOneRecord () { tradingTableTopic . pipeInput ( \"1\" , \"Metropolitan Museum of Art\" ); FinancialMessage mock = new FinancialMessage ( \"1\" , \"MET\" , \"SWISS\" , 12 , 1822.38 , 21868.55 , 94 , 7 , true ); inTopic . pipeInput ( \"1\" , mock ); KeyValueStore < String , ValueAndTimestamp < String >> joinedTableStore = testDriver . getTimestampedKeyValueStore ( joinedStoreName ); Assertions . assertEquals ( 1 , joinedTableStore . approximateNumEntries ()); System . out . println ( joinedTableStore . get ( \"1\" ). value ()); } Test the application by running the following: ./mvnw clean verify You should see the following output: [ INFO ] ------------------------------------------------------- [ INFO ] T E S T S [ INFO ] ------------------------------------------------------- [ INFO ] Running com.ibm.GreetingResourceTest 2021 -01-16 21 :53:03,682 INFO [ io.quarkus ] ( main ) Quarkus 1 .10.5.Final on JVM started in 2 .134s. Listening on: http://localhost:8081 2021 -01-16 21 :53:03,684 INFO [ io.quarkus ] ( main ) Profile test activated. 2021 -01-16 21 :53:03,685 INFO [ io.quarkus ] ( main ) Installed features: [ cdi, kafka-streams, resteasy, resteasy-jsonb ] [ INFO ] Tests run: 1 , Failures: 0 , Errors: 0 , Skipped: 0 , Time elapsed: 5 .574 s - in com.ibm.GreetingResourceTest [ INFO ] Running com.ibm.garage.cpat.lab.TestFinancialMessage userId = 1 companyName = Metropolitan Museum of Art 2021 -01-16 21 :53:05,432 INFO [ org.apa.kaf.str.pro.int.StateDirectory ] ( main ) stream-thread [ main ] Deleting state directory 0_0 for task 0_0 as user calling cleanup. [ INFO ] Tests run: 1 , Failures: 0 , Errors: 0 , Skipped: 0 , Time elapsed: 0 .294 s - in com.ibm.garage.cpat.lab.TestFinancialMessage [ INFO ] Running com.ibm.garage.cpat.lab.TestLoadKtableFromTopic C01:Health Care C02:Finance C03:Consumer Services C04:Transportation C05:Capital Goods C06:Public Utilities sector-types-store 2021 -01-16 21 :53:05,482 WARN [ org.apa.kaf.str.sta.int.RocksDBStore ] ( main ) Closing 1 open iterators for store sector-types-store 2021 -01-16 21 :53:05,484 INFO [ org.apa.kaf.str.pro.int.StateDirectory ] ( main ) stream-thread [ main ] Deleting state directory 0_0 for task 0_0 as user calling cleanup. [ INFO ] Tests run: 1 , Failures: 0 , Errors: 0 , Skipped: 0 , Time elapsed: 0 .044 s - in com.ibm.garage.cpat.lab.TestLoadKtableFromTopic 2021 -01-16 21 :53:05,518 INFO [ io.quarkus ] ( main ) Quarkus stopped in 0 .026s [ INFO ] [ INFO ] Results: [ INFO ] [ INFO ] Tests run: 3 , Failures: 0 , Errors: 0 , Skipped: 0 We can see that our test passes successfully as the FinancialMessage with userId=1 and the KTable record with key 1 successfully joined to produce the output message: userId = 1 companyName = Metropolitan Museum of Art We are now going to create yet another new test class. But first, we are going to create two new POJO classes to work with. These are EnrichedMessage.java and AggregatedMessage.java . Both of them should be placed where the previos POJO class is: src/main/java/com/ibm/garage/cpat/domain . Create the EnrichedMessage.java Java file with the following code: package com.ibm.garage.cpat.domain ; public class EnrichedMessage { public String userId ; public String stockSymbol ; public int quantity ; public double stockPrice ; public double totalCost ; public double adjustedCost ; public boolean technicalValidation ; public String companyName ; public EnrichedMessage ( FinancialMessage message , String companyName ) { this . userId = message . userId ; this . stockSymbol = message . stockSymbol ; this . quantity = message . quantity ; this . stockPrice = message . stockPrice ; this . totalCost = message . totalCost ; this . companyName = companyName ; if ( message . technicalValidation ) { this . technicalValidation = message . technicalValidation ; this . adjustedCost = message . totalCost * 1.15 ; } else { this . technicalValidation = message . technicalValidation ; this . adjustedCost = message . totalCost ; } } } Create the AggregatedMessage.java Java file with the following code: package com.ibm.garage.cpat.domain ; import java.math.BigDecimal ; import java.math.RoundingMode ; public class AggregatedMessage { public String userId ; public String stockSymbol ; public int quantity ; public double stockPrice ; public double totalCost ; public double adjustedCost ; public boolean technicalValidation ; public String companyName ; public int count ; public double sum ; public double average ; public AggregatedMessage updateFrom ( EnrichedMessage message ) { this . userId = message . userId ; this . stockSymbol = message . stockSymbol ; this . quantity = message . quantity ; this . stockPrice = message . stockPrice ; this . totalCost = message . totalCost ; this . companyName = message . companyName ; this . adjustedCost = message . adjustedCost ; this . technicalValidation = message . technicalValidation ; this . count ++ ; this . sum += message . adjustedCost ; this . average = BigDecimal . valueOf ( sum / count ) . setScale ( 1 , RoundingMode . HALF_UP ). doubleValue (); return this ; } } Now create the new test class named TestAggregate.java in the same path we have the other test classes ( src/test/java/com/ibm/garage/cpat ) and paste the following code: package com.ibm.garage.cpat.lab ; import java.util.Properties ; import org.apache.kafka.common.serialization.LongDeserializer ; import org.apache.kafka.common.serialization.Serde ; import org.apache.kafka.common.serialization.Serdes ; import org.apache.kafka.common.serialization.StringDeserializer ; import org.apache.kafka.common.serialization.StringSerializer ; import org.apache.kafka.streams.KeyValue ; import org.apache.kafka.streams.StreamsBuilder ; import org.apache.kafka.streams.StreamsConfig ; import org.apache.kafka.streams.TestInputTopic ; import org.apache.kafka.streams.TestOutputTopic ; import org.apache.kafka.streams.TopologyTestDriver ; import org.apache.kafka.streams.kstream.Consumed ; import org.apache.kafka.streams.kstream.KGroupedStream ; import org.apache.kafka.streams.kstream.KStream ; import org.apache.kafka.streams.kstream.KTable ; import org.apache.kafka.streams.kstream.Materialized ; import org.apache.kafka.streams.kstream.Produced ; import org.apache.kafka.streams.kstream.Windowed ; import org.apache.kafka.streams.kstream.WindowedSerdes ; import org.apache.kafka.streams.processor.StateStore ; import org.apache.kafka.streams.state.KeyValueBytesStoreSupplier ; import org.apache.kafka.streams.state.KeyValueIterator ; import org.apache.kafka.streams.state.KeyValueStore ; import org.apache.kafka.streams.state.Stores ; import org.apache.kafka.streams.state.ValueAndTimestamp ; import org.junit.jupiter.api.AfterAll ; import org.junit.jupiter.api.Assertions ; import org.junit.jupiter.api.BeforeAll ; import org.junit.jupiter.api.Test ; import io.quarkus.kafka.client.serialization.JsonbDeserializer ; import io.quarkus.kafka.client.serialization.JsonbSerde ; import io.quarkus.kafka.client.serialization.JsonbSerializer ; import io.quarkus.test.junit.QuarkusTest ; import com.ibm.garage.cpat.domain.* ; @QuarkusTest public class TestAggregate { private static TopologyTestDriver testDriver ; private static String inTopicName = \"financialMessages\" ; private static String outTopicName = \"enrichedMessages\" ; private static String storeName = \"financialStore\" ; private static String aggregatedTopicName = \"aggregatedMessages\" ; private static String companyTable = \"companyTable\" ; private static String companyStoreName = \"companyStore\" ; private static TestInputTopic < String , FinancialMessage > inTopic ; private static TestOutputTopic < String , EnrichedMessage > outTopic ; private static TestOutputTopic < String , AggregatedMessage > aggregatedTopic ; private static TestInputTopic < String , String > companyTableTopic ; private static final JsonbSerde < FinancialMessage > financialMessageSerde = new JsonbSerde <> ( FinancialMessage . class ); private static final JsonbSerde < EnrichedMessage > enrichedMessageSerde = new JsonbSerde <> ( EnrichedMessage . class ); private static final JsonbSerde < AggregatedMessage > aggregatedMessageSerde = new JsonbSerde <> ( AggregatedMessage . class ); public static Properties getStreamsConfig () { final Properties props = new Properties (); props . put ( StreamsConfig . APPLICATION_ID_CONFIG , \"kstream-lab3\" ); props . put ( StreamsConfig . BOOTSTRAP_SERVERS_CONFIG , \"dummmy:3456\" ); props . put ( StreamsConfig . DEFAULT_KEY_SERDE_CLASS_CONFIG , Serdes . String (). getClass ()); //props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, financialMessageSerde); return props ; } @BeforeAll public static void buildTopology () { final StreamsBuilder builder = new StreamsBuilder (); KeyValueBytesStoreSupplier storeSupplier = Stores . persistentKeyValueStore ( storeName ); KeyValueBytesStoreSupplier companyStoreSupplier = Stores . persistentKeyValueStore ( companyStoreName ); // create a KStream for financial messages. KStream < String , FinancialMessage > financialStream = builder . stream ( inTopicName , Consumed . with ( Serdes . String (), financialMessageSerde ) ); // create a KTable from a topic for companies. KTable < String , String > companyStore = builder . table ( companyTable , Consumed . with ( Serdes . String (), Serdes . String ()), Materialized . as ( companyStoreSupplier )); // join KStream with KTable and use aggregate. KStream < String , EnrichedMessage > enrichedStream = financialStream . join ( companyStore , //(financialMessage, companyName) -> financialMessage.userId, ( financialMessage , companyName ) -> { return new EnrichedMessage ( financialMessage , companyName ); } ); enrichedStream . groupByKey () . aggregate ( AggregatedMessage :: new , ( userId , value , aggregatedMessage ) -> aggregatedMessage . updateFrom ( value ), Materialized . < String , AggregatedMessage > as ( storeSupplier ) . withKeySerde ( Serdes . String ()) . withValueSerde ( aggregatedMessageSerde ) ) . toStream () . to ( aggregatedTopicName , Produced . with ( Serdes . String (), aggregatedMessageSerde ) ); testDriver = new TopologyTestDriver ( builder . build (), getStreamsConfig ()); inTopic = testDriver . createInputTopic ( inTopicName , new StringSerializer (), new JsonbSerializer < FinancialMessage > ()); //outTopic = testDriver.createOutputTopic(outTopicName, new StringDeserializer(), new JsonbDeserializer<>(EnrichedMessage.class)); companyTableTopic = testDriver . createInputTopic ( companyTable , new StringSerializer (), new StringSerializer ()); aggregatedTopic = testDriver . createOutputTopic ( aggregatedTopicName , new StringDeserializer (), new JsonbDeserializer <> ( AggregatedMessage . class )); } @AfterAll public static void close (){ testDriver . close (); } } Even though the code above might seem a completely new one at first glance, it is just an extra step of the previous code we have been working with. We can see that we still have both our financialStream KStream receiving FinancialMessage from the inTopicName topic and our companyStore KTable holding the latest being received from the companyTable input topic. Then, we also still have the join of the previous two KStream and KTable in our enrichedStream KStream that will be a KStream of EnrichedMessage . What is new in this code is the following aggregation we are doing on the resulting enrichedStream KStream. We are grouping EnrichedMessage objects by key and doing an aggregation of that grouping. The aggregation result will be an AggregatedMessage which will get materialized as storeSupplier so that we can query it later on. We are also converting that KTable to a KStream that will get outputed to aggregatedTopicName . The aggregate logic will simply work out some average and count of EnrichedMessage you can check out in the EnrichedMessage.java Java file. Now add a test that will make sure the functionality we have implemented in the buildTopology() function works as desired: @Test public void aggregatedMessageExists () { companyTableTopic . pipeInput ( \"1\" , \"Metropolitan Museum of Art\" ); FinancialMessage mock = new FinancialMessage ( \"1\" , \"MET\" , \"SWISS\" , 12 , 1822.38 , 21868.55 , 94 , 7 , true ); FinancialMessage mock2 = new FinancialMessage ( \"1\" , \"MET\" , \"SWISS\" , 12 , 1822.38 , 6634.56 , 94 , 7 , true ); inTopic . pipeInput ( \"1\" , mock ); inTopic . pipeInput ( \"1\" , mock2 ); KeyValueStore < String , ValueAndTimestamp < AggregatedMessage >> aggregatedTableStore = testDriver . getTimestampedKeyValueStore ( storeName ); Assertions . assertEquals ( 2 , aggregatedTableStore . approximateNumEntries ()); System . out . println ( \"Average = \" + aggregatedTableStore . get ( \"1\" ). value (). average ); Assertions . assertEquals ( 16389.3 , aggregatedTableStore . get ( \"2\" ). value (). average ); } Test the application by running the following: ./mvnw clean verify You should see the following output: [ INFO ] ------------------------------------------------------- [ INFO ] T E S T S [ INFO ] ------------------------------------------------------- [ INFO ] Running com.ibm.GreetingResourceTest 2021 -01-17 13 :13:02,533 INFO [ io.quarkus ] ( main ) Quarkus 1 .10.5.Final on JVM started in 2 .178s. Listening on: http://localhost:8081 2021 -01-17 13 :13:02,535 INFO [ io.quarkus ] ( main ) Profile test activated. 2021 -01-17 13 :13:02,535 INFO [ io.quarkus ] ( main ) Installed features: [ cdi, kafka-streams, resteasy, resteasy-jsonb ] [ INFO ] Tests run: 1 , Failures: 0 , Errors: 0 , Skipped: 0 , Time elapsed: 5 .441 s - in com.ibm.GreetingResourceTest [ INFO ] Running com.ibm.garage.cpat.lab.TestAggregate Average = 16389 .3 2021 -01-17 13 :13:04,216 INFO [ org.apa.kaf.str.pro.int.StateDirectory ] ( main ) stream-thread [ main ] Deleting state directory 0_0 for task 0_0 as user calling cleanup. [ ERROR ] Tests run: 1 , Failures: 0 , Errors: 1 , Skipped: 0 , Time elapsed: 0 .339 s <<< FAILURE! - in com.ibm.garage.cpat.lab.TestAggregate [ ERROR ] aggregatedMessageExists Time elapsed: 0 .122 s <<< ERROR! java.lang.NullPointerException at com.ibm.garage.cpat.lab.TestAggregate.aggregatedMessageExists ( TestAggregate.java:145 ) [ INFO ] Running com.ibm.garage.cpat.lab.TestFinancialMessage userId = 1 companyName = Metropolitan Museum of Art 2021 -01-17 13 :13:04,283 INFO [ org.apa.kaf.str.pro.int.StateDirectory ] ( main ) stream-thread [ main ] Deleting state directory 0_0 for task 0_0 as user calling cleanup. [ INFO ] Tests run: 1 , Failures: 0 , Errors: 0 , Skipped: 0 , Time elapsed: 0 .055 s - in com.ibm.garage.cpat.lab.TestFinancialMessage [ INFO ] Running com.ibm.garage.cpat.lab.TestLoadKtableFromTopic C01:Health Care C02:Finance C03:Consumer Services C04:Transportation C05:Capital Goods C06:Public Utilities sector-types-store 2021 -01-17 13 :13:04,333 WARN [ org.apa.kaf.str.sta.int.RocksDBStore ] ( main ) Closing 1 open iterators for store sector-types-store 2021 -01-17 13 :13:04,335 INFO [ org.apa.kaf.str.pro.int.StateDirectory ] ( main ) stream-thread [ main ] Deleting state directory 0_0 for task 0_0 as user calling cleanup. [ INFO ] Tests run: 1 , Failures: 0 , Errors: 0 , Skipped: 0 , Time elapsed: 0 .043 s - in com.ibm.garage.cpat.lab.TestLoadKtableFromTopic 2021 -01-17 13 :13:04,372 INFO [ io.quarkus ] ( main ) Quarkus stopped in 0 .029s [ INFO ] [ INFO ] Results: [ INFO ] [ ERROR ] Errors: [ ERROR ] TestAggregate.aggregatedMessageExists:145 NullPointer [ INFO ] [ ERROR ] Tests run: 4 , Failures: 0 , Errors: 1 , Skipped: 0 We see the test fails, but that is expected. In this test we have two different financialMessage inserted with a key of \"1\" and there is only one entry in the KTable (\"1\", \"Metropolitan Museum of Art\") . Those two financialMessage will get enriched with the only record in the KTable. Later on, those two EnrichedMessage should get grouped by as they have the same key and result in an AggregatedMessage . There are two assertions in this test. The first one passes as the aggregatedTableStore contains two entries: The first EnrichedMessage that eas aggregated with a new empty AggregatedMessage as the initializer and then the second EnrichedMessage that it was aggregated with the resulting AggregatedMessage of the previous EnrichedMessage . However, the second second assertion fails. The reason for this is that even though there are two records in the aggregatedTableStore , this store is key based and will return, as a result, the latest AggregatedMessage it holds for a particular key. Then, if we want to retrieve the latest AggregatedMessage for out key 1 we need to change the assetion: Assertions . assertEquals ( 16389.3 , aggregatedTableStore . get ( \"2\" ). value (). average ); to use the appropriate key: Assertions . assertEquals ( 16389.3 , aggregatedTableStore . get ( \"1\" ). value (). average ); Test the application by running the following: ./mvnw clean verify You should see the following output: [ INFO ] ------------------------------------------------------- [ INFO ] T E S T S [ INFO ] ------------------------------------------------------- [ INFO ] Running com.ibm.GreetingResourceTest 2021 -01-17 13 :14:40,370 INFO [ io.quarkus ] ( main ) Quarkus 1 .10.5.Final on JVM started in 2 .025s. Listening on: http://localhost:8081 2021 -01-17 13 :14:40,371 INFO [ io.quarkus ] ( main ) Profile test activated. 2021 -01-17 13 :14:40,372 INFO [ io.quarkus ] ( main ) Installed features: [ cdi, kafka-streams, resteasy, resteasy-jsonb ] [ INFO ] Tests run: 1 , Failures: 0 , Errors: 0 , Skipped: 0 , Time elapsed: 5 .495 s - in com.ibm.GreetingResourceTest [ INFO ] Running com.ibm.garage.cpat.lab.TestAggregate Average = 16389 .3 2021 -01-17 13 :14:42,114 INFO [ org.apa.kaf.str.pro.int.StateDirectory ] ( main ) stream-thread [ main ] Deleting state directory 0_0 for task 0_0 as user calling cleanup. [ INFO ] Tests run: 1 , Failures: 0 , Errors: 0 , Skipped: 0 , Time elapsed: 0 .33 s - in com.ibm.garage.cpat.lab.TestAggregate [ INFO ] Running com.ibm.garage.cpat.lab.TestFinancialMessage userId = 1 companyName = Metropolitan Museum of Art 2021 -01-17 13 :14:42,177 INFO [ org.apa.kaf.str.pro.int.StateDirectory ] ( main ) stream-thread [ main ] Deleting state directory 0_0 for task 0_0 as user calling cleanup. [ INFO ] Tests run: 1 , Failures: 0 , Errors: 0 , Skipped: 0 , Time elapsed: 0 .051 s - in com.ibm.garage.cpat.lab.TestFinancialMessage [ INFO ] Running com.ibm.garage.cpat.lab.TestLoadKtableFromTopic C01:Health Care C02:Finance C03:Consumer Services C04:Transportation C05:Capital Goods C06:Public Utilities sector-types-store 2021 -01-17 13 :14:42,227 WARN [ org.apa.kaf.str.sta.int.RocksDBStore ] ( main ) Closing 1 open iterators for store sector-types-store 2021 -01-17 13 :14:42,229 INFO [ org.apa.kaf.str.pro.int.StateDirectory ] ( main ) stream-thread [ main ] Deleting state directory 0_0 for task 0_0 as user calling cleanup. [ INFO ] Tests run: 1 , Failures: 0 , Errors: 0 , Skipped: 0 , Time elapsed: 0 .042 s - in com.ibm.garage.cpat.lab.TestLoadKtableFromTopic 2021 -01-17 13 :14:42,272 INFO [ io.quarkus ] ( main ) Quarkus stopped in 0 .035s [ INFO ] [ INFO ] Results: [ INFO ] [ INFO ] Tests run: 4 , Failures: 0 , Errors: 0 , Skipped: 0 Producing to and Consuming from a Kafka Topic on Event Streams \u00b6 When using the Kafka Streams API, we usually do it against a Kafka instance containing data already in, at least, one of its topics. In order to build up that scenario, we are going to use the MicroProfile Reactive Messaging library to send messages to a topic. For using the MicroProfile Reactive Messaging library, we first need to add such dependency to our pom.xml file: <dependency> <groupId> io.quarkus </groupId> <artifactId> quarkus-smallrye-reactive-messaging-kafka </artifactId> </dependency> Create the MockProducer.java Java file in src/main/java/com/ibm/garage/cpat/infrastructure with the following code: package com.ibm.garage.cpat.infrastructure ; import javax.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Outgoing ; import io.reactivex.Flowable ; import io.smallrye.reactive.messaging.kafka.KafkaRecord ; import java.util.concurrent.TimeUnit ; import java.util.Random ; import com.ibm.garage.cpat.domain.* ; @ApplicationScoped public class MockProducer { private Random random = new Random (); FinancialMessage mock = new FinancialMessage ( \"1\" , \"MET\" , \"SWISS\" , 12 , 1822.38 , 21868.55 , 94 , 7 , true ); @Outgoing ( \"mock-messages\" ) public Flowable < KafkaRecord < String , FinancialMessage >> produceMock () { return Flowable . interval ( 5 , TimeUnit . SECONDS ) . map ( tick -> { return setRandomUserId ( mock ); }); } public KafkaRecord < String , FinancialMessage > setRandomUserId ( FinancialMessage mock ) { mock . userId = String . valueOf ( random . nextInt ( 100 )); return KafkaRecord . of ( mock . userId , mock ); } } The producer code above produces a FinancialMessage every 5 seconds to the mock-messages channel with a random userId (out of 100). We will see later on how the mock-messages channel relates to a Kafka topic through configuration. Next, create the topology that we are going to build for processing the messages sent by our previous producer. Create a FinancialMessageTopology.java Java file in src/main/java/com/ibm/garage/cpat/domain with the following code: package com.ibm.garage.cpat.domain ; import javax.enterprise.context.ApplicationScoped ; import javax.enterprise.inject.Produces ; import org.eclipse.microprofile.config.inject.ConfigProperty ; import org.apache.kafka.common.serialization.Serdes ; import org.apache.kafka.streams.StreamsBuilder ; import org.apache.kafka.streams.Topology ; import org.apache.kafka.streams.kstream.Consumed ; import org.apache.kafka.streams.kstream.Produced ; import io.quarkus.kafka.client.serialization.JsonbSerde ; @ApplicationScoped public class FinancialMessageTopology { @ConfigProperty ( name = \"START_TOPIC_NAME\" ) private String INCOMING_TOPIC ; @ConfigProperty ( name = \"TARGET_TOPIC_NAME\" ) private String OUTGOING_TOPIC ; @Produces public Topology buildTopology () { StreamsBuilder builder = new StreamsBuilder (); JsonbSerde < FinancialMessage > financialMessageSerde = new JsonbSerde <> ( FinancialMessage . class ); // Stream reads from input topic, filters it by checking the boolean field on the message. // If the boolean is true, it gets passed to the mapValues function which will then send that record // to an outgoing topic. builder . stream ( INCOMING_TOPIC , Consumed . with ( Serdes . String (), financialMessageSerde ) ) . filter ( ( key , message ) -> checkValidation ( message ) ) . mapValues ( checkedMessage -> adjustPostValidation ( checkedMessage ) ) . to ( OUTGOING_TOPIC , Produced . with ( Serdes . String (), financialMessageSerde ) ); return builder . build (); } public boolean checkValidation ( FinancialMessage message ) { return ( message . technicalValidation ); } public FinancialMessage adjustPostValidation ( FinancialMessage message ) { message . totalCost = message . totalCost * 1.15 ; return message ; } } The code above builds a KStream from the messages in INCOMING_TOPIC . It will filter the FinanacialMessage based on a boolean property of them. Then, it will do some adjustment of the cost attribute within that FinancialMessage post message validation and send that out to the OUTGOING_TOPIC . Now, we have seen the code for both the mock producer and the topology we want to build are dependant on certain configuration variables such as the input and output topics they are meant to produce to and consume from. In a Quarkus application, all the configuration settings is done through a properties file, which makes the application portable across different environments. This property file, which we already had to configure in previous labs is called application.properties and is located in src/main/resources . In order to configure the application to work with an IBM Event Streams v10 or later, paste the following configuration in the application.properties file: quarkus.http.port = 8080 quarkus.log.console.enable = true quarkus.log.console.level = INFO # Base ES Connection Details mp.messaging.connector.smallrye-kafka.bootstrap.servers = ${BOOTSTRAP_SERVERS} mp.messaging.connector.smallrye-kafka.security.protocol = SASL_SSL mp.messaging.connector.smallrye-kafka.ssl.protocol = TLSv1.2 mp.messaging.connector.smallrye-kafka.sasl.mechanism = SCRAM-SHA-512 mp.messaging.connector.smallrye-kafka.sasl.jaas.config = org.apache.kafka.common.security.scram.ScramLoginModule required \\ username=${SCRAM_USERNAME} \\ password=${SCRAM_PASSWORD}; mp.messaging.connector.smallrye-kafka.ssl.truststore.location = ${CERT_LOCATION} mp.messaging.connector.smallrye-kafka.ssl.truststore.password = ${CERT_PASSWORD} mp.messaging.connector.smallrye-kafka.ssl.truststore.type = PKCS12 # Initial mock JSON message producer configuration mp.messaging.outgoing.mock-messages.connector = smallrye-kafka mp.messaging.outgoing.mock-messages.topic = ${START_TOPIC_NAME} mp.messaging.outgoing.mock-messages.value.serializer = io.quarkus.kafka.client.serialization.JsonbSerializer # Quarkus Kafka Streams configuration settings quarkus.kafka-streams.bootstrap-servers = ${BOOTSTRAP_SERVERS} quarkus.kafka-streams.application-id = financial-stream quarkus.kafka-streams.application-server = localhost:8080 quarkus.kafka-streams.topics = ${START_TOPIC_NAME},${TARGET_TOPIC_NAME} quarkus.kafka-streams.health.enabled = true quarkus.kafka-streams.security.protocol = SASL_SSL quarkus.kafka-streams.ssl.protocol = TLSv1.2 quarkus.kafka-streams.sasl.mechanism = SCRAM-SHA-512 quarkus.kafka-streams.sasl.jaas.config = org.apache.kafka.common.security.scram.ScramLoginModule required \\ username=${SCRAM_USERNAME} \\ password=${SCRAM_PASSWORD}; quarkus.kafka-streams.ssl.truststore.location = ${CERT_LOCATION} quarkus.kafka-streams.ssl.truststore.password = ${CERT_PASSWORD} quarkus.kafka-streams.ssl.truststore.type = PKCS12 # pass-through options kafka-streams.cache.max.bytes.buffering = 10240 kafka-streams.commit.interval.ms = 1000 kafka-streams.metadata.max.age.ms = 500 kafka-streams.auto.offset.reset = latest kafka-streams.metrics.recording.level = DEBUG If using a previous IBM Event Streams version (such as v2019.4.2) or on IBM Cloud, use the following configuration in the application.properties file: quarkus.http.port = 8080 quarkus.log.console.enable = true quarkus.log.console.level = INFO # Base ES Connection Details mp.messaging.connector.smallrye-kafka.bootstrap.servers = ${BOOTSTRAP_SERVERS} mp.messaging.connector.smallrye-kafka.security.protocol = SASL_SSL mp.messaging.connector.smallrye-kafka.ssl.protocol = TLSv1.2 mp.messaging.connector.smallrye-kafka.sasl.mechanism = PLAIN mp.messaging.connector.smallrye-kafka.sasl.jaas.config = org.apache.kafka.common.security.scram.PlainLoginModule required \\ username=\"token\" \\ password=${API_KEY}; # If connecting to Event Streams on IBM Cloud the following truststore options are not needed. mp.messaging.connector.smallrye-kafka.ssl.truststore.location = ${CERT_LOCATION} mp.messaging.connector.smallrye-kafka.ssl.truststore.password = password # Initial mock JSON message producer configuration mp.messaging.outgoing.mock-messages.connector = smallrye-kafka mp.messaging.outgoing.mock-messages.topic = ${START_TOPIC_NAME} mp.messaging.outgoing.mock-messages.value.serializer = io.quarkus.kafka.client.serialization.JsonbSerializer # Quarkus Kafka Streams configuration settings quarkus.kafka-streams.bootstrap-servers = ${BOOTSTRAP_SERVERS} quarkus.kafka-streams.application-id = financial-stream quarkus.kafka-streams.application-server = localhost:8080 quarkus.kafka-streams.topics = ${START_TOPIC_NAME},${TARGET_TOPIC_NAME} quarkus.kafka-streams.health.enabled = true quarkus.kafka-streams.security.protocol = SASL_SSL quarkus.kafka-streams.ssl.protocol = TLSv1.2 quarkus.kafka-streams.sasl.mechanism = PLAIN quarkus.kafka-streams.sasl.jaas.config = org.apache.kafka.common.security.scram.PlainLoginModule required \\ username=\"token\" \\ password=${API_KEY}; # If connecting to Event Streams on IBM Cloud the following truststore options are not needed. quarkus.kafka-streams.ssl.truststore.location = ${CERT_LOCATION} quarkus.kafka-streams.ssl.truststore.password = password # pass-through options kafka-streams.cache.max.bytes.buffering = 10240 kafka-streams.commit.interval.ms = 1000 kafka-streams.metadata.max.age.ms = 500 kafka-streams.auto.offset.reset = latest kafka-streams.metrics.recording.level = DEBUG There are some environment variables our application.properties file depends on: START_TOPIC_NAME : The Kafka topic the mock producer will produce FinancialMessage to and the topology consume messages from for processing. IMPORTANT: Use a topic name with some unique identifier if you are sharing the IBM Event Streams instance with other lab students. Also, you must create this topic in IBM Event Streams. See here for more details as to how to create a topic. TARGET_TOPIC_NAME : The Kafka topic the topology will produce the processed FinancialMessage to. IMPORTANT: Use a topic name with some unique identifier if you are sharing the IBM Event Streams instance with other lab students. Also, you must create this topic in IBM Event Streams. See [here]((/use-cases/overview/pre-requisites) for more details as to how to create a topic BOOTSTRAP_SERVERS : Your IBM Event Streams bootstrap server. See here for more details as to how to obtain these. CERT_LOCATION : The location where the PKCS12 certificate for the SSL connection to the IBM Event Streams instance is. See here for more details as to how to obtain these. CERT_PASSWORD : The password of the PKCS12 certificate. See here for more details as to how to obtain these. API_KEY if you are using an IBM Event Streams instance in IBM Cloud or SCRAM_USERNAME and SCRAM_PASSWORD : The SCRAM credentials for your application to get authenticated and authorized to work with IBM Event Streams. See here for more details as to how to obtain these. Export the variables and values on the terminal you will run the application from: IBM Event Strams v10 or later: export BOOTSTRAP_SERVERS = your-bootstrap-server-address:443 \\ export START_TOPIC_NAME = name-of-topic-to-consume-from \\ export TARGET_TOPIC_NAME = name-of-topic-to-produce-to \\ export CERT_LOCATION = /path-to-pkcs12-cert/es-cert.p12 \\ export CERT_PASSWORD = certificate-password \\ export SCRAM_USERNAME = your-scram-username \\ export SCRAM_PASSWORD = your-scram-password Previous IBM Event Streams versions: export BOOTSTRAP_SERVERS = your-bootstrap-server-address:443 \\ export START_TOPIC_NAME = name-of-topic-to-consume-from \\ export TARGET_TOPIC_NAME = name-of-topic-to-produce-to \\ export CERT_LOCATION = /path-to-jks-cert/es-cert.jks \\ export API_KEY = your-api-key Local Kafka Cluster: export BOOTSTRAP_SERVERS = your-bootstrap-server-address:443 \\ export START_TOPIC_NAME = name-of-topic-to-consume-from \\ export TARGET_TOPIC_NAME = name-of-topic-to-produce-to You can now test the Quarkus application ./mvnw quarkus:dev You should now be able to see FinancialMessage events in the input topic and those processed messages in the output topic you have specified above in the IBM Event Streams user interface.","title":"Kafka Streams Test Lab 2"},{"location":"use-cases/kafka-streams/lab-2/#kafka-streams-test-lab-2","text":"","title":"Kafka Streams Test Lab 2"},{"location":"use-cases/kafka-streams/lab-2/#overview","text":"This is a continuation of the previous Lab 1 . You should complete Lab 1 first before you get started here. There's a few more pre-reqs (if you so choose to use them) outlined below.","title":"Overview"},{"location":"use-cases/kafka-streams/lab-2/#scenario-prerequisites","text":"Java For the purposes of this lab we suggest Java 8+ Maven Maven will be needed for bootstrapping our application from the command-line and running our application. An IDE of your choice Ideally an IDE that supports Quarkus (such as Visual Studio Code) OpenShift Container Platform, IBM Cloud Pak for Integration and IBM Event Streams This is an optional portion of the lab for those who have access to an OCP Cluster where IBM Cloud Pak for Integration has been installed on top and an IBM Event Streams instance deployed. The following are optional OpenShift Container Platform v4.4.x IBM Cloud Pak for Integration CP4I2022.2 IBM Event Streams : IBM Event Streams v10 or later preferrably. If you are using a previous version of IBM Event Streams, there are some differences as to how you would configure application.properties to establish the connection to IBM Event Streams.","title":"Scenario Prerequisites"},{"location":"use-cases/kafka-streams/lab-2/#adding-in-more-kafka-streams-operators","text":"In this section we are going to to add more functionality to our previous test class in order to see, work with and understand more Kafka Streams operators. Add the following definitions to the TestFinancialMessage.java Java class: private static String tradingTable = \"tradingTable\" ; private static String tradingStoreName = \"tradingStore\" ; private static TestInputTopic < String , String > tradingTableTopic ; Add the following Store and KTable definitions inside the buildTopology() function to support the trading fuctionality we are adding to our application: KeyValueBytesStoreSupplier tradingStoreSupplier = Stores . persistentKeyValueStore ( tradingStoreName ); KTable < String , String > stockTradingStore = builder . table ( tradingTable , Consumed . with ( Serdes . String (), Serdes . String ()), Materialized . as ( tradingStoreSupplier )); Add the following import to your Java class so that you can use objects of type KTable: import org.apache.kafka.streams.kstream.KTable ; Edit the branch[1] logic again to create new KeyValue pairs of userId and stockSymbol branches [ 1 ] . filter ( ( key , value ) -> ( value . totalCost > 5000 ) ) . map ( ( key , value ) -> KeyValue . pair ( value . userId , value . stockSymbol ) ) . to ( tradingTable , Produced . with ( Serdes . String (), Serdes . String ()) ); Notice that, previously, we wrote straight to outTopic . However, we are now writing to a KTable which we can query in our tests by using the State Store it is materialised as. Before we create a test for the new functionality, remove or comment out the previous existing tests cases as these do no longer apply. Create a new test with the code below: @Test public void filterAndMapNewPair () { FinancialMessage mock = new FinancialMessage ( \"1\" , \"MET\" , \"SWISS\" , 12 , 1822.38 , 21868.55 , 94 , 7 , true ); inTopic . pipeInput ( \"1\" , mock ); KeyValueStore < String , ValueAndTimestamp < String >> tableStore = testDriver . getTimestampedKeyValueStore ( tradingStoreName ); Assertions . assertEquals ( 1 , tableStore . approximateNumEntries ()); Assertions . assertEquals ( \"MET\" , tableStore . get ( \"1\" ). value ()); } The first assertion checks whether the store has a record. The second assertion checks that the mock record that we inserted has the correct value as our map function created new KeyValue pairs of type <userId, stockSymbol> . Test the application by running the following: ./mvnw clean verify You should see the tests pass with the following output: [ INFO ] ------------------------------------------------------- [ INFO ] T E S T S [ INFO ] ------------------------------------------------------- [ INFO ] Running com.ibm.GreetingResourceTest 2021 -01-16 20 :47:06,478 INFO [ io.quarkus ] ( main ) Quarkus 1 .10.5.Final on JVM started in 2 .086s. Listening on: http://localhost:8081 2021 -01-16 20 :47:06,479 INFO [ io.quarkus ] ( main ) Profile test activated. 2021 -01-16 20 :47:06,479 INFO [ io.quarkus ] ( main ) Installed features: [ cdi, kafka-streams, resteasy, resteasy-jsonb ] [ INFO ] Tests run: 1 , Failures: 0 , Errors: 0 , Skipped: 0 , Time elapsed: 5 .611 s - in com.ibm.GreetingResourceTest [ INFO ] Running com.ibm.garage.cpat.lab.TestFinancialMessage 2021 -01-16 20 :47:08,248 INFO [ org.apa.kaf.str.pro.int.StateDirectory ] ( main ) stream-thread [ main ] Deleting state directory 0_0 for task 0_0 as user calling cleanup. [ INFO ] Tests run: 1 , Failures: 0 , Errors: 0 , Skipped: 0 , Time elapsed: 0 .276 s - in com.ibm.garage.cpat.lab.TestFinancialMessage [ INFO ] Running com.ibm.garage.cpat.lab.TestLoadKtableFromTopic C01:Health Care C02:Finance C03:Consumer Services C04:Transportation C05:Capital Goods C06:Public Utilities sector-types-store 2021 -01-16 20 :47:08,290 WARN [ org.apa.kaf.str.sta.int.RocksDBStore ] ( main ) Closing 1 open iterators for store sector-types-store 2021 -01-16 20 :47:08,292 INFO [ org.apa.kaf.str.pro.int.StateDirectory ] ( main ) stream-thread [ main ] Deleting state directory 0_0 for task 0_0 as user calling cleanup. [ INFO ] Tests run: 1 , Failures: 0 , Errors: 0 , Skipped: 0 , Time elapsed: 0 .035 s - in com.ibm.garage.cpat.lab.TestLoadKtableFromTopic 2021 -01-16 20 :47:08,325 INFO [ io.quarkus ] ( main ) Quarkus stopped in 0 .026s [ INFO ] [ INFO ] Results: [ INFO ] [ INFO ] Tests run: 3 , Failures: 0 , Errors: 0 , Skipped: 0 Now we are going to do something a little bit more advanced. We are going to join a KStream with a KTable. The Streams API has an inner join, left join, and an outer join. KStream-KTable joins are non-windowed and asymmetric. By asymmetric we mean that a join only gets triggered if the left input KStream gets a new record while the right KTable holds the latest input records materialized. Add the following new attributes: private static String joinedTopicName = \"joinedTopic\" ; private static TestOutputTopic < String , String > joinedTopic ; private static String joinedStoreName = \"joinedStore\" ; Replace the buildTopology() function for the following new one: public static void buildTopology () { final StreamsBuilder builder = new StreamsBuilder (); KeyValueBytesStoreSupplier storeSupplier = Stores . persistentKeyValueStore ( storeName ); KeyValueBytesStoreSupplier tradingStoreSupplier = Stores . persistentKeyValueStore ( tradingStoreName ); KeyValueBytesStoreSupplier joinedStoreSupplier = Stores . persistentKeyValueStore ( joinedStoreName ); KStream < String , FinancialMessage > transactionStream = builder . stream ( inTopicName , Consumed . with ( Serdes . String (), financialMessageSerde ) ); KTable < String , String > stockTradingStore = builder . table ( tradingTable , Consumed . with ( Serdes . String (), Serdes . String ()), Materialized . as ( tradingStoreSupplier )); KTable < String , String > joinedMessageStore = builder . table ( joinedTopicName , Consumed . with ( Serdes . String (), Serdes . String ()), Materialized . as ( joinedStoreSupplier )); KStream < String , String > joinedStream = transactionStream . join ( stockTradingStore , ( financialMessage , companyName ) -> \"userId = \" + financialMessage . userId + \" companyName = \" + companyName ); joinedStream . to ( joinedTopicName , Produced . with ( Serdes . String (), Serdes . String ())); testDriver = new TopologyTestDriver ( builder . build (), getStreamsConfig ()); inTopic = testDriver . createInputTopic ( inTopicName , new StringSerializer (), new JsonbSerializer < FinancialMessage > ()); tradingTableTopic = testDriver . createInputTopic ( tradingTable , new StringSerializer (), new StringSerializer ()); joinedTopic = testDriver . createOutputTopic ( joinedTopicName , new StringDeserializer (), new StringDeserializer ()); } We can see that our buildTopology() function still contains the transactionsStream KStream that will contain the stream of FinancialMessages being received through the inTopicName . Then, We can see a KTable called stockTradingStore , which will get materialized as a State Store, that will contain the messages comming in through the input topic called tradingTable . This KTable will hold data about the tradding companies that will serve to enhance the incoming FinancialMessages with. The join between the KStream and the KTable is being done below and is called joinedStream . The result of this join is being outputed into a topic called joinedTopicName . Finally, this output topic is being store in a KTable called joinedMessageStore , and materialized in its respective State Store, in order to be able to query it later on in our tests. The inner join is performed on matching keys between the KStream and the KTable and the matched records produce a new <String, String> pair with the value of userId and companyName . In order to test the above new functionality of the buildTopology() function, remove or comment out the existing test and create the following new one: @Test public void checkStreamAndTableJoinHasOneRecord () { tradingTableTopic . pipeInput ( \"1\" , \"Metropolitan Museum of Art\" ); FinancialMessage mock = new FinancialMessage ( \"1\" , \"MET\" , \"SWISS\" , 12 , 1822.38 , 21868.55 , 94 , 7 , true ); inTopic . pipeInput ( \"1\" , mock ); KeyValueStore < String , ValueAndTimestamp < String >> joinedTableStore = testDriver . getTimestampedKeyValueStore ( joinedStoreName ); Assertions . assertEquals ( 1 , joinedTableStore . approximateNumEntries ()); System . out . println ( joinedTableStore . get ( \"1\" ). value ()); } Test the application by running the following: ./mvnw clean verify You should see the following output: [ INFO ] ------------------------------------------------------- [ INFO ] T E S T S [ INFO ] ------------------------------------------------------- [ INFO ] Running com.ibm.GreetingResourceTest 2021 -01-16 21 :53:03,682 INFO [ io.quarkus ] ( main ) Quarkus 1 .10.5.Final on JVM started in 2 .134s. Listening on: http://localhost:8081 2021 -01-16 21 :53:03,684 INFO [ io.quarkus ] ( main ) Profile test activated. 2021 -01-16 21 :53:03,685 INFO [ io.quarkus ] ( main ) Installed features: [ cdi, kafka-streams, resteasy, resteasy-jsonb ] [ INFO ] Tests run: 1 , Failures: 0 , Errors: 0 , Skipped: 0 , Time elapsed: 5 .574 s - in com.ibm.GreetingResourceTest [ INFO ] Running com.ibm.garage.cpat.lab.TestFinancialMessage userId = 1 companyName = Metropolitan Museum of Art 2021 -01-16 21 :53:05,432 INFO [ org.apa.kaf.str.pro.int.StateDirectory ] ( main ) stream-thread [ main ] Deleting state directory 0_0 for task 0_0 as user calling cleanup. [ INFO ] Tests run: 1 , Failures: 0 , Errors: 0 , Skipped: 0 , Time elapsed: 0 .294 s - in com.ibm.garage.cpat.lab.TestFinancialMessage [ INFO ] Running com.ibm.garage.cpat.lab.TestLoadKtableFromTopic C01:Health Care C02:Finance C03:Consumer Services C04:Transportation C05:Capital Goods C06:Public Utilities sector-types-store 2021 -01-16 21 :53:05,482 WARN [ org.apa.kaf.str.sta.int.RocksDBStore ] ( main ) Closing 1 open iterators for store sector-types-store 2021 -01-16 21 :53:05,484 INFO [ org.apa.kaf.str.pro.int.StateDirectory ] ( main ) stream-thread [ main ] Deleting state directory 0_0 for task 0_0 as user calling cleanup. [ INFO ] Tests run: 1 , Failures: 0 , Errors: 0 , Skipped: 0 , Time elapsed: 0 .044 s - in com.ibm.garage.cpat.lab.TestLoadKtableFromTopic 2021 -01-16 21 :53:05,518 INFO [ io.quarkus ] ( main ) Quarkus stopped in 0 .026s [ INFO ] [ INFO ] Results: [ INFO ] [ INFO ] Tests run: 3 , Failures: 0 , Errors: 0 , Skipped: 0 We can see that our test passes successfully as the FinancialMessage with userId=1 and the KTable record with key 1 successfully joined to produce the output message: userId = 1 companyName = Metropolitan Museum of Art We are now going to create yet another new test class. But first, we are going to create two new POJO classes to work with. These are EnrichedMessage.java and AggregatedMessage.java . Both of them should be placed where the previos POJO class is: src/main/java/com/ibm/garage/cpat/domain . Create the EnrichedMessage.java Java file with the following code: package com.ibm.garage.cpat.domain ; public class EnrichedMessage { public String userId ; public String stockSymbol ; public int quantity ; public double stockPrice ; public double totalCost ; public double adjustedCost ; public boolean technicalValidation ; public String companyName ; public EnrichedMessage ( FinancialMessage message , String companyName ) { this . userId = message . userId ; this . stockSymbol = message . stockSymbol ; this . quantity = message . quantity ; this . stockPrice = message . stockPrice ; this . totalCost = message . totalCost ; this . companyName = companyName ; if ( message . technicalValidation ) { this . technicalValidation = message . technicalValidation ; this . adjustedCost = message . totalCost * 1.15 ; } else { this . technicalValidation = message . technicalValidation ; this . adjustedCost = message . totalCost ; } } } Create the AggregatedMessage.java Java file with the following code: package com.ibm.garage.cpat.domain ; import java.math.BigDecimal ; import java.math.RoundingMode ; public class AggregatedMessage { public String userId ; public String stockSymbol ; public int quantity ; public double stockPrice ; public double totalCost ; public double adjustedCost ; public boolean technicalValidation ; public String companyName ; public int count ; public double sum ; public double average ; public AggregatedMessage updateFrom ( EnrichedMessage message ) { this . userId = message . userId ; this . stockSymbol = message . stockSymbol ; this . quantity = message . quantity ; this . stockPrice = message . stockPrice ; this . totalCost = message . totalCost ; this . companyName = message . companyName ; this . adjustedCost = message . adjustedCost ; this . technicalValidation = message . technicalValidation ; this . count ++ ; this . sum += message . adjustedCost ; this . average = BigDecimal . valueOf ( sum / count ) . setScale ( 1 , RoundingMode . HALF_UP ). doubleValue (); return this ; } } Now create the new test class named TestAggregate.java in the same path we have the other test classes ( src/test/java/com/ibm/garage/cpat ) and paste the following code: package com.ibm.garage.cpat.lab ; import java.util.Properties ; import org.apache.kafka.common.serialization.LongDeserializer ; import org.apache.kafka.common.serialization.Serde ; import org.apache.kafka.common.serialization.Serdes ; import org.apache.kafka.common.serialization.StringDeserializer ; import org.apache.kafka.common.serialization.StringSerializer ; import org.apache.kafka.streams.KeyValue ; import org.apache.kafka.streams.StreamsBuilder ; import org.apache.kafka.streams.StreamsConfig ; import org.apache.kafka.streams.TestInputTopic ; import org.apache.kafka.streams.TestOutputTopic ; import org.apache.kafka.streams.TopologyTestDriver ; import org.apache.kafka.streams.kstream.Consumed ; import org.apache.kafka.streams.kstream.KGroupedStream ; import org.apache.kafka.streams.kstream.KStream ; import org.apache.kafka.streams.kstream.KTable ; import org.apache.kafka.streams.kstream.Materialized ; import org.apache.kafka.streams.kstream.Produced ; import org.apache.kafka.streams.kstream.Windowed ; import org.apache.kafka.streams.kstream.WindowedSerdes ; import org.apache.kafka.streams.processor.StateStore ; import org.apache.kafka.streams.state.KeyValueBytesStoreSupplier ; import org.apache.kafka.streams.state.KeyValueIterator ; import org.apache.kafka.streams.state.KeyValueStore ; import org.apache.kafka.streams.state.Stores ; import org.apache.kafka.streams.state.ValueAndTimestamp ; import org.junit.jupiter.api.AfterAll ; import org.junit.jupiter.api.Assertions ; import org.junit.jupiter.api.BeforeAll ; import org.junit.jupiter.api.Test ; import io.quarkus.kafka.client.serialization.JsonbDeserializer ; import io.quarkus.kafka.client.serialization.JsonbSerde ; import io.quarkus.kafka.client.serialization.JsonbSerializer ; import io.quarkus.test.junit.QuarkusTest ; import com.ibm.garage.cpat.domain.* ; @QuarkusTest public class TestAggregate { private static TopologyTestDriver testDriver ; private static String inTopicName = \"financialMessages\" ; private static String outTopicName = \"enrichedMessages\" ; private static String storeName = \"financialStore\" ; private static String aggregatedTopicName = \"aggregatedMessages\" ; private static String companyTable = \"companyTable\" ; private static String companyStoreName = \"companyStore\" ; private static TestInputTopic < String , FinancialMessage > inTopic ; private static TestOutputTopic < String , EnrichedMessage > outTopic ; private static TestOutputTopic < String , AggregatedMessage > aggregatedTopic ; private static TestInputTopic < String , String > companyTableTopic ; private static final JsonbSerde < FinancialMessage > financialMessageSerde = new JsonbSerde <> ( FinancialMessage . class ); private static final JsonbSerde < EnrichedMessage > enrichedMessageSerde = new JsonbSerde <> ( EnrichedMessage . class ); private static final JsonbSerde < AggregatedMessage > aggregatedMessageSerde = new JsonbSerde <> ( AggregatedMessage . class ); public static Properties getStreamsConfig () { final Properties props = new Properties (); props . put ( StreamsConfig . APPLICATION_ID_CONFIG , \"kstream-lab3\" ); props . put ( StreamsConfig . BOOTSTRAP_SERVERS_CONFIG , \"dummmy:3456\" ); props . put ( StreamsConfig . DEFAULT_KEY_SERDE_CLASS_CONFIG , Serdes . String (). getClass ()); //props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, financialMessageSerde); return props ; } @BeforeAll public static void buildTopology () { final StreamsBuilder builder = new StreamsBuilder (); KeyValueBytesStoreSupplier storeSupplier = Stores . persistentKeyValueStore ( storeName ); KeyValueBytesStoreSupplier companyStoreSupplier = Stores . persistentKeyValueStore ( companyStoreName ); // create a KStream for financial messages. KStream < String , FinancialMessage > financialStream = builder . stream ( inTopicName , Consumed . with ( Serdes . String (), financialMessageSerde ) ); // create a KTable from a topic for companies. KTable < String , String > companyStore = builder . table ( companyTable , Consumed . with ( Serdes . String (), Serdes . String ()), Materialized . as ( companyStoreSupplier )); // join KStream with KTable and use aggregate. KStream < String , EnrichedMessage > enrichedStream = financialStream . join ( companyStore , //(financialMessage, companyName) -> financialMessage.userId, ( financialMessage , companyName ) -> { return new EnrichedMessage ( financialMessage , companyName ); } ); enrichedStream . groupByKey () . aggregate ( AggregatedMessage :: new , ( userId , value , aggregatedMessage ) -> aggregatedMessage . updateFrom ( value ), Materialized . < String , AggregatedMessage > as ( storeSupplier ) . withKeySerde ( Serdes . String ()) . withValueSerde ( aggregatedMessageSerde ) ) . toStream () . to ( aggregatedTopicName , Produced . with ( Serdes . String (), aggregatedMessageSerde ) ); testDriver = new TopologyTestDriver ( builder . build (), getStreamsConfig ()); inTopic = testDriver . createInputTopic ( inTopicName , new StringSerializer (), new JsonbSerializer < FinancialMessage > ()); //outTopic = testDriver.createOutputTopic(outTopicName, new StringDeserializer(), new JsonbDeserializer<>(EnrichedMessage.class)); companyTableTopic = testDriver . createInputTopic ( companyTable , new StringSerializer (), new StringSerializer ()); aggregatedTopic = testDriver . createOutputTopic ( aggregatedTopicName , new StringDeserializer (), new JsonbDeserializer <> ( AggregatedMessage . class )); } @AfterAll public static void close (){ testDriver . close (); } } Even though the code above might seem a completely new one at first glance, it is just an extra step of the previous code we have been working with. We can see that we still have both our financialStream KStream receiving FinancialMessage from the inTopicName topic and our companyStore KTable holding the latest being received from the companyTable input topic. Then, we also still have the join of the previous two KStream and KTable in our enrichedStream KStream that will be a KStream of EnrichedMessage . What is new in this code is the following aggregation we are doing on the resulting enrichedStream KStream. We are grouping EnrichedMessage objects by key and doing an aggregation of that grouping. The aggregation result will be an AggregatedMessage which will get materialized as storeSupplier so that we can query it later on. We are also converting that KTable to a KStream that will get outputed to aggregatedTopicName . The aggregate logic will simply work out some average and count of EnrichedMessage you can check out in the EnrichedMessage.java Java file. Now add a test that will make sure the functionality we have implemented in the buildTopology() function works as desired: @Test public void aggregatedMessageExists () { companyTableTopic . pipeInput ( \"1\" , \"Metropolitan Museum of Art\" ); FinancialMessage mock = new FinancialMessage ( \"1\" , \"MET\" , \"SWISS\" , 12 , 1822.38 , 21868.55 , 94 , 7 , true ); FinancialMessage mock2 = new FinancialMessage ( \"1\" , \"MET\" , \"SWISS\" , 12 , 1822.38 , 6634.56 , 94 , 7 , true ); inTopic . pipeInput ( \"1\" , mock ); inTopic . pipeInput ( \"1\" , mock2 ); KeyValueStore < String , ValueAndTimestamp < AggregatedMessage >> aggregatedTableStore = testDriver . getTimestampedKeyValueStore ( storeName ); Assertions . assertEquals ( 2 , aggregatedTableStore . approximateNumEntries ()); System . out . println ( \"Average = \" + aggregatedTableStore . get ( \"1\" ). value (). average ); Assertions . assertEquals ( 16389.3 , aggregatedTableStore . get ( \"2\" ). value (). average ); } Test the application by running the following: ./mvnw clean verify You should see the following output: [ INFO ] ------------------------------------------------------- [ INFO ] T E S T S [ INFO ] ------------------------------------------------------- [ INFO ] Running com.ibm.GreetingResourceTest 2021 -01-17 13 :13:02,533 INFO [ io.quarkus ] ( main ) Quarkus 1 .10.5.Final on JVM started in 2 .178s. Listening on: http://localhost:8081 2021 -01-17 13 :13:02,535 INFO [ io.quarkus ] ( main ) Profile test activated. 2021 -01-17 13 :13:02,535 INFO [ io.quarkus ] ( main ) Installed features: [ cdi, kafka-streams, resteasy, resteasy-jsonb ] [ INFO ] Tests run: 1 , Failures: 0 , Errors: 0 , Skipped: 0 , Time elapsed: 5 .441 s - in com.ibm.GreetingResourceTest [ INFO ] Running com.ibm.garage.cpat.lab.TestAggregate Average = 16389 .3 2021 -01-17 13 :13:04,216 INFO [ org.apa.kaf.str.pro.int.StateDirectory ] ( main ) stream-thread [ main ] Deleting state directory 0_0 for task 0_0 as user calling cleanup. [ ERROR ] Tests run: 1 , Failures: 0 , Errors: 1 , Skipped: 0 , Time elapsed: 0 .339 s <<< FAILURE! - in com.ibm.garage.cpat.lab.TestAggregate [ ERROR ] aggregatedMessageExists Time elapsed: 0 .122 s <<< ERROR! java.lang.NullPointerException at com.ibm.garage.cpat.lab.TestAggregate.aggregatedMessageExists ( TestAggregate.java:145 ) [ INFO ] Running com.ibm.garage.cpat.lab.TestFinancialMessage userId = 1 companyName = Metropolitan Museum of Art 2021 -01-17 13 :13:04,283 INFO [ org.apa.kaf.str.pro.int.StateDirectory ] ( main ) stream-thread [ main ] Deleting state directory 0_0 for task 0_0 as user calling cleanup. [ INFO ] Tests run: 1 , Failures: 0 , Errors: 0 , Skipped: 0 , Time elapsed: 0 .055 s - in com.ibm.garage.cpat.lab.TestFinancialMessage [ INFO ] Running com.ibm.garage.cpat.lab.TestLoadKtableFromTopic C01:Health Care C02:Finance C03:Consumer Services C04:Transportation C05:Capital Goods C06:Public Utilities sector-types-store 2021 -01-17 13 :13:04,333 WARN [ org.apa.kaf.str.sta.int.RocksDBStore ] ( main ) Closing 1 open iterators for store sector-types-store 2021 -01-17 13 :13:04,335 INFO [ org.apa.kaf.str.pro.int.StateDirectory ] ( main ) stream-thread [ main ] Deleting state directory 0_0 for task 0_0 as user calling cleanup. [ INFO ] Tests run: 1 , Failures: 0 , Errors: 0 , Skipped: 0 , Time elapsed: 0 .043 s - in com.ibm.garage.cpat.lab.TestLoadKtableFromTopic 2021 -01-17 13 :13:04,372 INFO [ io.quarkus ] ( main ) Quarkus stopped in 0 .029s [ INFO ] [ INFO ] Results: [ INFO ] [ ERROR ] Errors: [ ERROR ] TestAggregate.aggregatedMessageExists:145 NullPointer [ INFO ] [ ERROR ] Tests run: 4 , Failures: 0 , Errors: 1 , Skipped: 0 We see the test fails, but that is expected. In this test we have two different financialMessage inserted with a key of \"1\" and there is only one entry in the KTable (\"1\", \"Metropolitan Museum of Art\") . Those two financialMessage will get enriched with the only record in the KTable. Later on, those two EnrichedMessage should get grouped by as they have the same key and result in an AggregatedMessage . There are two assertions in this test. The first one passes as the aggregatedTableStore contains two entries: The first EnrichedMessage that eas aggregated with a new empty AggregatedMessage as the initializer and then the second EnrichedMessage that it was aggregated with the resulting AggregatedMessage of the previous EnrichedMessage . However, the second second assertion fails. The reason for this is that even though there are two records in the aggregatedTableStore , this store is key based and will return, as a result, the latest AggregatedMessage it holds for a particular key. Then, if we want to retrieve the latest AggregatedMessage for out key 1 we need to change the assetion: Assertions . assertEquals ( 16389.3 , aggregatedTableStore . get ( \"2\" ). value (). average ); to use the appropriate key: Assertions . assertEquals ( 16389.3 , aggregatedTableStore . get ( \"1\" ). value (). average ); Test the application by running the following: ./mvnw clean verify You should see the following output: [ INFO ] ------------------------------------------------------- [ INFO ] T E S T S [ INFO ] ------------------------------------------------------- [ INFO ] Running com.ibm.GreetingResourceTest 2021 -01-17 13 :14:40,370 INFO [ io.quarkus ] ( main ) Quarkus 1 .10.5.Final on JVM started in 2 .025s. Listening on: http://localhost:8081 2021 -01-17 13 :14:40,371 INFO [ io.quarkus ] ( main ) Profile test activated. 2021 -01-17 13 :14:40,372 INFO [ io.quarkus ] ( main ) Installed features: [ cdi, kafka-streams, resteasy, resteasy-jsonb ] [ INFO ] Tests run: 1 , Failures: 0 , Errors: 0 , Skipped: 0 , Time elapsed: 5 .495 s - in com.ibm.GreetingResourceTest [ INFO ] Running com.ibm.garage.cpat.lab.TestAggregate Average = 16389 .3 2021 -01-17 13 :14:42,114 INFO [ org.apa.kaf.str.pro.int.StateDirectory ] ( main ) stream-thread [ main ] Deleting state directory 0_0 for task 0_0 as user calling cleanup. [ INFO ] Tests run: 1 , Failures: 0 , Errors: 0 , Skipped: 0 , Time elapsed: 0 .33 s - in com.ibm.garage.cpat.lab.TestAggregate [ INFO ] Running com.ibm.garage.cpat.lab.TestFinancialMessage userId = 1 companyName = Metropolitan Museum of Art 2021 -01-17 13 :14:42,177 INFO [ org.apa.kaf.str.pro.int.StateDirectory ] ( main ) stream-thread [ main ] Deleting state directory 0_0 for task 0_0 as user calling cleanup. [ INFO ] Tests run: 1 , Failures: 0 , Errors: 0 , Skipped: 0 , Time elapsed: 0 .051 s - in com.ibm.garage.cpat.lab.TestFinancialMessage [ INFO ] Running com.ibm.garage.cpat.lab.TestLoadKtableFromTopic C01:Health Care C02:Finance C03:Consumer Services C04:Transportation C05:Capital Goods C06:Public Utilities sector-types-store 2021 -01-17 13 :14:42,227 WARN [ org.apa.kaf.str.sta.int.RocksDBStore ] ( main ) Closing 1 open iterators for store sector-types-store 2021 -01-17 13 :14:42,229 INFO [ org.apa.kaf.str.pro.int.StateDirectory ] ( main ) stream-thread [ main ] Deleting state directory 0_0 for task 0_0 as user calling cleanup. [ INFO ] Tests run: 1 , Failures: 0 , Errors: 0 , Skipped: 0 , Time elapsed: 0 .042 s - in com.ibm.garage.cpat.lab.TestLoadKtableFromTopic 2021 -01-17 13 :14:42,272 INFO [ io.quarkus ] ( main ) Quarkus stopped in 0 .035s [ INFO ] [ INFO ] Results: [ INFO ] [ INFO ] Tests run: 4 , Failures: 0 , Errors: 0 , Skipped: 0","title":"Adding in more Kafka Streams operators"},{"location":"use-cases/kafka-streams/lab-2/#producing-to-and-consuming-from-a-kafka-topic-on-event-streams","text":"When using the Kafka Streams API, we usually do it against a Kafka instance containing data already in, at least, one of its topics. In order to build up that scenario, we are going to use the MicroProfile Reactive Messaging library to send messages to a topic. For using the MicroProfile Reactive Messaging library, we first need to add such dependency to our pom.xml file: <dependency> <groupId> io.quarkus </groupId> <artifactId> quarkus-smallrye-reactive-messaging-kafka </artifactId> </dependency> Create the MockProducer.java Java file in src/main/java/com/ibm/garage/cpat/infrastructure with the following code: package com.ibm.garage.cpat.infrastructure ; import javax.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Outgoing ; import io.reactivex.Flowable ; import io.smallrye.reactive.messaging.kafka.KafkaRecord ; import java.util.concurrent.TimeUnit ; import java.util.Random ; import com.ibm.garage.cpat.domain.* ; @ApplicationScoped public class MockProducer { private Random random = new Random (); FinancialMessage mock = new FinancialMessage ( \"1\" , \"MET\" , \"SWISS\" , 12 , 1822.38 , 21868.55 , 94 , 7 , true ); @Outgoing ( \"mock-messages\" ) public Flowable < KafkaRecord < String , FinancialMessage >> produceMock () { return Flowable . interval ( 5 , TimeUnit . SECONDS ) . map ( tick -> { return setRandomUserId ( mock ); }); } public KafkaRecord < String , FinancialMessage > setRandomUserId ( FinancialMessage mock ) { mock . userId = String . valueOf ( random . nextInt ( 100 )); return KafkaRecord . of ( mock . userId , mock ); } } The producer code above produces a FinancialMessage every 5 seconds to the mock-messages channel with a random userId (out of 100). We will see later on how the mock-messages channel relates to a Kafka topic through configuration. Next, create the topology that we are going to build for processing the messages sent by our previous producer. Create a FinancialMessageTopology.java Java file in src/main/java/com/ibm/garage/cpat/domain with the following code: package com.ibm.garage.cpat.domain ; import javax.enterprise.context.ApplicationScoped ; import javax.enterprise.inject.Produces ; import org.eclipse.microprofile.config.inject.ConfigProperty ; import org.apache.kafka.common.serialization.Serdes ; import org.apache.kafka.streams.StreamsBuilder ; import org.apache.kafka.streams.Topology ; import org.apache.kafka.streams.kstream.Consumed ; import org.apache.kafka.streams.kstream.Produced ; import io.quarkus.kafka.client.serialization.JsonbSerde ; @ApplicationScoped public class FinancialMessageTopology { @ConfigProperty ( name = \"START_TOPIC_NAME\" ) private String INCOMING_TOPIC ; @ConfigProperty ( name = \"TARGET_TOPIC_NAME\" ) private String OUTGOING_TOPIC ; @Produces public Topology buildTopology () { StreamsBuilder builder = new StreamsBuilder (); JsonbSerde < FinancialMessage > financialMessageSerde = new JsonbSerde <> ( FinancialMessage . class ); // Stream reads from input topic, filters it by checking the boolean field on the message. // If the boolean is true, it gets passed to the mapValues function which will then send that record // to an outgoing topic. builder . stream ( INCOMING_TOPIC , Consumed . with ( Serdes . String (), financialMessageSerde ) ) . filter ( ( key , message ) -> checkValidation ( message ) ) . mapValues ( checkedMessage -> adjustPostValidation ( checkedMessage ) ) . to ( OUTGOING_TOPIC , Produced . with ( Serdes . String (), financialMessageSerde ) ); return builder . build (); } public boolean checkValidation ( FinancialMessage message ) { return ( message . technicalValidation ); } public FinancialMessage adjustPostValidation ( FinancialMessage message ) { message . totalCost = message . totalCost * 1.15 ; return message ; } } The code above builds a KStream from the messages in INCOMING_TOPIC . It will filter the FinanacialMessage based on a boolean property of them. Then, it will do some adjustment of the cost attribute within that FinancialMessage post message validation and send that out to the OUTGOING_TOPIC . Now, we have seen the code for both the mock producer and the topology we want to build are dependant on certain configuration variables such as the input and output topics they are meant to produce to and consume from. In a Quarkus application, all the configuration settings is done through a properties file, which makes the application portable across different environments. This property file, which we already had to configure in previous labs is called application.properties and is located in src/main/resources . In order to configure the application to work with an IBM Event Streams v10 or later, paste the following configuration in the application.properties file: quarkus.http.port = 8080 quarkus.log.console.enable = true quarkus.log.console.level = INFO # Base ES Connection Details mp.messaging.connector.smallrye-kafka.bootstrap.servers = ${BOOTSTRAP_SERVERS} mp.messaging.connector.smallrye-kafka.security.protocol = SASL_SSL mp.messaging.connector.smallrye-kafka.ssl.protocol = TLSv1.2 mp.messaging.connector.smallrye-kafka.sasl.mechanism = SCRAM-SHA-512 mp.messaging.connector.smallrye-kafka.sasl.jaas.config = org.apache.kafka.common.security.scram.ScramLoginModule required \\ username=${SCRAM_USERNAME} \\ password=${SCRAM_PASSWORD}; mp.messaging.connector.smallrye-kafka.ssl.truststore.location = ${CERT_LOCATION} mp.messaging.connector.smallrye-kafka.ssl.truststore.password = ${CERT_PASSWORD} mp.messaging.connector.smallrye-kafka.ssl.truststore.type = PKCS12 # Initial mock JSON message producer configuration mp.messaging.outgoing.mock-messages.connector = smallrye-kafka mp.messaging.outgoing.mock-messages.topic = ${START_TOPIC_NAME} mp.messaging.outgoing.mock-messages.value.serializer = io.quarkus.kafka.client.serialization.JsonbSerializer # Quarkus Kafka Streams configuration settings quarkus.kafka-streams.bootstrap-servers = ${BOOTSTRAP_SERVERS} quarkus.kafka-streams.application-id = financial-stream quarkus.kafka-streams.application-server = localhost:8080 quarkus.kafka-streams.topics = ${START_TOPIC_NAME},${TARGET_TOPIC_NAME} quarkus.kafka-streams.health.enabled = true quarkus.kafka-streams.security.protocol = SASL_SSL quarkus.kafka-streams.ssl.protocol = TLSv1.2 quarkus.kafka-streams.sasl.mechanism = SCRAM-SHA-512 quarkus.kafka-streams.sasl.jaas.config = org.apache.kafka.common.security.scram.ScramLoginModule required \\ username=${SCRAM_USERNAME} \\ password=${SCRAM_PASSWORD}; quarkus.kafka-streams.ssl.truststore.location = ${CERT_LOCATION} quarkus.kafka-streams.ssl.truststore.password = ${CERT_PASSWORD} quarkus.kafka-streams.ssl.truststore.type = PKCS12 # pass-through options kafka-streams.cache.max.bytes.buffering = 10240 kafka-streams.commit.interval.ms = 1000 kafka-streams.metadata.max.age.ms = 500 kafka-streams.auto.offset.reset = latest kafka-streams.metrics.recording.level = DEBUG If using a previous IBM Event Streams version (such as v2019.4.2) or on IBM Cloud, use the following configuration in the application.properties file: quarkus.http.port = 8080 quarkus.log.console.enable = true quarkus.log.console.level = INFO # Base ES Connection Details mp.messaging.connector.smallrye-kafka.bootstrap.servers = ${BOOTSTRAP_SERVERS} mp.messaging.connector.smallrye-kafka.security.protocol = SASL_SSL mp.messaging.connector.smallrye-kafka.ssl.protocol = TLSv1.2 mp.messaging.connector.smallrye-kafka.sasl.mechanism = PLAIN mp.messaging.connector.smallrye-kafka.sasl.jaas.config = org.apache.kafka.common.security.scram.PlainLoginModule required \\ username=\"token\" \\ password=${API_KEY}; # If connecting to Event Streams on IBM Cloud the following truststore options are not needed. mp.messaging.connector.smallrye-kafka.ssl.truststore.location = ${CERT_LOCATION} mp.messaging.connector.smallrye-kafka.ssl.truststore.password = password # Initial mock JSON message producer configuration mp.messaging.outgoing.mock-messages.connector = smallrye-kafka mp.messaging.outgoing.mock-messages.topic = ${START_TOPIC_NAME} mp.messaging.outgoing.mock-messages.value.serializer = io.quarkus.kafka.client.serialization.JsonbSerializer # Quarkus Kafka Streams configuration settings quarkus.kafka-streams.bootstrap-servers = ${BOOTSTRAP_SERVERS} quarkus.kafka-streams.application-id = financial-stream quarkus.kafka-streams.application-server = localhost:8080 quarkus.kafka-streams.topics = ${START_TOPIC_NAME},${TARGET_TOPIC_NAME} quarkus.kafka-streams.health.enabled = true quarkus.kafka-streams.security.protocol = SASL_SSL quarkus.kafka-streams.ssl.protocol = TLSv1.2 quarkus.kafka-streams.sasl.mechanism = PLAIN quarkus.kafka-streams.sasl.jaas.config = org.apache.kafka.common.security.scram.PlainLoginModule required \\ username=\"token\" \\ password=${API_KEY}; # If connecting to Event Streams on IBM Cloud the following truststore options are not needed. quarkus.kafka-streams.ssl.truststore.location = ${CERT_LOCATION} quarkus.kafka-streams.ssl.truststore.password = password # pass-through options kafka-streams.cache.max.bytes.buffering = 10240 kafka-streams.commit.interval.ms = 1000 kafka-streams.metadata.max.age.ms = 500 kafka-streams.auto.offset.reset = latest kafka-streams.metrics.recording.level = DEBUG There are some environment variables our application.properties file depends on: START_TOPIC_NAME : The Kafka topic the mock producer will produce FinancialMessage to and the topology consume messages from for processing. IMPORTANT: Use a topic name with some unique identifier if you are sharing the IBM Event Streams instance with other lab students. Also, you must create this topic in IBM Event Streams. See here for more details as to how to create a topic. TARGET_TOPIC_NAME : The Kafka topic the topology will produce the processed FinancialMessage to. IMPORTANT: Use a topic name with some unique identifier if you are sharing the IBM Event Streams instance with other lab students. Also, you must create this topic in IBM Event Streams. See [here]((/use-cases/overview/pre-requisites) for more details as to how to create a topic BOOTSTRAP_SERVERS : Your IBM Event Streams bootstrap server. See here for more details as to how to obtain these. CERT_LOCATION : The location where the PKCS12 certificate for the SSL connection to the IBM Event Streams instance is. See here for more details as to how to obtain these. CERT_PASSWORD : The password of the PKCS12 certificate. See here for more details as to how to obtain these. API_KEY if you are using an IBM Event Streams instance in IBM Cloud or SCRAM_USERNAME and SCRAM_PASSWORD : The SCRAM credentials for your application to get authenticated and authorized to work with IBM Event Streams. See here for more details as to how to obtain these. Export the variables and values on the terminal you will run the application from: IBM Event Strams v10 or later: export BOOTSTRAP_SERVERS = your-bootstrap-server-address:443 \\ export START_TOPIC_NAME = name-of-topic-to-consume-from \\ export TARGET_TOPIC_NAME = name-of-topic-to-produce-to \\ export CERT_LOCATION = /path-to-pkcs12-cert/es-cert.p12 \\ export CERT_PASSWORD = certificate-password \\ export SCRAM_USERNAME = your-scram-username \\ export SCRAM_PASSWORD = your-scram-password Previous IBM Event Streams versions: export BOOTSTRAP_SERVERS = your-bootstrap-server-address:443 \\ export START_TOPIC_NAME = name-of-topic-to-consume-from \\ export TARGET_TOPIC_NAME = name-of-topic-to-produce-to \\ export CERT_LOCATION = /path-to-jks-cert/es-cert.jks \\ export API_KEY = your-api-key Local Kafka Cluster: export BOOTSTRAP_SERVERS = your-bootstrap-server-address:443 \\ export START_TOPIC_NAME = name-of-topic-to-consume-from \\ export TARGET_TOPIC_NAME = name-of-topic-to-produce-to You can now test the Quarkus application ./mvnw quarkus:dev You should now be able to see FinancialMessage events in the input topic and those processed messages in the output topic you have specified above in the IBM Event Streams user interface.","title":"Producing to and Consuming from a Kafka Topic on Event Streams"},{"location":"use-cases/kafka-streams/lab-3/","text":"Kafka Streams Test Lab 3 \u00b6 Using Kafka Streams to compute real time inventory stock Overview \u00b6 In this lab, we're going to use Quarkus to develop the near real-time inventory logic using Kafka Streams APIs and microprofile reactive messaging. The requirements to address are: consume item sold events from the items topic. Item has SKU as unique key. Item event has store ID reference the Kafka record in the items topic, uses the Store unique ID as key compute for each item its current stock cross stores compute the store's stock for each item generate inventory event for store - item - stock expose APIs to get stock for a store or for an item The solution is using Kafka Streams and it includes two services. The components used are: The goal of this lab, is to develop the green components which expose APIs to support Kafka Streams interactive query on top of the aggregates to keep the inventory views and saved in state store (light blue storage/per service deployed and persisted in Kafka as topic). We will be unit testing the stream logic using Apache Kafka Streams TopologyTestDriver class. This solution is deployed to OpenShift cluster with Strimzi running in the same cluster and namespace. This application needs the Item Store sell simulator to perform the end to end testing and to demonstrate the end to end scenario. Pre-requisites \u00b6 Access to an OpenShift Container Platform v4.6.x Kafka : The lab can use Event Streams on Cloud or Kafka Strimzi deployed on OpenShift. Code Source : from the git repositories: https://github.com/ibm-cloud-architecture/refarch-eda-item-inventory to compute item inventory cross store https://github.com/ibm-cloud-architecture/refarch-eda-store-inventory to compute store inventory. https://github.com/ibm-cloud-architecture/eda-lab-inventory to get access to deployment configuration to deploy on OpenShift. git clone https://github.com/ibm-cloud-architecture/refarch-eda-item-inventory git clone https://github.com/ibm-cloud-architecture/refarch-eda-store-inventory git clone https://github.com/ibm-cloud-architecture/eda-rt-inventory-gitops OC CLI As in previous labs, be connected to your Openshift cluster. We have automated the deployment of all the pieces making up this use case. IBM OpenLabs \u00b6 In this section, we are going to see use the IBM OpenLabs hosted environment. Go to IBM OpenLabs in a browser and click on Launch Lab button for Bring Your Own Application . Sign in with your IBM Cloud account or register for an IBM Cloud account. You will be presented with a dialog asking you whether you have an Opportunity Id or not. If you don't have it or don't no, just select No and click on Launch Lab . You should now see your IBM OpenLabs environment. On the left hand side navigation menu, click on the Quick Links and Common Commands section. Now, if you scroll down on the instructions shown on your screen, you should reach the Commonly Used Commands section of these and in there you should see an oc login ... command to get your terminal associated to this IBM OpenLabs logged into the OpenShift cluster that you will be working with for this quickstart tutorial. Click on the oc login... command and you should see a Login successful message on the terminal. One Click Deploy to OpenShift \u00b6 The different components are deployed in the same namespace as the Kafka cluster, and use internal route to access Kafka bootstrap URL. The images for each of the components used are in the quay.io ibmcase repository: The Gitops repository includes a makefile to deploy to different environment types. You can use the following command to deploy Event Streams, MQ broker and the apps in the same namespace: make deploy_rt_inventory Testing the solution \u00b6 We have moved the demonstration script to the gitop repository . Understanding the Kafka Streams implementation \u00b6 The item and store aggregator code are based on the same code structure, reflecting the DDD onion architecture: \u2514\u2500\u2500 ibm \u2514\u2500\u2500 gse \u2514\u2500\u2500 eda \u2514\u2500\u2500 inventory \u251c\u2500\u2500 app \u2502 \u2514\u2500\u2500 ItemAggregatorApplication.java \u251c\u2500\u2500 domain \u2502 \u251c\u2500\u2500 ItemInventory.java \u2502 \u251c\u2500\u2500 ItemProcessingAgent.java \u2502 \u2514\u2500\u2500 ItemTransaction.java \u2514\u2500\u2500 infra \u251c\u2500\u2500 ItemTransactionDeserializer.java \u251c\u2500\u2500 ItemTransactionStream.java \u2514\u2500\u2500 api \u251c\u2500\u2500 InventoryResource.java \u251c\u2500\u2500 ItemCountQueries.java \u2514\u2500\u2500 dto \u251c\u2500\u2500 ItemCountQueryResult.java \u2514\u2500\u2500 PipelineMetadata.java The interesting class that supports the business logic is in ItemProcessingAgent.java . Basically the logic to compute the different stocks are in the processItemTransaction method, which builds a Kafla Stream topology For the stock of items cross store computation the code looks like: @Produces public Topology processItemTransaction (){ KStream < String , ItemTransaction > items = inItemsAsStream . getItemStreams (); // process items and aggregate at the store level KTable < String , ItemInventory > itemItemInventory = items // use store name as key, which is what the item event is also using . map (( k , transaction ) -> { ItemInventory newRecord = new ItemInventory (); newRecord . updateStockQuantityFromTransaction ( transaction . sku , transaction ); return new KeyValue < String , ItemInventory > ( newRecord . itemID , newRecord ); }) . groupByKey ( Grouped . with ( Serdes . String (), ItemInventory . itemInventorySerde )). aggregate ( () -> new ItemInventory (), ( itemID , newValue , currentValue ) -> currentValue . updateStockQuantity ( itemID , newValue . currentStock ), materializeAsStoreInventoryKafkaStore ()); produceStoreInventoryToOutputStream ( itemItemInventory ); return inItemsAsStream . run (); } while for the store the code is also in ItemProcessingAgent public Topology processItemTransaction (){ KStream < String , ItemTransaction > items = inItemsAsStream . getItemStreams (); // process items and aggregate at the store level KTable < String , StoreInventory > storeItemInventory = items // use store name as key, which is what the item event is also using . groupByKey ( ItemStream . buildGroupDefinitionType ()) // update the current stock for this <store,item> pair // change the value type . aggregate ( () -> new StoreInventory (), // initializer when there was no store in the table ( store , newItem , existingStoreInventory ) -> existingStoreInventory . updateStockQuantity ( store , newItem ), materializeAsStoreInventoryKafkaStore ()); produceStoreInventoryToInventoryOutputStream ( storeItemInventory ); return inItemsAsStream . run (); Each project includes a set of unit tests to validate the logic. Integration tests \u00b6 For running the integration test, we propose to copy the e2e folder from the solution repository and follow the readme instructions section end-to-end-testing . Interactive queries \u00b6 Warning With Kafka 3.x release this section needs to be revisited as API changed. We already addressed the interactive queries concept in the kafka stream technology summary article . Each of the store and item aggregator implements those queries via two classes: ItemCountQueries StoreInventoryQueries The principles are the same: Get the metadata about each \"kafka store\" supporting the stateful KTables which are keeping the aggregate per item or per store. Get the value of the aggregate for the given key, locally or remotely.","title":"Kafka Streams Test Lab 3"},{"location":"use-cases/kafka-streams/lab-3/#kafka-streams-test-lab-3","text":"Using Kafka Streams to compute real time inventory stock","title":"Kafka Streams Test Lab 3"},{"location":"use-cases/kafka-streams/lab-3/#overview","text":"In this lab, we're going to use Quarkus to develop the near real-time inventory logic using Kafka Streams APIs and microprofile reactive messaging. The requirements to address are: consume item sold events from the items topic. Item has SKU as unique key. Item event has store ID reference the Kafka record in the items topic, uses the Store unique ID as key compute for each item its current stock cross stores compute the store's stock for each item generate inventory event for store - item - stock expose APIs to get stock for a store or for an item The solution is using Kafka Streams and it includes two services. The components used are: The goal of this lab, is to develop the green components which expose APIs to support Kafka Streams interactive query on top of the aggregates to keep the inventory views and saved in state store (light blue storage/per service deployed and persisted in Kafka as topic). We will be unit testing the stream logic using Apache Kafka Streams TopologyTestDriver class. This solution is deployed to OpenShift cluster with Strimzi running in the same cluster and namespace. This application needs the Item Store sell simulator to perform the end to end testing and to demonstrate the end to end scenario.","title":"Overview"},{"location":"use-cases/kafka-streams/lab-3/#pre-requisites","text":"Access to an OpenShift Container Platform v4.6.x Kafka : The lab can use Event Streams on Cloud or Kafka Strimzi deployed on OpenShift. Code Source : from the git repositories: https://github.com/ibm-cloud-architecture/refarch-eda-item-inventory to compute item inventory cross store https://github.com/ibm-cloud-architecture/refarch-eda-store-inventory to compute store inventory. https://github.com/ibm-cloud-architecture/eda-lab-inventory to get access to deployment configuration to deploy on OpenShift. git clone https://github.com/ibm-cloud-architecture/refarch-eda-item-inventory git clone https://github.com/ibm-cloud-architecture/refarch-eda-store-inventory git clone https://github.com/ibm-cloud-architecture/eda-rt-inventory-gitops OC CLI As in previous labs, be connected to your Openshift cluster. We have automated the deployment of all the pieces making up this use case.","title":"Pre-requisites"},{"location":"use-cases/kafka-streams/lab-3/#ibm-openlabs","text":"In this section, we are going to see use the IBM OpenLabs hosted environment. Go to IBM OpenLabs in a browser and click on Launch Lab button for Bring Your Own Application . Sign in with your IBM Cloud account or register for an IBM Cloud account. You will be presented with a dialog asking you whether you have an Opportunity Id or not. If you don't have it or don't no, just select No and click on Launch Lab . You should now see your IBM OpenLabs environment. On the left hand side navigation menu, click on the Quick Links and Common Commands section. Now, if you scroll down on the instructions shown on your screen, you should reach the Commonly Used Commands section of these and in there you should see an oc login ... command to get your terminal associated to this IBM OpenLabs logged into the OpenShift cluster that you will be working with for this quickstart tutorial. Click on the oc login... command and you should see a Login successful message on the terminal.","title":"IBM OpenLabs"},{"location":"use-cases/kafka-streams/lab-3/#one-click-deploy-to-openshift","text":"The different components are deployed in the same namespace as the Kafka cluster, and use internal route to access Kafka bootstrap URL. The images for each of the components used are in the quay.io ibmcase repository: The Gitops repository includes a makefile to deploy to different environment types. You can use the following command to deploy Event Streams, MQ broker and the apps in the same namespace: make deploy_rt_inventory","title":"One Click Deploy to OpenShift"},{"location":"use-cases/kafka-streams/lab-3/#testing-the-solution","text":"We have moved the demonstration script to the gitop repository .","title":"Testing the solution"},{"location":"use-cases/kafka-streams/lab-3/#understanding-the-kafka-streams-implementation","text":"The item and store aggregator code are based on the same code structure, reflecting the DDD onion architecture: \u2514\u2500\u2500 ibm \u2514\u2500\u2500 gse \u2514\u2500\u2500 eda \u2514\u2500\u2500 inventory \u251c\u2500\u2500 app \u2502 \u2514\u2500\u2500 ItemAggregatorApplication.java \u251c\u2500\u2500 domain \u2502 \u251c\u2500\u2500 ItemInventory.java \u2502 \u251c\u2500\u2500 ItemProcessingAgent.java \u2502 \u2514\u2500\u2500 ItemTransaction.java \u2514\u2500\u2500 infra \u251c\u2500\u2500 ItemTransactionDeserializer.java \u251c\u2500\u2500 ItemTransactionStream.java \u2514\u2500\u2500 api \u251c\u2500\u2500 InventoryResource.java \u251c\u2500\u2500 ItemCountQueries.java \u2514\u2500\u2500 dto \u251c\u2500\u2500 ItemCountQueryResult.java \u2514\u2500\u2500 PipelineMetadata.java The interesting class that supports the business logic is in ItemProcessingAgent.java . Basically the logic to compute the different stocks are in the processItemTransaction method, which builds a Kafla Stream topology For the stock of items cross store computation the code looks like: @Produces public Topology processItemTransaction (){ KStream < String , ItemTransaction > items = inItemsAsStream . getItemStreams (); // process items and aggregate at the store level KTable < String , ItemInventory > itemItemInventory = items // use store name as key, which is what the item event is also using . map (( k , transaction ) -> { ItemInventory newRecord = new ItemInventory (); newRecord . updateStockQuantityFromTransaction ( transaction . sku , transaction ); return new KeyValue < String , ItemInventory > ( newRecord . itemID , newRecord ); }) . groupByKey ( Grouped . with ( Serdes . String (), ItemInventory . itemInventorySerde )). aggregate ( () -> new ItemInventory (), ( itemID , newValue , currentValue ) -> currentValue . updateStockQuantity ( itemID , newValue . currentStock ), materializeAsStoreInventoryKafkaStore ()); produceStoreInventoryToOutputStream ( itemItemInventory ); return inItemsAsStream . run (); } while for the store the code is also in ItemProcessingAgent public Topology processItemTransaction (){ KStream < String , ItemTransaction > items = inItemsAsStream . getItemStreams (); // process items and aggregate at the store level KTable < String , StoreInventory > storeItemInventory = items // use store name as key, which is what the item event is also using . groupByKey ( ItemStream . buildGroupDefinitionType ()) // update the current stock for this <store,item> pair // change the value type . aggregate ( () -> new StoreInventory (), // initializer when there was no store in the table ( store , newItem , existingStoreInventory ) -> existingStoreInventory . updateStockQuantity ( store , newItem ), materializeAsStoreInventoryKafkaStore ()); produceStoreInventoryToInventoryOutputStream ( storeItemInventory ); return inItemsAsStream . run (); Each project includes a set of unit tests to validate the logic.","title":"Understanding the Kafka Streams implementation"},{"location":"use-cases/kafka-streams/lab-3/#integration-tests","text":"For running the integration test, we propose to copy the e2e folder from the solution repository and follow the readme instructions section end-to-end-testing .","title":"Integration tests"},{"location":"use-cases/kafka-streams/lab-3/#interactive-queries","text":"Warning With Kafka 3.x release this section needs to be revisited as API changed. We already addressed the interactive queries concept in the kafka stream technology summary article . Each of the store and item aggregator implements those queries via two classes: ItemCountQueries StoreInventoryQueries The principles are the same: Get the metadata about each \"kafka store\" supporting the stateful KTables which are keeping the aggregate per item or per store. Get the value of the aggregate for the given key, locally or remotely.","title":"Interactive queries"},{"location":"use-cases/monitoring-on-cloud/","text":"General monitoring practices Monitoring with SYSDIG Event Streams Monitoring General monitoring practices \u00b6 We have documented in this note the major tools for monitoring a Kafka cluster. like prometheus and Grafana. Monitoring with SYSDIG \u00b6 With Event Streams on cloud, the monitoring capability is supported by Sysdig. To enable monitoring approach includes the following steps: Create a IBM Cloud monitoring with Sysdig service. Enter name, resource group and enable platform monitoring, if you want to combine Event Streams and Cluster monitoring: You may want to do the Sysdig getting started tutorial to understand the different features of this product. Enable Event Streams to be monitored and select the monitoring service. Link to an existing sysdig instance: Event Streams Monitoring \u00b6 Now open the Event Streams dashboard in Sysdig. This can be done from the Event Streams Dashboard -> The available metrics are discribed in this product documentation page . The following present the network throughput derived by aggregating cross brokers bytes per second. For example the enterprise plan is described here , so it is possible to define an alert when the instance byte in and out per second are going over a given threshold (35MB?). Here is an example of such alert: Other interesting gauges are related to consumer groups, where it can be interesting to alert on inactive consumer groups, and the number of rebalancing consumer groups (something may be linked to the state of the consumers within the group or the brokers themselves). Here are other reported interesting gauges: Authentication failures will help to assess if API keys expired or some security attack. Producer or consumer conversion time are metrics to assess if some codes are using older API version and some time is consumed by doing conversion. This need to be at zero.","title":"Monitoring ES on Cloud"},{"location":"use-cases/monitoring-on-cloud/#general-monitoring-practices","text":"We have documented in this note the major tools for monitoring a Kafka cluster. like prometheus and Grafana.","title":"General monitoring practices"},{"location":"use-cases/monitoring-on-cloud/#monitoring-with-sysdig","text":"With Event Streams on cloud, the monitoring capability is supported by Sysdig. To enable monitoring approach includes the following steps: Create a IBM Cloud monitoring with Sysdig service. Enter name, resource group and enable platform monitoring, if you want to combine Event Streams and Cluster monitoring: You may want to do the Sysdig getting started tutorial to understand the different features of this product. Enable Event Streams to be monitored and select the monitoring service. Link to an existing sysdig instance:","title":"Monitoring with SYSDIG"},{"location":"use-cases/monitoring-on-cloud/#event-streams-monitoring","text":"Now open the Event Streams dashboard in Sysdig. This can be done from the Event Streams Dashboard -> The available metrics are discribed in this product documentation page . The following present the network throughput derived by aggregating cross brokers bytes per second. For example the enterprise plan is described here , so it is possible to define an alert when the instance byte in and out per second are going over a given threshold (35MB?). Here is an example of such alert: Other interesting gauges are related to consumer groups, where it can be interesting to alert on inactive consumer groups, and the number of rebalancing consumer groups (something may be linked to the state of the consumers within the group or the brokers themselves). Here are other reported interesting gauges: Authentication failures will help to assess if API keys expired or some security attack. Producer or consumer conversion time are metrics to assess if some codes are using older API version and some time is consumed by doing conversion. This need to be at zero.","title":"Event Streams Monitoring"},{"location":"use-cases/monitoring-on-ocp/","text":"This tutorial was developed with and validated against IBM Cloud Pak for Integration Version 2020.2.1 and IBM Event Streams Version 10.0.0. Any deviation from those versions while performing the tasks in this tutorial may produced unexpected results. Overview \u00b6 Deploying IBM Event Streams on OpenShift Cloud Platform (OCP) as the Apache Kafka-based event backbone is a great first step in your Event-Driven Architecture implementation. However, now you must maintain that Kafka cluster and understand the intricate details of what a \"healthy\" cluster looks like. This tutorial will walk you through some of the initial monitoring scenarios that are available for IBM Event Streams deployed on OCP. The raw monitoring use cases and capabilities are available from the official IBM Event Streams documentation via the links below: - Monitoring deployment health - Monitoring Kafka cluster health - Monitoring topic health - Monitoring Kafka consumer group lag This tutorial will focus on a more guided approach to understanding the foundation of Apache Kafka monitoring capabilities provided by IBM Event Streams and the IBM Cloud Pak for Integration. Upon completion of this tutorial, you can extend your own experience through the Advanced Scenarios section to adapt Kafka monitoring capabilites to your project's needs. Scenario Prereqs \u00b6 OpenShift Container Platform This deployment scenario was developed for use on the OpenShift Container Platform, with a minimum version of 4.4 . Cloud Pak for Integration This deployment scenario was developed for use with the 2020.2.x release of the IBM Cloud Pak for Integration , installed on OpenShift 4.4. IBM Event Streams This deployment scenario requires a working installation of IBM Event Streams V10.0 or greater, deployed on the Cloud Pak for Integration environment mentioned above. For Cloud Pak installation guidance, you can follow the Cloud Pak Playbook installation instructions. Git We will need to clone repositories. Java Java Development Kit (JDK) v1.8+ (Java 8+) Maven The scenario uses Maven v3.6.x Generate Event Load \u00b6 See now separate note on Starter application Explore the preconfigured Event Streams Dashboard \u00b6 This section will walk through the default dashboard and user interface available on every IBM Event Streams deployment. Log into the Event Streams Dashboard. Click the Monitoring tab from the primary navigation menu on the left hand side. From here, you can view information on messages, partitions, and replicas for the past hour, day, week, or month. Click the Topics tab from the primary navigation menu on the left hand side. Click the name of your topic that you previously created in the Generate Event Load section. This should be in the format of monitoring-lab-topic-[your-initials] . You are presented with a Producers page showing the number of active producers, as well as the average message size produced per second and average number of messages produced per second. You can modify the time window by changing the values in the View producers by time box. Click the Messages tab to view all the data and metadata for events stored in the topic. You can view messages across partitions or on specific partitions, as well as jump to specific offsets or timestamps. Click Consumer Groups to be shown the number of consumer groups that have previously registered or are currently registered as consuming from the topic. You are able to see how many active members a consumer group has, as well as have many unconsumed partitions a topic has inside of a consumer group (also known as consumer group lag )- a key metric for driving parallelism in event-driven microservices! Import Grafana Dashboards \u00b6 This section will walk through the Grafana Dashboard capabilities documented in the official IBM Event Streams documentation . Apply the Grafana Dashboard for overall Kafka Health via a MonitoringDashboard custom resource: oc apply -f https://raw.githubusercontent.com/ibm-messaging/event-streams-operator-resources/master/grafana-dashboards/ibm-eventstreams-kafka-health-dashboard.yaml View Grafana Dashboards \u00b6 To view the newly imported Event Streams Grafana dashboard for overall Kafka Health, follow these steps: Navigate to the IBM Cloud Platform Common Services console homepage via https://cp-console.[cluster-name] , click the hamburger icon in the top left and click the Monitoring in the expanded menu to open the Grafana homepage. Click the user icon in the bottom left corner to open the user profile page. In the Organizations table, find the namespace where you installed the Event Streams monitoringdashboard custom resource (most likely the eventstreams ), and switch the user profile to that namespace. Hover over the Dashboards square on the left and click Manage . Click on IBM Event Streams Kafka dashboard in the Dashboard table to view the newly imported resource. Using the drop-down selectors at the top, select the following: Namespace which has the running instance of your Event Streams deployment, Cluster Name for the desired Event Streams cluster Topic that matches desired topics for viewing (only topics that have been published to will appear in this list) Broker to select individual or multiple brokers in the cluster. Note: Not all of the metrics that Kafka uses are published to Prometheus by default. The metrics that are published are controlled by a ConfigMap. You can publish metrics by adding them to the ConfigMap. Create an Alert \u00b6 A monitoring system is only as good as the alerts it can send out, since you're not going to be watching that Grafana dashboard all day and night! This section will walk through the creation of a quick alert rule which will automatically trigger, as well as how to view and silence that alert in the provided Alertmanager interface. The official Event Streams documentation provides a walkthrough of selecting the desired metrics to monitor, but for our example, we will leverage the kafka_server_replicamanager_partitioncount_value metric as an indicator of topic creation (as the overall partition count will increase when a topic is first created) . On the command line, create this sample rule which will fire whenever the partition count is over 50 (which is the baseline number of partitions the Event Streams system uses for its internal topic partitions) . In order to do this, create prom-rule-partitions.yaml file with the following content in it: apiVersion : monitoring.coreos.com/v1 kind : PrometheusRule metadata : labels : component : icp-prometheus name : demo-partition-count spec : groups : - name : PartitionCount rules : - alert : PartitionCount expr : kafka_server_replicamanager_partitioncount_value > 50 for : 10s labels : severity : critical annotations : identifier : 'Partition count' description : 'There are {{ $value }} partition(s) reported by broker {{ $labels.kafka }}' Create the alert rule via the OpenShift CLI: oc apply -f prom-rule-partitions.yaml You can view the creation and status of your alert via the OpenShift CLI: oc get PrometheusRule demo-partition-count oc describe PrometheusRule demo-partition-count 1. Access the Prometheus monitoring backend that is provided in the IBM Cloud Pak Common Services via https://cp-console.[cluster-name]/prometheus . Click the Alerts button in the header. You should see your new PartitionCount rule firing and highlighted in red. NOTE: If you do not see your PrometheusRule, you may need to create it in the ibm-common-services namespace depending upon your OpenShift cluster and Cloud Pak operator configuration. This can be done by supplying the -n ibm-common-services flag to the oc apply -f prom-rule-partitions.yaml command. Click on the PartitionCount alert to expand the details and see which components are triggering the alert. Now that we have created alerts from the monitoring system, you will want a way to manage those alerts. The default Alertmanager component provides a way to manage firing alerts, notifications, and silences. Prometheus is capable of integrating with many notification systems - from Slack to PagerDuty to HipChat to common HTTP webhooks. For further information on the extensibility of Prometheus, you can view the Alerting configuration section of the official docs. For configuring the IBM Cloud Pak Common Services deployed instance of Prometheus, you can view the Configuring Alertmanager section of the official docs. In this section of the tutorial, you will walk through the Alertmanager interface and silence the previously created alerts. Access the default Alertmanager instance via https://cp-console.apps.[cluster-name]/alertmanager/ . You should see the newly created PartitionCount alerts listed as firing. Click on the Info button for the first alert to see the additional context provided by the alert definition (ie there are more than 50 partitions) As alerts fire and become acknowledged, you can silence them to mark them as known, acknowledged, or resolved. To do this, you create a Silence. Click the Silence button for one of the alerts in the list. You will see a start time, a duration, and an end time by default. This gives you initial control over what you are silencing and for how long. Next, you will see a list of Name and Value pairs that are filled with the information from the alert instance you clicked on. Delete the elements in the Matchers list until only the following items are left. This will allow for a robust capture of all the PartitionCount alerts for the same Event Streams cluster. alertname app_kubernetes_io_instance app_kubernetes_io_part_of kubernetes_namespace Your username should already be filled in for the Creator , so enter a Comment of \"Silencing demo alerts\" and click Preview Alerts . Once the affected number of alerts matches the same number of PartitionCount alerts that were listed as firing in Prometheus, click Create . Clicking on the Alerts tab in the header, you will now see those alerts are silenced - meaning you acknowledged them. To make them visible again prior to the expiration of the created Silence, click on the Silences tab from the header. This page lists all the Active, Pending, and Expired silences in the system. You can view, edit, and expire any active Silence to again have the alerts show up in Alertmanager or anywhere else Prometheus is sending notifications. Next Steps \u00b6 External Monitoring Tools \u00b6 IBM Event Streams supports additional monitoring capabilities with third-party monitoring tools via a connection to the clusters JMX port on the Kafka brokers. You must first configure your IBM Event Streams instance for specific access by these external monitoring tools. You can then follow along with the tutorials defined in the official IBM Event Streams documentation to monitor Event Streams with tools such as Datadog and Splunk. Advanced Scenarios \u00b6 As shown in this tutorial, IBM Event Streams provides a robust default set of monitoring metrics which are available to use right out of the box. However, you will most likely need to define custom metrics or extend existing metrics for use in custom dashboards or reporting processes. The following links (in order of recommended usage) discuss additional monitoring capabilities, technologies, and endpoints that are supported with IBM Event Streams to extend your custom monitoring solution as needed: Kafka Exporter - You can use Event Streams to export metrics to Prometheus. These metrics are otherwise only accessible through the Kafka command line tools and allow per-topic metrics, such as consumer group lag, to be colleced. JMX Exporter - You can use Event Streams to collect JMX metrics from Kafka brokers, ZooKeeper nodes, and Kafka Connect nodes, and export them to Prometheus via the Prometheus JMX Exporter . JmxTrans - JmxTrans can be used to push JMX metrics from Kafka brokers to external applications or databases. Additional Reading \u00b6 Monitoring Kafka via official Apache Kafka documentation Monitoring Kafka performance metrics via Datadog How to Monitor Kafka via Server Density OpenShift Day 2 Monitoring via IBM Cloud Paks Playbook Monitoring Kafka cluster health via IBM Event Streams documentation Configuring the monitoring stack via Red Hat OpenShift documentation Examining cluster metrics via Red Hat OpenShift documentation","title":"Monitoring ES"},{"location":"use-cases/monitoring-on-ocp/#overview","text":"Deploying IBM Event Streams on OpenShift Cloud Platform (OCP) as the Apache Kafka-based event backbone is a great first step in your Event-Driven Architecture implementation. However, now you must maintain that Kafka cluster and understand the intricate details of what a \"healthy\" cluster looks like. This tutorial will walk you through some of the initial monitoring scenarios that are available for IBM Event Streams deployed on OCP. The raw monitoring use cases and capabilities are available from the official IBM Event Streams documentation via the links below: - Monitoring deployment health - Monitoring Kafka cluster health - Monitoring topic health - Monitoring Kafka consumer group lag This tutorial will focus on a more guided approach to understanding the foundation of Apache Kafka monitoring capabilities provided by IBM Event Streams and the IBM Cloud Pak for Integration. Upon completion of this tutorial, you can extend your own experience through the Advanced Scenarios section to adapt Kafka monitoring capabilites to your project's needs.","title":"Overview"},{"location":"use-cases/monitoring-on-ocp/#scenario-prereqs","text":"OpenShift Container Platform This deployment scenario was developed for use on the OpenShift Container Platform, with a minimum version of 4.4 . Cloud Pak for Integration This deployment scenario was developed for use with the 2020.2.x release of the IBM Cloud Pak for Integration , installed on OpenShift 4.4. IBM Event Streams This deployment scenario requires a working installation of IBM Event Streams V10.0 or greater, deployed on the Cloud Pak for Integration environment mentioned above. For Cloud Pak installation guidance, you can follow the Cloud Pak Playbook installation instructions. Git We will need to clone repositories. Java Java Development Kit (JDK) v1.8+ (Java 8+) Maven The scenario uses Maven v3.6.x","title":"Scenario Prereqs"},{"location":"use-cases/monitoring-on-ocp/#generate-event-load","text":"See now separate note on Starter application","title":"Generate Event Load"},{"location":"use-cases/monitoring-on-ocp/#explore-the-preconfigured-event-streams-dashboard","text":"This section will walk through the default dashboard and user interface available on every IBM Event Streams deployment. Log into the Event Streams Dashboard. Click the Monitoring tab from the primary navigation menu on the left hand side. From here, you can view information on messages, partitions, and replicas for the past hour, day, week, or month. Click the Topics tab from the primary navigation menu on the left hand side. Click the name of your topic that you previously created in the Generate Event Load section. This should be in the format of monitoring-lab-topic-[your-initials] . You are presented with a Producers page showing the number of active producers, as well as the average message size produced per second and average number of messages produced per second. You can modify the time window by changing the values in the View producers by time box. Click the Messages tab to view all the data and metadata for events stored in the topic. You can view messages across partitions or on specific partitions, as well as jump to specific offsets or timestamps. Click Consumer Groups to be shown the number of consumer groups that have previously registered or are currently registered as consuming from the topic. You are able to see how many active members a consumer group has, as well as have many unconsumed partitions a topic has inside of a consumer group (also known as consumer group lag )- a key metric for driving parallelism in event-driven microservices!","title":"Explore the preconfigured Event Streams Dashboard"},{"location":"use-cases/monitoring-on-ocp/#import-grafana-dashboards","text":"This section will walk through the Grafana Dashboard capabilities documented in the official IBM Event Streams documentation . Apply the Grafana Dashboard for overall Kafka Health via a MonitoringDashboard custom resource: oc apply -f https://raw.githubusercontent.com/ibm-messaging/event-streams-operator-resources/master/grafana-dashboards/ibm-eventstreams-kafka-health-dashboard.yaml","title":"Import Grafana Dashboards"},{"location":"use-cases/monitoring-on-ocp/#view-grafana-dashboards","text":"To view the newly imported Event Streams Grafana dashboard for overall Kafka Health, follow these steps: Navigate to the IBM Cloud Platform Common Services console homepage via https://cp-console.[cluster-name] , click the hamburger icon in the top left and click the Monitoring in the expanded menu to open the Grafana homepage. Click the user icon in the bottom left corner to open the user profile page. In the Organizations table, find the namespace where you installed the Event Streams monitoringdashboard custom resource (most likely the eventstreams ), and switch the user profile to that namespace. Hover over the Dashboards square on the left and click Manage . Click on IBM Event Streams Kafka dashboard in the Dashboard table to view the newly imported resource. Using the drop-down selectors at the top, select the following: Namespace which has the running instance of your Event Streams deployment, Cluster Name for the desired Event Streams cluster Topic that matches desired topics for viewing (only topics that have been published to will appear in this list) Broker to select individual or multiple brokers in the cluster. Note: Not all of the metrics that Kafka uses are published to Prometheus by default. The metrics that are published are controlled by a ConfigMap. You can publish metrics by adding them to the ConfigMap.","title":"View Grafana Dashboards"},{"location":"use-cases/monitoring-on-ocp/#create-an-alert","text":"A monitoring system is only as good as the alerts it can send out, since you're not going to be watching that Grafana dashboard all day and night! This section will walk through the creation of a quick alert rule which will automatically trigger, as well as how to view and silence that alert in the provided Alertmanager interface. The official Event Streams documentation provides a walkthrough of selecting the desired metrics to monitor, but for our example, we will leverage the kafka_server_replicamanager_partitioncount_value metric as an indicator of topic creation (as the overall partition count will increase when a topic is first created) . On the command line, create this sample rule which will fire whenever the partition count is over 50 (which is the baseline number of partitions the Event Streams system uses for its internal topic partitions) . In order to do this, create prom-rule-partitions.yaml file with the following content in it: apiVersion : monitoring.coreos.com/v1 kind : PrometheusRule metadata : labels : component : icp-prometheus name : demo-partition-count spec : groups : - name : PartitionCount rules : - alert : PartitionCount expr : kafka_server_replicamanager_partitioncount_value > 50 for : 10s labels : severity : critical annotations : identifier : 'Partition count' description : 'There are {{ $value }} partition(s) reported by broker {{ $labels.kafka }}' Create the alert rule via the OpenShift CLI: oc apply -f prom-rule-partitions.yaml You can view the creation and status of your alert via the OpenShift CLI: oc get PrometheusRule demo-partition-count oc describe PrometheusRule demo-partition-count 1. Access the Prometheus monitoring backend that is provided in the IBM Cloud Pak Common Services via https://cp-console.[cluster-name]/prometheus . Click the Alerts button in the header. You should see your new PartitionCount rule firing and highlighted in red. NOTE: If you do not see your PrometheusRule, you may need to create it in the ibm-common-services namespace depending upon your OpenShift cluster and Cloud Pak operator configuration. This can be done by supplying the -n ibm-common-services flag to the oc apply -f prom-rule-partitions.yaml command. Click on the PartitionCount alert to expand the details and see which components are triggering the alert. Now that we have created alerts from the monitoring system, you will want a way to manage those alerts. The default Alertmanager component provides a way to manage firing alerts, notifications, and silences. Prometheus is capable of integrating with many notification systems - from Slack to PagerDuty to HipChat to common HTTP webhooks. For further information on the extensibility of Prometheus, you can view the Alerting configuration section of the official docs. For configuring the IBM Cloud Pak Common Services deployed instance of Prometheus, you can view the Configuring Alertmanager section of the official docs. In this section of the tutorial, you will walk through the Alertmanager interface and silence the previously created alerts. Access the default Alertmanager instance via https://cp-console.apps.[cluster-name]/alertmanager/ . You should see the newly created PartitionCount alerts listed as firing. Click on the Info button for the first alert to see the additional context provided by the alert definition (ie there are more than 50 partitions) As alerts fire and become acknowledged, you can silence them to mark them as known, acknowledged, or resolved. To do this, you create a Silence. Click the Silence button for one of the alerts in the list. You will see a start time, a duration, and an end time by default. This gives you initial control over what you are silencing and for how long. Next, you will see a list of Name and Value pairs that are filled with the information from the alert instance you clicked on. Delete the elements in the Matchers list until only the following items are left. This will allow for a robust capture of all the PartitionCount alerts for the same Event Streams cluster. alertname app_kubernetes_io_instance app_kubernetes_io_part_of kubernetes_namespace Your username should already be filled in for the Creator , so enter a Comment of \"Silencing demo alerts\" and click Preview Alerts . Once the affected number of alerts matches the same number of PartitionCount alerts that were listed as firing in Prometheus, click Create . Clicking on the Alerts tab in the header, you will now see those alerts are silenced - meaning you acknowledged them. To make them visible again prior to the expiration of the created Silence, click on the Silences tab from the header. This page lists all the Active, Pending, and Expired silences in the system. You can view, edit, and expire any active Silence to again have the alerts show up in Alertmanager or anywhere else Prometheus is sending notifications.","title":"Create an Alert"},{"location":"use-cases/monitoring-on-ocp/#next-steps","text":"","title":"Next Steps"},{"location":"use-cases/monitoring-on-ocp/#external-monitoring-tools","text":"IBM Event Streams supports additional monitoring capabilities with third-party monitoring tools via a connection to the clusters JMX port on the Kafka brokers. You must first configure your IBM Event Streams instance for specific access by these external monitoring tools. You can then follow along with the tutorials defined in the official IBM Event Streams documentation to monitor Event Streams with tools such as Datadog and Splunk.","title":"External Monitoring Tools"},{"location":"use-cases/monitoring-on-ocp/#advanced-scenarios","text":"As shown in this tutorial, IBM Event Streams provides a robust default set of monitoring metrics which are available to use right out of the box. However, you will most likely need to define custom metrics or extend existing metrics for use in custom dashboards or reporting processes. The following links (in order of recommended usage) discuss additional monitoring capabilities, technologies, and endpoints that are supported with IBM Event Streams to extend your custom monitoring solution as needed: Kafka Exporter - You can use Event Streams to export metrics to Prometheus. These metrics are otherwise only accessible through the Kafka command line tools and allow per-topic metrics, such as consumer group lag, to be colleced. JMX Exporter - You can use Event Streams to collect JMX metrics from Kafka brokers, ZooKeeper nodes, and Kafka Connect nodes, and export them to Prometheus via the Prometheus JMX Exporter . JmxTrans - JmxTrans can be used to push JMX metrics from Kafka brokers to external applications or databases.","title":"Advanced Scenarios"},{"location":"use-cases/monitoring-on-ocp/#additional-reading","text":"Monitoring Kafka via official Apache Kafka documentation Monitoring Kafka performance metrics via Datadog How to Monitor Kafka via Server Density OpenShift Day 2 Monitoring via IBM Cloud Paks Playbook Monitoring Kafka cluster health via IBM Event Streams documentation Configuring the monitoring stack via Red Hat OpenShift documentation Examining cluster metrics via Red Hat OpenShift documentation","title":"Additional Reading"},{"location":"use-cases/schema-registry-on-cloud/","text":"This documentation aims to be a introductory hands-on lab on IBM Event Streams on IBM Cloud Schema Registry where we will go through the different capabilities of the Schema Registry that is available for IBM Event Streams on IBM Cloud users. We strongly recommend to complete first the IBM Event Streams on IBM Cloud lab you can find here . Index \u00b6 Requirements IBM Event Streams Service Credentials Kafdrop Python Demo Environment Schema Registry Schemas Python Avro Producer Python Avro Consumer Schemas and Messages Data Evolution Security Requirements \u00b6 This lab requires the following components to work against: An IBM Cloud account. An IBM Event Streams instance with the Enterprise plan (Schema Registry is only available to these instance for now) - https://cloud.ibm.com/docs/EventStreams?topic=eventstreams-getting_started On your development workstation you will need: (optional) IBM Cloud CLI - https://cloud.ibm.com/docs/cli?topic=cloud-cli-getting-started (optional) IBM CLoud CLI Event Streams plugin - https://cloud.ibm.com/docs/EventStreams?topic=eventstreams-cli#step5_es_cli Docker - https://docs.docker.com/get-docker/ Kafdrop - https://github.com/obsidiandynamics/kafdrop IBM Event Streams Service Credentials \u00b6 At this point, we want to create the needed service credentials in order to allow other applications, tools, scripts to interact with our IBM Event Streams instance. For doing so, we need to: In your IBM Event Streams instance service page, click on Service credentials on the left hand side menu: Observe, there is no service credentials yet and click on the New credential button on the top right corner: Enter a name for your service, choose Manager role for now and click on Add : You should now see your new service credential and be able to inspect its details if you click on its dropdown arrow on it left: We could have created the service credentials using the CLI as well but we leave that as extra homework for students to try on their own. However, we can explore the service credentials using the CLI with ibmcloud resource service-key <service_credentials_name> : $ ibmcloud resource service-key demo-serv-cred Retrieving service key demo-serv-cred in all resource groups under account Kedar Kulkarni ' s Account as ALMARAZJ@ie.ibm.com... Name: demo-serv-cred ID: crn:v1:bluemix:public:messagehub:eu-de:a/b636d1d83e34d7ae7e904591ac248cfa:b05be932-2a60-4315-951d-a6dd902e687a:resource-key:4ba348d2-5fcf-4c13-a265-360e983d99c5 Created At: Tue May 12 10 :53:02 UTC 2020 State: active Credentials: api_key: ***** apikey: ***** iam_apikey_description: Auto-generated for key 4ba348d2-5fcf-4c13-a265-360e983d99c5 iam_apikey_name: demo-serv-cred iam_role_crn: crn:v1:bluemix:public:iam::::serviceRole:Manager iam_serviceid_crn: crn:v1:bluemix:public:iam-identity::a/b636d1d83e34d7ae7e904591ac248cfa::serviceid:ServiceId-380e866c-5914-4e01-85c4-d80bd1b8a899 instance_id: b05be932-2a60-4315-951d-a6dd902e687a kafka_admin_url: https://mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud kafka_brokers_sasl: [ kafka-2.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093 kafka-0.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093 kafka-1.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093 ] kafka_http_url: https://mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud password: ***** user: token IMPORTANT: Out of number 4 above, we are going to define the following important environment variables that will be used throughout this lab so please make sure you understand and define these properly KAFKA_BROKERS which should take the value of kafka_brokers_sasl comma separated: export KAFKA_BROKERS = kafka-2.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093,kafka-1.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093,kafka-0.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093 KAFKA_APIKEY which should take the value of apikey : export KAFKA_APIKEY = ***** URL which should take the value of kafka_http_url : export URL = https://mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud SCHEMA_REGISTRY_URL which should be a combination of the three before in the form of: https://token:<apikey>@<kafka_http_url>/confluent export SCHEMA_REGISTRY_URL = https://token:*****@mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud/confluent INFO: The reason for this \"weird\" schema registry url configuration is because we are using the Confluent SerDes as we will see in the Python Avro Producer and Python Avro Consumer sections later on. Otherwise, the schema registry url would simply be the same as the kafka_http_url in your service credentials. Kafdrop \u00b6 Kafdrop is a web UI for viewing Kafka topics and browsing consumer groups. The tool displays information such as brokers, topics, partitions, consumers, and lets you view messages: https://github.com/obsidiandynamics/kafdrop Kafdrop runs as a Docker container in your wokstation and in order to run it, Set the password value for the sasl.jaas.config property in the kafka.properties file you can find in this repo. The value for password is the api_key/password attributes of your service credentials. If you don't remember how to get this, review that section here security.protocol = SASL_SSL ssl.protocol = TLSv1.2 ssl.enabled.protocols = TLSv1.2 ssl.endpoint.identification.algorithm = HTTPS sasl.mechanism = PLAIN sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"<YOUR_PASSWORD/API_KEY_HERE>\"; Run the Kafdrop Docker container by executing: $ docker run -d --rm -p 9000 :9000 \\ -e KAFKA_BROKERCONNECT = $KAFKA_BROKERS \\ -e KAFKA_PROPERTIES = $( cat kafka.properties | base64 ) \\ -e JVM_OPTS = \"-Xms32M -Xmx64M\" \\ -e SERVER_SERVLET_CONTEXTPATH = \"/\" \\ obsidiandynamics/kafdrop (*) After running the above command, you can use the docker container id returned on the screen to debug any possible issue with docker logs -f <container_id> . This container id will be used to stop the Kafdrop container once we have finished. You can point your browser to http://localhost:9000/ to access the Kafdrop application: You can see the topic details by clicking on the name of that topic: You can watch the messages on this topic by clicking on View Messages button under the topic name and configuring the partition, offset and number of messages option that you are presented with in the next screen. Finally, click on View Messages button at the right: To stop the Kafdrop container once you have finsihed the tutorial, simply list the containers running on your workstation, find the container id for your Kafdrop container and stop it: $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES ba6c8eaf6a3a ibmcase/python-schema-registry-lab:latest \"bash\" 31 seconds ago Up 30 seconds keen_neumann bab9cae43f15 obsidiandynamics/kafdrop \"/kafdrop.sh\" 17 minutes ago Up 17 minutes 0 .0.0.0:9000->9000/tcp laughing_dirac $ docker stop bab9cae43f15 bab9cae43f15 Python Demo Environment \u00b6 Given that students' workstations may vary quite a lot, not only on their operating system but also on the tools installed on them and the tools we need for our lab might install differently, we have opted to provide a python demo environment in the form of a Docker container where all the libraries and tools needed are already pre-installed. Clone \u00b6 In order to build our python demo environment we first need to clone the github repository where the assets live. This github repository is https://github.com/ibm-cloud-architecture/refarch-eda-tools and the specific assets we refer to can be found under the labs/es-cloud-schema-lab folder: Clone the github repository on your workstation on the location of your choice: $ git clone https://github.com/ibm-cloud-architecture/refarch-eda-tools.git Cloning into 'refarch-eda-tools' ... remote: Enumerating objects: 185 , done . remote: Counting objects: 100 % ( 185 /185 ) , done . remote: Compressing objects: 100 % ( 148 /148 ) , done . remote: Total 185 ( delta 23 ) , reused 176 ( delta 16 ) , pack-reused 0 Receiving objects: 100 % ( 185 /185 ) , 6 .17 MiB | 4 .61 MiB/s, done . Resolving deltas: 100 % ( 23 /23 ) , done . Change directory into refarch-eda-tools/labs/es-cloud-schema-lab to find the assets we will we working from now on for the python demo environment and few other scripts/applications: $ cd refarch-eda-tools/labs/es-cloud-schema-lab $ ls -all total 240 drwxr-xr-x 9 user staff 288 20 May 19 :33 . drwxr-xr-x 3 user staff 96 20 May 19 :33 .. -rw-r--r-- 1 user staff 1012 20 May 19 :33 Dockerfile -rw-r--r-- 1 user staff 112578 20 May 19 :33 README.md drwxr-xr-x 5 user staff 160 20 May 19 :33 avro_files drwxr-xr-x 27 user staff 864 20 May 19 :33 images drwxr-xr-x 6 user staff 192 20 May 19 :33 kafka -rw-r--r-- 1 user staff 286 20 May 19 :33 kafka.properties drwxr-xr-x 6 user staff 192 20 May 19 :33 src Build \u00b6 This Docker container can be built by using the Dockerfile provided within this github repository. To build your python demo environment Docker container, execute the following on your workstation: docker build -t \"ibmcase/python-schema-registry-lab:latest\" . WARNING: Mind the dot at the end of the command. Be consistent throughout the lab with the name you give to the Docker container. Run \u00b6 In order to run the python demo environment Docker container, execute the following on your workstation: Make sure you have declared your KAFKA_BROKERS , KAFKA_APIKEY and SCHEMA_REGISTRY_URL environment variables as explain in the IBM Event Streams Service Credentials section. Run the python demo environment container $ docker run -e KAFKA_BROKERS = $KAFKA_BROKERS \\ -e KAFKA_APIKEY = $KAFKA_APIKEY \\ -e SCHEMA_REGISTRY_URL = $SCHEMA_REGISTRY_URL \\ -v ${ PWD } :/tmp/lab \\ --rm \\ -ti ibmcase/python-schema-registry-lab:latest bash Go to /tmp/lab to find all the assets you will need to complete this lab. INFO: we have mounted this working directory into the container so any changes to any of the files apply within the container. This is good as we do not need to restart the python demo environment Docker container if we want to do any changes to the files. Exit \u00b6 Once you are done with the python demo environment container, just execute exit and you will get out of the container and the container will automatically be removed from your system. Schema Registry \u00b6 INFO: The following documentation about the IBM Event Streams on IBM Cloud Schema registry until the end of this lab is based on the Schema Registry status as of mid May 2020 when this tutorial was developed One of the most common technologies used in the industry these days to define, serialize and deserialize messages flowing through your Kafka topics is Apache Avro ( https://avro.apache.org/docs/current/ ). To learn more about Apache Avro, how to define Apache Avro data schemas and more see our documentation here IBM Event Streams on IBM Cloud development team has developed a Schema Registry to work along your Kafka cluster to provide centralized schema management and compatibility checks as schemas evolve so that the communication between Kafka producers and consumers follow these schemas for consistency or as many like to say, meet the contracts that schemas are. Overview \u00b6 The schema registry capability is being developed for the IBM Event Streams managed Kafka service running in IBM Cloud. The purpose of the schema registry is to provide a place to store descriptions of the message formats used by your producers and consumers. The benefit of storing these descriptions is that you are able to validate that your producing and consuming applications will correctly inter-operate. Currently the schema registry is in early access status. This means that a limited function version of the registry is being made available to a small group of users for the purpose of gathering feedback, and rapidly iterating on the design of the registry. Please note that while the schema registry is in early access, there may be occasions when IBM will delete all of the schema data held within the registry. Current features \u00b6 Support for creating, listing and deleting schemas via a REST interface Support for creating, listing and deleting versions of a schema, via a REST interface Support for using schema information in Kafka producer and consumer applications via the Confluent AVRO SerDes Support for Apache AVRO as the format used to express schemas Support for applying constraints on schema compatibility, either at a global scope, or on a per-schema basis Access to schema registry requires authentication and access is controlled via IAM Access to individual schemas, and compatibility rules can be controlled via a new IAM schema resource type Constraints on maximum schema size (64K), the maximum number of schemas that can be stored in the registry (1000) and the maximum number of versions a schema can have (100) SLA of 99.99% availability, consistent with that of the Event Streams service Current limitations \u00b6 No caching performed for frequently requested schemas Does not publish metrics to Sysdig Does not generate Activity Tracker events There may be other missing functions that you require. Please let IBM know! Enabling the Schema Registry \u00b6 Currently the schema registry is not enabled by default and can only be enabled for IBM Event Streams Enterprise plan service instances. To request the enablement of the schema registry, please raise a support ticket here . Ensure that you include the CRN of your Event Streams service instance in the ticket . The CRN of the service instance can be found using the following IBM Cloud CLI command ibmcloud resource service-instance <SERVICENAME> (where SERVICENAME is the name of your Event Streams service instance). For more info on how to get your IBM Event Streams service instance CRN review the Event Streams on Cloud hands on lab . If you have already enabled the schema registry capability for an Event Streams Enterprise plan service instance, it will automatically receive updates as new capabilities become available. Accessing the Schema Registry \u00b6 To access the schema registry, you will need the URL of the schema registry as well as a set of credentials that can be used to authenticate with the registry. Both of these pieces of information can be found by inspecting the service credentials for your service. To view these in the UI, click on your service instance, select Service credentials in the left-hand navigation pane, then click on the dropdown arrow next to one of the service credentials name listed in the table. You should see something like this: You will need the value of kafka_http_url , which is also the URL of the schema registry, and the value of apikey which you can use as the credential for authenticating with the schema registry. To check we have appropriate permissions to work with the Schema Registry, we can execute the following command that would actually list the schemas stored in the schema registry in our terminal: curl -i \u2013u token:<KAFKA_APIKEY> <URL>/artifacts curl -i -u token:***** https://mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud/artifacts HTTP/1.1 200 OK Date: Wed, 13 May 2020 11 :38:36 GMT Content-Type: application/json Content-Length: 3 Connection: keep-alive [] As you can see, no schema is being retrieved which makes sense with the just out of the box schema registry status we are at. INFO: For easiness executing the following schema registry REST API commands, we recommend you set your kafka_http_url and apikey as URL and KAFKA_APIKEY environment variables respectively as explained in the IBM Event Streams Service Credentials section. Schemas \u00b6 In this section we will finally get our hands dirty with the IBM Event Steams on IBM Cloud Schema Registry capability by working with Apache Avro schemas and the schema registry. Create a schema \u00b6 This endpoint is used to store a schema in the registry. The schema data is sent as the body of the post request. An ID for the schema can be included using the X-Registry-ArtifactId request header. If this header is not present in the request, then an ID will be generated. The content type header must be set to application/json . Creating a schema requires at least both: Reader role access to the Event Streams cluster resource type Writer role access to the schema resource that matches the schema being created Create a schema with: curl -u token:$KAFKA_APIKEY -H 'Content-Type: application/json' -H 'X- Registry-ArtifactId: <SCHEMA_ID>' $URL/artifacts -d '<AVRO_SCHEMA>' $ curl -u token: $KAFKA_APIKEY \\ -H 'Content-Type: application/json' \\ -H 'X-Registry-ArtifactId: demo-schema' \\ $URL /artifacts \\ -d '{ \"type\":\"record\", \"name\":\"demoSchema\", \"fields\":[ {\"name\": \"eventKey\",\"type\":\"string\"}, {\"name\": \"message\",\"type\":\"string\"}] }' { \"id\" : \"demo-schema\" , \"type\" : \"AVRO\" , \"version\" :1, \"createdBy\" : \"\" , \"createdOn\" :1589371190273, \"modifiedBy\" : \"\" , \"modifiedOn\" :1589371190273, \"globalId\" :1 } Create a few more schemas. List schemas \u00b6 Listing schemas requires at least: Reader role access to the Event Streams cluster resource type List the schemas and see the ones you have created previously with: curl -u token:$KAFKA_APIKEY $URL/artifacts $ curl -u token: $KAFKA_APIKEY $URL /artifacts [ \"demo-schema\" , \"demo-schema-2\" ] Delete schemas \u00b6 Schemas are deleted from the registry by issuing a DELETE request to the /artifacts/{schema-id} endpoint (where {schema-id} is the ID of the schema). If successful an empty response, and a status code of 204 (no content) is returned. Deleting a schema requires at least both: Reader role access to the Event Streams cluster resource type Manager role access to the schema resource that matches the schema being deleted Delete one (or all) of the schemas you created previously with: curl -u token:$KAFKA_APIKEY -X DELETE $URL/artifacts/my-schema $ curl -i -u token: $KAFKA_APIKEY -X DELETE $URL /artifacts/demo-schema HTTP/1.1 204 No Content Date: Wed, 13 May 2020 12 :37:48 GMT Connection: keep-alive Create new schema version \u00b6 To create a new version of a schema, make a POST request to the /artifacts/{schema-id}/versions endpoint, (where {schema-id} is the ID of the schema). The body of the request must contain the new version of the schema. If the request is successful the new schema version is created as the new latest version of the schema, with an appropriate version number, and a response with status code 200 (OK) and a payload containing metadata describing the new version, (including the version number), is returned. Creating a new version of a schema requires at least both: Reader role access to the Event Streams cluster resource type Writer role access to the schema resource that matches the schema getting a new version Create a new schema: $ curl -u token: $KAFKA_APIKEY \\ -H 'Content-Type: application/json' \\ -H 'X-Registry-ArtifactId: version-demo-schema' \\ $URL /artifacts \\ -d '{\"type\":\"record\", \"name\":\"versionDemoSchema\", \"fields\":[ {\"name\": \"eventKey\",\"type\":\"string\"}, {\"name\": \"message\",\"type\":\"string\"}] }' { \"id\" : \"version-demo-schema\" , \"type\" : \"AVRO\" , \"version\" :1, \"createdBy\" : \"\" , \"createdOn\" :1589380529049, \"modifiedBy\" : \"\" , \"modifiedOn\" :1589380529049, \"globalId\" :9 } Add a new attribute to the schema and create a new version for it with: curl -u token:$KAFKA_APIKEY -H 'Content-Type: application/json' $URL/artifacts/<SCHEMA_ID>/versions -d '<AVRO_SCHEMA' $ curl -u token: $KAFKA_APIKEY \\ -H 'Content-Type: application/json' \\ -H 'X-Registry-ArtifactId: version-demo-schema' \\ $URL /artifacts/version-demo-schema/versions \\ -d '{\"type\":\"record\", \"name\":\"versionDemoSchema\", \"fields\":[ {\"name\": \"eventKey\",\"type\":\"string\"}, {\"name\": \"message\",\"type\":\"string\"}, {\"name\": \"attribute1\",\"type\":\"string\"}]}' { \"id\" : \"version-demo-schema\" , \"type\" : \"AVRO\" , \"version\" :2, \"createdBy\" : \"\" , \"createdOn\" :1589380529049, \"modifiedBy\" : \"\" , \"modifiedOn\" :1589380728324, \"globalId\" :10 } (*) See that the returned JSON object includes the version for the schema and that this has increased Create yet another new attribute and version for the schema: $ curl -u token: $KAFKA_APIKEY \\ -H 'Content-Type: application/json' \\ -H 'X-Registry-ArtifactId: version-demo-schema' \\ $URL /artifacts/version-demo-schema/versions \\ -d '{\"type\":\"record\", \"name\":\"versionDemoSchema\", \"fields\":[ {\"name\": \"eventKey\",\"type\":\"string\"}, {\"name\": \"message\",\"type\":\"string\"}, {\"name\": \"attribute1\",\"type\":\"string\"}, {\"name\": \"attribute2\",\"type\":\"string\"}]}' { \"id\" : \"version-demo-schema\" , \"type\" : \"AVRO\" , \"version\" :3, \"createdBy\" : \"\" , \"createdOn\" :1589380529049, \"modifiedBy\" : \"\" , \"modifiedOn\" :1589380955324, \"globalId\" :11 } Get latest version of a schema \u00b6 To retrieve the latest version of a particular schema, make a GET request to the /artifacts/{schema-id} endpoint, (where {schema-id} is the ID of the schema). If successful, the latest version of the schema is returned in the payload of the response. Getting the latest version of a schema requires at least both: Reader role access to the Event Streams cluster resource type Reader role access to the schema resource that matches the schema being retrieved Get the latest version of the schema with: curl -u token:$KAFKA_APIKEY $URL/artifacts/<SCHEMA_ID> $ curl -u token: $KAFKA_APIKEY $URL /artifacts/version-demo-schema { \"type\" : \"record\" , \"name\" : \"versionDemoSchema\" , \"fields\" : [ { \"name\" : \"eventKey\" , \"type\" : \"string\" } , { \"name\" : \"message\" , \"type\" : \"string\" } , { \"name\" : \"attribute1\" , \"type\" : \"string\" } , { \"name\" : \"attribute2\" , \"type\" : \"string\" }] } Get specific version of a schema \u00b6 To retrieve a specific version of a schema, make a GET request to the /artifacts/{schema-id}/versions/{version} endpoint, (where {schema-id} is the ID of the schema, and {version} is the version number of the specific version you need to retrieve). If successful, the specified version of the schema is returned in the payload of the response. Getting the latest version of a schema requires at least both: Reader role access to the Event Streams cluster resource type Reader role access to the schema resource that matches the schema being retrieved Get a specific version of a schema with: curl -u token:$KAFKA_APIKEY $URL/artifacts/<SCHEMA_ID>/versions/<VERSION_ID> $ curl -u token: $KAFKA_APIKEY $URL /artifacts/version-demo-schema/versions/2 { \"type\" : \"record\" , \"name\" : \"versionDemoSchema\" , \"fields\" : [ { \"name\" : \"eventKey\" , \"type\" : \"string\" } , { \"name\" : \"message\" , \"type\" : \"string\" } , { \"name\" : \"attribute1\" , \"type\" : \"string\" }] } Listing all versions of a schema \u00b6 To list all versions of a schema currently stored in the registry, make a GET request to the /artifacts/{schema-id}/versions endpoint, (where {schema-id} is the ID of the schema). If successful, a list of all current version numbers for the schema is returned in the payload of the response. Getting the list of available versions of a schema requires at least both: Reader role access to the Event Streams cluster resource type Reader role access to the schema resource that matches the schema being retrieved Get all the versions for a schema with: curl -u token:$KAFKA_APIKEY $URL/artifacts/<SCHEMA_ID>/versions $ curl -u token: $KAFKA_APIKEY $URL /artifacts/version-demo-schema/versions [ 1 ,2,3 ] Deleting a version of a schema \u00b6 Schema versions are deleted from the registry by issuing a DELETE request to the /artifacts/{schema-id}/versions/{version} endpoint (where {schema-id} is the ID of the schema, and {version} is the version number of the schema version). If successful an empty response, and a status code of 204 (no content) is returned. Deleting the only remaining version of a schema will also delete the schema. Deleting a schema version requires at least both: Reader role access to the Event Streams cluster resource type Manager role access to the schema resource that matches the schema being deleted Delete a version of a schema with: curl -u token:$KAFKA_APIKEY -X DELETE $URL/artifacts/<SCHEMA_ID>/versions/<VERSION_ID> $ curl -u token: $KAFKA_APIKEY -X DELETE $URL /artifacts/version-demo-schema/versions/2 Make sure your specific version has been deleted by listing all the version for that schema: $ curl -u token: $KAFKA_APIKEY $URL /artifacts/version-demo-schema/versions [ 1 ,3 ] If you deleted a schema, it will get deleted along with all its versions. Python Avro Producer \u00b6 In this section we describe the python scripts we will be using in order to be able to produce avro messages to a Kafka topic. Produce Message \u00b6 The python script that we will use to send an avro message to a Kafka topic is ProduceAvroMessage.py where we have the following: A function to parse the arguments: def parseArguments (): global TOPIC_NAME print ( \"The arguments for this script are: \" , str ( sys . argv )) if len ( sys . argv ) == 2 : TOPIC_NAME = sys . argv [ 1 ] else : print ( \"[ERROR] - The produceAvroMessage.py script expects one argument: The Kafka topic to publish the message to\" ) exit ( 1 ) A function to create the event to be sent: def createEvent (): print ( 'Creating event...' ) event = { \"eventKey\" : \"1\" , \"message\" : \"This is a test message\" } print ( \"DONE\" ) return json . dumps ( event ) The main where we will: Parse the arguments Get the Avro schemas for the key and value of the event Create the Event to be sent Print it out for reference Create the Kafka Avro Producer and configure it Send the event if __name__ == '__main__' : # Get the Kafka topic name parseArguments () # Get the avro schemas for the message's key and value event_value_schema = getDefaultEventValueSchema ( DATA_SCHEMAS ) event_key_schema = getDefaultEventKeySchema ( DATA_SCHEMAS ) # Create the event message_event = createEvent () # Print out the event to be sent print ( \"--- Container event to be published: ---\" ) print ( json . loads ( message_event )) print ( \"----------------------------------------\" ) # Create the Kafka Avro Producer kafka_producer = KafkaProducer ( KAFKA_BROKERS , KAFKA_APIKEY , SCHEMA_REGISTRY_URL ) # Prepare the Kafka Avro Producer kafka_producer . prepareProducer ( \"ProduceAvroMessagePython\" , event_key_schema , event_value_schema ) # Publish the event kafka_producer . publishEvent ( TOPIC_NAME , message_event , \"eventKey\" ) As you can see, this python code depends on a Kafka Avro Producer and an Avro Utils for loading the Avro schemas which are explained next. Avro Utils \u00b6 This script, called avroEDAUtils.py , contains some very simple utility functions to be able to load Avro schemas from their avsc files in order to be used by the Kafka Avro Producer. A function to get the key and value Avro schemas for the messages to be sent: def getDefaultEventValueSchema ( schema_files_location ): # Get the default event value data schema known_schemas = avro . schema . Names () default_event_value_schema = LoadAvsc ( schema_files_location + \"/default_value.avsc\" , known_schemas ) return default_event_value_schema def getDefaultEventKeySchema ( schema_files_location ): # Get the default event key data schema known_schemas = avro . schema . Names () default_event_key_schema = LoadAvsc ( schema_files_location + \"/default_key.avsc\" , known_schemas ) return default_event_key_schema (*) Where known_schemas is an Avro schema dictionary where all Avro schemas read get stored in order to be able to read nested Avro schemas afterwards. See the python script in detail for examples of this. A function to open a file, read its content as an Avro schema and store it in the Avro schema dictionary: def LoadAvsc ( file_path , names = None ): # Load avsc file # file_path: path to schema file # names(optional): avro.schema.Names object file_text = open ( file_path ) . read () json_data = json . loads ( file_text ) schema = avro . schema . SchemaFromJSONData ( json_data , names ) return schema Kafka Avro Producer \u00b6 This script, called KcAvroProducer.py , will actually be the responsible for creating the Kafka Avro Producer, initialize and configure it and provide the publish method: Initialize and prepare the Kafka Producer class KafkaProducer : def __init__ ( self , kafka_brokers = \"\" , kafka_apikey = \"\" , schema_registry_url = \"\" ): self . kafka_brokers = kafka_brokers self . kafka_apikey = kafka_apikey self . schema_registry_url = schema_registry_url def prepareProducer ( self , groupID = \"pythonproducers\" , key_schema = \"\" , value_schema = \"\" ): options = { 'bootstrap.servers' : self . kafka_brokers , 'schema.registry.url' : self . schema_registry_url , 'group.id' : groupID , 'security.protocol' : 'SASL_SSL' , 'sasl.mechanisms' : 'PLAIN' , 'sasl.username' : 'token' , 'sasl.password' : self . kafka_apikey } # Print out the configuration print ( \"--- This is the configuration for the producer: ---\" ) print ( options ) print ( \"---------------------------------------------------\" ) # Create the Avro Producer self . producer = AvroProducer ( options , default_key_schema = key_schema , default_value_schema = value_schema ) Publish method def publishEvent ( self , topicName , value , key ): # Produce the Avro message # Important: value DOES NOT come in JSON format from ContainerAvroProducer.py. Therefore, we must convert it to JSON format first self . producer . produce ( topic = topicName , value = json . loads ( value ), key = json . loads ( value )[ key ], callback = self . delivery_report ) # Flush self . producer . flush () Run \u00b6 We will see in the following section Schemas and Messages how to send Avro messages according with their schemas to IBM Event Streams. Python Avro Consumer \u00b6 In this section we describe the python scripts we will be using in order to be able to consume Avro messages from a Kafka topic. Consume Message \u00b6 The python script that we will use to consume an Avro message from a Kafka topic is ConsumeAvroMessage.py where we have the following: A function to parse arguments: # Parse arguments to get the container ID to poll for def parseArguments (): global TOPIC_NAME print ( \"The arguments for the script are: \" , str ( sys . argv )) if len ( sys . argv ) != 2 : print ( \"[ERROR] - The ConsumeAvroMessage.py script expects one arguments: The Kafka topic to events from.\" ) exit ( 1 ) TOPIC_NAME = sys . argv [ 1 ] The main where we will: Parse the arguments to get the topic to read from Create the Kafka Consumer and configure it Poll for next avro message Close the Kafka consumer if __name__ == '__main__' : # Parse arguments parseArguments () # Create the Kafka Avro consumer kafka_consumer = KafkaConsumer ( KAFKA_BROKERS , KAFKA_APIKEY , TOPIC_NAME , SCHEMA_REGISTRY_URL ) # Prepare the consumer kafka_consumer . prepareConsumer () # Consume next Avro event kafka_consumer . pollNextEvent () # Close the Avro consumer kafka_consumer . close () As you can see, this python code depends on a Kafka Consumer which is explained next. Kafka Avro Consumer \u00b6 This script, called KcAvroConsumer.py , will actually be the responsible for creating the Kafka Avro Consumer, initialize and configure it and provide the poll next event method: Initialize and prepare the new Kafka consumer: class KafkaConsumer : def __init__ ( self , kafka_brokers = \"\" , kafka_apikey = \"\" , topic_name = \"\" , schema_registry_url = \"\" , autocommit = True ): self . kafka_brokers = kafka_brokers self . kafka_apikey = kafka_apikey self . topic_name = topic_name self . schema_registry_url = schema_registry_url self . kafka_auto_commit = autocommit # See https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md def prepareConsumer ( self , groupID = \"pythonconsumers\" ): options = { 'bootstrap.servers' : self . kafka_brokers , 'group.id' : groupID , 'auto.offset.reset' : 'earliest' , 'schema.registry.url' : self . schema_registry_url , 'enable.auto.commit' : self . kafka_auto_commit , 'security.protocol' : 'SASL_SSL' , 'sasl.mechanisms' : 'PLAIN' , 'sasl.username' : 'token' , 'sasl.password' : self . kafka_apikey } # Print the configuration print ( \"--- This is the configuration for the Avro consumer: ---\" ) print ( options ) print ( \"---------------------------------------------------\" ) # Create the Avro consumer self . consumer = AvroConsumer ( options ) # Subscribe to the topic self . consumer . subscribe ([ self . topic_name ]) Poll next event method: # Prints out the message def traceResponse ( self , msg ): print ( '@@@ pollNextOrder - {} partition: [ {} ] at offset {} with key {} : \\n\\t value: {} ' . format ( msg . topic (), msg . partition (), msg . offset (), msg . key (), msg . value () )) # Polls for next event def pollNextEvent ( self ): # Poll for messages msg = self . consumer . poll ( timeout = 10.0 ) # Validate the returned message if msg is None : print ( \"[INFO] - No new messages on the topic\" ) elif msg . error (): if ( \"PARTITION_EOF\" in msg . error ()): print ( \"[INFO] - End of partition\" ) else : print ( \"[ERROR] - Consumer error: {} \" . format ( msg . error ())) else : # Print the message msgStr = self . traceResponse ( msg ) Run \u00b6 We will see in the following section Schemas and Messages how to consume Avro messages. Schemas and Messages \u00b6 In this section we are going to see how Schema Registry works when you have an application that produces and consumes messages based on Avro data schemas. The application we are going to use for this is the python scripts presented above in the Python Avro Producer and Python Avro Consumer . Once again, we are going to run these scripts in the python demo environment we presented earlier in this lab in this section . Please, review that section in order to understand how to run the environment in your local workstation. Make sure you have a newly created topic for this exercise (review the IBM Event Streams on IBM Cloud lab if needed): $ ibmcloud es topic-create test Created topic test OK $ ibmcloud es topics Topic name test OK Make sure you dont have any schema registered (preferably for clarity): $ curl -u token: $KAFKA_APIKEY $URL /artifacts [] Start your python environment with: $ docker run -e KAFKA_BROKERS = $KAFKA_BROKERS \\ -e KAFKA_APIKEY = $KAFKA_APIKEY \\ -e SCHEMA_REGISTRY_URL = $SCHEMA_REGISTRY_URL \\ -v ${ PWD } :/tmp/lab \\ --rm \\ -ti ibmcase/python-schema-registry-lab:latest bash Create a message \u00b6 In order to create a message, we execute the ProduceAvroMessage.py within the /tmp/lab/src folder in our python demo environment. This script, as you could see in the Python Avro Producer section, it is sending the event {'eventKey': '1', 'message': 'This is a test message'} according to the schemas defined in default_key.avsc and default_value.avsc for the key and value of the event respectively. python ProduceAvroMessage.py test @@@ Executing script: ProduceAvroMessage.py The arguments for the script are: [ 'ProduceAvroMessage.py' , 'test' ] Creating event... DONE --- Container event to be published: --- { 'eventKey' : '1' , 'message' : 'This is a test message' } ---------------------------------------- --- This is the configuration for the avro producer: --- { 'bootstrap.servers' : 'kafka-2.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093,kafka-1.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093,kafka-0.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093' , 'schema.registry.url' : 'https://token:4uk9gZ-n85a2esMoMZ5wtW-yIq_29o3PrHVBEFBj67N0@mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud/confluent' , 'group.id' : 'ProduceAvroMessagePython' , 'security.protocol' : 'SASL_SSL' , 'sasl.mechanisms' : 'PLAIN' , 'sasl.username' : 'token' , 'sasl.password' : '*****' } --------------------------------------------------- Message delivered to test [ 0 ] We should now have a new message in our test kafka topic. We can check that out using Kafdrop : INFO: Mind the message now is not in JSON format as Avro does not repeat every field name with every single record which makes Avro more efficient than JSON for high-volume usage. This is thanks to having Avro schemas. WARNING: Most of the Avro producer clients, whether it is in Java, Python or many other languages, give users the ability to auto-register a schema automatically with the specified schema registry in its configuration. If we look know at the schemas our schema registry has: $ curl -u token: $KAFKA_APIKEY $URL /artifacts [ \"test-key\" , \"test-value\" ] we see two schemas, test-key and test-value , which in fact correspond to the Avro data schema used for the key ( default_key.avsc ) and the value ( default_value.avsc ) of events sent to the test topic in the ProduceAvroMessage.py as explained before sending the message. To make sure of what we are saying, we can inspect those schemas: $ curl -u token: $KAFKA_APIKEY $URL /artifacts/test-key \"string\" $ curl -s -u token: $KAFKA_APIKEY $URL /artifacts/test-value | jq . { \"type\" : \"record\" , \"name\" : \"defaultValue\" , \"namespace\" : \"ibm.eda.default\" , \"fields\" : [ { \"type\" : \"string\" , \"name\" : \"eventKey\" , \"doc\" : \"We expect any string as the event key\" } , { \"type\" : \"string\" , \"name\" : \"message\" , \"doc\" : \"Any string message\" } ] , \"doc\" : \"Default Message's value Avro data schema, composed of the key again and any string message\" } If I now decided that my events should contain another attribute, I would modify the event value schema ( default_value.avsc ) to reflect that as well as ProduceAvroMessage.py to send that new attribute in the event it sends: # python ProduceAvroMessage.py test @@@ Executing script: ProduceAvroMessage.py The arguments for the script are: [ 'ProduceAvroMessage.py' , 'test' ] Creating event... DONE --- Container event to be published: --- { 'eventKey' : '1' , 'message' : 'This is a test message' , 'anotherAttribute' : 'This is another atttribute' } ---------------------------------------- --- This is the configuration for the avro producer: --- { 'bootstrap.servers' : 'kafka-2.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093,kafka-1.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093,kafka-0.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093' , 'schema.registry.url' : 'https://token:4uk9gZ-n85a2esMoMZ5wtW-yIq_29o3PrHVBEFBj67N0@mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud/confluent' , 'group.id' : 'ProduceAvroMessagePython' , 'security.protocol' : 'SASL_SSL' , 'sasl.mechanisms' : 'PLAIN' , 'sasl.username' : 'token' , 'sasl.password' : '*****' } --------------------------------------------------- Message delivered to test [ 0 ] I can see that an event with a new attribute has been sent: And I can also see that the new shcema has got registered as well as a new version for the already exisiting schema: $ curl -s -u token: $KAFKA_APIKEY $URL /artifacts/test-value/versions [ 1 ,2 ] $ curl -s -u token: $KAFKA_APIKEY $URL /artifacts/test-value | jq . { \"type\" : \"record\" , \"name\" : \"defaultValue\" , \"namespace\" : \"ibm.eda.default\" , \"fields\" : [ { \"type\" : \"string\" , \"name\" : \"eventKey\" , \"doc\" : \"We expect any string as the event key\" } , { \"type\" : \"string\" , \"name\" : \"message\" , \"doc\" : \"Any string message\" } , { \"type\" : \"string\" , \"name\" : \"anotherAttribute\" , \"doc\" : \"Another string attribute for demo\" } ] , \"doc\" : \"Default Message's value Avro data schema, composed of the key again and any string message\" } SECURITY: As some of you may have already thought, having your clients (that is your applications), auto-register the Avro data schemas that are in the end kind of the contracts that your components of your overal solution agree on in order to understand each other and collaborate between them is NOT a good idea. Specially in microservices architectures where you might have hundreds of microservices talking and collaborating among themselsves. We will see in the Security section how we can control schema registration and evolution based on roles at the schema level also. Create a non-compliant message \u00b6 Now, we are trying to send a non-compliant message according to the Avro data schema we have for our events. Im going to try to send the following event: { 'eventKey' : '1' , 'message' : 'This is a test message' , 'anotherAttribute' : 'This is another atttribute' , 'yetAnotherAttribute' : 'This should fail' } and this is the output of that attempt: # python ProduceAvroMessage.py test @@@ Executing script: ProduceAvroMessage.py The arguments for the script are: [ 'ProduceAvroMessage.py' , 'test' ] Creating event... DONE --- Container event to be published: --- { 'eventKey' : '1' , 'message' : 'This is a test message' , 'anotherAttribute' : 'This is another atttribute' , 'yetAnotherAttribute' : 'This should fail' } ---------------------------------------- --- This is the configuration for the avro producer: --- { 'bootstrap.servers' : 'kafka-2.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093,kafka-1.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093,kafka-0.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093' , 'schema.registry.url' : 'https://token:4uk9gZ-n85a2esMoMZ5wtW-yIq_29o3PrHVBEFBj67N0@mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud/confluent' , 'group.id' : 'ProduceAvroMessagePython' , 'security.protocol' : 'SASL_SSL' , 'sasl.mechanisms' : 'PLAIN' , 'sasl.username' : 'token' , 'sasl.password' : '*****' } --------------------------------------------------- Traceback ( most recent call last ) : File \"ProduceAvroMessage.py\" , line 77 , in <module> kp.publishEvent ( TOPIC_NAME,message_event, \"eventKey\" ) File \"/tmp/lab/kafka/KcAvroProducer.py\" , line 39 , in publishEvent self.producer.produce ( topic = topicName,value = json.loads ( value ) ,key = json.loads ( value )[ key ] , callback = self.delivery_report ) File \"/root/.local/lib/python3.7/site-packages/confluent_kafka/avro/__init__.py\" , line 99 , in produce value = self._serializer.encode_record_with_schema ( topic, value_schema, value ) File \"/root/.local/lib/python3.7/site-packages/confluent_kafka/avro/serializer/message_serializer.py\" , line 118 , in encode_record_with_schema return self.encode_record_with_schema_id ( schema_id, record, is_key = is_key ) File \"/root/.local/lib/python3.7/site-packages/confluent_kafka/avro/serializer/message_serializer.py\" , line 152 , in encode_record_with_schema_id writer ( record, outf ) File \"/root/.local/lib/python3.7/site-packages/confluent_kafka/avro/serializer/message_serializer.py\" , line 86 , in <lambda> return lambda record, fp: writer.write ( record, avro.io.BinaryEncoder ( fp )) File \"/root/.local/lib/python3.7/site-packages/avro/io.py\" , line 771 , in write raise AvroTypeException ( self.writer_schema, datum ) avro.io.AvroTypeException: The datum { 'eventKey' : '1' , 'message' : 'This is a test message' , 'anotherAttribute' : 'This is another atttribute' , 'yetAnotherAttribute' : 'This should fail' } is not an example of the schema { \"type\" : \"record\" , \"name\" : \"defaultValue\" , \"namespace\" : \"ibm.eda.default\" , \"fields\" : [ { \"type\" : \"string\" , \"name\" : \"eventKey\" , \"doc\" : \"We expect any string as the event key\" } , { \"type\" : \"string\" , \"name\" : \"message\" , \"doc\" : \"Any string message\" } , { \"type\" : \"string\" , \"name\" : \"anotherAttribute\" , \"doc\" : \"Another string attribute for demo\" } ] , \"doc\" : \"Default Message's value Avro data schema, composed of the key again and any string message\" } As we can see, the attempt failed as the Avro producer will check the message against the Avro data schema defined for the topic we want to send the message to and yield that this message does not comply. Therefore, using Avro schemas with IBM Event Streams give us the ability to build our system with robustness protecting downstream data consumers from malformed data, as only valid data will be permitted in the topic. Consume a message \u00b6 In order to consume a message, we execute the ConsumeAvroMessage.py within the /tmp/lab/src folder in our python demo environment: # python ConsumeAvroMessage.py test @@@ Executing script: ConsumeAvroMessage.py The arguments for the script are: [ 'ConsumeAvroMessage.py' , 'test' ] --- This is the configuration for the Avro consumer: --- { 'bootstrap.servers' : 'kafka-2.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093,kafka-1.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093,kafka-0.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093' , 'group.id' : 'pythonconsumers' , 'auto.offset.reset' : 'earliest' , 'schema.registry.url' : 'https://token:4uk9gZ-n85a2esMoMZ5wtW-yIq_29o3PrHVBEFBj67N0@mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud/confluent' , 'enable.auto.commit' : True, 'security.protocol' : 'SASL_SSL' , 'sasl.mechanisms' : 'PLAIN' , 'sasl.username' : 'token' , 'sasl.password' : '*****' } --------------------------------------------------- [ Message ] - Next message consumed from test partition: [ 0 ] at offset 0 with key 1 : value: { 'eventKey' : '1' , 'message' : 'This is a test message' } # python ConsumeAvroMessage.py test @@@ Executing script: ConsumeAvroMessage.py The arguments for the script are: [ 'ConsumeAvroMessage.py' , 'test' ] --- This is the configuration for the Avro consumer: --- { 'bootstrap.servers' : 'kafka-2.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093,kafka-1.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093,kafka-0.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093' , 'group.id' : 'pythonconsumers' , 'auto.offset.reset' : 'earliest' , 'schema.registry.url' : 'https://token:4uk9gZ-n85a2esMoMZ5wtW-yIq_29o3PrHVBEFBj67N0@mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud/confluent' , 'enable.auto.commit' : True, 'security.protocol' : 'SASL_SSL' , 'sasl.mechanisms' : 'PLAIN' , 'sasl.username' : 'token' , 'sasl.password' : '*****' } --------------------------------------------------- [ Message ] - Next message consumed from test partition: [ 0 ] at offset 1 with key 1 : value: { 'eventKey' : '1' , 'message' : 'This is a test message' , 'anotherAttribute' : 'This is another atttribute' } As you can see, our script was able to read the Avro messages from the test topic and map that back to their original structure thanks to the Avro schemas: [ Message ] - Next message consumed from test partition: [ 0 ] at offset 0 with key 1 : value: { 'eventKey' : '1' , 'message' : 'This is a test message' } [ Message ] - Next message consumed from - test partition: [ 0 ] at offset 1 with key 1 : value: { 'eventKey' : '1' , 'message' : 'This is a test message' , 'anotherAttribute' : 'This is another atttribute' } Data Evolution \u00b6 So far we have more or less seen what Avro is, what an Avro data schema is, what a schema registry is and how this all works together. From creating an Avro data schema for your messages/events to comply with to how the schema registry and Avro data schemas work together. And we have also seen the code for doing all this, from the python code to send and receive Avro encoded messages based on their Avro data schemas to the rich API the IBM Event Streams on IBM Cloud schema registry provides to interact with. However, we have said little about the need for data to evolve. When you design an Event Driven architecture for your application (by applying Event Storming or Domain Driven Design for example), it is very hard to come up with data structures/schemas that will not need to evolve/change in time. That is, your data, like your use or business cases, may need to evolve. As a result, Avro data schemas must be somehow flexible to allow your data to evolve along with your application and use cases. But it is not as easy as adding or removing data that travels in your events/messages or modifying the type of such data. And one of the reasons for this is that Kafka (or any other type of event backbone) is many times used as the source of truth. That is, a place that you can trust as to what has happened. Hence, Kafka will serve as the event source of truth where all the events (that is, data) that happened (which could be bank transactions, communications, etc) will get stored (sometimes up to hundreds of years ) and will be able to be replayed if needed. As a result, there must be a data schema management and data schema evolution put in place that allow the compatibility of old and new data schemas and, in fact, old and new data at the end of the day. The IBM Event Streams on IBM Cloud schema registry supports enforcing compatibility rules when creating a new version of a schema. If a request is made to create a new schema version that does not conform to the required compatibility rule, then the registry will reject the request. The following rules are supported: Compatibility Tested against Descrition NONE N/A No compatibility checking is performed when a new schema version is created BACKWARD Latest version of the schema A new version of the schema can omit fields that are present in the existing version of the schema. A new version of the schema can add optional fields that are not present in the existing version of the schema. BACKWARD_TRANSITIVE All versions of the schema Same as above FORWARD Latest version of the schema A new version of the schema can add fields that are not present in the existing version of the schema. A new version of the schema can omit optional fields that are present in the existing version of the schema. FORWARD_TRANSITIVE All versions of the schema Same as above FULL Latest version of the schema A new version of the schema can add optional fields that are not present in the existing version of the schema. A new version of the schema can omit optional fields that are present in the existing version of the schema. FULL_TRANSITIVE All versions of the schema Same as above These rules can be applied at two scopes : At a global scope , which is the default that At a per-schema level . If a per-schema level rule is defined, then this overrides the global default for the particular schema. By default, the registry has a global compatibility rule setting of NONE . Per-schema level rules must be defined otherwise the schema will default to using the global setting. Rules \u00b6 In this section we are going to see how to interact with the schema registry in order to alter the compatibility rules both at the global and per-schema level. Get the current value of a global rule \u00b6 The current value of a global rule is retrieved by issuing a GET request to the /rules/{rule-type} endpoint, (where {rule-type} is the type of global rule to be retrieved - currently the only supported type is COMPATIBILITY ). If the request is successful then the current rule configuration is returned in the payload of the response, together with a status code of 200 (OK). Getting global rule configuration requires at least: Reader role access to the Event Streams cluster resource type Get the current value for the COMPATIBILITY global rule with curl -u token:$KAFKA_APIKEY $URL/rules/COMPATIBILITY $ curl -u token: $KAFKA_APIKEY $URL /rules/COMPATIBILITY { \"type\" : \"COMPATIBILITY\" , \"config\" : \"NONE\" } As already explained, the default out of the box global compatibility rule for IBM Event Streams on IBM Cloud schema registry is NONE . Update a global rule \u00b6 Global compatibility rules can be updated by issuing a PUT request to the /rules/{rule-type} endpoint, (where {rule-type} identifies the type of global rule to be updated - currently the only supported type is COMPATIBILITY), with the new rule configuration in the body of the request. If the request is successful then the newly updated rule config is returned in the payload of the response, together with a status code of 200 (OK). Updating a global rule configuration requires at least: Manager role access to the Event Streams cluster resource type Update the compatibility globar rule from NONE to FORWARD with: curl -u token:$KAFKA_APIKEY \u2013X PUT $URL/rules/COMPATIBILITY -d '{\"type\":\"COMPATIBILITY\",\"config\":\"<COMPATIBILITY_MODE>\"}' $ curl -u token: $KAFKA_APIKEY -X PUT $URL /rules/COMPATIBILITY -d '{\"type\":\"COMPATIBILITY\",\"config\":\"FORWARD\"}' { \"type\" : \"COMPATIBILITY\" , \"config\" : \"FORWARD\" } 1. Verify it has been changed: $ curl -u token: $KAFKA_APIKEY $URL /rules/COMPATIBILITY { \"type\" : \"COMPATIBILITY\" , \"config\" : \"FORWARD\" } Get a per-schema rule \u00b6 To retrieve the current value of a type of rule being applied to a specific schema, a GET request is made to the /artifacts/{schema-id}/rules/{rule-type} endpoint, (where {schema-id} is the ID of the schema, and {rule-type} is the type of global rule to be retrieved - currently the only supported type is COMPATIBILITY). If the request is successful then the current rule value is returned in the payload of the response, together with a status code of 200 (OK). Getting per-schema rules requires at least: Reader role access to the Event Streams cluster resource type Reader role access to the schema resource to which the rule applies Get the compatibility rule for our test-value schema with: curl -u token:$KAFKA_APIKEY $URL/artifacts/<YOUR_SCHEMA_ID>/rules/COMPATIBILITY $ curl -u token: $KAFKA_APIKEY $URL /artifacts/test-value/rules/COMPATIBILITY { \"error_code\" :404, \"message\" : \"no compatibility rule exists for artifact 'test-value'\" } which makes sense as we have not yet created a compatibility rule for the test-value schema Create a per-schema rule \u00b6 Rules can be applied to a specific schema, overriding any global rules which have been set, by making a POST request to the /artifacts/{schema-id}/rules endpoint, (where {schema-id} is the ID of the schema), with the type and value of the new rule contained in the body of the request, (currently the only supported type is COMPATIBILITY). If successful an empty response, and a status code of 204 (no content) is returned. Creating per-schema rules requires at least: Reader role access to the Event Streams cluster resource type Manager role access to the schema resource for which the rule will apply Create a compatibility rule for our test-value schema with: curl -u token:$KAFKA_APIKEY $URL/artifacts/<YOUR_SCHEMA_ID>/rules -d '{\"type\":\"COMPATIBILITY\",\"config\":\"<COMPATIBILITY_MODE>\"}' $ curl -u token: $KAFKA_APIKEY $URL /artifacts/test-value/rules -d '{\"type\":\"COMPATIBILITY\",\"config\":\"FULL\"}' { \"type\" : \"COMPATIBILITY\" , \"config\" : \"FULL\" } Make sure this compatibility rules has been created: $ curl -u token: $KAFKA_APIKEY $URL /artifacts/test-value/rules/COMPATIBILITY { \"type\" : \"COMPATIBILITY\" , \"config\" : \"FULL\" } Update a per-schema rule \u00b6 The rules applied to a specific schema are modified by making a PUT request to the /artifacts/{schema-id}/rules/{rule-type} endpoint, (where {schema-id} is the ID of the schema, and {rule-type} is the type of global rule to be retrieved - currently the only supported type is COMPATIBILITY). If the request is successful then the newly updated rule config is returned in the payload of the response, together with a status code of 200 (OK). Updating a per-schema rule requires at least: Reader role access to the Event Streams cluster resource type Manager role access to the schema resource to which the rule applies Update the compatibility rule we have for our schema test-value with: curl -u token:$KAFKA_APIKEY \u2013X PUT $URL/artifacts/<YOUR_SCHEMA_ID>/rules/COMPATIBILITY -d '{\"type\":\"COMPATIBILITY\",\"config\":\"<COMPATIBILITY_MODE>\"}' $ curl -u token: $KAFKA_APIKEY -X PUT $URL /artifacts/test-value/rules/COMPATIBILITY -d '{\"type\":\"COMPATIBILITY\",\"config\":\"BACKWARD\"}' { \"type\" : \"COMPATIBILITY\" , \"config\" : \"BACKWARD\" } Make sure the compatibility mode for our test-value schema is as expected: $ curl -u token: $KAFKA_APIKEY $URL /artifacts/test-value/rules/COMPATIBILITY { \"type\" : \"COMPATIBILITY\" , \"config\" : \"BACKWARD\" } Delete a per-schema rule \u00b6 The rules applied to a specific schema are deleted by making a DELETE request to the /artifacts/{schema-id}/rules/{rule-type} endpoint, (where {schema-id} is the ID of the schema, and {rule-type} is the type of global rule to be retrieved - currently the only supported type is COMPATIBILITY). If the request is successful then an empty response is returned, with a status code of 204 (no content). Deleting a per-schema rule requires at least: Reader role access to the Event Streams cluster resource type Manager role access to the schema resource to which the rule applies Delete the compatibility rule we have for our test-value schema with: curl -u token:$KAFKA_APIKEY \u2013X DELETE $URL/artifacts/<YOUR_SCHEMA_ID>/rules/COMPATIBILITY $ curl -u token: $KAFKA_APIKEY -X DELETE $URL /artifacts/test-value/rules/COMPATIBILITY Make sure there is no compatibility rule for our test-value schema now: $ curl -u token: $KAFKA_APIKEY $URL /artifacts/test-value/rules/COMPATIBILITY { \"error_code\" :404, \"message\" : \"no compatibility rule exists for artifact 'test-value'\" } Evolve your schemas \u00b6 In this section, we are going to review the different compatibility modes for our Avro data schemas to evolve (hence our data). For simplicity though, we are going to to review only the following types of schema compatibility and at the global level: None Backward Forward Full None \u00b6 First of all, set the compatibility mode to backwards at the global level (for simplicity): $ curl -u token: $KAFKA_APIKEY -X PUT $URL /rules/COMPATIBILITY -d '{\"type\":\"COMPATIBILITY\",\"config\":\"NONE\"}' { \"type\" : \"COMPATIBILITY\" , \"config\" : \"NONE\" } This compatibility rule does not perform any compatibility check at all when a new schema version is created. That is, if we use the test-value schema we've used through this tutorial, and try to register a completely different schema like: { \"namespace\" : \"ibm.eda.default\" , \"doc\" : \"New Default Message's value Avro data schema, completely different than its predecesors to demonstrate None compatibility\" , \"type\" : \"record\" , \"name\" : \"defaultValue\" , \"fields\" :[ { \"name\" : \"anAttribute\" , \"type\" : \"int\" , \"doc\" : \"an attribute that is an integer\" }, { \"name\" : \"anotherAttribute\" , \"type\" : \"long\" , \"doc\" : \"Just a long number\" } ] } it should work: $ curl -u token: $KAFKA_APIKEY \\ -H 'Content-Type: application/json' \\ -H 'X-Registry-ArtifactId: test-value' \\ $URL /artifacts/test-value/versions \\ -d '{ \"namespace\": \"ibm.eda.default\", \"doc\": \"New Default Message value Avro data schema, completely different than its predecesors to demonstrate None compatibility\", \"type\":\"record\", \"name\":\"defaultValue\", \"fields\":[ {\"name\": \"anAttribute\",\"type\":\"int\",\"doc\": \"an attribute that is an integer\"}, {\"name\": \"anotherAttribute\",\"type\":\"long\",\"doc\": \"Just a long number\"}]}' { \"id\" : \"test-value\" , \"type\" : \"AVRO\" , \"version\" :3, \"createdBy\" : \"\" , \"createdOn\" :1589479701588, \"modifiedBy\" : \"\" , \"modifiedOn\" :1589551226293, \"globalId\" :23 } As we can see, now the schema for the test topic will enforce that events coming into the topic are formed by an integer and a long number. New Schema \u00b6 For the following compatibility modes, we are going to use a new schema to describe a person. It will be called demo-schema and will looks like this: { \"namespace\" : \"schema.compatibility.test\" , \"name\" : \"person\" , \"type\" : \"record\" , \"fields\" : [ { \"name\" : \"name\" , \"type\" : \"string\" }, { \"name\" : \"age\" , \"type\" : \"int\" }, { \"name\" : \"gender\" , \"type\" : \"string\" } ] } Register the schema: $ curl -u token: $KAFKA_APIKEY \\ -H 'Content-Type: application/json' \\ -H 'X-Registry-ArtifactId: demo-schema' \\ $URL /artifacts \\ -d '{ \"namespace\": \"schema.compatibility.test\", \"name\": \"person\", \"type\": \"record\", \"fields\" : [ {\"name\" : \"name\", \"type\" : \"string\"}, {\"name\" : \"age\", \"type\" : \"int\"}, {\"name\" : \"gender\", \"type\" : \"string\"}]}' { \"id\" : \"demo-schema\" , \"type\" : \"AVRO\" , \"version\" :1, \"createdBy\" : \"\" , \"createdOn\" :1589553367867, \"modifiedBy\" : \"\" , \"modifiedOn\" :1589553367867, \"globalId\" :24 } Backward \u00b6 First of all, set the compatibility mode to backwards at the global level (for simplicity): $ curl -u token: $KAFKA_APIKEY -X PUT $URL /rules/COMPATIBILITY -d '{\"type\":\"COMPATIBILITY\",\"config\":\"BACKWARD\"}' { \"type\" : \"COMPATIBILITY\" , \"config\" : \"BACKWARD\" } Now, What if we decide to change the data schema to add a new attribute such as place of birth? That is, the new schema would look like: { \"namespace\" : \"schema.compatibility.test\" , \"name\" : \"person\" , \"type\" : \"record\" , \"fields\" : [ { \"name\" : \"name\" , \"type\" : \"string\" }, { \"name\" : \"age\" , \"type\" : \"int\" }, { \"name\" : \"gender\" , \"type\" : \"string\" }, { \"name\" : \"place_of_birth\" , \"type\" : \"string\" } ] } If we try to create a new version for our demo-schema to include the place_of_birth attribute with the compatibility mode set to backwards, we will get the following: $ curl -u token: $KAFKA_APIKEY \\ -H 'Content-Type: application/json' \\ -H 'X-Registry-ArtifactId: demo-schema' \\ $URL /artifacts/demo-schema/versions \\ -d '{ \"namespace\": \"schema.compatibility.test\", \"name\": \"person\", \"type\": \"record\", \"fields\" : [ {\"name\" : \"name\", \"type\" : \"string\"}, {\"name\" : \"age\", \"type\" : \"int\"}, {\"name\" : \"gender\", \"type\" : \"string\"}, {\"name\" : \"place_of_birth\",\"type\" : \"string\"}]}' { \"error_code\" :409, \"message\" : \"Schema failed compatibility check with version ID: demo-schema, schema not backward compatible: reader's 'record' schema named 'schema.compatibility.test.person' contains a field named 'place_of_birth' that does not match any field in the corresponding writer's schema. The reader's schema does not specify a default value for this field\" } We see we get an error that says that the new schema version we are trying to register for our demo-schema is not backward compatible because \"reader's 'record' schema named 'schema.compatibility.test.person' contains a field named 'place_of_birth' that does not match any field in the corresponding writer's schema. The reader's schema does not specify a default value for this field\" As the reason above explains, backward compatibility means that consumers using the new schema can read data produced with the last schema . As it stands at the moment, this is not satisfied since the consumer with the newer version expects the attribute place_of_birth . As the error explanation above also suggests, in order to include a new attribute in our schema when we have backward compatibility mode enabled, we need to provide a default value for it so that the consumer uses it when reading messages produced with the older version of the schema that will not include the newer attribute. That is, we need our newer schema version to be like: { \"namespace\" : \"schema.compatibility.test\" , \"name\" : \"person\" , \"type\" : \"record\" , \"fields\" : [ { \"name\" : \"name\" , \"type\" : \"string\" }, { \"name\" : \"age\" , \"type\" : \"int\" }, { \"name\" : \"gender\" , \"type\" : \"string\" }, { \"name\" : \"place_of_birth\" , \"type\" : \"string\" , \"default\" : \"nonDefined\" } ] } so that the consumer will use nonDefined as the value for place_of_birth whenever it consumes messages produced with the older shcema version that do not include the attribute. Let's check: $ curl -u token: $KAFKA_APIKEY \\ -H 'Content-Type: application/json' \\ -H 'X-Registry-ArtifactId: demo-schema' \\ $URL /artifacts/demo-schema/versions \\ -d '{ \"namespace\": \"schema.compatibility.test\", \"name\": \"person\", \"type\": \"record\", \"fields\" : [ {\"name\" : \"name\", \"type\" : \"string\"}, {\"name\" : \"age\", \"type\" : \"int\"}, {\"name\" : \"gender\", \"type\" : \"string\"}, {\"name\" : \"place_of_birth\",\"type\" : \"string\",\"default\": \"nonDefined\"}]}' { \"id\" : \"demo-schema\" , \"type\" : \"AVRO\" , \"version\" :2, \"createdBy\" : \"\" , \"createdOn\" :1589553367867, \"modifiedBy\" : \"\" , \"modifiedOn\" :1589556982143, \"globalId\" :27 } Effectively, we have just evolved our Avro data demo-schema schema to include a new attribute called place_of_birth . Now, how about if we wanted to delete an attribute in our schema? Let's try to remove the gender attribute: $ curl -u token: $KAFKA_APIKEY \\ -H 'Content-Type: application/json' \\ -H 'X-Registry-ArtifactId: demo-schema' \\ $URL /artifacts/demo-schema/versions \\ -d '{ \"namespace\": \"schema.compatibility.test\", \"name\": \"person\", \"type\": \"record\", \"fields\" : [ {\"name\" : \"name\", \"type\" : \"string\"}, {\"name\" : \"age\", \"type\" : \"int\"}, {\"name\" : \"place_of_birth\",\"type\" : \"string\",\"default\": \"nonDefined\"}]}' { \"id\" : \"demo-schema\" , \"type\" : \"AVRO\" , \"version\" :3, \"createdBy\" : \"\" , \"createdOn\" :1589553367867, \"modifiedBy\" : \"\" , \"modifiedOn\" :1589557152931, \"globalId\" :28 } It worked. The resaon for this is that the consumer reading messages with the newer schema that does not contain the gender attribute, will simply ignore/drop all those attributes in the old person events/messages that are not defined in the newer data schema and just take in those that are defined. In this case, if it reads messages produced with the older data schema that come with the gender attribute, it will simply drop it. Forward \u00b6 First of all, set the compatibility mode to forward at the global level (for simplicity): $ curl -u token: $KAFKA_APIKEY -X PUT $URL /rules/COMPATIBILITY -d '{\"type\":\"COMPATIBILITY\",\"config\":\"FORWARD\"}' { \"type\" : \"COMPATIBILITY\" , \"config\" : \"FORWARD\" } Now, how about removing an attribute when the compatibility type configured is set to FORWARD? Let's try to remove the age attribute: $ curl -u token: $KAFKA_APIKEY \\ -H 'Content-Type: application/json' \\ -H 'X-Registry-ArtifactId: demo-schema' \\ $URL /artifacts/demo-schema/versions \\ -d '{ \"namespace\": \"schema.compatibility.test\", \"name\": \"person\", \"type\": \"record\", \"fields\" : [ {\"name\" : \"name\", \"type\" : \"string\"}, {\"name\" : \"place_of_birth\",\"type\" : \"string\",\"default\": \"nonDefined\"}]}' { \"error_code\" :409, \"message\" : \"Schema failed compatibility check with version ID: demo-schema, schema not forward compatible: reader's 'record' schema named 'schema.compatibility.test.person' contains a field named 'age' that does not match any field in the corresponding writer's schema. The reader's schema does not specify a default value for this field\" } As the error above explains, the problem we have when we want to remove an attribute from our Avro data schema with the compatibility mode set to forward is that the producer will be sending messages without the removed attribute ( age in our case) while the consumer will be reading messages with the older Avro data schema that expects such attribute, which is what the forward compatibility required: forward compatibility means that data produced with a new schema can be read by consumers using the last schema . What can we do to have a schema that allows the producer to send messages that do not contain an attribute that the consumer expects? The trick here is to first register an \"intermediate\" data schema that adds a default value to the attibute we want to remove from the schema ( age in our case). This way, the \"intermediate\" data schema will become the older data schema for the consumers so when we register the new schema without the attribute we wanted to remove and produce data according to it (that is, without the age attribute) afterwards, the consumers will not complain since they will have in the schema they use (the \"intermediate\" schema) a default value for that attribute. And they will use that default value when reading messages without the age attribute. That is, our intermediate schema will be: { \"namespace\" : \"schema.compatibility.test\" , \"name\" : \"person\" , \"type\" : \"record\" , \"fields\" : [ { \"name\" : \"name\" , \"type\" : \"string\" }, { \"name\" : \"age\" , \"type\" : \"int\" , \"default\" : 0 , }, { \"name\" : \"place_of_birth\" , \"type\" : \"string\" , \"default\" : \"nonDefined\" } ] } If we try to register that new version of our demo-schema schema: $ curl -u token: $KAFKA_APIKEY \\ -H 'Content-Type: application/json' \\ -H 'X-Registry-ArtifactId: demo-schema' \\ $URL /artifacts/demo-schema/versions \\ -d '{ \"namespace\": \"schema.compatibility.test\", \"name\": \"person\", \"type\": \"record\", \"fields\" : [ {\"name\" : \"name\", \"type\" : \"string\"}, {\"name\" : \"age\", \"type\" : \"int\", \"default\": 0}, {\"name\" : \"place_of_birth\",\"type\" : \"string\", \"default\": \"nonDefined\"}]}' { \"id\" : \"demo-schema\" , \"type\" : \"AVRO\" , \"version\" :4, \"createdBy\" : \"\" , \"createdOn\" :1589553367867, \"modifiedBy\" : \"\" , \"modifiedOn\" :1589560847136, \"globalId\" :30 } we see we succeed since we are simply adding a default value for the newer schema. This intermediate schema, will become now the older schema at the consumer side but will still be forward compatible with the newer schema at the producer side because it now has a default value for age . Let's try now to add a new version of our demo-schema schema without the age attribute: $ curl -u token: $KAFKA_APIKEY \\ -H 'Content-Type: application/json' \\ -H 'X-Registry-ArtifactId: demo-schema' \\ $URL /artifacts/demo-schema/versions \\ -d '{ \"namespace\": \"schema.compatibility.test\", \"name\": \"person\", \"type\": \"record\", \"fields\" : [ {\"name\" : \"name\", \"type\" : \"string\"}, {\"name\" : \"place_of_birth\",\"type\" : \"string\", \"default\": \"nonDefined\"}]}' { \"id\" : \"demo-schema\" , \"type\" : \"AVRO\" , \"version\" :5, \"createdBy\" : \"\" , \"createdOn\" :1589553367867, \"modifiedBy\" : \"\" , \"modifiedOn\" :1589561057270, \"globalId\" :31 } tada! we succeeded. As we said, the older schema now at the consumer side has a default value for age so the new messages produced with the newer schema coming without the age attribute can be successfully read by the consumer using the default value. How about adding a new attribute when the compatibility mode is set to forward? Let's try to add the attribute siblings to denote the number of borthers and sisters a person might have: $ curl -u token: $KAFKA_APIKEY \\ -H 'Content-Type: application/json' \\ -H 'X-Registry-ArtifactId: demo-schema' \\ $URL /artifacts/demo-schema/versions \\ -d '{ \"namespace\": \"schema.compatibility.test\", \"name\": \"person\", \"type\": \"record\", \"fields\" : [ {\"name\" : \"name\", \"type\" : \"string\"}, {\"name\" : \"place_of_birth\",\"type\" : \"string\", \"default\": \"nonDefined\"}, {\"name\" : \"siblings\", \"type\" : \"int\"}]}' { \"id\" : \"demo-schema\" , \"type\" : \"AVRO\" , \"version\" :6, \"createdBy\" : \"\" , \"createdOn\" :1589553367867, \"modifiedBy\" : \"\" , \"modifiedOn\" :1589561302428, \"globalId\" :32 } Contrary to what happened when addign a new attribute to an Avro data schema when the compatibility mode is set to backward, adding an attribute to an Avro data schema when the compatibility mode is set to forward is not a problem because the new attributes coming with the messages that the consumer does not expect based on the older Avro data schema it uses will simply get dropped/skipped. Full \u00b6 First of all, set the compatibility mode to full at the global level (for simplicity): $ curl -u token: $KAFKA_APIKEY -X PUT $URL /rules/COMPATIBILITY -d '{\"type\":\"COMPATIBILITY\",\"config\":\"FULL\"}' { \"type\" : \"COMPATIBILITY\" , \"config\" : \"FULL\" } Full compatibility means data schemas are both backward and forward compatible . Data schemas evolve in a fully compatible way: old data can be read with the new data schema, and new data can also be read with the last data schema . In some data formats, such as JSON, there are no full-compatible changes. Every modification is either only forward or only backward compatible. But in other data formats, like Avro, you can define fields with default values. In that case adding or removing a field with a default value is a fully compatible change. So let's see if we can delete the place_of_birth attribute (the only attribute in our data schema that defines a default value): $ curl -u token: $KAFKA_APIKEY \\ -H 'Content-Type: application/json' \\ -H 'X-Registry-ArtifactId: demo-schema' \\ $URL /artifacts/demo-schema/versions \\ -d '{ \"namespace\": \"schema.compatibility.test\", \"name\": \"person\", \"type\": \"record\", \"fields\" : [ {\"name\" : \"name\", \"type\" : \"string\"}, {\"name\" : \"siblings\", \"type\" : \"int\"}]}' { \"id\" : \"demo-schema\" , \"type\" : \"AVRO\" , \"version\" :7, \"createdBy\" : \"\" , \"createdOn\" :1589553367867, \"modifiedBy\" : \"\" , \"modifiedOn\" :1589562453126, \"globalId\" :33 } It seems it works. But how about removing an attribute that does not have a default value? Well, we need to do the trick of the \"intermediate\" schema again where we first add a default for that attribute and register the schema to later register a newer schema where we remove that attibute. Let's see adding a new attribute called height (remember we need to add a default value for it because of the full compatibility mode): $ curl -u token: $KAFKA_APIKEY \\ -H 'Content-Type: application/json' \\ -H 'X-Registry-ArtifactId: demo-schema' \\ $URL /artifacts/demo-schema/versions \\ -d '{ \"namespace\": \"schema.compatibility.test\", \"name\": \"person\", \"type\": \"record\", \"fields\" : [ {\"name\" : \"name\", \"type\" : \"string\"}, {\"name\" : \"siblings\", \"type\" : \"int\"}, {\"name\" : \"height\", \"type\" : \"int\", \"default\": 0}]}' { \"id\" : \"demo-schema\" , \"type\" : \"AVRO\" , \"version\" :8, \"createdBy\" : \"\" , \"createdOn\" :1589553367867, \"modifiedBy\" : \"\" , \"modifiedOn\" :1589562591316, \"globalId\" :34 } We see it also works. Now we know how a data schema can evolve when full compatibility is required. As a final exercise, check how many times we have evolve our demo-schema schema for the compatibility rules: $ curl -u token: $KAFKA_APIKEY $URL /artifacts/demo-schema/versions [ 1 ,2,3,4,5,6,7,8 ] Security \u00b6 As we have already mentioned during the this tutorial, we need to pay attention to the permissions we give to users, groups, applications (and thefore the clients they used to interact with IBM Event Streams on IBM Cloud), etc since we dont want everyone and everything to be, for instance, creating or deleting schemas or updating the compatibility modes for schema evolution. As a result, the IBM Event Streams on IBM Cloud schema registry introduces a new IAM resource type: schema . This can be used as part of a CRN to identify a particular schema. For example: A CRN that names a specific schema (test-schema), for a particular instance of an Event Streams service: crn:v1:bluemix:public:messagehub:us-south:a/6db1b0d0b5c54ee5c201552547febcd8:91191e31-5642-4f2d-936f-647332dce3ae:schema:test-schema A CRN that describes all of the schemas for a particular instance of the Event Streams service: crn:v1:bluemix:public:messagehub:us-south:a/6db1b0d0b5c54ee5c201552547febcd8:91191e31-5642-4f2d-936f-647332dce3ae:schema: While not directly related to the addition of the schema resource type, it is also worth noting that it is possible to apply policies to a CRN describing a particular Event Streams instance. For example, policies granted at the scope of this CRN would affect all resources (topics, schemas, etc.) belonging to the cluster: crn:v1:bluemix:public:messagehub:us-south:a/6db1b0d0b5c54ee5c201552547febcd8:91191e31-5642-4f2d-936f-647332dce3ae:: With the addition of the new schema IAM resource type it is possible to create policies that control access using varying degrees of granularity, for example: a specific schema a set of schemas selected via a wildcard expression all of the schemas stored by an instance of IBM Event Streams all of the schemas stored by all of the instances of IBM Event Streams in an account Please, review the security documentation you can find here in order to understand how to create the policies for assigning specific permissions at the differentt levels of a cloud resource (that is, at the level of any Event Streams, a specific Event Streams instance but all topics, etc). Schema security role mapping \u00b6 Action Event Streams cluster resource Schema resource Create schema Read Write List schemas Read Read Delete schema Read Manager Create schema version Read Write Get latest schema version Read Read Get specific schema version Read Read List all schema versions Read Read Delete a schema version Read Manager Compatibilitty rules security role mapping \u00b6 Action Event Streams cluster resource Schema resource Update global rule Manager - Get global rule Read - Create per schema rule Read Manager Get per schema rule Read Read Update per schema rule Read Manager Delete per schema rule Read Manager","title":"Schema registry ES on Cloud"},{"location":"use-cases/schema-registry-on-cloud/#index","text":"Requirements IBM Event Streams Service Credentials Kafdrop Python Demo Environment Schema Registry Schemas Python Avro Producer Python Avro Consumer Schemas and Messages Data Evolution Security","title":"Index"},{"location":"use-cases/schema-registry-on-cloud/#requirements","text":"This lab requires the following components to work against: An IBM Cloud account. An IBM Event Streams instance with the Enterprise plan (Schema Registry is only available to these instance for now) - https://cloud.ibm.com/docs/EventStreams?topic=eventstreams-getting_started On your development workstation you will need: (optional) IBM Cloud CLI - https://cloud.ibm.com/docs/cli?topic=cloud-cli-getting-started (optional) IBM CLoud CLI Event Streams plugin - https://cloud.ibm.com/docs/EventStreams?topic=eventstreams-cli#step5_es_cli Docker - https://docs.docker.com/get-docker/ Kafdrop - https://github.com/obsidiandynamics/kafdrop","title":"Requirements"},{"location":"use-cases/schema-registry-on-cloud/#ibm-event-streams-service-credentials","text":"At this point, we want to create the needed service credentials in order to allow other applications, tools, scripts to interact with our IBM Event Streams instance. For doing so, we need to: In your IBM Event Streams instance service page, click on Service credentials on the left hand side menu: Observe, there is no service credentials yet and click on the New credential button on the top right corner: Enter a name for your service, choose Manager role for now and click on Add : You should now see your new service credential and be able to inspect its details if you click on its dropdown arrow on it left: We could have created the service credentials using the CLI as well but we leave that as extra homework for students to try on their own. However, we can explore the service credentials using the CLI with ibmcloud resource service-key <service_credentials_name> : $ ibmcloud resource service-key demo-serv-cred Retrieving service key demo-serv-cred in all resource groups under account Kedar Kulkarni ' s Account as ALMARAZJ@ie.ibm.com... Name: demo-serv-cred ID: crn:v1:bluemix:public:messagehub:eu-de:a/b636d1d83e34d7ae7e904591ac248cfa:b05be932-2a60-4315-951d-a6dd902e687a:resource-key:4ba348d2-5fcf-4c13-a265-360e983d99c5 Created At: Tue May 12 10 :53:02 UTC 2020 State: active Credentials: api_key: ***** apikey: ***** iam_apikey_description: Auto-generated for key 4ba348d2-5fcf-4c13-a265-360e983d99c5 iam_apikey_name: demo-serv-cred iam_role_crn: crn:v1:bluemix:public:iam::::serviceRole:Manager iam_serviceid_crn: crn:v1:bluemix:public:iam-identity::a/b636d1d83e34d7ae7e904591ac248cfa::serviceid:ServiceId-380e866c-5914-4e01-85c4-d80bd1b8a899 instance_id: b05be932-2a60-4315-951d-a6dd902e687a kafka_admin_url: https://mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud kafka_brokers_sasl: [ kafka-2.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093 kafka-0.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093 kafka-1.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093 ] kafka_http_url: https://mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud password: ***** user: token IMPORTANT: Out of number 4 above, we are going to define the following important environment variables that will be used throughout this lab so please make sure you understand and define these properly KAFKA_BROKERS which should take the value of kafka_brokers_sasl comma separated: export KAFKA_BROKERS = kafka-2.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093,kafka-1.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093,kafka-0.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093 KAFKA_APIKEY which should take the value of apikey : export KAFKA_APIKEY = ***** URL which should take the value of kafka_http_url : export URL = https://mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud SCHEMA_REGISTRY_URL which should be a combination of the three before in the form of: https://token:<apikey>@<kafka_http_url>/confluent export SCHEMA_REGISTRY_URL = https://token:*****@mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud/confluent INFO: The reason for this \"weird\" schema registry url configuration is because we are using the Confluent SerDes as we will see in the Python Avro Producer and Python Avro Consumer sections later on. Otherwise, the schema registry url would simply be the same as the kafka_http_url in your service credentials.","title":"IBM Event Streams Service Credentials"},{"location":"use-cases/schema-registry-on-cloud/#kafdrop","text":"Kafdrop is a web UI for viewing Kafka topics and browsing consumer groups. The tool displays information such as brokers, topics, partitions, consumers, and lets you view messages: https://github.com/obsidiandynamics/kafdrop Kafdrop runs as a Docker container in your wokstation and in order to run it, Set the password value for the sasl.jaas.config property in the kafka.properties file you can find in this repo. The value for password is the api_key/password attributes of your service credentials. If you don't remember how to get this, review that section here security.protocol = SASL_SSL ssl.protocol = TLSv1.2 ssl.enabled.protocols = TLSv1.2 ssl.endpoint.identification.algorithm = HTTPS sasl.mechanism = PLAIN sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"<YOUR_PASSWORD/API_KEY_HERE>\"; Run the Kafdrop Docker container by executing: $ docker run -d --rm -p 9000 :9000 \\ -e KAFKA_BROKERCONNECT = $KAFKA_BROKERS \\ -e KAFKA_PROPERTIES = $( cat kafka.properties | base64 ) \\ -e JVM_OPTS = \"-Xms32M -Xmx64M\" \\ -e SERVER_SERVLET_CONTEXTPATH = \"/\" \\ obsidiandynamics/kafdrop (*) After running the above command, you can use the docker container id returned on the screen to debug any possible issue with docker logs -f <container_id> . This container id will be used to stop the Kafdrop container once we have finished. You can point your browser to http://localhost:9000/ to access the Kafdrop application: You can see the topic details by clicking on the name of that topic: You can watch the messages on this topic by clicking on View Messages button under the topic name and configuring the partition, offset and number of messages option that you are presented with in the next screen. Finally, click on View Messages button at the right: To stop the Kafdrop container once you have finsihed the tutorial, simply list the containers running on your workstation, find the container id for your Kafdrop container and stop it: $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES ba6c8eaf6a3a ibmcase/python-schema-registry-lab:latest \"bash\" 31 seconds ago Up 30 seconds keen_neumann bab9cae43f15 obsidiandynamics/kafdrop \"/kafdrop.sh\" 17 minutes ago Up 17 minutes 0 .0.0.0:9000->9000/tcp laughing_dirac $ docker stop bab9cae43f15 bab9cae43f15","title":"Kafdrop"},{"location":"use-cases/schema-registry-on-cloud/#python-demo-environment","text":"Given that students' workstations may vary quite a lot, not only on their operating system but also on the tools installed on them and the tools we need for our lab might install differently, we have opted to provide a python demo environment in the form of a Docker container where all the libraries and tools needed are already pre-installed.","title":"Python Demo Environment"},{"location":"use-cases/schema-registry-on-cloud/#clone","text":"In order to build our python demo environment we first need to clone the github repository where the assets live. This github repository is https://github.com/ibm-cloud-architecture/refarch-eda-tools and the specific assets we refer to can be found under the labs/es-cloud-schema-lab folder: Clone the github repository on your workstation on the location of your choice: $ git clone https://github.com/ibm-cloud-architecture/refarch-eda-tools.git Cloning into 'refarch-eda-tools' ... remote: Enumerating objects: 185 , done . remote: Counting objects: 100 % ( 185 /185 ) , done . remote: Compressing objects: 100 % ( 148 /148 ) , done . remote: Total 185 ( delta 23 ) , reused 176 ( delta 16 ) , pack-reused 0 Receiving objects: 100 % ( 185 /185 ) , 6 .17 MiB | 4 .61 MiB/s, done . Resolving deltas: 100 % ( 23 /23 ) , done . Change directory into refarch-eda-tools/labs/es-cloud-schema-lab to find the assets we will we working from now on for the python demo environment and few other scripts/applications: $ cd refarch-eda-tools/labs/es-cloud-schema-lab $ ls -all total 240 drwxr-xr-x 9 user staff 288 20 May 19 :33 . drwxr-xr-x 3 user staff 96 20 May 19 :33 .. -rw-r--r-- 1 user staff 1012 20 May 19 :33 Dockerfile -rw-r--r-- 1 user staff 112578 20 May 19 :33 README.md drwxr-xr-x 5 user staff 160 20 May 19 :33 avro_files drwxr-xr-x 27 user staff 864 20 May 19 :33 images drwxr-xr-x 6 user staff 192 20 May 19 :33 kafka -rw-r--r-- 1 user staff 286 20 May 19 :33 kafka.properties drwxr-xr-x 6 user staff 192 20 May 19 :33 src","title":"Clone"},{"location":"use-cases/schema-registry-on-cloud/#build","text":"This Docker container can be built by using the Dockerfile provided within this github repository. To build your python demo environment Docker container, execute the following on your workstation: docker build -t \"ibmcase/python-schema-registry-lab:latest\" . WARNING: Mind the dot at the end of the command. Be consistent throughout the lab with the name you give to the Docker container.","title":"Build"},{"location":"use-cases/schema-registry-on-cloud/#run","text":"In order to run the python demo environment Docker container, execute the following on your workstation: Make sure you have declared your KAFKA_BROKERS , KAFKA_APIKEY and SCHEMA_REGISTRY_URL environment variables as explain in the IBM Event Streams Service Credentials section. Run the python demo environment container $ docker run -e KAFKA_BROKERS = $KAFKA_BROKERS \\ -e KAFKA_APIKEY = $KAFKA_APIKEY \\ -e SCHEMA_REGISTRY_URL = $SCHEMA_REGISTRY_URL \\ -v ${ PWD } :/tmp/lab \\ --rm \\ -ti ibmcase/python-schema-registry-lab:latest bash Go to /tmp/lab to find all the assets you will need to complete this lab. INFO: we have mounted this working directory into the container so any changes to any of the files apply within the container. This is good as we do not need to restart the python demo environment Docker container if we want to do any changes to the files.","title":"Run"},{"location":"use-cases/schema-registry-on-cloud/#exit","text":"Once you are done with the python demo environment container, just execute exit and you will get out of the container and the container will automatically be removed from your system.","title":"Exit"},{"location":"use-cases/schema-registry-on-cloud/#schema-registry","text":"INFO: The following documentation about the IBM Event Streams on IBM Cloud Schema registry until the end of this lab is based on the Schema Registry status as of mid May 2020 when this tutorial was developed One of the most common technologies used in the industry these days to define, serialize and deserialize messages flowing through your Kafka topics is Apache Avro ( https://avro.apache.org/docs/current/ ). To learn more about Apache Avro, how to define Apache Avro data schemas and more see our documentation here IBM Event Streams on IBM Cloud development team has developed a Schema Registry to work along your Kafka cluster to provide centralized schema management and compatibility checks as schemas evolve so that the communication between Kafka producers and consumers follow these schemas for consistency or as many like to say, meet the contracts that schemas are.","title":"Schema Registry"},{"location":"use-cases/schema-registry-on-cloud/#overview","text":"The schema registry capability is being developed for the IBM Event Streams managed Kafka service running in IBM Cloud. The purpose of the schema registry is to provide a place to store descriptions of the message formats used by your producers and consumers. The benefit of storing these descriptions is that you are able to validate that your producing and consuming applications will correctly inter-operate. Currently the schema registry is in early access status. This means that a limited function version of the registry is being made available to a small group of users for the purpose of gathering feedback, and rapidly iterating on the design of the registry. Please note that while the schema registry is in early access, there may be occasions when IBM will delete all of the schema data held within the registry.","title":"Overview"},{"location":"use-cases/schema-registry-on-cloud/#current-features","text":"Support for creating, listing and deleting schemas via a REST interface Support for creating, listing and deleting versions of a schema, via a REST interface Support for using schema information in Kafka producer and consumer applications via the Confluent AVRO SerDes Support for Apache AVRO as the format used to express schemas Support for applying constraints on schema compatibility, either at a global scope, or on a per-schema basis Access to schema registry requires authentication and access is controlled via IAM Access to individual schemas, and compatibility rules can be controlled via a new IAM schema resource type Constraints on maximum schema size (64K), the maximum number of schemas that can be stored in the registry (1000) and the maximum number of versions a schema can have (100) SLA of 99.99% availability, consistent with that of the Event Streams service","title":"Current features"},{"location":"use-cases/schema-registry-on-cloud/#current-limitations","text":"No caching performed for frequently requested schemas Does not publish metrics to Sysdig Does not generate Activity Tracker events There may be other missing functions that you require. Please let IBM know!","title":"Current limitations"},{"location":"use-cases/schema-registry-on-cloud/#enabling-the-schema-registry","text":"Currently the schema registry is not enabled by default and can only be enabled for IBM Event Streams Enterprise plan service instances. To request the enablement of the schema registry, please raise a support ticket here . Ensure that you include the CRN of your Event Streams service instance in the ticket . The CRN of the service instance can be found using the following IBM Cloud CLI command ibmcloud resource service-instance <SERVICENAME> (where SERVICENAME is the name of your Event Streams service instance). For more info on how to get your IBM Event Streams service instance CRN review the Event Streams on Cloud hands on lab . If you have already enabled the schema registry capability for an Event Streams Enterprise plan service instance, it will automatically receive updates as new capabilities become available.","title":"Enabling the Schema Registry"},{"location":"use-cases/schema-registry-on-cloud/#accessing-the-schema-registry","text":"To access the schema registry, you will need the URL of the schema registry as well as a set of credentials that can be used to authenticate with the registry. Both of these pieces of information can be found by inspecting the service credentials for your service. To view these in the UI, click on your service instance, select Service credentials in the left-hand navigation pane, then click on the dropdown arrow next to one of the service credentials name listed in the table. You should see something like this: You will need the value of kafka_http_url , which is also the URL of the schema registry, and the value of apikey which you can use as the credential for authenticating with the schema registry. To check we have appropriate permissions to work with the Schema Registry, we can execute the following command that would actually list the schemas stored in the schema registry in our terminal: curl -i \u2013u token:<KAFKA_APIKEY> <URL>/artifacts curl -i -u token:***** https://mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud/artifacts HTTP/1.1 200 OK Date: Wed, 13 May 2020 11 :38:36 GMT Content-Type: application/json Content-Length: 3 Connection: keep-alive [] As you can see, no schema is being retrieved which makes sense with the just out of the box schema registry status we are at. INFO: For easiness executing the following schema registry REST API commands, we recommend you set your kafka_http_url and apikey as URL and KAFKA_APIKEY environment variables respectively as explained in the IBM Event Streams Service Credentials section.","title":"Accessing the Schema Registry"},{"location":"use-cases/schema-registry-on-cloud/#schemas","text":"In this section we will finally get our hands dirty with the IBM Event Steams on IBM Cloud Schema Registry capability by working with Apache Avro schemas and the schema registry.","title":"Schemas"},{"location":"use-cases/schema-registry-on-cloud/#create-a-schema","text":"This endpoint is used to store a schema in the registry. The schema data is sent as the body of the post request. An ID for the schema can be included using the X-Registry-ArtifactId request header. If this header is not present in the request, then an ID will be generated. The content type header must be set to application/json . Creating a schema requires at least both: Reader role access to the Event Streams cluster resource type Writer role access to the schema resource that matches the schema being created Create a schema with: curl -u token:$KAFKA_APIKEY -H 'Content-Type: application/json' -H 'X- Registry-ArtifactId: <SCHEMA_ID>' $URL/artifacts -d '<AVRO_SCHEMA>' $ curl -u token: $KAFKA_APIKEY \\ -H 'Content-Type: application/json' \\ -H 'X-Registry-ArtifactId: demo-schema' \\ $URL /artifacts \\ -d '{ \"type\":\"record\", \"name\":\"demoSchema\", \"fields\":[ {\"name\": \"eventKey\",\"type\":\"string\"}, {\"name\": \"message\",\"type\":\"string\"}] }' { \"id\" : \"demo-schema\" , \"type\" : \"AVRO\" , \"version\" :1, \"createdBy\" : \"\" , \"createdOn\" :1589371190273, \"modifiedBy\" : \"\" , \"modifiedOn\" :1589371190273, \"globalId\" :1 } Create a few more schemas.","title":"Create a schema"},{"location":"use-cases/schema-registry-on-cloud/#list-schemas","text":"Listing schemas requires at least: Reader role access to the Event Streams cluster resource type List the schemas and see the ones you have created previously with: curl -u token:$KAFKA_APIKEY $URL/artifacts $ curl -u token: $KAFKA_APIKEY $URL /artifacts [ \"demo-schema\" , \"demo-schema-2\" ]","title":"List schemas"},{"location":"use-cases/schema-registry-on-cloud/#delete-schemas","text":"Schemas are deleted from the registry by issuing a DELETE request to the /artifacts/{schema-id} endpoint (where {schema-id} is the ID of the schema). If successful an empty response, and a status code of 204 (no content) is returned. Deleting a schema requires at least both: Reader role access to the Event Streams cluster resource type Manager role access to the schema resource that matches the schema being deleted Delete one (or all) of the schemas you created previously with: curl -u token:$KAFKA_APIKEY -X DELETE $URL/artifacts/my-schema $ curl -i -u token: $KAFKA_APIKEY -X DELETE $URL /artifacts/demo-schema HTTP/1.1 204 No Content Date: Wed, 13 May 2020 12 :37:48 GMT Connection: keep-alive","title":"Delete schemas"},{"location":"use-cases/schema-registry-on-cloud/#create-new-schema-version","text":"To create a new version of a schema, make a POST request to the /artifacts/{schema-id}/versions endpoint, (where {schema-id} is the ID of the schema). The body of the request must contain the new version of the schema. If the request is successful the new schema version is created as the new latest version of the schema, with an appropriate version number, and a response with status code 200 (OK) and a payload containing metadata describing the new version, (including the version number), is returned. Creating a new version of a schema requires at least both: Reader role access to the Event Streams cluster resource type Writer role access to the schema resource that matches the schema getting a new version Create a new schema: $ curl -u token: $KAFKA_APIKEY \\ -H 'Content-Type: application/json' \\ -H 'X-Registry-ArtifactId: version-demo-schema' \\ $URL /artifacts \\ -d '{\"type\":\"record\", \"name\":\"versionDemoSchema\", \"fields\":[ {\"name\": \"eventKey\",\"type\":\"string\"}, {\"name\": \"message\",\"type\":\"string\"}] }' { \"id\" : \"version-demo-schema\" , \"type\" : \"AVRO\" , \"version\" :1, \"createdBy\" : \"\" , \"createdOn\" :1589380529049, \"modifiedBy\" : \"\" , \"modifiedOn\" :1589380529049, \"globalId\" :9 } Add a new attribute to the schema and create a new version for it with: curl -u token:$KAFKA_APIKEY -H 'Content-Type: application/json' $URL/artifacts/<SCHEMA_ID>/versions -d '<AVRO_SCHEMA' $ curl -u token: $KAFKA_APIKEY \\ -H 'Content-Type: application/json' \\ -H 'X-Registry-ArtifactId: version-demo-schema' \\ $URL /artifacts/version-demo-schema/versions \\ -d '{\"type\":\"record\", \"name\":\"versionDemoSchema\", \"fields\":[ {\"name\": \"eventKey\",\"type\":\"string\"}, {\"name\": \"message\",\"type\":\"string\"}, {\"name\": \"attribute1\",\"type\":\"string\"}]}' { \"id\" : \"version-demo-schema\" , \"type\" : \"AVRO\" , \"version\" :2, \"createdBy\" : \"\" , \"createdOn\" :1589380529049, \"modifiedBy\" : \"\" , \"modifiedOn\" :1589380728324, \"globalId\" :10 } (*) See that the returned JSON object includes the version for the schema and that this has increased Create yet another new attribute and version for the schema: $ curl -u token: $KAFKA_APIKEY \\ -H 'Content-Type: application/json' \\ -H 'X-Registry-ArtifactId: version-demo-schema' \\ $URL /artifacts/version-demo-schema/versions \\ -d '{\"type\":\"record\", \"name\":\"versionDemoSchema\", \"fields\":[ {\"name\": \"eventKey\",\"type\":\"string\"}, {\"name\": \"message\",\"type\":\"string\"}, {\"name\": \"attribute1\",\"type\":\"string\"}, {\"name\": \"attribute2\",\"type\":\"string\"}]}' { \"id\" : \"version-demo-schema\" , \"type\" : \"AVRO\" , \"version\" :3, \"createdBy\" : \"\" , \"createdOn\" :1589380529049, \"modifiedBy\" : \"\" , \"modifiedOn\" :1589380955324, \"globalId\" :11 }","title":"Create new schema version"},{"location":"use-cases/schema-registry-on-cloud/#get-latest-version-of-a-schema","text":"To retrieve the latest version of a particular schema, make a GET request to the /artifacts/{schema-id} endpoint, (where {schema-id} is the ID of the schema). If successful, the latest version of the schema is returned in the payload of the response. Getting the latest version of a schema requires at least both: Reader role access to the Event Streams cluster resource type Reader role access to the schema resource that matches the schema being retrieved Get the latest version of the schema with: curl -u token:$KAFKA_APIKEY $URL/artifacts/<SCHEMA_ID> $ curl -u token: $KAFKA_APIKEY $URL /artifacts/version-demo-schema { \"type\" : \"record\" , \"name\" : \"versionDemoSchema\" , \"fields\" : [ { \"name\" : \"eventKey\" , \"type\" : \"string\" } , { \"name\" : \"message\" , \"type\" : \"string\" } , { \"name\" : \"attribute1\" , \"type\" : \"string\" } , { \"name\" : \"attribute2\" , \"type\" : \"string\" }] }","title":"Get latest version of a schema"},{"location":"use-cases/schema-registry-on-cloud/#get-specific-version-of-a-schema","text":"To retrieve a specific version of a schema, make a GET request to the /artifacts/{schema-id}/versions/{version} endpoint, (where {schema-id} is the ID of the schema, and {version} is the version number of the specific version you need to retrieve). If successful, the specified version of the schema is returned in the payload of the response. Getting the latest version of a schema requires at least both: Reader role access to the Event Streams cluster resource type Reader role access to the schema resource that matches the schema being retrieved Get a specific version of a schema with: curl -u token:$KAFKA_APIKEY $URL/artifacts/<SCHEMA_ID>/versions/<VERSION_ID> $ curl -u token: $KAFKA_APIKEY $URL /artifacts/version-demo-schema/versions/2 { \"type\" : \"record\" , \"name\" : \"versionDemoSchema\" , \"fields\" : [ { \"name\" : \"eventKey\" , \"type\" : \"string\" } , { \"name\" : \"message\" , \"type\" : \"string\" } , { \"name\" : \"attribute1\" , \"type\" : \"string\" }] }","title":"Get specific version of a schema"},{"location":"use-cases/schema-registry-on-cloud/#listing-all-versions-of-a-schema","text":"To list all versions of a schema currently stored in the registry, make a GET request to the /artifacts/{schema-id}/versions endpoint, (where {schema-id} is the ID of the schema). If successful, a list of all current version numbers for the schema is returned in the payload of the response. Getting the list of available versions of a schema requires at least both: Reader role access to the Event Streams cluster resource type Reader role access to the schema resource that matches the schema being retrieved Get all the versions for a schema with: curl -u token:$KAFKA_APIKEY $URL/artifacts/<SCHEMA_ID>/versions $ curl -u token: $KAFKA_APIKEY $URL /artifacts/version-demo-schema/versions [ 1 ,2,3 ]","title":"Listing all versions of a schema"},{"location":"use-cases/schema-registry-on-cloud/#deleting-a-version-of-a-schema","text":"Schema versions are deleted from the registry by issuing a DELETE request to the /artifacts/{schema-id}/versions/{version} endpoint (where {schema-id} is the ID of the schema, and {version} is the version number of the schema version). If successful an empty response, and a status code of 204 (no content) is returned. Deleting the only remaining version of a schema will also delete the schema. Deleting a schema version requires at least both: Reader role access to the Event Streams cluster resource type Manager role access to the schema resource that matches the schema being deleted Delete a version of a schema with: curl -u token:$KAFKA_APIKEY -X DELETE $URL/artifacts/<SCHEMA_ID>/versions/<VERSION_ID> $ curl -u token: $KAFKA_APIKEY -X DELETE $URL /artifacts/version-demo-schema/versions/2 Make sure your specific version has been deleted by listing all the version for that schema: $ curl -u token: $KAFKA_APIKEY $URL /artifacts/version-demo-schema/versions [ 1 ,3 ] If you deleted a schema, it will get deleted along with all its versions.","title":"Deleting a version of a schema"},{"location":"use-cases/schema-registry-on-cloud/#python-avro-producer","text":"In this section we describe the python scripts we will be using in order to be able to produce avro messages to a Kafka topic.","title":"Python Avro Producer"},{"location":"use-cases/schema-registry-on-cloud/#produce-message","text":"The python script that we will use to send an avro message to a Kafka topic is ProduceAvroMessage.py where we have the following: A function to parse the arguments: def parseArguments (): global TOPIC_NAME print ( \"The arguments for this script are: \" , str ( sys . argv )) if len ( sys . argv ) == 2 : TOPIC_NAME = sys . argv [ 1 ] else : print ( \"[ERROR] - The produceAvroMessage.py script expects one argument: The Kafka topic to publish the message to\" ) exit ( 1 ) A function to create the event to be sent: def createEvent (): print ( 'Creating event...' ) event = { \"eventKey\" : \"1\" , \"message\" : \"This is a test message\" } print ( \"DONE\" ) return json . dumps ( event ) The main where we will: Parse the arguments Get the Avro schemas for the key and value of the event Create the Event to be sent Print it out for reference Create the Kafka Avro Producer and configure it Send the event if __name__ == '__main__' : # Get the Kafka topic name parseArguments () # Get the avro schemas for the message's key and value event_value_schema = getDefaultEventValueSchema ( DATA_SCHEMAS ) event_key_schema = getDefaultEventKeySchema ( DATA_SCHEMAS ) # Create the event message_event = createEvent () # Print out the event to be sent print ( \"--- Container event to be published: ---\" ) print ( json . loads ( message_event )) print ( \"----------------------------------------\" ) # Create the Kafka Avro Producer kafka_producer = KafkaProducer ( KAFKA_BROKERS , KAFKA_APIKEY , SCHEMA_REGISTRY_URL ) # Prepare the Kafka Avro Producer kafka_producer . prepareProducer ( \"ProduceAvroMessagePython\" , event_key_schema , event_value_schema ) # Publish the event kafka_producer . publishEvent ( TOPIC_NAME , message_event , \"eventKey\" ) As you can see, this python code depends on a Kafka Avro Producer and an Avro Utils for loading the Avro schemas which are explained next.","title":"Produce Message"},{"location":"use-cases/schema-registry-on-cloud/#avro-utils","text":"This script, called avroEDAUtils.py , contains some very simple utility functions to be able to load Avro schemas from their avsc files in order to be used by the Kafka Avro Producer. A function to get the key and value Avro schemas for the messages to be sent: def getDefaultEventValueSchema ( schema_files_location ): # Get the default event value data schema known_schemas = avro . schema . Names () default_event_value_schema = LoadAvsc ( schema_files_location + \"/default_value.avsc\" , known_schemas ) return default_event_value_schema def getDefaultEventKeySchema ( schema_files_location ): # Get the default event key data schema known_schemas = avro . schema . Names () default_event_key_schema = LoadAvsc ( schema_files_location + \"/default_key.avsc\" , known_schemas ) return default_event_key_schema (*) Where known_schemas is an Avro schema dictionary where all Avro schemas read get stored in order to be able to read nested Avro schemas afterwards. See the python script in detail for examples of this. A function to open a file, read its content as an Avro schema and store it in the Avro schema dictionary: def LoadAvsc ( file_path , names = None ): # Load avsc file # file_path: path to schema file # names(optional): avro.schema.Names object file_text = open ( file_path ) . read () json_data = json . loads ( file_text ) schema = avro . schema . SchemaFromJSONData ( json_data , names ) return schema","title":"Avro Utils"},{"location":"use-cases/schema-registry-on-cloud/#kafka-avro-producer","text":"This script, called KcAvroProducer.py , will actually be the responsible for creating the Kafka Avro Producer, initialize and configure it and provide the publish method: Initialize and prepare the Kafka Producer class KafkaProducer : def __init__ ( self , kafka_brokers = \"\" , kafka_apikey = \"\" , schema_registry_url = \"\" ): self . kafka_brokers = kafka_brokers self . kafka_apikey = kafka_apikey self . schema_registry_url = schema_registry_url def prepareProducer ( self , groupID = \"pythonproducers\" , key_schema = \"\" , value_schema = \"\" ): options = { 'bootstrap.servers' : self . kafka_brokers , 'schema.registry.url' : self . schema_registry_url , 'group.id' : groupID , 'security.protocol' : 'SASL_SSL' , 'sasl.mechanisms' : 'PLAIN' , 'sasl.username' : 'token' , 'sasl.password' : self . kafka_apikey } # Print out the configuration print ( \"--- This is the configuration for the producer: ---\" ) print ( options ) print ( \"---------------------------------------------------\" ) # Create the Avro Producer self . producer = AvroProducer ( options , default_key_schema = key_schema , default_value_schema = value_schema ) Publish method def publishEvent ( self , topicName , value , key ): # Produce the Avro message # Important: value DOES NOT come in JSON format from ContainerAvroProducer.py. Therefore, we must convert it to JSON format first self . producer . produce ( topic = topicName , value = json . loads ( value ), key = json . loads ( value )[ key ], callback = self . delivery_report ) # Flush self . producer . flush ()","title":"Kafka Avro Producer"},{"location":"use-cases/schema-registry-on-cloud/#run_1","text":"We will see in the following section Schemas and Messages how to send Avro messages according with their schemas to IBM Event Streams.","title":"Run"},{"location":"use-cases/schema-registry-on-cloud/#python-avro-consumer","text":"In this section we describe the python scripts we will be using in order to be able to consume Avro messages from a Kafka topic.","title":"Python Avro Consumer"},{"location":"use-cases/schema-registry-on-cloud/#consume-message","text":"The python script that we will use to consume an Avro message from a Kafka topic is ConsumeAvroMessage.py where we have the following: A function to parse arguments: # Parse arguments to get the container ID to poll for def parseArguments (): global TOPIC_NAME print ( \"The arguments for the script are: \" , str ( sys . argv )) if len ( sys . argv ) != 2 : print ( \"[ERROR] - The ConsumeAvroMessage.py script expects one arguments: The Kafka topic to events from.\" ) exit ( 1 ) TOPIC_NAME = sys . argv [ 1 ] The main where we will: Parse the arguments to get the topic to read from Create the Kafka Consumer and configure it Poll for next avro message Close the Kafka consumer if __name__ == '__main__' : # Parse arguments parseArguments () # Create the Kafka Avro consumer kafka_consumer = KafkaConsumer ( KAFKA_BROKERS , KAFKA_APIKEY , TOPIC_NAME , SCHEMA_REGISTRY_URL ) # Prepare the consumer kafka_consumer . prepareConsumer () # Consume next Avro event kafka_consumer . pollNextEvent () # Close the Avro consumer kafka_consumer . close () As you can see, this python code depends on a Kafka Consumer which is explained next.","title":"Consume Message"},{"location":"use-cases/schema-registry-on-cloud/#kafka-avro-consumer","text":"This script, called KcAvroConsumer.py , will actually be the responsible for creating the Kafka Avro Consumer, initialize and configure it and provide the poll next event method: Initialize and prepare the new Kafka consumer: class KafkaConsumer : def __init__ ( self , kafka_brokers = \"\" , kafka_apikey = \"\" , topic_name = \"\" , schema_registry_url = \"\" , autocommit = True ): self . kafka_brokers = kafka_brokers self . kafka_apikey = kafka_apikey self . topic_name = topic_name self . schema_registry_url = schema_registry_url self . kafka_auto_commit = autocommit # See https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md def prepareConsumer ( self , groupID = \"pythonconsumers\" ): options = { 'bootstrap.servers' : self . kafka_brokers , 'group.id' : groupID , 'auto.offset.reset' : 'earliest' , 'schema.registry.url' : self . schema_registry_url , 'enable.auto.commit' : self . kafka_auto_commit , 'security.protocol' : 'SASL_SSL' , 'sasl.mechanisms' : 'PLAIN' , 'sasl.username' : 'token' , 'sasl.password' : self . kafka_apikey } # Print the configuration print ( \"--- This is the configuration for the Avro consumer: ---\" ) print ( options ) print ( \"---------------------------------------------------\" ) # Create the Avro consumer self . consumer = AvroConsumer ( options ) # Subscribe to the topic self . consumer . subscribe ([ self . topic_name ]) Poll next event method: # Prints out the message def traceResponse ( self , msg ): print ( '@@@ pollNextOrder - {} partition: [ {} ] at offset {} with key {} : \\n\\t value: {} ' . format ( msg . topic (), msg . partition (), msg . offset (), msg . key (), msg . value () )) # Polls for next event def pollNextEvent ( self ): # Poll for messages msg = self . consumer . poll ( timeout = 10.0 ) # Validate the returned message if msg is None : print ( \"[INFO] - No new messages on the topic\" ) elif msg . error (): if ( \"PARTITION_EOF\" in msg . error ()): print ( \"[INFO] - End of partition\" ) else : print ( \"[ERROR] - Consumer error: {} \" . format ( msg . error ())) else : # Print the message msgStr = self . traceResponse ( msg )","title":"Kafka Avro Consumer"},{"location":"use-cases/schema-registry-on-cloud/#run_2","text":"We will see in the following section Schemas and Messages how to consume Avro messages.","title":"Run"},{"location":"use-cases/schema-registry-on-cloud/#schemas-and-messages","text":"In this section we are going to see how Schema Registry works when you have an application that produces and consumes messages based on Avro data schemas. The application we are going to use for this is the python scripts presented above in the Python Avro Producer and Python Avro Consumer . Once again, we are going to run these scripts in the python demo environment we presented earlier in this lab in this section . Please, review that section in order to understand how to run the environment in your local workstation. Make sure you have a newly created topic for this exercise (review the IBM Event Streams on IBM Cloud lab if needed): $ ibmcloud es topic-create test Created topic test OK $ ibmcloud es topics Topic name test OK Make sure you dont have any schema registered (preferably for clarity): $ curl -u token: $KAFKA_APIKEY $URL /artifacts [] Start your python environment with: $ docker run -e KAFKA_BROKERS = $KAFKA_BROKERS \\ -e KAFKA_APIKEY = $KAFKA_APIKEY \\ -e SCHEMA_REGISTRY_URL = $SCHEMA_REGISTRY_URL \\ -v ${ PWD } :/tmp/lab \\ --rm \\ -ti ibmcase/python-schema-registry-lab:latest bash","title":"Schemas and Messages"},{"location":"use-cases/schema-registry-on-cloud/#create-a-message","text":"In order to create a message, we execute the ProduceAvroMessage.py within the /tmp/lab/src folder in our python demo environment. This script, as you could see in the Python Avro Producer section, it is sending the event {'eventKey': '1', 'message': 'This is a test message'} according to the schemas defined in default_key.avsc and default_value.avsc for the key and value of the event respectively. python ProduceAvroMessage.py test @@@ Executing script: ProduceAvroMessage.py The arguments for the script are: [ 'ProduceAvroMessage.py' , 'test' ] Creating event... DONE --- Container event to be published: --- { 'eventKey' : '1' , 'message' : 'This is a test message' } ---------------------------------------- --- This is the configuration for the avro producer: --- { 'bootstrap.servers' : 'kafka-2.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093,kafka-1.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093,kafka-0.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093' , 'schema.registry.url' : 'https://token:4uk9gZ-n85a2esMoMZ5wtW-yIq_29o3PrHVBEFBj67N0@mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud/confluent' , 'group.id' : 'ProduceAvroMessagePython' , 'security.protocol' : 'SASL_SSL' , 'sasl.mechanisms' : 'PLAIN' , 'sasl.username' : 'token' , 'sasl.password' : '*****' } --------------------------------------------------- Message delivered to test [ 0 ] We should now have a new message in our test kafka topic. We can check that out using Kafdrop : INFO: Mind the message now is not in JSON format as Avro does not repeat every field name with every single record which makes Avro more efficient than JSON for high-volume usage. This is thanks to having Avro schemas. WARNING: Most of the Avro producer clients, whether it is in Java, Python or many other languages, give users the ability to auto-register a schema automatically with the specified schema registry in its configuration. If we look know at the schemas our schema registry has: $ curl -u token: $KAFKA_APIKEY $URL /artifacts [ \"test-key\" , \"test-value\" ] we see two schemas, test-key and test-value , which in fact correspond to the Avro data schema used for the key ( default_key.avsc ) and the value ( default_value.avsc ) of events sent to the test topic in the ProduceAvroMessage.py as explained before sending the message. To make sure of what we are saying, we can inspect those schemas: $ curl -u token: $KAFKA_APIKEY $URL /artifacts/test-key \"string\" $ curl -s -u token: $KAFKA_APIKEY $URL /artifacts/test-value | jq . { \"type\" : \"record\" , \"name\" : \"defaultValue\" , \"namespace\" : \"ibm.eda.default\" , \"fields\" : [ { \"type\" : \"string\" , \"name\" : \"eventKey\" , \"doc\" : \"We expect any string as the event key\" } , { \"type\" : \"string\" , \"name\" : \"message\" , \"doc\" : \"Any string message\" } ] , \"doc\" : \"Default Message's value Avro data schema, composed of the key again and any string message\" } If I now decided that my events should contain another attribute, I would modify the event value schema ( default_value.avsc ) to reflect that as well as ProduceAvroMessage.py to send that new attribute in the event it sends: # python ProduceAvroMessage.py test @@@ Executing script: ProduceAvroMessage.py The arguments for the script are: [ 'ProduceAvroMessage.py' , 'test' ] Creating event... DONE --- Container event to be published: --- { 'eventKey' : '1' , 'message' : 'This is a test message' , 'anotherAttribute' : 'This is another atttribute' } ---------------------------------------- --- This is the configuration for the avro producer: --- { 'bootstrap.servers' : 'kafka-2.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093,kafka-1.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093,kafka-0.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093' , 'schema.registry.url' : 'https://token:4uk9gZ-n85a2esMoMZ5wtW-yIq_29o3PrHVBEFBj67N0@mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud/confluent' , 'group.id' : 'ProduceAvroMessagePython' , 'security.protocol' : 'SASL_SSL' , 'sasl.mechanisms' : 'PLAIN' , 'sasl.username' : 'token' , 'sasl.password' : '*****' } --------------------------------------------------- Message delivered to test [ 0 ] I can see that an event with a new attribute has been sent: And I can also see that the new shcema has got registered as well as a new version for the already exisiting schema: $ curl -s -u token: $KAFKA_APIKEY $URL /artifacts/test-value/versions [ 1 ,2 ] $ curl -s -u token: $KAFKA_APIKEY $URL /artifacts/test-value | jq . { \"type\" : \"record\" , \"name\" : \"defaultValue\" , \"namespace\" : \"ibm.eda.default\" , \"fields\" : [ { \"type\" : \"string\" , \"name\" : \"eventKey\" , \"doc\" : \"We expect any string as the event key\" } , { \"type\" : \"string\" , \"name\" : \"message\" , \"doc\" : \"Any string message\" } , { \"type\" : \"string\" , \"name\" : \"anotherAttribute\" , \"doc\" : \"Another string attribute for demo\" } ] , \"doc\" : \"Default Message's value Avro data schema, composed of the key again and any string message\" } SECURITY: As some of you may have already thought, having your clients (that is your applications), auto-register the Avro data schemas that are in the end kind of the contracts that your components of your overal solution agree on in order to understand each other and collaborate between them is NOT a good idea. Specially in microservices architectures where you might have hundreds of microservices talking and collaborating among themselsves. We will see in the Security section how we can control schema registration and evolution based on roles at the schema level also.","title":"Create a message"},{"location":"use-cases/schema-registry-on-cloud/#create-a-non-compliant-message","text":"Now, we are trying to send a non-compliant message according to the Avro data schema we have for our events. Im going to try to send the following event: { 'eventKey' : '1' , 'message' : 'This is a test message' , 'anotherAttribute' : 'This is another atttribute' , 'yetAnotherAttribute' : 'This should fail' } and this is the output of that attempt: # python ProduceAvroMessage.py test @@@ Executing script: ProduceAvroMessage.py The arguments for the script are: [ 'ProduceAvroMessage.py' , 'test' ] Creating event... DONE --- Container event to be published: --- { 'eventKey' : '1' , 'message' : 'This is a test message' , 'anotherAttribute' : 'This is another atttribute' , 'yetAnotherAttribute' : 'This should fail' } ---------------------------------------- --- This is the configuration for the avro producer: --- { 'bootstrap.servers' : 'kafka-2.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093,kafka-1.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093,kafka-0.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093' , 'schema.registry.url' : 'https://token:4uk9gZ-n85a2esMoMZ5wtW-yIq_29o3PrHVBEFBj67N0@mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud/confluent' , 'group.id' : 'ProduceAvroMessagePython' , 'security.protocol' : 'SASL_SSL' , 'sasl.mechanisms' : 'PLAIN' , 'sasl.username' : 'token' , 'sasl.password' : '*****' } --------------------------------------------------- Traceback ( most recent call last ) : File \"ProduceAvroMessage.py\" , line 77 , in <module> kp.publishEvent ( TOPIC_NAME,message_event, \"eventKey\" ) File \"/tmp/lab/kafka/KcAvroProducer.py\" , line 39 , in publishEvent self.producer.produce ( topic = topicName,value = json.loads ( value ) ,key = json.loads ( value )[ key ] , callback = self.delivery_report ) File \"/root/.local/lib/python3.7/site-packages/confluent_kafka/avro/__init__.py\" , line 99 , in produce value = self._serializer.encode_record_with_schema ( topic, value_schema, value ) File \"/root/.local/lib/python3.7/site-packages/confluent_kafka/avro/serializer/message_serializer.py\" , line 118 , in encode_record_with_schema return self.encode_record_with_schema_id ( schema_id, record, is_key = is_key ) File \"/root/.local/lib/python3.7/site-packages/confluent_kafka/avro/serializer/message_serializer.py\" , line 152 , in encode_record_with_schema_id writer ( record, outf ) File \"/root/.local/lib/python3.7/site-packages/confluent_kafka/avro/serializer/message_serializer.py\" , line 86 , in <lambda> return lambda record, fp: writer.write ( record, avro.io.BinaryEncoder ( fp )) File \"/root/.local/lib/python3.7/site-packages/avro/io.py\" , line 771 , in write raise AvroTypeException ( self.writer_schema, datum ) avro.io.AvroTypeException: The datum { 'eventKey' : '1' , 'message' : 'This is a test message' , 'anotherAttribute' : 'This is another atttribute' , 'yetAnotherAttribute' : 'This should fail' } is not an example of the schema { \"type\" : \"record\" , \"name\" : \"defaultValue\" , \"namespace\" : \"ibm.eda.default\" , \"fields\" : [ { \"type\" : \"string\" , \"name\" : \"eventKey\" , \"doc\" : \"We expect any string as the event key\" } , { \"type\" : \"string\" , \"name\" : \"message\" , \"doc\" : \"Any string message\" } , { \"type\" : \"string\" , \"name\" : \"anotherAttribute\" , \"doc\" : \"Another string attribute for demo\" } ] , \"doc\" : \"Default Message's value Avro data schema, composed of the key again and any string message\" } As we can see, the attempt failed as the Avro producer will check the message against the Avro data schema defined for the topic we want to send the message to and yield that this message does not comply. Therefore, using Avro schemas with IBM Event Streams give us the ability to build our system with robustness protecting downstream data consumers from malformed data, as only valid data will be permitted in the topic.","title":"Create a non-compliant message"},{"location":"use-cases/schema-registry-on-cloud/#consume-a-message","text":"In order to consume a message, we execute the ConsumeAvroMessage.py within the /tmp/lab/src folder in our python demo environment: # python ConsumeAvroMessage.py test @@@ Executing script: ConsumeAvroMessage.py The arguments for the script are: [ 'ConsumeAvroMessage.py' , 'test' ] --- This is the configuration for the Avro consumer: --- { 'bootstrap.servers' : 'kafka-2.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093,kafka-1.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093,kafka-0.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093' , 'group.id' : 'pythonconsumers' , 'auto.offset.reset' : 'earliest' , 'schema.registry.url' : 'https://token:4uk9gZ-n85a2esMoMZ5wtW-yIq_29o3PrHVBEFBj67N0@mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud/confluent' , 'enable.auto.commit' : True, 'security.protocol' : 'SASL_SSL' , 'sasl.mechanisms' : 'PLAIN' , 'sasl.username' : 'token' , 'sasl.password' : '*****' } --------------------------------------------------- [ Message ] - Next message consumed from test partition: [ 0 ] at offset 0 with key 1 : value: { 'eventKey' : '1' , 'message' : 'This is a test message' } # python ConsumeAvroMessage.py test @@@ Executing script: ConsumeAvroMessage.py The arguments for the script are: [ 'ConsumeAvroMessage.py' , 'test' ] --- This is the configuration for the Avro consumer: --- { 'bootstrap.servers' : 'kafka-2.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093,kafka-1.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093,kafka-0.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093' , 'group.id' : 'pythonconsumers' , 'auto.offset.reset' : 'earliest' , 'schema.registry.url' : 'https://token:4uk9gZ-n85a2esMoMZ5wtW-yIq_29o3PrHVBEFBj67N0@mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud/confluent' , 'enable.auto.commit' : True, 'security.protocol' : 'SASL_SSL' , 'sasl.mechanisms' : 'PLAIN' , 'sasl.username' : 'token' , 'sasl.password' : '*****' } --------------------------------------------------- [ Message ] - Next message consumed from test partition: [ 0 ] at offset 1 with key 1 : value: { 'eventKey' : '1' , 'message' : 'This is a test message' , 'anotherAttribute' : 'This is another atttribute' } As you can see, our script was able to read the Avro messages from the test topic and map that back to their original structure thanks to the Avro schemas: [ Message ] - Next message consumed from test partition: [ 0 ] at offset 0 with key 1 : value: { 'eventKey' : '1' , 'message' : 'This is a test message' } [ Message ] - Next message consumed from - test partition: [ 0 ] at offset 1 with key 1 : value: { 'eventKey' : '1' , 'message' : 'This is a test message' , 'anotherAttribute' : 'This is another atttribute' }","title":"Consume a message"},{"location":"use-cases/schema-registry-on-cloud/#data-evolution","text":"So far we have more or less seen what Avro is, what an Avro data schema is, what a schema registry is and how this all works together. From creating an Avro data schema for your messages/events to comply with to how the schema registry and Avro data schemas work together. And we have also seen the code for doing all this, from the python code to send and receive Avro encoded messages based on their Avro data schemas to the rich API the IBM Event Streams on IBM Cloud schema registry provides to interact with. However, we have said little about the need for data to evolve. When you design an Event Driven architecture for your application (by applying Event Storming or Domain Driven Design for example), it is very hard to come up with data structures/schemas that will not need to evolve/change in time. That is, your data, like your use or business cases, may need to evolve. As a result, Avro data schemas must be somehow flexible to allow your data to evolve along with your application and use cases. But it is not as easy as adding or removing data that travels in your events/messages or modifying the type of such data. And one of the reasons for this is that Kafka (or any other type of event backbone) is many times used as the source of truth. That is, a place that you can trust as to what has happened. Hence, Kafka will serve as the event source of truth where all the events (that is, data) that happened (which could be bank transactions, communications, etc) will get stored (sometimes up to hundreds of years ) and will be able to be replayed if needed. As a result, there must be a data schema management and data schema evolution put in place that allow the compatibility of old and new data schemas and, in fact, old and new data at the end of the day. The IBM Event Streams on IBM Cloud schema registry supports enforcing compatibility rules when creating a new version of a schema. If a request is made to create a new schema version that does not conform to the required compatibility rule, then the registry will reject the request. The following rules are supported: Compatibility Tested against Descrition NONE N/A No compatibility checking is performed when a new schema version is created BACKWARD Latest version of the schema A new version of the schema can omit fields that are present in the existing version of the schema. A new version of the schema can add optional fields that are not present in the existing version of the schema. BACKWARD_TRANSITIVE All versions of the schema Same as above FORWARD Latest version of the schema A new version of the schema can add fields that are not present in the existing version of the schema. A new version of the schema can omit optional fields that are present in the existing version of the schema. FORWARD_TRANSITIVE All versions of the schema Same as above FULL Latest version of the schema A new version of the schema can add optional fields that are not present in the existing version of the schema. A new version of the schema can omit optional fields that are present in the existing version of the schema. FULL_TRANSITIVE All versions of the schema Same as above These rules can be applied at two scopes : At a global scope , which is the default that At a per-schema level . If a per-schema level rule is defined, then this overrides the global default for the particular schema. By default, the registry has a global compatibility rule setting of NONE . Per-schema level rules must be defined otherwise the schema will default to using the global setting.","title":"Data Evolution"},{"location":"use-cases/schema-registry-on-cloud/#rules","text":"In this section we are going to see how to interact with the schema registry in order to alter the compatibility rules both at the global and per-schema level.","title":"Rules"},{"location":"use-cases/schema-registry-on-cloud/#get-the-current-value-of-a-global-rule","text":"The current value of a global rule is retrieved by issuing a GET request to the /rules/{rule-type} endpoint, (where {rule-type} is the type of global rule to be retrieved - currently the only supported type is COMPATIBILITY ). If the request is successful then the current rule configuration is returned in the payload of the response, together with a status code of 200 (OK). Getting global rule configuration requires at least: Reader role access to the Event Streams cluster resource type Get the current value for the COMPATIBILITY global rule with curl -u token:$KAFKA_APIKEY $URL/rules/COMPATIBILITY $ curl -u token: $KAFKA_APIKEY $URL /rules/COMPATIBILITY { \"type\" : \"COMPATIBILITY\" , \"config\" : \"NONE\" } As already explained, the default out of the box global compatibility rule for IBM Event Streams on IBM Cloud schema registry is NONE .","title":"Get the current value of a global rule"},{"location":"use-cases/schema-registry-on-cloud/#update-a-global-rule","text":"Global compatibility rules can be updated by issuing a PUT request to the /rules/{rule-type} endpoint, (where {rule-type} identifies the type of global rule to be updated - currently the only supported type is COMPATIBILITY), with the new rule configuration in the body of the request. If the request is successful then the newly updated rule config is returned in the payload of the response, together with a status code of 200 (OK). Updating a global rule configuration requires at least: Manager role access to the Event Streams cluster resource type Update the compatibility globar rule from NONE to FORWARD with: curl -u token:$KAFKA_APIKEY \u2013X PUT $URL/rules/COMPATIBILITY -d '{\"type\":\"COMPATIBILITY\",\"config\":\"<COMPATIBILITY_MODE>\"}' $ curl -u token: $KAFKA_APIKEY -X PUT $URL /rules/COMPATIBILITY -d '{\"type\":\"COMPATIBILITY\",\"config\":\"FORWARD\"}' { \"type\" : \"COMPATIBILITY\" , \"config\" : \"FORWARD\" } 1. Verify it has been changed: $ curl -u token: $KAFKA_APIKEY $URL /rules/COMPATIBILITY { \"type\" : \"COMPATIBILITY\" , \"config\" : \"FORWARD\" }","title":"Update a global rule"},{"location":"use-cases/schema-registry-on-cloud/#get-a-per-schema-rule","text":"To retrieve the current value of a type of rule being applied to a specific schema, a GET request is made to the /artifacts/{schema-id}/rules/{rule-type} endpoint, (where {schema-id} is the ID of the schema, and {rule-type} is the type of global rule to be retrieved - currently the only supported type is COMPATIBILITY). If the request is successful then the current rule value is returned in the payload of the response, together with a status code of 200 (OK). Getting per-schema rules requires at least: Reader role access to the Event Streams cluster resource type Reader role access to the schema resource to which the rule applies Get the compatibility rule for our test-value schema with: curl -u token:$KAFKA_APIKEY $URL/artifacts/<YOUR_SCHEMA_ID>/rules/COMPATIBILITY $ curl -u token: $KAFKA_APIKEY $URL /artifacts/test-value/rules/COMPATIBILITY { \"error_code\" :404, \"message\" : \"no compatibility rule exists for artifact 'test-value'\" } which makes sense as we have not yet created a compatibility rule for the test-value schema","title":"Get a per-schema rule"},{"location":"use-cases/schema-registry-on-cloud/#create-a-per-schema-rule","text":"Rules can be applied to a specific schema, overriding any global rules which have been set, by making a POST request to the /artifacts/{schema-id}/rules endpoint, (where {schema-id} is the ID of the schema), with the type and value of the new rule contained in the body of the request, (currently the only supported type is COMPATIBILITY). If successful an empty response, and a status code of 204 (no content) is returned. Creating per-schema rules requires at least: Reader role access to the Event Streams cluster resource type Manager role access to the schema resource for which the rule will apply Create a compatibility rule for our test-value schema with: curl -u token:$KAFKA_APIKEY $URL/artifacts/<YOUR_SCHEMA_ID>/rules -d '{\"type\":\"COMPATIBILITY\",\"config\":\"<COMPATIBILITY_MODE>\"}' $ curl -u token: $KAFKA_APIKEY $URL /artifacts/test-value/rules -d '{\"type\":\"COMPATIBILITY\",\"config\":\"FULL\"}' { \"type\" : \"COMPATIBILITY\" , \"config\" : \"FULL\" } Make sure this compatibility rules has been created: $ curl -u token: $KAFKA_APIKEY $URL /artifacts/test-value/rules/COMPATIBILITY { \"type\" : \"COMPATIBILITY\" , \"config\" : \"FULL\" }","title":"Create a per-schema rule"},{"location":"use-cases/schema-registry-on-cloud/#update-a-per-schema-rule","text":"The rules applied to a specific schema are modified by making a PUT request to the /artifacts/{schema-id}/rules/{rule-type} endpoint, (where {schema-id} is the ID of the schema, and {rule-type} is the type of global rule to be retrieved - currently the only supported type is COMPATIBILITY). If the request is successful then the newly updated rule config is returned in the payload of the response, together with a status code of 200 (OK). Updating a per-schema rule requires at least: Reader role access to the Event Streams cluster resource type Manager role access to the schema resource to which the rule applies Update the compatibility rule we have for our schema test-value with: curl -u token:$KAFKA_APIKEY \u2013X PUT $URL/artifacts/<YOUR_SCHEMA_ID>/rules/COMPATIBILITY -d '{\"type\":\"COMPATIBILITY\",\"config\":\"<COMPATIBILITY_MODE>\"}' $ curl -u token: $KAFKA_APIKEY -X PUT $URL /artifacts/test-value/rules/COMPATIBILITY -d '{\"type\":\"COMPATIBILITY\",\"config\":\"BACKWARD\"}' { \"type\" : \"COMPATIBILITY\" , \"config\" : \"BACKWARD\" } Make sure the compatibility mode for our test-value schema is as expected: $ curl -u token: $KAFKA_APIKEY $URL /artifacts/test-value/rules/COMPATIBILITY { \"type\" : \"COMPATIBILITY\" , \"config\" : \"BACKWARD\" }","title":"Update a per-schema rule"},{"location":"use-cases/schema-registry-on-cloud/#delete-a-per-schema-rule","text":"The rules applied to a specific schema are deleted by making a DELETE request to the /artifacts/{schema-id}/rules/{rule-type} endpoint, (where {schema-id} is the ID of the schema, and {rule-type} is the type of global rule to be retrieved - currently the only supported type is COMPATIBILITY). If the request is successful then an empty response is returned, with a status code of 204 (no content). Deleting a per-schema rule requires at least: Reader role access to the Event Streams cluster resource type Manager role access to the schema resource to which the rule applies Delete the compatibility rule we have for our test-value schema with: curl -u token:$KAFKA_APIKEY \u2013X DELETE $URL/artifacts/<YOUR_SCHEMA_ID>/rules/COMPATIBILITY $ curl -u token: $KAFKA_APIKEY -X DELETE $URL /artifacts/test-value/rules/COMPATIBILITY Make sure there is no compatibility rule for our test-value schema now: $ curl -u token: $KAFKA_APIKEY $URL /artifacts/test-value/rules/COMPATIBILITY { \"error_code\" :404, \"message\" : \"no compatibility rule exists for artifact 'test-value'\" }","title":"Delete a per-schema rule"},{"location":"use-cases/schema-registry-on-cloud/#evolve-your-schemas","text":"In this section, we are going to review the different compatibility modes for our Avro data schemas to evolve (hence our data). For simplicity though, we are going to to review only the following types of schema compatibility and at the global level: None Backward Forward Full","title":"Evolve your schemas"},{"location":"use-cases/schema-registry-on-cloud/#none","text":"First of all, set the compatibility mode to backwards at the global level (for simplicity): $ curl -u token: $KAFKA_APIKEY -X PUT $URL /rules/COMPATIBILITY -d '{\"type\":\"COMPATIBILITY\",\"config\":\"NONE\"}' { \"type\" : \"COMPATIBILITY\" , \"config\" : \"NONE\" } This compatibility rule does not perform any compatibility check at all when a new schema version is created. That is, if we use the test-value schema we've used through this tutorial, and try to register a completely different schema like: { \"namespace\" : \"ibm.eda.default\" , \"doc\" : \"New Default Message's value Avro data schema, completely different than its predecesors to demonstrate None compatibility\" , \"type\" : \"record\" , \"name\" : \"defaultValue\" , \"fields\" :[ { \"name\" : \"anAttribute\" , \"type\" : \"int\" , \"doc\" : \"an attribute that is an integer\" }, { \"name\" : \"anotherAttribute\" , \"type\" : \"long\" , \"doc\" : \"Just a long number\" } ] } it should work: $ curl -u token: $KAFKA_APIKEY \\ -H 'Content-Type: application/json' \\ -H 'X-Registry-ArtifactId: test-value' \\ $URL /artifacts/test-value/versions \\ -d '{ \"namespace\": \"ibm.eda.default\", \"doc\": \"New Default Message value Avro data schema, completely different than its predecesors to demonstrate None compatibility\", \"type\":\"record\", \"name\":\"defaultValue\", \"fields\":[ {\"name\": \"anAttribute\",\"type\":\"int\",\"doc\": \"an attribute that is an integer\"}, {\"name\": \"anotherAttribute\",\"type\":\"long\",\"doc\": \"Just a long number\"}]}' { \"id\" : \"test-value\" , \"type\" : \"AVRO\" , \"version\" :3, \"createdBy\" : \"\" , \"createdOn\" :1589479701588, \"modifiedBy\" : \"\" , \"modifiedOn\" :1589551226293, \"globalId\" :23 } As we can see, now the schema for the test topic will enforce that events coming into the topic are formed by an integer and a long number.","title":"None"},{"location":"use-cases/schema-registry-on-cloud/#new-schema","text":"For the following compatibility modes, we are going to use a new schema to describe a person. It will be called demo-schema and will looks like this: { \"namespace\" : \"schema.compatibility.test\" , \"name\" : \"person\" , \"type\" : \"record\" , \"fields\" : [ { \"name\" : \"name\" , \"type\" : \"string\" }, { \"name\" : \"age\" , \"type\" : \"int\" }, { \"name\" : \"gender\" , \"type\" : \"string\" } ] } Register the schema: $ curl -u token: $KAFKA_APIKEY \\ -H 'Content-Type: application/json' \\ -H 'X-Registry-ArtifactId: demo-schema' \\ $URL /artifacts \\ -d '{ \"namespace\": \"schema.compatibility.test\", \"name\": \"person\", \"type\": \"record\", \"fields\" : [ {\"name\" : \"name\", \"type\" : \"string\"}, {\"name\" : \"age\", \"type\" : \"int\"}, {\"name\" : \"gender\", \"type\" : \"string\"}]}' { \"id\" : \"demo-schema\" , \"type\" : \"AVRO\" , \"version\" :1, \"createdBy\" : \"\" , \"createdOn\" :1589553367867, \"modifiedBy\" : \"\" , \"modifiedOn\" :1589553367867, \"globalId\" :24 }","title":"New Schema"},{"location":"use-cases/schema-registry-on-cloud/#backward","text":"First of all, set the compatibility mode to backwards at the global level (for simplicity): $ curl -u token: $KAFKA_APIKEY -X PUT $URL /rules/COMPATIBILITY -d '{\"type\":\"COMPATIBILITY\",\"config\":\"BACKWARD\"}' { \"type\" : \"COMPATIBILITY\" , \"config\" : \"BACKWARD\" } Now, What if we decide to change the data schema to add a new attribute such as place of birth? That is, the new schema would look like: { \"namespace\" : \"schema.compatibility.test\" , \"name\" : \"person\" , \"type\" : \"record\" , \"fields\" : [ { \"name\" : \"name\" , \"type\" : \"string\" }, { \"name\" : \"age\" , \"type\" : \"int\" }, { \"name\" : \"gender\" , \"type\" : \"string\" }, { \"name\" : \"place_of_birth\" , \"type\" : \"string\" } ] } If we try to create a new version for our demo-schema to include the place_of_birth attribute with the compatibility mode set to backwards, we will get the following: $ curl -u token: $KAFKA_APIKEY \\ -H 'Content-Type: application/json' \\ -H 'X-Registry-ArtifactId: demo-schema' \\ $URL /artifacts/demo-schema/versions \\ -d '{ \"namespace\": \"schema.compatibility.test\", \"name\": \"person\", \"type\": \"record\", \"fields\" : [ {\"name\" : \"name\", \"type\" : \"string\"}, {\"name\" : \"age\", \"type\" : \"int\"}, {\"name\" : \"gender\", \"type\" : \"string\"}, {\"name\" : \"place_of_birth\",\"type\" : \"string\"}]}' { \"error_code\" :409, \"message\" : \"Schema failed compatibility check with version ID: demo-schema, schema not backward compatible: reader's 'record' schema named 'schema.compatibility.test.person' contains a field named 'place_of_birth' that does not match any field in the corresponding writer's schema. The reader's schema does not specify a default value for this field\" } We see we get an error that says that the new schema version we are trying to register for our demo-schema is not backward compatible because \"reader's 'record' schema named 'schema.compatibility.test.person' contains a field named 'place_of_birth' that does not match any field in the corresponding writer's schema. The reader's schema does not specify a default value for this field\" As the reason above explains, backward compatibility means that consumers using the new schema can read data produced with the last schema . As it stands at the moment, this is not satisfied since the consumer with the newer version expects the attribute place_of_birth . As the error explanation above also suggests, in order to include a new attribute in our schema when we have backward compatibility mode enabled, we need to provide a default value for it so that the consumer uses it when reading messages produced with the older version of the schema that will not include the newer attribute. That is, we need our newer schema version to be like: { \"namespace\" : \"schema.compatibility.test\" , \"name\" : \"person\" , \"type\" : \"record\" , \"fields\" : [ { \"name\" : \"name\" , \"type\" : \"string\" }, { \"name\" : \"age\" , \"type\" : \"int\" }, { \"name\" : \"gender\" , \"type\" : \"string\" }, { \"name\" : \"place_of_birth\" , \"type\" : \"string\" , \"default\" : \"nonDefined\" } ] } so that the consumer will use nonDefined as the value for place_of_birth whenever it consumes messages produced with the older shcema version that do not include the attribute. Let's check: $ curl -u token: $KAFKA_APIKEY \\ -H 'Content-Type: application/json' \\ -H 'X-Registry-ArtifactId: demo-schema' \\ $URL /artifacts/demo-schema/versions \\ -d '{ \"namespace\": \"schema.compatibility.test\", \"name\": \"person\", \"type\": \"record\", \"fields\" : [ {\"name\" : \"name\", \"type\" : \"string\"}, {\"name\" : \"age\", \"type\" : \"int\"}, {\"name\" : \"gender\", \"type\" : \"string\"}, {\"name\" : \"place_of_birth\",\"type\" : \"string\",\"default\": \"nonDefined\"}]}' { \"id\" : \"demo-schema\" , \"type\" : \"AVRO\" , \"version\" :2, \"createdBy\" : \"\" , \"createdOn\" :1589553367867, \"modifiedBy\" : \"\" , \"modifiedOn\" :1589556982143, \"globalId\" :27 } Effectively, we have just evolved our Avro data demo-schema schema to include a new attribute called place_of_birth . Now, how about if we wanted to delete an attribute in our schema? Let's try to remove the gender attribute: $ curl -u token: $KAFKA_APIKEY \\ -H 'Content-Type: application/json' \\ -H 'X-Registry-ArtifactId: demo-schema' \\ $URL /artifacts/demo-schema/versions \\ -d '{ \"namespace\": \"schema.compatibility.test\", \"name\": \"person\", \"type\": \"record\", \"fields\" : [ {\"name\" : \"name\", \"type\" : \"string\"}, {\"name\" : \"age\", \"type\" : \"int\"}, {\"name\" : \"place_of_birth\",\"type\" : \"string\",\"default\": \"nonDefined\"}]}' { \"id\" : \"demo-schema\" , \"type\" : \"AVRO\" , \"version\" :3, \"createdBy\" : \"\" , \"createdOn\" :1589553367867, \"modifiedBy\" : \"\" , \"modifiedOn\" :1589557152931, \"globalId\" :28 } It worked. The resaon for this is that the consumer reading messages with the newer schema that does not contain the gender attribute, will simply ignore/drop all those attributes in the old person events/messages that are not defined in the newer data schema and just take in those that are defined. In this case, if it reads messages produced with the older data schema that come with the gender attribute, it will simply drop it.","title":"Backward"},{"location":"use-cases/schema-registry-on-cloud/#forward","text":"First of all, set the compatibility mode to forward at the global level (for simplicity): $ curl -u token: $KAFKA_APIKEY -X PUT $URL /rules/COMPATIBILITY -d '{\"type\":\"COMPATIBILITY\",\"config\":\"FORWARD\"}' { \"type\" : \"COMPATIBILITY\" , \"config\" : \"FORWARD\" } Now, how about removing an attribute when the compatibility type configured is set to FORWARD? Let's try to remove the age attribute: $ curl -u token: $KAFKA_APIKEY \\ -H 'Content-Type: application/json' \\ -H 'X-Registry-ArtifactId: demo-schema' \\ $URL /artifacts/demo-schema/versions \\ -d '{ \"namespace\": \"schema.compatibility.test\", \"name\": \"person\", \"type\": \"record\", \"fields\" : [ {\"name\" : \"name\", \"type\" : \"string\"}, {\"name\" : \"place_of_birth\",\"type\" : \"string\",\"default\": \"nonDefined\"}]}' { \"error_code\" :409, \"message\" : \"Schema failed compatibility check with version ID: demo-schema, schema not forward compatible: reader's 'record' schema named 'schema.compatibility.test.person' contains a field named 'age' that does not match any field in the corresponding writer's schema. The reader's schema does not specify a default value for this field\" } As the error above explains, the problem we have when we want to remove an attribute from our Avro data schema with the compatibility mode set to forward is that the producer will be sending messages without the removed attribute ( age in our case) while the consumer will be reading messages with the older Avro data schema that expects such attribute, which is what the forward compatibility required: forward compatibility means that data produced with a new schema can be read by consumers using the last schema . What can we do to have a schema that allows the producer to send messages that do not contain an attribute that the consumer expects? The trick here is to first register an \"intermediate\" data schema that adds a default value to the attibute we want to remove from the schema ( age in our case). This way, the \"intermediate\" data schema will become the older data schema for the consumers so when we register the new schema without the attribute we wanted to remove and produce data according to it (that is, without the age attribute) afterwards, the consumers will not complain since they will have in the schema they use (the \"intermediate\" schema) a default value for that attribute. And they will use that default value when reading messages without the age attribute. That is, our intermediate schema will be: { \"namespace\" : \"schema.compatibility.test\" , \"name\" : \"person\" , \"type\" : \"record\" , \"fields\" : [ { \"name\" : \"name\" , \"type\" : \"string\" }, { \"name\" : \"age\" , \"type\" : \"int\" , \"default\" : 0 , }, { \"name\" : \"place_of_birth\" , \"type\" : \"string\" , \"default\" : \"nonDefined\" } ] } If we try to register that new version of our demo-schema schema: $ curl -u token: $KAFKA_APIKEY \\ -H 'Content-Type: application/json' \\ -H 'X-Registry-ArtifactId: demo-schema' \\ $URL /artifacts/demo-schema/versions \\ -d '{ \"namespace\": \"schema.compatibility.test\", \"name\": \"person\", \"type\": \"record\", \"fields\" : [ {\"name\" : \"name\", \"type\" : \"string\"}, {\"name\" : \"age\", \"type\" : \"int\", \"default\": 0}, {\"name\" : \"place_of_birth\",\"type\" : \"string\", \"default\": \"nonDefined\"}]}' { \"id\" : \"demo-schema\" , \"type\" : \"AVRO\" , \"version\" :4, \"createdBy\" : \"\" , \"createdOn\" :1589553367867, \"modifiedBy\" : \"\" , \"modifiedOn\" :1589560847136, \"globalId\" :30 } we see we succeed since we are simply adding a default value for the newer schema. This intermediate schema, will become now the older schema at the consumer side but will still be forward compatible with the newer schema at the producer side because it now has a default value for age . Let's try now to add a new version of our demo-schema schema without the age attribute: $ curl -u token: $KAFKA_APIKEY \\ -H 'Content-Type: application/json' \\ -H 'X-Registry-ArtifactId: demo-schema' \\ $URL /artifacts/demo-schema/versions \\ -d '{ \"namespace\": \"schema.compatibility.test\", \"name\": \"person\", \"type\": \"record\", \"fields\" : [ {\"name\" : \"name\", \"type\" : \"string\"}, {\"name\" : \"place_of_birth\",\"type\" : \"string\", \"default\": \"nonDefined\"}]}' { \"id\" : \"demo-schema\" , \"type\" : \"AVRO\" , \"version\" :5, \"createdBy\" : \"\" , \"createdOn\" :1589553367867, \"modifiedBy\" : \"\" , \"modifiedOn\" :1589561057270, \"globalId\" :31 } tada! we succeeded. As we said, the older schema now at the consumer side has a default value for age so the new messages produced with the newer schema coming without the age attribute can be successfully read by the consumer using the default value. How about adding a new attribute when the compatibility mode is set to forward? Let's try to add the attribute siblings to denote the number of borthers and sisters a person might have: $ curl -u token: $KAFKA_APIKEY \\ -H 'Content-Type: application/json' \\ -H 'X-Registry-ArtifactId: demo-schema' \\ $URL /artifacts/demo-schema/versions \\ -d '{ \"namespace\": \"schema.compatibility.test\", \"name\": \"person\", \"type\": \"record\", \"fields\" : [ {\"name\" : \"name\", \"type\" : \"string\"}, {\"name\" : \"place_of_birth\",\"type\" : \"string\", \"default\": \"nonDefined\"}, {\"name\" : \"siblings\", \"type\" : \"int\"}]}' { \"id\" : \"demo-schema\" , \"type\" : \"AVRO\" , \"version\" :6, \"createdBy\" : \"\" , \"createdOn\" :1589553367867, \"modifiedBy\" : \"\" , \"modifiedOn\" :1589561302428, \"globalId\" :32 } Contrary to what happened when addign a new attribute to an Avro data schema when the compatibility mode is set to backward, adding an attribute to an Avro data schema when the compatibility mode is set to forward is not a problem because the new attributes coming with the messages that the consumer does not expect based on the older Avro data schema it uses will simply get dropped/skipped.","title":"Forward"},{"location":"use-cases/schema-registry-on-cloud/#full","text":"First of all, set the compatibility mode to full at the global level (for simplicity): $ curl -u token: $KAFKA_APIKEY -X PUT $URL /rules/COMPATIBILITY -d '{\"type\":\"COMPATIBILITY\",\"config\":\"FULL\"}' { \"type\" : \"COMPATIBILITY\" , \"config\" : \"FULL\" } Full compatibility means data schemas are both backward and forward compatible . Data schemas evolve in a fully compatible way: old data can be read with the new data schema, and new data can also be read with the last data schema . In some data formats, such as JSON, there are no full-compatible changes. Every modification is either only forward or only backward compatible. But in other data formats, like Avro, you can define fields with default values. In that case adding or removing a field with a default value is a fully compatible change. So let's see if we can delete the place_of_birth attribute (the only attribute in our data schema that defines a default value): $ curl -u token: $KAFKA_APIKEY \\ -H 'Content-Type: application/json' \\ -H 'X-Registry-ArtifactId: demo-schema' \\ $URL /artifacts/demo-schema/versions \\ -d '{ \"namespace\": \"schema.compatibility.test\", \"name\": \"person\", \"type\": \"record\", \"fields\" : [ {\"name\" : \"name\", \"type\" : \"string\"}, {\"name\" : \"siblings\", \"type\" : \"int\"}]}' { \"id\" : \"demo-schema\" , \"type\" : \"AVRO\" , \"version\" :7, \"createdBy\" : \"\" , \"createdOn\" :1589553367867, \"modifiedBy\" : \"\" , \"modifiedOn\" :1589562453126, \"globalId\" :33 } It seems it works. But how about removing an attribute that does not have a default value? Well, we need to do the trick of the \"intermediate\" schema again where we first add a default for that attribute and register the schema to later register a newer schema where we remove that attibute. Let's see adding a new attribute called height (remember we need to add a default value for it because of the full compatibility mode): $ curl -u token: $KAFKA_APIKEY \\ -H 'Content-Type: application/json' \\ -H 'X-Registry-ArtifactId: demo-schema' \\ $URL /artifacts/demo-schema/versions \\ -d '{ \"namespace\": \"schema.compatibility.test\", \"name\": \"person\", \"type\": \"record\", \"fields\" : [ {\"name\" : \"name\", \"type\" : \"string\"}, {\"name\" : \"siblings\", \"type\" : \"int\"}, {\"name\" : \"height\", \"type\" : \"int\", \"default\": 0}]}' { \"id\" : \"demo-schema\" , \"type\" : \"AVRO\" , \"version\" :8, \"createdBy\" : \"\" , \"createdOn\" :1589553367867, \"modifiedBy\" : \"\" , \"modifiedOn\" :1589562591316, \"globalId\" :34 } We see it also works. Now we know how a data schema can evolve when full compatibility is required. As a final exercise, check how many times we have evolve our demo-schema schema for the compatibility rules: $ curl -u token: $KAFKA_APIKEY $URL /artifacts/demo-schema/versions [ 1 ,2,3,4,5,6,7,8 ]","title":"Full"},{"location":"use-cases/schema-registry-on-cloud/#security","text":"As we have already mentioned during the this tutorial, we need to pay attention to the permissions we give to users, groups, applications (and thefore the clients they used to interact with IBM Event Streams on IBM Cloud), etc since we dont want everyone and everything to be, for instance, creating or deleting schemas or updating the compatibility modes for schema evolution. As a result, the IBM Event Streams on IBM Cloud schema registry introduces a new IAM resource type: schema . This can be used as part of a CRN to identify a particular schema. For example: A CRN that names a specific schema (test-schema), for a particular instance of an Event Streams service: crn:v1:bluemix:public:messagehub:us-south:a/6db1b0d0b5c54ee5c201552547febcd8:91191e31-5642-4f2d-936f-647332dce3ae:schema:test-schema A CRN that describes all of the schemas for a particular instance of the Event Streams service: crn:v1:bluemix:public:messagehub:us-south:a/6db1b0d0b5c54ee5c201552547febcd8:91191e31-5642-4f2d-936f-647332dce3ae:schema: While not directly related to the addition of the schema resource type, it is also worth noting that it is possible to apply policies to a CRN describing a particular Event Streams instance. For example, policies granted at the scope of this CRN would affect all resources (topics, schemas, etc.) belonging to the cluster: crn:v1:bluemix:public:messagehub:us-south:a/6db1b0d0b5c54ee5c201552547febcd8:91191e31-5642-4f2d-936f-647332dce3ae:: With the addition of the new schema IAM resource type it is possible to create policies that control access using varying degrees of granularity, for example: a specific schema a set of schemas selected via a wildcard expression all of the schemas stored by an instance of IBM Event Streams all of the schemas stored by all of the instances of IBM Event Streams in an account Please, review the security documentation you can find here in order to understand how to create the policies for assigning specific permissions at the differentt levels of a cloud resource (that is, at the level of any Event Streams, a specific Event Streams instance but all topics, etc).","title":"Security"},{"location":"use-cases/schema-registry-on-cloud/#schema-security-role-mapping","text":"Action Event Streams cluster resource Schema resource Create schema Read Write List schemas Read Read Delete schema Read Manager Create schema version Read Write Get latest schema version Read Read Get specific schema version Read Read List all schema versions Read Read Delete a schema version Read Manager","title":"Schema security role mapping"},{"location":"use-cases/schema-registry-on-cloud/#compatibilitty-rules-security-role-mapping","text":"Action Event Streams cluster resource Schema resource Update global rule Manager - Get global rule Read - Create per schema rule Read Manager Get per schema rule Read Read Update per schema rule Read Manager Delete per schema rule Read Manager","title":"Compatibilitty rules security role mapping"},{"location":"use-cases/schema-registry-on-ocp/","text":"This documentation aims to be a introductory hands-on lab on the IBM Event Streams (v11.0.2) Schema Registry installed throught the IBM Cloud Pak for Integration V2022.2 on an Openshift cluster. It uses Python applications for producer and consumer with schema registry API. For a Quarkus based producer and consumer see the EDA-quickstarts project sub-folders: quarkus-reactive-kafka-producer and quarkus-reactive-kafka-consumer which includes docker compose with Apicur.io and reactive messaging implementation, plus all needed instructions to test schema management. Requirements \u00b6 This lab requires the following components to work against: An IBM Event Streams V10 instance installed through the IBM CloudPak for Integration V2020.2.X or greater. An IBM Cloud Shell - https://www.ibm.com/cloud/cloud-shell IBM Cloud Shell \u00b6 Here we are going to set up our IBM Cloud Shell with all the tools required to carry out this lab. Start your IBM Cloud Shell by pointing your browser to https://cloud.ibm.com/shell IBM Cloud Pak CLI \u00b6 Cloudctl is a command line tool to manage Container Application Software for Enterprises (CASEs). This CLI will allow us to manage Cloud Pak related components as well as software, like IBM Event Streams, installed through any IBM Cloud Pak. In order to install it, execute the following commands in your IBM Cloud Shell: Download the IBM Cloud Pak CLI - curl -L https://github.com/IBM/cloud-pak-cli/releases/latest/download/cloudctl-linux-amd64.tar.gz -o cloudctl-linux-amd64.tar.gz Untar it - tar -xvf cloudctl-linux-amd64.tar.gz Rename it for ease of use - mv cloudctl-linux-amd64 cloudctl Include it to the PATH environment variable - export PATH=$PATH:$PWD Make sure your IBM Cloud Pak CLI is in the path- which cloudctl Make sure your IBM Cloud Pak CLI works - cloudctl help Note: If you are not using the IBM Cloud Shell to run the lab, be aware that the cloudctl CLI requires the kubectl CLI. To install the kubectl CLI on your personal environment, follow the instructions here Event Streams plugin for IBM Cloud Pak CLI \u00b6 This plugin will allow us to manage IBM Event Streams. In order to install it, execute the following commands in your IBM Cloud Shell: Download the Event Streams plugin for IBM Cloud Pak CLI - curl -L http://ibm.biz/es-cli-linux -o es-plugin Install it - cloudctl plugin install es-plugin Make sure it works - cloudctl es help Git \u00b6 IBM Cloud Shell comes with Git already installed out of the box. Vi \u00b6 IBM Cloud Shell comes with Vi already installed out of the box. Python 3 \u00b6 IBM Cloud Shell comes with Python 3 already installed out of the box. However, we need to install the following modules that will be used later on in this tutorial when we run a Python application to work with Avro, Schemas and messages. These modules are confluent_kafka and avro-python3 In order to install these modules, execute the following command in your IBM Cloud Shell: Install the modules - python3 -mpip install avro-python3 confluent_kafka --user Congrats! you have now your IBM Cloud Shell ready to start working. Schema Registry \u00b6 One of the most common technologies used in the industry these days to define, serialize and deserialize messages flowing through your Kafka topics is Apache Avro ( https://avro.apache.org/docs/current/ ). To learn more about Apache Avro, how to define Apache Avro data schemas and more, we strongly recommend to read through our documentation on Avro and data schemas here IBM Event Streams development team has developed a Schema Registry to work along your Kafka cluster to provide a place to store descriptions of the message formats used by your producers and consumers. The benefit of storing these descriptions is that you are able to validate that your producing and consuming applications will correctly inter-operate. The Schema Registry will also provide the ability for schemas to evolve in time. Accessing the Schema Registry \u00b6 UI \u00b6 To access the schema registry, we first need to log into IBM Event Streams. Point your browser to your IBM Event Streams instace's user interface url and introduce your credentials Once you are logged into your IBM Event Streams instance, you simply need to click on the Schema Registry button on the main left hand vertical menu bar: CLI \u00b6 We can also interact with the Schema Registry through the IBM Event Streams CLI. In order to do so, we first need to log in with the IBM Cloud Pak CLI: Log into your cluster with the IBM CloudPak CLI Make sure to use the appropriate credentials and select the namespace where your IBM Event Streams instance is installed cloudctl login -a https://cp-console.apps.eda-solutions.gse-ocp.net --skip-ssl-validation Username> admin Password> Authenticating... OK Targeted account mycluster Account Enter a namespace > integration Targeted namespace integration Configuring kubectl ... Property \"clusters.mycluster\" unset. Property \"users.mycluster-user\" unset. Property \"contexts.mycluster-context\" unset. Cluster \"mycluster\" set. User \"mycluster-user\" set. Context \"mycluster-context\" created. Switched to context \"mycluster-context\" . OK Configuring helm: /Users/user/.helm OK Initialize the Event Streams CLI plugin cloudctl es init IBM Cloud Platform Common Services endpoint: https://cp-console.apps.eda-solutions.gse-ocp.net Namespace: integration Name: es-1 IBM Cloud Pak for Integration UI address: No instance of Cloud Pak for Integration has been found. Please check that you have access to it. Event Streams API endpoint: https://es-1-ibm-es-admapi-external-integration.apps.eda-solutions.gse-ocp.net Event Streams API status: OK Event Streams UI address: https://es-1-ibm-es-ui-integration.apps.eda-solutions.gse-ocp.net Event Streams Schema Registry endpoint: https://es-1-ibm-es-schema-external-integration.apps.eda-solutions.gse-ocp.net Event Streams bootstrap address: es-1-kafka-bootstrap-integration.apps.eda-solutions.gse-ocp.net:443 OK (*)The above information will later be used in the IBM Event Streams Credentials section as these are neeeded by the Python application we will work with. Make sure you can access the IBM Event Streams Schema Registry: cloudctl es schemas No schemas were found. OK Schemas \u00b6 In this section we will finally get our hands dirty with the IBM Event Steams Schema Registry capability by working with Apache Avro schemas and the Schema Registry. We recommend to complete most of the UI steps from your local workstation since these will require you to upload the files your create/modify to IBM Event Streams and that requires having your files available locally on your workstation rather than on the IBM Cloud Shell Create a schema \u00b6 Let's see how can we create a schema to start playing with. UI \u00b6 The IBM EVent Streams user interface allow us to create schemas only from json or Avro schema avsc files. Create an Avro schema file avsc with your schema: Change USER1 echo '{ \"type\":\"record\", \"name\":\"demoSchema_UI_USER1\", \"namespace\": \"schemas.demo.ui\", \"fields\":[ {\"name\": \"eventKey\",\"type\":\"string\"}, {\"name\": \"message\",\"type\":\"string\"}] }' > demoshema-ui.avsc On the IBM Event Streams Schema Registry User Interface, Click on Add schema button on the top right corner. Click on Upload definition button on the left hand side and select the demoschema-ui.avsc file we just created. You should now see you Avro schema loaded in the UI with two tabs, definition and preview to make sure your schema looks as desired: Click on Add schema button at the top right corner and you should now see that schema listed among your other schemas. CLI \u00b6 Create another Avro schema avsc file with a different schema: Change USER1 echo '{ \"type\":\"record\", \"name\":\"demoSchema_CLI_USER1\", \"namespace\": \"schemas.demo.cli\", \"fields\":[ {\"name\": \"eventKey\",\"type\":\"string\"}, {\"name\": \"message\",\"type\":\"string\"}] }' > demoshema-cli.avsc Create a schema by executing the following command: cloudctl es schema-add --file demoshema-cli.avsc Schema demoSchema_CLI_USER1 is active. Version Version ID Schema State Updated Comment 1 .0.0 1 demoSchema_CLI_USER1 active Thu, 25 Jun 2020 11 :30:42 UTC Added version 1 .0.0 of schema demoSchema_CLI_USER1 to the registry. OK List schemas \u00b6 UI \u00b6 In order to list the schemas in the UI you just simply need to open up the Schema Registry User Interface and schemas will get listed in there automatically. You also have a search tool bar at the top. You can also see more details about your schema by clicking the drop down arrow on its left: CLI \u00b6 Execute the following command to list the schemas in your Schema Registry: cloudctl es schemas Schema State Latest version Latest version ID Updated demoSchema_CLI_USER1 active 1 .0.0 1 Fri, 24 Jul 2020 13 :55:49 UTC demoSchema_UI_USER1 active 1 .0.0 1 Fri, 24 Jul 2020 13 :55:51 UTC OK Delete schemas \u00b6 UI \u00b6 Click on the schema you want to delete. Click on the Manage schema tab at the top. Click on Remove schema CLI \u00b6 To remove a schema using the CLI, simply execute the following command and confirm: Change USER1 cloudctl es schema-remove demoSchema_CLI_USER1 Remove schema demoSchema_CLI_USER1 and all versions? [ y/n ] > y Schema demoSchema_CLI_USER1 and all versions removed. OK Create new schema version \u00b6 To create a new version of a schema, Let's first create again the previous two schemas: Change USER1 cloudctl es schema-add --file demoshema-ui.avsc Schema demoSchema_UI_USER1 is active. Version Version ID Schema State Updated Comment 1 .0.0 1 demoSchema_UI_USER1 active Fri, 24 Jul 2020 13 :59:55 UTC Added version 1 .0.0 of schema demoSchema_UI_USER1 to the registry. OK cloudctl es schema-add --file demoshema-cli.avsc Schema demoSchema_CLI_USER1 is active. Version Version ID Schema State Updated Comment 1 .0.0 1 demoSchema_CLI_USER1 active Fri, 24 Jul 2020 14 :00:45 UTC Added version 1 .0.0 of schema demoSchema_CLI_USER1 to the registry. OK Add a new attribute to the schemas by editing their Avro schema avsc files: Change USER1 cat demoshema-ui.avsc { \"type\" : \"record\" , \"name\" : \"demoSchema_UI_USER1\" , \"namespace\" : \"schemas.demo.ui\" , \"fields\" : [ { \"name\" : \"eventKey\" , \"type\" : \"string\" } , { \"name\" : \"message\" , \"type\" : \"string\" } , { \"name\" : \"attribute1\" , \"type\" : \"string\" }] } UI \u00b6 Click on the schema you want to create a new version for. Click on the Add new version button on the left hand side. Click on Upload definition button on the left hand side. Select the Avro schema avsc file and click ok. ERROR: The error we are seeing on the screen is because the IBM Event Streams Schema Registtry enforces full compatibility: https://ibm.github.io/event-streams/schemas/creating/#adding-new-schema-versions Full compatibility for data schemas means that messages that have been serialized with an earlier version of a schema can be deserialized with a later version. To be compatible, fields in later versions of a schema cannot be removed, and any new schema field must have a default value. More on data schema compatibility on the section Data Evolution towards the end of this lab. As explained in the error notification above, we need to add a default value for our new attribute in our data schema so that messages serialized with an older version of the data schema which won't contain this new attribute can later be deserialized with the newer version of the data schema that expects such attribute. By providing a default value, we allow deserializers to consume messages that do not contain newer attributes. Add a default value for the new attribute: Change USER1 cat demoshema-ui.avsc { \"type\" : \"record\" , \"name\" : \"demoSchema_UI_USER1\" , \"namespace\" : \"schemas.demo.ui\" , \"fields\" : [ { \"name\" : \"eventKey\" , \"type\" : \"string\" } , { \"name\" : \"message\" , \"type\" : \"string\" } , { \"name\" : \"attribute1\" , \"type\" : \"string\" , \"default\" : \"whatever\" }] } Repeat the steps for adding a new version of a schema above. This time you should see that the schema is valid: However, it still does not let us add this new version to the data schema until we actually provide a version for it. Click on the Add + link on the right of the version attribute of the schema and give it 2.0.0 for example (hit enter for the version to take the value you type in). Click on Add schema . You should now see the two versions for your data schema on the left hand side. If you go back to the Schema Registry page where all your schemas are listed, you should now see that the latest version for your data schema is 2.0.0 now. CLI \u00b6 If we try to add the new version of the schema from its demoschema-cli.avsc Avro schema file, we will get the same error as in the previous UI example: cloudctl es schema-add --file demoshema-cli.avsc FAILED Event Streams API request failed: Error response from server. Status code: 400 . Avro schema is not compatible with latest schema version: Compatibility type 'MUTUAL_READ' does not hold between 1 schema ( s ) in the chronology because: Schema [ 0 ] has incompatibilities: [ 'READER_FIELD_MISSING_DEFAULT_VALUE: attribute1' at '/fields/2' ] . Unable to add version 1 .0.0 of schema demoSchema_CLI_USER1 to the registry. Add the default value for the new attribute in your Avro schema avsc file and try to add that new version of the schema: cloudctl es schema-add --file demoshema-cli.avsc FAILED Event Streams API request failed: Error response from server. Status code: 409 . Schema version name already exists Unable to add version 1 .0.0 of schema demoSchema_CLI_USER1 to the registry. We see that we still have an error because we have not specified a new version value. Specify a new version value when adding this new version of the schema: cloudctl es schema-add --file demoshema-cli.avsc --version 2 .0.0 Schema demoSchema_CLI_USER1 is active. Version Version ID Schema State Updated Comment 1 .0.0 1 demoSchema_CLI_USER1 active Fri, 24 Jul 2020 14 :00:45 UTC 2 .0.0 2 demoSchema_CLI_USER1 active Fri, 24 Jul 2020 14 :09:37 UTC Added version 2 .0.0 of schema demoSchema_CLI_USER1 to the registry. OK Get latest version of a schema \u00b6 UI \u00b6 In order to see the latest version of a data schema using the UI, we just need to go to the Schema Registry web user interface and click on the expand arrow buttton that is on the left: CLI \u00b6 In order to see the latest version of a data schema using the CLI, we simply need to run the following command: cloudctl es schema demoSchema_CLI_USER1 --version 2 { \"type\" : \"record\" , \"name\" : \"demoSchema_CLI_USER1\" , \"namespace\" : \"schemas.demo.cli\" , \"fields\" : [ { \"name\" : \"eventKey\" , \"type\" : \"string\" } , { \"name\" : \"message\" , \"type\" : \"string\" } , { \"name\" : \"attribute1\" , \"type\" : \"string\" , \"default\" : \"whatever\" } ] } (*) The version you specify is actually the version ID (2) rather than the version name we gave to the newer schema version (2.0.0): cloudctl es schema demoSchema_CLI_USER1 Schema demoSchema_CLI_USER1 is active. Version Version ID Schema State Updated Comment 1 .0.0 1 demoSchema_CLI_USER1 active Fri, 24 Jul 2020 14 :00:45 UTC 2 .0.0 2 demoSchema_CLI_USER1 active Fri, 24 Jul 2020 14 :09:37 UTC OK Get specific version of a schema \u00b6 UI \u00b6 To see a specific version of a schema, go to the Schema Registry web user interface and click on the schema you want to see the version for. You will now see how many version of the schema you have and you can click on any of these in order to see more details about it. CLI \u00b6 To see a specific version of a schema using the CLI, simply run the following command with the version ID you would like to get retrieved: cloudctl es schema demoSchema_CLI_USER1 --version 1 { \"type\" : \"record\" , \"name\" : \"demoSchema_CLI_USER1\" , \"namespace\" : \"schemas.demo.cli\" , \"fields\" : [ { \"name\" : \"eventKey\" , \"type\" : \"string\" } , { \"name\" : \"message\" , \"type\" : \"string\" } ] } Listing all versions of a schema \u00b6 UI \u00b6 To list all versions of schema in the Schema Registry user interface, you simply need to click on the data schema you want and a new page will display these: CLI \u00b6 In order to display all versions of a schema, run the following command: cloudctl es schema demoSchema_CLI_USER1 Schema demoSchema_CLI_USER1 is active. Version Version ID Schema State Updated Comment 1 .0.0 1 demoSchema_CLI_USER1 active Fri, 24 Jul 2020 14 :00:45 UTC 2 .0.0 2 demoSchema_CLI_USER1 active Fri, 24 Jul 2020 14 :09:37 UTC OK Deleting a version of a schema \u00b6 UI \u00b6 In order to delete a version of a schema using the Schema Registry user interface, Click on the data schema you want a version of it deleted for. Select the version you want to delete on the left hand side. Click on Manage version button that is on the top right corner within the main box in the center of the page. Click on Remove version . CLI \u00b6 In order to delete a version of a schema through the CLI, execute the following command: cloudctl es schema-remove demoSchema_CLI_USER1 --version 1 Remove version with ID 1 of schema demoSchema_CLI_USER1? [ y/n ] > y Version with ID 1 of schema demoSchema_CLI_USER1 removed. OK We can see only version 2 now: cloudctl es schema demoSchema_CLI_USER1 Schema demoSchema_CLI_USER1 is active. Version Version ID Schema State Updated Comment 2 .0.0 2 demoSchema_CLI_USER1 active Fri, 24 Jul 2020 14 :09:37 UTC OK IBM Event Streams Credentials \u00b6 We have seen how to interact with the IBM Event Streams Schema Registry in order to create, delete, update, etc schemas that our applications will theoretically used for data correctness and application robusteness. However, the first thing that we need to set up in our IBM Event Streams instance are these applications' service credentials to be able to interact with IBM Event Streams and its Schema Registry. For doing so, we can either use either the GUI or the CLI. GUI \u00b6 Go to you IBM Event Streams instance console Click on Connect to this cluster In this panel, you will find The Botstrap server to connect your applications to in order to send and receive messages from your IBM Event Streams instance. We can see we have one external listener (whith SCRAM-SHA authentication) and one internal listener (Depending your IBM Event Streams installation you might have different listeners and authentications for these). The Schema Registry url your applications will need to work with Apache Avro data schemas. A Generate SCRAM credentials button to generate the SCRAM credentials your applications will need to authenticate with. A Certificates section to download either the Java PKCS12 or the PEM certificates (or both) that your applications will need in order to be able to establish the communitaction with your IBM Event Streams instance. To generate the SCRAM credentials needed by your application to get authenticated against IBM Event Streams to be able to produce and consume messages as well as to create, delete, etc topics and schemas, we need to create a KafkaUser (this happens behind the scenes) which we will set some permissions and get the corresponding SCRAM usernname and password for to be used in our applications kafka clients configuration: Click on Generate SCRAM credentials Enter a user name for your credentials and click next (leave the last option selected: Produce messages, consume messages and create topics and schemas so that we give full access to our user for simplicity) Select all topics and click next Select all consumer groups and click next Select all transactional IDs and click next Once you have created your new KafkaUser, you get the SCRAM credentials displayed on the screen: You can download the PEM certificate from the UI and then use the IBM Cloud Shell upload file option on the top bar or you can download it from within your IBM Cloud Shell by using the CLI (see below) CLI \u00b6 The following two steps should have been completed already in the previous Accessing the Schema Registry section Log into your cluster with the IBM CloudPak CLI cloudctl login -a https://cp-console.apps.eda-solutions.gse-ocp.net --skip-ssl-validation Username> admin Password> Authenticating... OK Targeted account mycluster Account Enter a namespace > integration Targeted namespace integration Configuring kubectl ... Property \"clusters.mycluster\" unset. Property \"users.mycluster-user\" unset. Property \"contexts.mycluster-context\" unset. Cluster \"mycluster\" set. User \"mycluster-user\" set. Context \"mycluster-context\" created. Switched to context \"mycluster-context\" . OK Configuring helm: /Users/jesusalmaraz/.helm OK Initialize the Event Streams CLI plugin cloudctl es init IBM Cloud Platform Common Services endpoint: https://cp-console.apps.eda-solutions.gse-ocp.net Namespace: integration Name: es-1 IBM Cloud Pak for Integration UI address: No instance of Cloud Pak for Integration has been found. Please check that you have access to it. Event Streams API endpoint: https://es-1-ibm-es-admapi-external-integration.apps.eda-solutions.gse-ocp.net Event Streams API status: OK Event Streams UI address: https://es-1-ibm-es-ui-integration.apps.eda-solutions.gse-ocp.net Event Streams Schema Registry endpoint: https://es-1-ibm-es-schema-external-integration.apps.eda-solutions.gse-ocp.net Event Streams bootstrap address: es-1-kafka-bootstrap-integration.apps.eda-solutions.gse-ocp.net:443 OK We can see above the Event Streams bootstrap address and Schema Registry url that our applications will need in order to connect to this Event Streams instance To be able to establish communication and authenticate against your IBM Event Streams instance, you will need the PEM certificate and an the SCRAM credentials: To download your PEM certificate , you can use the following command: cloudctl es certificates --format pem Certificate successfully written to /home/ALMARAZJ/es-cert.pem. OK To generate your SCRAM credentials, we first need to create a KafkaUser , you can use the following command: cloudctl es kafka-user-create --name my-user1 --consumer --producer --schema-topic-create --all-topics --all-groups --all-txnids --auth-type scram-sha-512 KafkaUser name Authentication Authorization Username Secret my-user1 scram-sha-512 simple EntityOperator has not created corresponding username EntityOperator has not created corresponding secret Resource type Name Pattern type Host Operation topic * literal * Read topic __schema_ prefix * Read topic * literal * Write topic * literal * Create topic __schema_ prefix * Alter group * literal * Read transactionalId * literal * Write Created KafkaUser my-user1. OK We recommend to carefully set appropriate roles as well as access to topics, groups, transaction IDs and schemas for the API keys that you generate. When a KafkaUser custom resource is created, the Entity Operator within Event Streams will create the principal in ZooKeeper with appropriate ACL entries. It will also create a Kubernetes Secret that contains the Base64-encoded SCRAM password for the scram-sha-512 authentication type, or the Base64-encoded certificates and keys for the tls authentication type. Retrieve the username and the secret name containing the password of your SCRAM credentials for your KafkaUser: oc get kafkauser my-user1 --namespace integration -o jsonpath = '{\"username: \"}{.status.username}{\"\\nsecret-name: \"}{.status.secret}{\"\\n\"}' username: my-user1 secret-name: my-user1 1. Retrieve the password of your SCRAM credentials from the secret above: oc get secret my-user1 --namespace integration -o jsonpath = '{.data.password}' | base64 --decode ***** Environment variables \u00b6 Now that we have generated the appropriate IBM Event Streams credentials for applications to be able to establish communication and authenticate against our IBM Event Streams instance, we are going to set some environment variables that will be used by our Python application: KAFKA_BROKERS which should take the value of bootstrap server : export KAFKA_BROKERS = es-1-kafka-bootstrap-integration.apps.eda-solutions.gse-ocp.net:443 SCRAM_USERNAME which should take the value of the SCRAM username you have generated: export SCRAM_USERNAME = my-user1 SCRAM_PASSWORD which should take the value of the SCRAM password you have generated: export SCRAM_PASSWORD = ***** PEM_CERT which should take the value of the location where the PEM certificate is within your IBM Cloud Shell: Set the path appropriately to your IBM Cloud Shell export PEM_CERT = ~/es-cert.pem (*) Don't forget to download both the PEM certificate to your IBM Cloud Shell through the CLI or upload it to your IBM Cloud Shell from your laptop if you used the UI to get the certificate. Review previous section if needed. SCHEMA_REGISTRY_URL which should be a combination of the SCRAM username , the SCRAM password and the Schema Registry url in the form of: https://<SCRAM_username>:<SCRAM_password>@<Schema_Registry_url> export SCHEMA_REGISTRY_URL = https:// ${ SCRAM_USERNAME } : ${ SCRAM_PASSWORD } @es-1-ibm-es-schema-external-integration.apps.eda-solutions.gse-ocp.net Python Application \u00b6 The Python application we have built to see how to produce and consume messages (either plain messages or Avro encoded messages based on Avro Data Schemas) to and from an IBM Event Streams instance installed through the IBM Cloud Pak for Integration is public at the following GitHub repo: https://github.com/ibm-cloud-architecture/refarch-eda-tools/tree/master/labs/es-cp4i-schema-lab-v10 Clone \u00b6 In order to use and work with this Python application, the first thing we need to do is to clone the GitHub repository where it is published. Clone the github repository on your workstation on the location of your choice: git clone https://github.com/ibm-cloud-architecture/refarch-eda-tools.git Cloning into 'refarch-eda-tools' ... remote: Enumerating objects: 185 , done . remote: Counting objects: 100 % ( 185 /185 ) , done . remote: Compressing objects: 100 % ( 148 /148 ) , done . remote: Total 185 ( delta 23 ) , reused 176 ( delta 16 ) , pack-reused 0 Receiving objects: 100 % ( 185 /185 ) , 6 .17 MiB | 4 .61 MiB/s, done . Resolving deltas: 100 % ( 23 /23 ) , done . Change directory into refarch-eda-tools/labs/es-cp4i-schema-lab-v10 to find the assets we will we working from now on for the python demo environment and few other scripts/applications: cd refarch-eda-tools/labs/es-cp4i-schema-lab-v10 $ ls -all total 240 drwxr-xr-x 9 user staff 288 20 May 19 :33 . drwxr-xr-x 3 user staff 96 20 May 19 :33 .. -rw-r--r-- 1 user staff 112578 20 May 19 :33 README.md drwxr-xr-x 5 user staff 160 20 May 19 :33 avro_files drwxr-xr-x 6 user staff 192 20 May 19 :33 kafka drwxr-xr-x 6 user staff 192 20 May 19 :33 src In the next sections, we are going to briefly explain the implementation of this Python application so that you understand what is being done behind the scenes and more importantly, if you are a developer, how to do so. Python Avro Producer \u00b6 In this section we describe the Python scripts we will be using in order to be able to produce avro messages to a Kafka topic. Produce Message \u00b6 The python script that we will use to send an avro message to a Kafka topic is ProduceAvroMessage.py where we have the following: A function to parse the arguments: def parseArguments (): global TOPIC_NAME print ( \"The arguments for this script are: \" , str ( sys . argv )) if len ( sys . argv ) == 2 : TOPIC_NAME = sys . argv [ 1 ] else : print ( \"[ERROR] - The produceAvroMessage.py script expects one argument: The Kafka topic to publish the message to\" ) exit ( 1 ) A function to create the event to be sent: def createEvent (): print ( 'Creating event...' ) key = { \"key\" : 1 } value = { \"message\" : \"This is a test message\" } print ( \"DONE\" ) return json . dumps ( value ), json . dumps ( key ) The main where we will: Parse the arguments Get the Avro schemas for the key and value of the event Create the Event to be sent Print it out for reference Create the Kafka Avro Producer and configure it Send the event if __name__ == '__main__' : # Get the Kafka topic name parseArguments () # Get the avro schemas for the message's key and value event_value_schema = getDefaultEventValueSchema ( DATA_SCHEMAS ) event_key_schema = getDefaultEventKeySchema ( DATA_SCHEMAS ) # Create the event event_value , event_key = createEvent () # Print out the event to be sent print ( \"--- Event to be published: ---\" ) print ( event_key ) print ( event_value ) print ( \"----------------------------------------\" ) # Create the Kafka Avro Producer kafka_producer = KafkaProducer ( KAFKA_BROKERS , SCRAM_USERNAME , SCRAM_PASSWORD , SCHEMA_REGISTRY_URL ) # Prepare the Kafka Avro Producer kafka_producer . prepareProducer ( \"ProduceAvroMessagePython\" , event_key_schema , event_value_schema ) # Publish the event kafka_producer . publishEvent ( TOPIC_NAME , event_value , event_key ) As you can see, this python code depends on an Avro Utils for loading the Avro schemas and a Kafka Avro Producer to send the messages. These are explained next. Avro Utils \u00b6 This script, called avroEDAUtils.py , contains some very simple utility functions to be able to load Avro schemas from their avsc files in order to be used by the Kafka Avro Producer. A function to get the key and value Avro schemas for the messages to be sent: def getDefaultEventValueSchema ( schema_files_location ): # Get the default event value data schema known_schemas = avro . schema . Names () default_event_value_schema = LoadAvsc ( schema_files_location + \"/default_value.avsc\" , known_schemas ) return default_event_value_schema def getDefaultEventKeySchema ( schema_files_location ): # Get the default event key data schema known_schemas = avro . schema . Names () default_event_key_schema = LoadAvsc ( schema_files_location + \"/default_key.avsc\" , known_schemas ) return default_event_key_schema (*) Where known_schemas is an Avro schema dictionary where all Avro schemas read get stored in order to be able to read nested Avro schemas afterwards. See the python script in detail for examples of this. A function to open a file, read its content as an Avro schema and store it in the Avro schema dictionary: def LoadAvsc ( file_path , names = None ): # Load avsc file # file_path: path to schema file # names(optional): avro.schema.Names object file_text = open ( file_path ) . read () json_data = json . loads ( file_text ) schema = avro . schema . SchemaFromJSONData ( json_data , names ) return schema Kafka Avro Producer \u00b6 This script, called KcAvroProducer.py , will actually be the responsible for creating the Kafka Avro Producer, initialize and configure it and provide the publish method: Initialize and prepare the Kafka Producer class KafkaProducer : def __init__ ( self , kafka_brokers = \"\" , scram_username = \"\" , scram_password = \"\" , schema_registry_url = \"\" ): self . kafka_brokers = kafka_brokers self . scram_username = scram_username self . scram_password = scram_password self . schema_registry_url = schema_registry_url def prepareProducer ( self , groupID = \"pythonproducers\" , key_schema = \"\" , value_schema = \"\" ): options = { 'bootstrap.servers' : self . kafka_brokers , 'schema.registry.url' : self . schema_registry_url , 'group.id' : groupID , 'security.protocol' : 'SASL_SSL' , 'sasl.mechanisms' : 'SCRAM-SHA-512' , 'sasl.username' : self . scram_username , 'sasl.password' : self . scram_password , 'ssl.ca.location' : os . environ [ 'PEM_CERT' ], 'schema.registry.ssl.ca.location' : os . environ [ 'PEM_CERT' ] } # Print out the configuration print ( \"--- This is the configuration for the avro producer: ---\" ) print ( options ) print ( \"---------------------------------------------------\" ) # Create the Avro Producer self . producer = AvroProducer ( options , default_key_schema = key_schema , default_value_schema = value_schema ) Publish method def publishEvent ( self , topicName , value , key ): # Produce the Avro message # Important: value DOES NOT come in JSON format from ContainerAvroProducer.py. Therefore, we must convert it to JSON format first self . producer . produce ( topic = topicName , value = json . loads ( value ), key = json . loads ( value )[ key ], callback = self . delivery_report ) # Flush self . producer . flush () Python Avro Consumer \u00b6 In this section we describe the python scripts we will be using in order to be able to consume Avro messages from a Kafka topic. Consume Message \u00b6 The python script that we will use to consume an Avro message from a Kafka topic is ConsumeAvroMessage.py where we have the following: A function to parse arguments: # Parse arguments to get the container ID to poll for def parseArguments (): global TOPIC_NAME print ( \"The arguments for the script are: \" , str ( sys . argv )) if len ( sys . argv ) != 2 : print ( \"[ERROR] - The ConsumeAvroMessage.py script expects one arguments: The Kafka topic to events from.\" ) exit ( 1 ) TOPIC_NAME = sys . argv [ 1 ] The main where we will: Parse the arguments to get the topic to read from Create the Kafka Consumer and configure it Poll for next avro message Close the Kafka consumer if __name__ == '__main__' : # Parse arguments parseArguments () # Create the Kafka Avro consumer kafka_consumer = KafkaConsumer ( KAFKA_BROKERS , SCRAM_USERNAME , SCRAM_PASSWORD , TOPIC_NAME , SCHEMA_REGISTRY_URL ) # Prepare the consumer kafka_consumer . prepareConsumer () # Consume next Avro event kafka_consumer . pollNextEvent () # Close the Avro consumer kafka_consumer . close () As you can see, this python code depends on a Kafka Avro Consumer to consume messages. This is explained next. Kafka Avro Consumer \u00b6 This script, called KcAvroConsumer.py , will actually be the responsible for creating the Kafka Avro Consumer, initialize and configure it and provide the poll next event method: Initialize and prepare the new Kafka consumer: class KafkaConsumer : def __init__ ( self , kafka_brokers = \"\" , scram_username = \"\" , scram_password = \"\" , topic_name = \"\" , schema_registry_url = \"\" , autocommit = True ): self . kafka_brokers = kafka_brokers self . scram_username = scram_username self . scram_password = scram_password self . topic_name = topic_name self . schema_registry_url = schema_registry_url self . kafka_auto_commit = autocommit # See https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md def prepareConsumer ( self , groupID = \"pythonconsumers\" ): options = { 'bootstrap.servers' : self . kafka_brokers , 'group.id' : groupID , 'auto.offset.reset' : 'earliest' , 'schema.registry.url' : self . schema_registry_url , 'enable.auto.commit' : self . kafka_auto_commit , 'security.protocol' : 'SASL_SSL' , 'sasl.mechanisms' : 'SCRAM-SHA-512' , 'sasl.username' : self . scram_username , 'sasl.password' : self . scram_password , 'ssl.ca.location' : os . environ [ 'PEM_CERT' ], 'schema.registry.ssl.ca.location' : os . environ [ 'PEM_CERT' ] } # Print the configuration print ( \"--- This is the configuration for the Avro consumer: ---\" ) print ( options ) print ( \"---------------------------------------------------\" ) # Create the Avro consumer self . consumer = AvroConsumer ( options ) # Subscribe to the topic self . consumer . subscribe ([ self . topic_name ]) Poll next event method: # Prints out the message def traceResponse ( self , msg ): print ( '[Message] - Next message consumed from {} partition: [ {} ] at offset {} with key {} and value {} ' . format ( msg . topic (), msg . partition (), msg . offset (), msg . key (), msg . value () )) # Polls for next event def pollNextEvent ( self ): # Poll for messages msg = self . consumer . poll ( timeout = 10.0 ) # Validate the returned message if msg is None : print ( \"[INFO] - No new messages on the topic\" ) elif msg . error (): if ( \"PARTITION_EOF\" in msg . error ()): print ( \"[INFO] - End of partition\" ) else : print ( \"[ERROR] - Consumer error: {} \" . format ( msg . error ())) else : # Print the message msgStr = self . traceResponse ( msg ) Schemas and Messages \u00b6 In this section we are going to see how Schema Registry works when you have an application that produces and consumes messages based on Avro data schemas. The application we are going to use for this is the python application presented above in the Python Avro Producer and Python Avro Consumer sections. IMPORTANT: Before start using our Python application we must set the PYTHONPATH environment variable to point to where we have all the Python scripts that make up our application in order for Python to find these at execution time. Set the PYTHONPATH variable to the location where you cloned the GitHub repository containing the Python application we are going to be working with export PYTHONPATH = ~/refarch-eda-tools/labs/es-cp4i-schema-lab-v10 Produce a message \u00b6 In order to produce a message, we execute the ProduceAvroMessage.py . This script, as you could see in the Python Avro Producer section, is sending the event with key {'key': '1'} and value {'message': 'This is a test message'} according to the schemas defined in default_key.avsc and default_value.avsc for the key and value of the event respectively. Make sure you are on the right path where the python scripts live: ~/refarch-eda-tools/labs/es-cp4i-schema-lab-v10/src Change user1 python3 ProduceAvroMessage.py test-schema-user1 @@@ Executing script: ProduceAvroMessage.py The arguments for the script are: [ 'ProduceAvroMessage.py' , 'test-schema-user1' ] Creating event... DONE --- Event to be published: --- { \"key\" : 1 } { \"message\" : \"This is a test message\" } ---------------------------------------- --- This is the configuration for the avro producer: --- { 'bootstrap.servers' : 'es-1-kafka-bootstrap-integration.apps.eda-solutions.gse-ocp.net:443' , 'schema.registry.url' : 'https://my-user1:*****@es-1-ibm-es-schema-external-integration.apps.eda-solutions.gse-ocp.net' , 'group.id' : 'ProduceAvroMessagePython' , 'security.protocol' : 'SASL_SSL' , 'sasl.mechanisms' : 'SCRAM-SHA-512' , 'sasl.username' : 'my-user1' , 'sasl.password' : '*****' , 'ssl.ca.location' : '/home/ALMARAZJ/es-cert.pem' , 'schema.registry.ssl.ca.location' : '/home/ALMARAZJ/es-cert.pem' } --------------------------------------------------- Message delivered to test-schema-user1 [ 0 ] We can see our new message delivered in the test-schema-user1 topic by Go into the topics page in the IBM Event Streams UI Click on the topic and then on the Messages tab at the top. Finally, click on a message to see it displayed on the right hand side of the screen INFO: Mind the message now is not in JSON format as Avro does not repeat every field name with every single record which makes Avro more efficient than JSON for high-volume usage. This is thanks to having Avro schemas. IMPORTANT: As you can see, we got the test-schema-user1 topic auto-created when we produced the message. The reason for this is that Kafka is set out of the box to let applications to auto-create topics. We created the SCRAM credentials for our application to allow the application to create topics. On a production-like environment, you don't want developers creating applications that auto-create topics in your IBM Event Streams instance without any control. For that, we would configure Kafka to forbid topic auto-creation ( https://kafka.apache.org/documentation/#auto.create.topics.enable ) as well as thoroughly created the SCRAM credentials with the most strict but appropriate permissions that our application needs. IMPORTANT: Similar to the auto-creation of topics, we can see below that our application got the Avro data schemas for both the key and value of the message produced auto-registered. This is because many client libraries come with a SerDes property to allow them to auto-register the Avro data schemas ( https://docs.confluent.io/current/clients/confluent-kafka-python/#avroserializer ). However, on a production-like environment we don't want applications to auto-register schemas without any control but yet we can not leave it to the developers to set the auto-registration property off on their libraries. Instead, we would create the SCRAM credentials with the most strict but appropriate permissions that our application needs. If we look now at the schemas our schema registry has: cloudctl es schemas Schema State Latest version Latest version ID Updated demoSchema_CLI_USER1 active 2 .0.0 2 Fri, 24 Jul 2020 14 :09:37 UTC demoSchema_UI_USER1 active 2 .0.0 2 Fri, 24 Jul 2020 14 :06:27 UTC test-schema-user1-key-d89uk active 1 1 Fri, 24 Jul 2020 15 :41:46 UTC test-schema-user1-value-tv5efr active 1 1 Fri, 24 Jul 2020 15 :41:45 UTC OK we see two schemas, test-schema-user1-key-d89uk and test-schema-user1-value-tv5efr , which in fact correspond to the Avro data schema used for the key ( default_key.avsc ) and the value ( default_value.avsc ) of events sent to the test-schema-user1 topic in the ProduceAvroMessage.py as explained before sending the message. To make sure of what we are saying, we can inspect those schemas: cloudctl es schema test-schema-user1-key-d89uk --version 1 { \"type\" : \"record\" , \"name\" : \"defaultKey\" , \"namespace\" : \"ibm.eda.default\" , \"fields\" : [ { \"type\" : \"int\" , \"name\" : \"key\" , \"doc\" : \"We expect any int as the event key\" } ] , \"doc\" : \"Default Message's key Avro data schema\" } cloudctl es schema test-schema-user1-value-tv5efr --version 1 { \"type\" : \"record\" , \"name\" : \"defaultValue\" , \"namespace\" : \"ibm.eda.default\" , \"fields\" : [ { \"type\" : \"string\" , \"name\" : \"message\" , \"doc\" : \"Any string\" } ] , \"doc\" : \"Default Message's value Avro data schema\" } If I now decided that my events should contain another attribute, I would modify the event value schema ( default_value.avsc ) to reflect that as well as ProduceAvroMessage.py to send that new attribute in the event it sends: Change user1 python3 ProduceAvroMessage.py test-schema-user1 @@@ Executing script: ProduceAvroMessage.py The arguments for the script are: [ 'ProduceAvroMessage.py' , 'test-schema-user1' ] Creating event... DONE --- Event to be published: --- { \"key\" : 1 } { \"message\" : \"This is a test message\" , \"anotherAttribute\" : \"Just another test string\" } ---------------------------------------- --- This is the configuration for the avro producer: --- { 'bootstrap.servers' : 'es-1-kafka-bootstrap-integration.apps.eda-solutions.gse-ocp.net:443' , 'schema.registry.url' : 'https://my-user1:*****@es-1-ibm-es-schema-external-integration.apps.eda-solutions.gse-ocp.net' , 'group.id' : 'ProduceAvroMessagePython' , 'security.protocol' : 'SASL_SSL' , 'sasl.mechanisms' : 'SCRAM-SHA-512' , 'sasl.username' : 'my-user1' , 'sasl.password' : '*****' , 'ssl.ca.location' : '/home/ALMARAZJ/es-cert.pem' , 'schema.registry.ssl.ca.location' : '/home/ALMARAZJ/es-cert.pem' } --------------------------------------------------- Message delivered to test-schema-user1 [ 0 ] I can see that an event with a new attribute has been sent: And I can also see that the new shcema has got registered as well: cloudctl es schemas Schema State Latest version Latest version ID Updated demoSchema_CLI_USER1 active 2 .0.0 2 Fri, 24 Jul 2020 14 :09:37 UTC demoSchema_UI_USER1 active 2 .0.0 2 Fri, 24 Jul 2020 14 :06:27 UTC test-schema-user1-key-d89uk active 1 1 Fri, 24 Jul 2020 15 :41:46 UTC test-schema-user1-value-a5bbaa active 1 1 Fri, 24 Jul 2020 15 :54:37 UTC test-schema-user1-value-tv5efr active 1 1 Fri, 24 Jul 2020 15 :41:45 UTC OK If I inspect that new schema, I see my new attribute in it: cloudctl es schema test-schema-user1-value-a5bbaa --version 1 { \"type\" : \"record\" , \"name\" : \"defaultValue\" , \"namespace\" : \"ibm.eda.default\" , \"fields\" : [ { \"type\" : \"string\" , \"name\" : \"message\" , \"doc\" : \"Any string\" } , { \"type\" : \"string\" , \"name\" : \"anotherAttribute\" , \"doc\" : \"Any other string\" } ] , \"doc\" : \"Default Message's value Avro data schema\" } The schema evolution above (test-schema-user1-value-a5bbaa) should have got registered as a new version of the already existing schema (test-schema-user1-value-tv5efr). IBM Event Streams allows schemas to auto-register themselves when these are sent along with a message from a producer application. However, the Schema Registry does not pick \"new\" schemas up as a new version of a previous schema and simply creates a new schema. Anyway, when reading messages off the topic, Schema Registry handles well what schema to return back to the receiver application so messages can get properly deserialized. Will see that in the next section. SECURITY: As some of you may have already thought, having your clients (that is your applications), auto-register the Avro data schemas that are in the end kind of the contracts that your components of your overal solution agree on in order to understand each other and collaborate between them is NOT a good idea. Specially in microservices architectures where you might have hundreds of microservices talking and collaborating among themselsves. We will see in the Security section how we can control schema registration and evolution based on roles at the schema level also. Create a non-compliant message \u00b6 Let's see what happens if we send a message that does not comply with its Avro data schema. Let's say that I send the following message: key = { \"key\" : 1 } value = { \"message\" : 12345 } and this is the output of that attempt: Change user1 python3 ProduceAvroMessage.py test-schema-user1 @@@ Executing script: ProduceAvroMessage.py The arguments for the script are: [ 'ProduceAvroMessage.py' , 'test-schema-user1' ] Creating event... DONE --- Event to be published: --- { \"key\" : 1 } { \"message\" : 12345 } ---------------------------------------- --- This is the configuration for the avro producer: --- { 'bootstrap.servers' : 'es-1-kafka-bootstrap-integration.apps.eda-solutions.gse-ocp.net:443' , 'schema.registry.url' : 'https://my-user1:*****@es-1-ibm-es-schema-external-integration.apps.eda-solutions.gse-ocp.net' , 'group.id' : 'ProduceAvroMessagePython' , 'security.protocol' : 'SASL_SSL' , 'sasl.mechanisms' : 'SCRAM-SHA-512' , 'sasl.username' : 'my-user1' , 'sasl.password' : '*****' , 'ssl.ca.location' : '/home/ALMARAZJ/es-cert.pem' , 'schema.registry.ssl.ca.location' : '/home/ALMARAZJ/es-cert.pem' } --------------------------------------------------- Traceback ( most recent call last ) : File \"ProduceAvroMessage.py\" , line 81 , in <module> kafka_producer.publishEvent ( TOPIC_NAME,event_value,event_key ) File \"/tmp/lab/kafka/KcAvroProducer.py\" , line 43 , in publishEvent self.producer.produce ( topic = topicName,value = json.loads ( value ) ,key = json.loads ( key ) , callback = self.delivery_report ) File \"/root/.local/lib/python3.7/site-packages/confluent_kafka/avro/__init__.py\" , line 99 , in produce value = self._serializer.encode_record_with_schema ( topic, value_schema, value ) File \"/root/.local/lib/python3.7/site-packages/confluent_kafka/avro/serializer/message_serializer.py\" , line 118 , in encode_record_with_schema return self.encode_record_with_schema_id ( schema_id, record, is_key = is_key ) File \"/root/.local/lib/python3.7/site-packages/confluent_kafka/avro/serializer/message_serializer.py\" , line 152 , in encode_record_with_schema_id writer ( record, outf ) File \"/root/.local/lib/python3.7/site-packages/confluent_kafka/avro/serializer/message_serializer.py\" , line 86 , in <lambda> return lambda record, fp: writer.write ( record, avro.io.BinaryEncoder ( fp )) File \"/root/.local/lib/python3.7/site-packages/avro/io.py\" , line 771 , in write raise AvroTypeException ( self.writer_schema, datum ) avro.io.AvroTypeException: The datum { 'message' : 12345 } is not an example of the schema { \"type\" : \"record\" , \"name\" : \"defaultValue\" , \"namespace\" : \"ibm.eda.default\" , \"fields\" : [ { \"type\" : \"string\" , \"name\" : \"message\" , \"doc\" : \"Any string\" } , { \"type\" : \"string\" , \"name\" : \"anotherAttribute\" , \"doc\" : \"Any other string\" } ] , \"doc\" : \"Default Message's value Avro data schema\" } As we can see, the attempt failed as the Avro producer will check the message against the Avro data schema defined for the topic we want to send the message to and yield that this message does not comply (the message value attribute we are sending is an integer rather than a string and we are missing the second attribute). Therefore, using Avro schemas with IBM Event Streams give us the ability to build our system with robustness protecting downstream data consumers from malformed data, as only valid data will be permitted in the topic. Consume a message \u00b6 In order to consume a message, we execute the ConsumeAvroMessage.py within the /tmp/lab/src folder in our python demo environment: Change user1 python3 ConsumeAvroMessage.py test-schema-user1 @@@ Executing script: ConsumeAvroMessage.py The arguments for this script are: [ 'ConsumeAvroMessage.py' , 'test-schema-user1' ] --- This is the configuration for the Avro consumer: --- { 'bootstrap.servers' : 'es-1-kafka-bootstrap-integration.apps.eda-solutions.gse-ocp.net:443' , 'group.id' : 'pythonconsumers' , 'auto.offset.reset' : 'earliest' , 'schema.registry.url' : 'https://my-user1:*****@es-1-ibm-es-schema-external-integration.apps.eda-solutions.gse-ocp.net' , 'enable.auto.commit' : True, 'security.protocol' : 'SASL_SSL' , 'sasl.mechanisms' : 'SCRAM-SHA-512' , 'sasl.username' : 'my-user1' , 'sasl.password' : '*****' , 'ssl.ca.location' : '/home/ALMARAZJ/es-cert.pem' , 'schema.registry.ssl.ca.location' : '/home/ALMARAZJ/es-cert.pem' } --------------------------------------------------- [ Message ] - Next message consumed from test-schema-user1 partition: [ 0 ] at offset 0 with key { 'key' : 1 } and value { 'message' : 'This is a test message' } python3 ConsumeAvroMessage.py test-schema-user1 @@@ Executing script: ConsumeAvroMessage.py The arguments for this script are: [ 'ConsumeAvroMessage.py' , 'test-schema-user1' ] --- This is the configuration for the Avro consumer: --- { 'bootstrap.servers' : 'es-1-kafka-bootstrap-integration.apps.eda-solutions.gse-ocp.net:443' , 'group.id' : 'pythonconsumers' , 'auto.offset.reset' : 'earliest' , 'schema.registry.url' : 'https://my-user1:*****@es-1-ibm-es-schema-external-integration.apps.eda-solutions.gse-ocp.net' , 'enable.auto.commit' : True, 'security.protocol' : 'SASL_SSL' , 'sasl.mechanisms' : 'SCRAM-SHA-512' , 'sasl.username' : 'my-user1' , 'sasl.password' : '*****' , 'ssl.ca.location' : '/home/ALMARAZJ/es-cert.pem' , 'schema.registry.ssl.ca.location' : '/home/ALMARAZJ/es-cert.pem' } --------------------------------------------------- [ Message ] - Next message consumed from test-schema-user1 partition: [ 0 ] at offset 1 with key { 'key' : 1 } and value { 'message' : 'This is a test message' , 'anotherAttribute' : 'Just another test string' } As you can see, our script was able to read the Avro messages from the test-schema-user1 topic and map that back to their original structure thanks to the Avro schemas: [ Message ] - Next message consumed from test-schema partition: [ 0 ] at offset 0 with key { 'key' : 1 } and value { 'message' : 'This is a test message' } [ Message ] - Next message consumed from test-schema partition: [ 0 ] at offset 1 with key { 'key' : 1 } and value { 'message' : 'This is a test message' , 'anotherAttribute' : 'Just another test string' } Data Evolution \u00b6 So far we have more or less seen what Avro is, what an Avro data schema is, what a schema registry is and how this all works together. From creating an Avro data schema for your messages/events to comply with to how the schema registry and Avro data schemas work together. And we have also seen the code for doing all this, from the python code to send and receive Avro encoded messages based on their Avro data schemas to the rich CLI IBM Event Streams provides to interact with. However, we have said little about the need for data to evolve. When you design an Event Driven architecture for your application (by applying Event Storming or Domain Driven Design for example), it is very hard to come up with data structures/schemas that will not need to evolve/change in time. That is, your data, like your use or business cases, may need to evolve. As a result, Avro data schemas must be somehow flexible to allow your data to evolve along with your application and use cases. But it is not as easy as adding or removing data that travels in your events/messages or modifying the type of such data. And one of the reasons for this is that Kafka (or any other type of event backbone) is many times used as the source of truth. That is, a place that you can trust as to what has happened. Hence, Kafka will serve as the source of truth where all the events (that is, data) that happened (which could be bank transactions, communications, etc) will get stored (sometimes up to hundreds of years ) and will be able to be replayed if needed. As a result, there must be a data schema management and data schema evolution put in place that allow the compatibility of old and new data schemas and, in fact, old and new data at the end of the day. The IBM Event Streams Schema Registry enforces full compatibility when creating a new version of a schema . Full compatibility means that old data can be read with the new data schema, and new data can also be read with the last data schema . In data formats like Avro, you can define fields with default values. In that case adding or removing a field with a default value is a fully compatible change. Let's see then how this affects when you want your data to evolve in a way that it needs to add a new attribute or delete an existing attribute. But let's see what that means in terms of adding and removing attributes from your data schema. Adding a new attribute \u00b6 Although we have already seen this in the adding a new version of a schema section, let's try to add a new version of our test-schema-value schema where we have a new attribute. Remember, our default_schema.avsc already contains a new attribute than the original one but that it got registered as a new schema rather than as a new version of the original one. Let's reuse that Avro schema file to register it as a new version ( INFO: you might need to copy/download that file to your local workstation in order to be able to then upload it to the IBM Event Streams through its UI) When doing so from the UI, we see the following error: The reason, as alread explained in the add a new version of a schema section, is because full compatibility dictates that you can only add new attributes to a schema if these have a default value. Reason being that a receiver should be able to deserialize messages produced with an older schema using the newer schema. Because old messages were written with an older schema that did not contain our new attribute, those messages won't have that attribute so we need to provide a default value for it in our never version of the schema so that the receiver is able to deserialize those older messages with the newer schema. If we add the default value for the new attribute, we see that our newer version is now compatible: and that it gets registered fine: Removing an existing attribute \u00b6 What if we now wanted to remove the original message attribute from our schema. Let's remove it from the default_value.avsc file and try to register that new version: We, again, get the same error. And the reason is because receivers must be able to read and deserialize messages produced with the newer schema (that is, without the message attribute) but with the older schema (that is, with the schema version that enforces the existence of the message attribute). In order to work this around, what we need to do is to register first an intermediate schema that defines a default value for the message attribute: Once we have a default value for the message attribute, we can register a new version of the schema that finally removes that attribute: Security \u00b6 As we have already mentioned during the this tutorial, we need to pay attention to the permissions we give to users, groups, applications (and thefore the clients they used to interact with IBM Event Streams), etc since we don't want everyone and everything to be, for instance, creating or deleting topics, schemas, etc. You can secure your IBM Event Streams resources in a fine-grained manner by managing the access each user and application has to each resource. Within IBM Event Streams, you can secure access to the following resource types, where the names in parentheses are the resource type names used in Access Control List (ACL) rules: Topics (topic): you can control the ability of users and applications to create, delete, read, and write to a topic. Consumer groups (group): you can control an application\u2019s ability to join a consumer group. Transactional IDs (transactionalId): you can control the ability to use the transaction capability in Kafka. Note: Schemas in the Event Streams Schema Registry are a special case and are secured using the resource type of topic combined with a prefix of __schema_. You can control the ability of users and applications to create, delete, read, and update schemas. You can find more information about how to secure your IBM Event Streams resources in the official documentation at: https://ibm.github.io/event-streams/security/managing-access/","title":"Schema registry"},{"location":"use-cases/schema-registry-on-ocp/#requirements","text":"This lab requires the following components to work against: An IBM Event Streams V10 instance installed through the IBM CloudPak for Integration V2020.2.X or greater. An IBM Cloud Shell - https://www.ibm.com/cloud/cloud-shell","title":"Requirements"},{"location":"use-cases/schema-registry-on-ocp/#ibm-cloud-shell","text":"Here we are going to set up our IBM Cloud Shell with all the tools required to carry out this lab. Start your IBM Cloud Shell by pointing your browser to https://cloud.ibm.com/shell","title":"IBM Cloud Shell"},{"location":"use-cases/schema-registry-on-ocp/#ibm-cloud-pak-cli","text":"Cloudctl is a command line tool to manage Container Application Software for Enterprises (CASEs). This CLI will allow us to manage Cloud Pak related components as well as software, like IBM Event Streams, installed through any IBM Cloud Pak. In order to install it, execute the following commands in your IBM Cloud Shell: Download the IBM Cloud Pak CLI - curl -L https://github.com/IBM/cloud-pak-cli/releases/latest/download/cloudctl-linux-amd64.tar.gz -o cloudctl-linux-amd64.tar.gz Untar it - tar -xvf cloudctl-linux-amd64.tar.gz Rename it for ease of use - mv cloudctl-linux-amd64 cloudctl Include it to the PATH environment variable - export PATH=$PATH:$PWD Make sure your IBM Cloud Pak CLI is in the path- which cloudctl Make sure your IBM Cloud Pak CLI works - cloudctl help Note: If you are not using the IBM Cloud Shell to run the lab, be aware that the cloudctl CLI requires the kubectl CLI. To install the kubectl CLI on your personal environment, follow the instructions here","title":"IBM Cloud Pak CLI"},{"location":"use-cases/schema-registry-on-ocp/#event-streams-plugin-for-ibm-cloud-pak-cli","text":"This plugin will allow us to manage IBM Event Streams. In order to install it, execute the following commands in your IBM Cloud Shell: Download the Event Streams plugin for IBM Cloud Pak CLI - curl -L http://ibm.biz/es-cli-linux -o es-plugin Install it - cloudctl plugin install es-plugin Make sure it works - cloudctl es help","title":"Event Streams plugin for IBM Cloud Pak CLI"},{"location":"use-cases/schema-registry-on-ocp/#git","text":"IBM Cloud Shell comes with Git already installed out of the box.","title":"Git"},{"location":"use-cases/schema-registry-on-ocp/#vi","text":"IBM Cloud Shell comes with Vi already installed out of the box.","title":"Vi"},{"location":"use-cases/schema-registry-on-ocp/#python-3","text":"IBM Cloud Shell comes with Python 3 already installed out of the box. However, we need to install the following modules that will be used later on in this tutorial when we run a Python application to work with Avro, Schemas and messages. These modules are confluent_kafka and avro-python3 In order to install these modules, execute the following command in your IBM Cloud Shell: Install the modules - python3 -mpip install avro-python3 confluent_kafka --user Congrats! you have now your IBM Cloud Shell ready to start working.","title":"Python 3"},{"location":"use-cases/schema-registry-on-ocp/#schema-registry","text":"One of the most common technologies used in the industry these days to define, serialize and deserialize messages flowing through your Kafka topics is Apache Avro ( https://avro.apache.org/docs/current/ ). To learn more about Apache Avro, how to define Apache Avro data schemas and more, we strongly recommend to read through our documentation on Avro and data schemas here IBM Event Streams development team has developed a Schema Registry to work along your Kafka cluster to provide a place to store descriptions of the message formats used by your producers and consumers. The benefit of storing these descriptions is that you are able to validate that your producing and consuming applications will correctly inter-operate. The Schema Registry will also provide the ability for schemas to evolve in time.","title":"Schema Registry"},{"location":"use-cases/schema-registry-on-ocp/#accessing-the-schema-registry","text":"","title":"Accessing the Schema Registry"},{"location":"use-cases/schema-registry-on-ocp/#ui","text":"To access the schema registry, we first need to log into IBM Event Streams. Point your browser to your IBM Event Streams instace's user interface url and introduce your credentials Once you are logged into your IBM Event Streams instance, you simply need to click on the Schema Registry button on the main left hand vertical menu bar:","title":"UI"},{"location":"use-cases/schema-registry-on-ocp/#cli","text":"We can also interact with the Schema Registry through the IBM Event Streams CLI. In order to do so, we first need to log in with the IBM Cloud Pak CLI: Log into your cluster with the IBM CloudPak CLI Make sure to use the appropriate credentials and select the namespace where your IBM Event Streams instance is installed cloudctl login -a https://cp-console.apps.eda-solutions.gse-ocp.net --skip-ssl-validation Username> admin Password> Authenticating... OK Targeted account mycluster Account Enter a namespace > integration Targeted namespace integration Configuring kubectl ... Property \"clusters.mycluster\" unset. Property \"users.mycluster-user\" unset. Property \"contexts.mycluster-context\" unset. Cluster \"mycluster\" set. User \"mycluster-user\" set. Context \"mycluster-context\" created. Switched to context \"mycluster-context\" . OK Configuring helm: /Users/user/.helm OK Initialize the Event Streams CLI plugin cloudctl es init IBM Cloud Platform Common Services endpoint: https://cp-console.apps.eda-solutions.gse-ocp.net Namespace: integration Name: es-1 IBM Cloud Pak for Integration UI address: No instance of Cloud Pak for Integration has been found. Please check that you have access to it. Event Streams API endpoint: https://es-1-ibm-es-admapi-external-integration.apps.eda-solutions.gse-ocp.net Event Streams API status: OK Event Streams UI address: https://es-1-ibm-es-ui-integration.apps.eda-solutions.gse-ocp.net Event Streams Schema Registry endpoint: https://es-1-ibm-es-schema-external-integration.apps.eda-solutions.gse-ocp.net Event Streams bootstrap address: es-1-kafka-bootstrap-integration.apps.eda-solutions.gse-ocp.net:443 OK (*)The above information will later be used in the IBM Event Streams Credentials section as these are neeeded by the Python application we will work with. Make sure you can access the IBM Event Streams Schema Registry: cloudctl es schemas No schemas were found. OK","title":"CLI"},{"location":"use-cases/schema-registry-on-ocp/#schemas","text":"In this section we will finally get our hands dirty with the IBM Event Steams Schema Registry capability by working with Apache Avro schemas and the Schema Registry. We recommend to complete most of the UI steps from your local workstation since these will require you to upload the files your create/modify to IBM Event Streams and that requires having your files available locally on your workstation rather than on the IBM Cloud Shell","title":"Schemas"},{"location":"use-cases/schema-registry-on-ocp/#create-a-schema","text":"Let's see how can we create a schema to start playing with.","title":"Create a schema"},{"location":"use-cases/schema-registry-on-ocp/#ui_1","text":"The IBM EVent Streams user interface allow us to create schemas only from json or Avro schema avsc files. Create an Avro schema file avsc with your schema: Change USER1 echo '{ \"type\":\"record\", \"name\":\"demoSchema_UI_USER1\", \"namespace\": \"schemas.demo.ui\", \"fields\":[ {\"name\": \"eventKey\",\"type\":\"string\"}, {\"name\": \"message\",\"type\":\"string\"}] }' > demoshema-ui.avsc On the IBM Event Streams Schema Registry User Interface, Click on Add schema button on the top right corner. Click on Upload definition button on the left hand side and select the demoschema-ui.avsc file we just created. You should now see you Avro schema loaded in the UI with two tabs, definition and preview to make sure your schema looks as desired: Click on Add schema button at the top right corner and you should now see that schema listed among your other schemas.","title":"UI"},{"location":"use-cases/schema-registry-on-ocp/#cli_1","text":"Create another Avro schema avsc file with a different schema: Change USER1 echo '{ \"type\":\"record\", \"name\":\"demoSchema_CLI_USER1\", \"namespace\": \"schemas.demo.cli\", \"fields\":[ {\"name\": \"eventKey\",\"type\":\"string\"}, {\"name\": \"message\",\"type\":\"string\"}] }' > demoshema-cli.avsc Create a schema by executing the following command: cloudctl es schema-add --file demoshema-cli.avsc Schema demoSchema_CLI_USER1 is active. Version Version ID Schema State Updated Comment 1 .0.0 1 demoSchema_CLI_USER1 active Thu, 25 Jun 2020 11 :30:42 UTC Added version 1 .0.0 of schema demoSchema_CLI_USER1 to the registry. OK","title":"CLI"},{"location":"use-cases/schema-registry-on-ocp/#list-schemas","text":"","title":"List schemas"},{"location":"use-cases/schema-registry-on-ocp/#ui_2","text":"In order to list the schemas in the UI you just simply need to open up the Schema Registry User Interface and schemas will get listed in there automatically. You also have a search tool bar at the top. You can also see more details about your schema by clicking the drop down arrow on its left:","title":"UI"},{"location":"use-cases/schema-registry-on-ocp/#cli_2","text":"Execute the following command to list the schemas in your Schema Registry: cloudctl es schemas Schema State Latest version Latest version ID Updated demoSchema_CLI_USER1 active 1 .0.0 1 Fri, 24 Jul 2020 13 :55:49 UTC demoSchema_UI_USER1 active 1 .0.0 1 Fri, 24 Jul 2020 13 :55:51 UTC OK","title":"CLI"},{"location":"use-cases/schema-registry-on-ocp/#delete-schemas","text":"","title":"Delete schemas"},{"location":"use-cases/schema-registry-on-ocp/#ui_3","text":"Click on the schema you want to delete. Click on the Manage schema tab at the top. Click on Remove schema","title":"UI"},{"location":"use-cases/schema-registry-on-ocp/#cli_3","text":"To remove a schema using the CLI, simply execute the following command and confirm: Change USER1 cloudctl es schema-remove demoSchema_CLI_USER1 Remove schema demoSchema_CLI_USER1 and all versions? [ y/n ] > y Schema demoSchema_CLI_USER1 and all versions removed. OK","title":"CLI"},{"location":"use-cases/schema-registry-on-ocp/#create-new-schema-version","text":"To create a new version of a schema, Let's first create again the previous two schemas: Change USER1 cloudctl es schema-add --file demoshema-ui.avsc Schema demoSchema_UI_USER1 is active. Version Version ID Schema State Updated Comment 1 .0.0 1 demoSchema_UI_USER1 active Fri, 24 Jul 2020 13 :59:55 UTC Added version 1 .0.0 of schema demoSchema_UI_USER1 to the registry. OK cloudctl es schema-add --file demoshema-cli.avsc Schema demoSchema_CLI_USER1 is active. Version Version ID Schema State Updated Comment 1 .0.0 1 demoSchema_CLI_USER1 active Fri, 24 Jul 2020 14 :00:45 UTC Added version 1 .0.0 of schema demoSchema_CLI_USER1 to the registry. OK Add a new attribute to the schemas by editing their Avro schema avsc files: Change USER1 cat demoshema-ui.avsc { \"type\" : \"record\" , \"name\" : \"demoSchema_UI_USER1\" , \"namespace\" : \"schemas.demo.ui\" , \"fields\" : [ { \"name\" : \"eventKey\" , \"type\" : \"string\" } , { \"name\" : \"message\" , \"type\" : \"string\" } , { \"name\" : \"attribute1\" , \"type\" : \"string\" }] }","title":"Create new schema version"},{"location":"use-cases/schema-registry-on-ocp/#ui_4","text":"Click on the schema you want to create a new version for. Click on the Add new version button on the left hand side. Click on Upload definition button on the left hand side. Select the Avro schema avsc file and click ok. ERROR: The error we are seeing on the screen is because the IBM Event Streams Schema Registtry enforces full compatibility: https://ibm.github.io/event-streams/schemas/creating/#adding-new-schema-versions Full compatibility for data schemas means that messages that have been serialized with an earlier version of a schema can be deserialized with a later version. To be compatible, fields in later versions of a schema cannot be removed, and any new schema field must have a default value. More on data schema compatibility on the section Data Evolution towards the end of this lab. As explained in the error notification above, we need to add a default value for our new attribute in our data schema so that messages serialized with an older version of the data schema which won't contain this new attribute can later be deserialized with the newer version of the data schema that expects such attribute. By providing a default value, we allow deserializers to consume messages that do not contain newer attributes. Add a default value for the new attribute: Change USER1 cat demoshema-ui.avsc { \"type\" : \"record\" , \"name\" : \"demoSchema_UI_USER1\" , \"namespace\" : \"schemas.demo.ui\" , \"fields\" : [ { \"name\" : \"eventKey\" , \"type\" : \"string\" } , { \"name\" : \"message\" , \"type\" : \"string\" } , { \"name\" : \"attribute1\" , \"type\" : \"string\" , \"default\" : \"whatever\" }] } Repeat the steps for adding a new version of a schema above. This time you should see that the schema is valid: However, it still does not let us add this new version to the data schema until we actually provide a version for it. Click on the Add + link on the right of the version attribute of the schema and give it 2.0.0 for example (hit enter for the version to take the value you type in). Click on Add schema . You should now see the two versions for your data schema on the left hand side. If you go back to the Schema Registry page where all your schemas are listed, you should now see that the latest version for your data schema is 2.0.0 now.","title":"UI"},{"location":"use-cases/schema-registry-on-ocp/#cli_4","text":"If we try to add the new version of the schema from its demoschema-cli.avsc Avro schema file, we will get the same error as in the previous UI example: cloudctl es schema-add --file demoshema-cli.avsc FAILED Event Streams API request failed: Error response from server. Status code: 400 . Avro schema is not compatible with latest schema version: Compatibility type 'MUTUAL_READ' does not hold between 1 schema ( s ) in the chronology because: Schema [ 0 ] has incompatibilities: [ 'READER_FIELD_MISSING_DEFAULT_VALUE: attribute1' at '/fields/2' ] . Unable to add version 1 .0.0 of schema demoSchema_CLI_USER1 to the registry. Add the default value for the new attribute in your Avro schema avsc file and try to add that new version of the schema: cloudctl es schema-add --file demoshema-cli.avsc FAILED Event Streams API request failed: Error response from server. Status code: 409 . Schema version name already exists Unable to add version 1 .0.0 of schema demoSchema_CLI_USER1 to the registry. We see that we still have an error because we have not specified a new version value. Specify a new version value when adding this new version of the schema: cloudctl es schema-add --file demoshema-cli.avsc --version 2 .0.0 Schema demoSchema_CLI_USER1 is active. Version Version ID Schema State Updated Comment 1 .0.0 1 demoSchema_CLI_USER1 active Fri, 24 Jul 2020 14 :00:45 UTC 2 .0.0 2 demoSchema_CLI_USER1 active Fri, 24 Jul 2020 14 :09:37 UTC Added version 2 .0.0 of schema demoSchema_CLI_USER1 to the registry. OK","title":"CLI"},{"location":"use-cases/schema-registry-on-ocp/#get-latest-version-of-a-schema","text":"","title":"Get latest version of a schema"},{"location":"use-cases/schema-registry-on-ocp/#ui_5","text":"In order to see the latest version of a data schema using the UI, we just need to go to the Schema Registry web user interface and click on the expand arrow buttton that is on the left:","title":"UI"},{"location":"use-cases/schema-registry-on-ocp/#cli_5","text":"In order to see the latest version of a data schema using the CLI, we simply need to run the following command: cloudctl es schema demoSchema_CLI_USER1 --version 2 { \"type\" : \"record\" , \"name\" : \"demoSchema_CLI_USER1\" , \"namespace\" : \"schemas.demo.cli\" , \"fields\" : [ { \"name\" : \"eventKey\" , \"type\" : \"string\" } , { \"name\" : \"message\" , \"type\" : \"string\" } , { \"name\" : \"attribute1\" , \"type\" : \"string\" , \"default\" : \"whatever\" } ] } (*) The version you specify is actually the version ID (2) rather than the version name we gave to the newer schema version (2.0.0): cloudctl es schema demoSchema_CLI_USER1 Schema demoSchema_CLI_USER1 is active. Version Version ID Schema State Updated Comment 1 .0.0 1 demoSchema_CLI_USER1 active Fri, 24 Jul 2020 14 :00:45 UTC 2 .0.0 2 demoSchema_CLI_USER1 active Fri, 24 Jul 2020 14 :09:37 UTC OK","title":"CLI"},{"location":"use-cases/schema-registry-on-ocp/#get-specific-version-of-a-schema","text":"","title":"Get specific version of a schema"},{"location":"use-cases/schema-registry-on-ocp/#ui_6","text":"To see a specific version of a schema, go to the Schema Registry web user interface and click on the schema you want to see the version for. You will now see how many version of the schema you have and you can click on any of these in order to see more details about it.","title":"UI"},{"location":"use-cases/schema-registry-on-ocp/#cli_6","text":"To see a specific version of a schema using the CLI, simply run the following command with the version ID you would like to get retrieved: cloudctl es schema demoSchema_CLI_USER1 --version 1 { \"type\" : \"record\" , \"name\" : \"demoSchema_CLI_USER1\" , \"namespace\" : \"schemas.demo.cli\" , \"fields\" : [ { \"name\" : \"eventKey\" , \"type\" : \"string\" } , { \"name\" : \"message\" , \"type\" : \"string\" } ] }","title":"CLI"},{"location":"use-cases/schema-registry-on-ocp/#listing-all-versions-of-a-schema","text":"","title":"Listing all versions of a schema"},{"location":"use-cases/schema-registry-on-ocp/#ui_7","text":"To list all versions of schema in the Schema Registry user interface, you simply need to click on the data schema you want and a new page will display these:","title":"UI"},{"location":"use-cases/schema-registry-on-ocp/#cli_7","text":"In order to display all versions of a schema, run the following command: cloudctl es schema demoSchema_CLI_USER1 Schema demoSchema_CLI_USER1 is active. Version Version ID Schema State Updated Comment 1 .0.0 1 demoSchema_CLI_USER1 active Fri, 24 Jul 2020 14 :00:45 UTC 2 .0.0 2 demoSchema_CLI_USER1 active Fri, 24 Jul 2020 14 :09:37 UTC OK","title":"CLI"},{"location":"use-cases/schema-registry-on-ocp/#deleting-a-version-of-a-schema","text":"","title":"Deleting a version of a schema"},{"location":"use-cases/schema-registry-on-ocp/#ui_8","text":"In order to delete a version of a schema using the Schema Registry user interface, Click on the data schema you want a version of it deleted for. Select the version you want to delete on the left hand side. Click on Manage version button that is on the top right corner within the main box in the center of the page. Click on Remove version .","title":"UI"},{"location":"use-cases/schema-registry-on-ocp/#cli_8","text":"In order to delete a version of a schema through the CLI, execute the following command: cloudctl es schema-remove demoSchema_CLI_USER1 --version 1 Remove version with ID 1 of schema demoSchema_CLI_USER1? [ y/n ] > y Version with ID 1 of schema demoSchema_CLI_USER1 removed. OK We can see only version 2 now: cloudctl es schema demoSchema_CLI_USER1 Schema demoSchema_CLI_USER1 is active. Version Version ID Schema State Updated Comment 2 .0.0 2 demoSchema_CLI_USER1 active Fri, 24 Jul 2020 14 :09:37 UTC OK","title":"CLI"},{"location":"use-cases/schema-registry-on-ocp/#ibm-event-streams-credentials","text":"We have seen how to interact with the IBM Event Streams Schema Registry in order to create, delete, update, etc schemas that our applications will theoretically used for data correctness and application robusteness. However, the first thing that we need to set up in our IBM Event Streams instance are these applications' service credentials to be able to interact with IBM Event Streams and its Schema Registry. For doing so, we can either use either the GUI or the CLI.","title":"IBM Event Streams Credentials"},{"location":"use-cases/schema-registry-on-ocp/#gui","text":"Go to you IBM Event Streams instance console Click on Connect to this cluster In this panel, you will find The Botstrap server to connect your applications to in order to send and receive messages from your IBM Event Streams instance. We can see we have one external listener (whith SCRAM-SHA authentication) and one internal listener (Depending your IBM Event Streams installation you might have different listeners and authentications for these). The Schema Registry url your applications will need to work with Apache Avro data schemas. A Generate SCRAM credentials button to generate the SCRAM credentials your applications will need to authenticate with. A Certificates section to download either the Java PKCS12 or the PEM certificates (or both) that your applications will need in order to be able to establish the communitaction with your IBM Event Streams instance. To generate the SCRAM credentials needed by your application to get authenticated against IBM Event Streams to be able to produce and consume messages as well as to create, delete, etc topics and schemas, we need to create a KafkaUser (this happens behind the scenes) which we will set some permissions and get the corresponding SCRAM usernname and password for to be used in our applications kafka clients configuration: Click on Generate SCRAM credentials Enter a user name for your credentials and click next (leave the last option selected: Produce messages, consume messages and create topics and schemas so that we give full access to our user for simplicity) Select all topics and click next Select all consumer groups and click next Select all transactional IDs and click next Once you have created your new KafkaUser, you get the SCRAM credentials displayed on the screen: You can download the PEM certificate from the UI and then use the IBM Cloud Shell upload file option on the top bar or you can download it from within your IBM Cloud Shell by using the CLI (see below)","title":"GUI"},{"location":"use-cases/schema-registry-on-ocp/#cli_9","text":"The following two steps should have been completed already in the previous Accessing the Schema Registry section Log into your cluster with the IBM CloudPak CLI cloudctl login -a https://cp-console.apps.eda-solutions.gse-ocp.net --skip-ssl-validation Username> admin Password> Authenticating... OK Targeted account mycluster Account Enter a namespace > integration Targeted namespace integration Configuring kubectl ... Property \"clusters.mycluster\" unset. Property \"users.mycluster-user\" unset. Property \"contexts.mycluster-context\" unset. Cluster \"mycluster\" set. User \"mycluster-user\" set. Context \"mycluster-context\" created. Switched to context \"mycluster-context\" . OK Configuring helm: /Users/jesusalmaraz/.helm OK Initialize the Event Streams CLI plugin cloudctl es init IBM Cloud Platform Common Services endpoint: https://cp-console.apps.eda-solutions.gse-ocp.net Namespace: integration Name: es-1 IBM Cloud Pak for Integration UI address: No instance of Cloud Pak for Integration has been found. Please check that you have access to it. Event Streams API endpoint: https://es-1-ibm-es-admapi-external-integration.apps.eda-solutions.gse-ocp.net Event Streams API status: OK Event Streams UI address: https://es-1-ibm-es-ui-integration.apps.eda-solutions.gse-ocp.net Event Streams Schema Registry endpoint: https://es-1-ibm-es-schema-external-integration.apps.eda-solutions.gse-ocp.net Event Streams bootstrap address: es-1-kafka-bootstrap-integration.apps.eda-solutions.gse-ocp.net:443 OK We can see above the Event Streams bootstrap address and Schema Registry url that our applications will need in order to connect to this Event Streams instance To be able to establish communication and authenticate against your IBM Event Streams instance, you will need the PEM certificate and an the SCRAM credentials: To download your PEM certificate , you can use the following command: cloudctl es certificates --format pem Certificate successfully written to /home/ALMARAZJ/es-cert.pem. OK To generate your SCRAM credentials, we first need to create a KafkaUser , you can use the following command: cloudctl es kafka-user-create --name my-user1 --consumer --producer --schema-topic-create --all-topics --all-groups --all-txnids --auth-type scram-sha-512 KafkaUser name Authentication Authorization Username Secret my-user1 scram-sha-512 simple EntityOperator has not created corresponding username EntityOperator has not created corresponding secret Resource type Name Pattern type Host Operation topic * literal * Read topic __schema_ prefix * Read topic * literal * Write topic * literal * Create topic __schema_ prefix * Alter group * literal * Read transactionalId * literal * Write Created KafkaUser my-user1. OK We recommend to carefully set appropriate roles as well as access to topics, groups, transaction IDs and schemas for the API keys that you generate. When a KafkaUser custom resource is created, the Entity Operator within Event Streams will create the principal in ZooKeeper with appropriate ACL entries. It will also create a Kubernetes Secret that contains the Base64-encoded SCRAM password for the scram-sha-512 authentication type, or the Base64-encoded certificates and keys for the tls authentication type. Retrieve the username and the secret name containing the password of your SCRAM credentials for your KafkaUser: oc get kafkauser my-user1 --namespace integration -o jsonpath = '{\"username: \"}{.status.username}{\"\\nsecret-name: \"}{.status.secret}{\"\\n\"}' username: my-user1 secret-name: my-user1 1. Retrieve the password of your SCRAM credentials from the secret above: oc get secret my-user1 --namespace integration -o jsonpath = '{.data.password}' | base64 --decode *****","title":"CLI"},{"location":"use-cases/schema-registry-on-ocp/#environment-variables","text":"Now that we have generated the appropriate IBM Event Streams credentials for applications to be able to establish communication and authenticate against our IBM Event Streams instance, we are going to set some environment variables that will be used by our Python application: KAFKA_BROKERS which should take the value of bootstrap server : export KAFKA_BROKERS = es-1-kafka-bootstrap-integration.apps.eda-solutions.gse-ocp.net:443 SCRAM_USERNAME which should take the value of the SCRAM username you have generated: export SCRAM_USERNAME = my-user1 SCRAM_PASSWORD which should take the value of the SCRAM password you have generated: export SCRAM_PASSWORD = ***** PEM_CERT which should take the value of the location where the PEM certificate is within your IBM Cloud Shell: Set the path appropriately to your IBM Cloud Shell export PEM_CERT = ~/es-cert.pem (*) Don't forget to download both the PEM certificate to your IBM Cloud Shell through the CLI or upload it to your IBM Cloud Shell from your laptop if you used the UI to get the certificate. Review previous section if needed. SCHEMA_REGISTRY_URL which should be a combination of the SCRAM username , the SCRAM password and the Schema Registry url in the form of: https://<SCRAM_username>:<SCRAM_password>@<Schema_Registry_url> export SCHEMA_REGISTRY_URL = https:// ${ SCRAM_USERNAME } : ${ SCRAM_PASSWORD } @es-1-ibm-es-schema-external-integration.apps.eda-solutions.gse-ocp.net","title":"Environment variables"},{"location":"use-cases/schema-registry-on-ocp/#python-application","text":"The Python application we have built to see how to produce and consume messages (either plain messages or Avro encoded messages based on Avro Data Schemas) to and from an IBM Event Streams instance installed through the IBM Cloud Pak for Integration is public at the following GitHub repo: https://github.com/ibm-cloud-architecture/refarch-eda-tools/tree/master/labs/es-cp4i-schema-lab-v10","title":"Python Application"},{"location":"use-cases/schema-registry-on-ocp/#clone","text":"In order to use and work with this Python application, the first thing we need to do is to clone the GitHub repository where it is published. Clone the github repository on your workstation on the location of your choice: git clone https://github.com/ibm-cloud-architecture/refarch-eda-tools.git Cloning into 'refarch-eda-tools' ... remote: Enumerating objects: 185 , done . remote: Counting objects: 100 % ( 185 /185 ) , done . remote: Compressing objects: 100 % ( 148 /148 ) , done . remote: Total 185 ( delta 23 ) , reused 176 ( delta 16 ) , pack-reused 0 Receiving objects: 100 % ( 185 /185 ) , 6 .17 MiB | 4 .61 MiB/s, done . Resolving deltas: 100 % ( 23 /23 ) , done . Change directory into refarch-eda-tools/labs/es-cp4i-schema-lab-v10 to find the assets we will we working from now on for the python demo environment and few other scripts/applications: cd refarch-eda-tools/labs/es-cp4i-schema-lab-v10 $ ls -all total 240 drwxr-xr-x 9 user staff 288 20 May 19 :33 . drwxr-xr-x 3 user staff 96 20 May 19 :33 .. -rw-r--r-- 1 user staff 112578 20 May 19 :33 README.md drwxr-xr-x 5 user staff 160 20 May 19 :33 avro_files drwxr-xr-x 6 user staff 192 20 May 19 :33 kafka drwxr-xr-x 6 user staff 192 20 May 19 :33 src In the next sections, we are going to briefly explain the implementation of this Python application so that you understand what is being done behind the scenes and more importantly, if you are a developer, how to do so.","title":"Clone"},{"location":"use-cases/schema-registry-on-ocp/#python-avro-producer","text":"In this section we describe the Python scripts we will be using in order to be able to produce avro messages to a Kafka topic.","title":"Python Avro Producer"},{"location":"use-cases/schema-registry-on-ocp/#produce-message","text":"The python script that we will use to send an avro message to a Kafka topic is ProduceAvroMessage.py where we have the following: A function to parse the arguments: def parseArguments (): global TOPIC_NAME print ( \"The arguments for this script are: \" , str ( sys . argv )) if len ( sys . argv ) == 2 : TOPIC_NAME = sys . argv [ 1 ] else : print ( \"[ERROR] - The produceAvroMessage.py script expects one argument: The Kafka topic to publish the message to\" ) exit ( 1 ) A function to create the event to be sent: def createEvent (): print ( 'Creating event...' ) key = { \"key\" : 1 } value = { \"message\" : \"This is a test message\" } print ( \"DONE\" ) return json . dumps ( value ), json . dumps ( key ) The main where we will: Parse the arguments Get the Avro schemas for the key and value of the event Create the Event to be sent Print it out for reference Create the Kafka Avro Producer and configure it Send the event if __name__ == '__main__' : # Get the Kafka topic name parseArguments () # Get the avro schemas for the message's key and value event_value_schema = getDefaultEventValueSchema ( DATA_SCHEMAS ) event_key_schema = getDefaultEventKeySchema ( DATA_SCHEMAS ) # Create the event event_value , event_key = createEvent () # Print out the event to be sent print ( \"--- Event to be published: ---\" ) print ( event_key ) print ( event_value ) print ( \"----------------------------------------\" ) # Create the Kafka Avro Producer kafka_producer = KafkaProducer ( KAFKA_BROKERS , SCRAM_USERNAME , SCRAM_PASSWORD , SCHEMA_REGISTRY_URL ) # Prepare the Kafka Avro Producer kafka_producer . prepareProducer ( \"ProduceAvroMessagePython\" , event_key_schema , event_value_schema ) # Publish the event kafka_producer . publishEvent ( TOPIC_NAME , event_value , event_key ) As you can see, this python code depends on an Avro Utils for loading the Avro schemas and a Kafka Avro Producer to send the messages. These are explained next.","title":"Produce Message"},{"location":"use-cases/schema-registry-on-ocp/#avro-utils","text":"This script, called avroEDAUtils.py , contains some very simple utility functions to be able to load Avro schemas from their avsc files in order to be used by the Kafka Avro Producer. A function to get the key and value Avro schemas for the messages to be sent: def getDefaultEventValueSchema ( schema_files_location ): # Get the default event value data schema known_schemas = avro . schema . Names () default_event_value_schema = LoadAvsc ( schema_files_location + \"/default_value.avsc\" , known_schemas ) return default_event_value_schema def getDefaultEventKeySchema ( schema_files_location ): # Get the default event key data schema known_schemas = avro . schema . Names () default_event_key_schema = LoadAvsc ( schema_files_location + \"/default_key.avsc\" , known_schemas ) return default_event_key_schema (*) Where known_schemas is an Avro schema dictionary where all Avro schemas read get stored in order to be able to read nested Avro schemas afterwards. See the python script in detail for examples of this. A function to open a file, read its content as an Avro schema and store it in the Avro schema dictionary: def LoadAvsc ( file_path , names = None ): # Load avsc file # file_path: path to schema file # names(optional): avro.schema.Names object file_text = open ( file_path ) . read () json_data = json . loads ( file_text ) schema = avro . schema . SchemaFromJSONData ( json_data , names ) return schema","title":"Avro Utils"},{"location":"use-cases/schema-registry-on-ocp/#kafka-avro-producer","text":"This script, called KcAvroProducer.py , will actually be the responsible for creating the Kafka Avro Producer, initialize and configure it and provide the publish method: Initialize and prepare the Kafka Producer class KafkaProducer : def __init__ ( self , kafka_brokers = \"\" , scram_username = \"\" , scram_password = \"\" , schema_registry_url = \"\" ): self . kafka_brokers = kafka_brokers self . scram_username = scram_username self . scram_password = scram_password self . schema_registry_url = schema_registry_url def prepareProducer ( self , groupID = \"pythonproducers\" , key_schema = \"\" , value_schema = \"\" ): options = { 'bootstrap.servers' : self . kafka_brokers , 'schema.registry.url' : self . schema_registry_url , 'group.id' : groupID , 'security.protocol' : 'SASL_SSL' , 'sasl.mechanisms' : 'SCRAM-SHA-512' , 'sasl.username' : self . scram_username , 'sasl.password' : self . scram_password , 'ssl.ca.location' : os . environ [ 'PEM_CERT' ], 'schema.registry.ssl.ca.location' : os . environ [ 'PEM_CERT' ] } # Print out the configuration print ( \"--- This is the configuration for the avro producer: ---\" ) print ( options ) print ( \"---------------------------------------------------\" ) # Create the Avro Producer self . producer = AvroProducer ( options , default_key_schema = key_schema , default_value_schema = value_schema ) Publish method def publishEvent ( self , topicName , value , key ): # Produce the Avro message # Important: value DOES NOT come in JSON format from ContainerAvroProducer.py. Therefore, we must convert it to JSON format first self . producer . produce ( topic = topicName , value = json . loads ( value ), key = json . loads ( value )[ key ], callback = self . delivery_report ) # Flush self . producer . flush ()","title":"Kafka Avro Producer"},{"location":"use-cases/schema-registry-on-ocp/#python-avro-consumer","text":"In this section we describe the python scripts we will be using in order to be able to consume Avro messages from a Kafka topic.","title":"Python Avro Consumer"},{"location":"use-cases/schema-registry-on-ocp/#consume-message","text":"The python script that we will use to consume an Avro message from a Kafka topic is ConsumeAvroMessage.py where we have the following: A function to parse arguments: # Parse arguments to get the container ID to poll for def parseArguments (): global TOPIC_NAME print ( \"The arguments for the script are: \" , str ( sys . argv )) if len ( sys . argv ) != 2 : print ( \"[ERROR] - The ConsumeAvroMessage.py script expects one arguments: The Kafka topic to events from.\" ) exit ( 1 ) TOPIC_NAME = sys . argv [ 1 ] The main where we will: Parse the arguments to get the topic to read from Create the Kafka Consumer and configure it Poll for next avro message Close the Kafka consumer if __name__ == '__main__' : # Parse arguments parseArguments () # Create the Kafka Avro consumer kafka_consumer = KafkaConsumer ( KAFKA_BROKERS , SCRAM_USERNAME , SCRAM_PASSWORD , TOPIC_NAME , SCHEMA_REGISTRY_URL ) # Prepare the consumer kafka_consumer . prepareConsumer () # Consume next Avro event kafka_consumer . pollNextEvent () # Close the Avro consumer kafka_consumer . close () As you can see, this python code depends on a Kafka Avro Consumer to consume messages. This is explained next.","title":"Consume Message"},{"location":"use-cases/schema-registry-on-ocp/#kafka-avro-consumer","text":"This script, called KcAvroConsumer.py , will actually be the responsible for creating the Kafka Avro Consumer, initialize and configure it and provide the poll next event method: Initialize and prepare the new Kafka consumer: class KafkaConsumer : def __init__ ( self , kafka_brokers = \"\" , scram_username = \"\" , scram_password = \"\" , topic_name = \"\" , schema_registry_url = \"\" , autocommit = True ): self . kafka_brokers = kafka_brokers self . scram_username = scram_username self . scram_password = scram_password self . topic_name = topic_name self . schema_registry_url = schema_registry_url self . kafka_auto_commit = autocommit # See https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md def prepareConsumer ( self , groupID = \"pythonconsumers\" ): options = { 'bootstrap.servers' : self . kafka_brokers , 'group.id' : groupID , 'auto.offset.reset' : 'earliest' , 'schema.registry.url' : self . schema_registry_url , 'enable.auto.commit' : self . kafka_auto_commit , 'security.protocol' : 'SASL_SSL' , 'sasl.mechanisms' : 'SCRAM-SHA-512' , 'sasl.username' : self . scram_username , 'sasl.password' : self . scram_password , 'ssl.ca.location' : os . environ [ 'PEM_CERT' ], 'schema.registry.ssl.ca.location' : os . environ [ 'PEM_CERT' ] } # Print the configuration print ( \"--- This is the configuration for the Avro consumer: ---\" ) print ( options ) print ( \"---------------------------------------------------\" ) # Create the Avro consumer self . consumer = AvroConsumer ( options ) # Subscribe to the topic self . consumer . subscribe ([ self . topic_name ]) Poll next event method: # Prints out the message def traceResponse ( self , msg ): print ( '[Message] - Next message consumed from {} partition: [ {} ] at offset {} with key {} and value {} ' . format ( msg . topic (), msg . partition (), msg . offset (), msg . key (), msg . value () )) # Polls for next event def pollNextEvent ( self ): # Poll for messages msg = self . consumer . poll ( timeout = 10.0 ) # Validate the returned message if msg is None : print ( \"[INFO] - No new messages on the topic\" ) elif msg . error (): if ( \"PARTITION_EOF\" in msg . error ()): print ( \"[INFO] - End of partition\" ) else : print ( \"[ERROR] - Consumer error: {} \" . format ( msg . error ())) else : # Print the message msgStr = self . traceResponse ( msg )","title":"Kafka Avro Consumer"},{"location":"use-cases/schema-registry-on-ocp/#schemas-and-messages","text":"In this section we are going to see how Schema Registry works when you have an application that produces and consumes messages based on Avro data schemas. The application we are going to use for this is the python application presented above in the Python Avro Producer and Python Avro Consumer sections. IMPORTANT: Before start using our Python application we must set the PYTHONPATH environment variable to point to where we have all the Python scripts that make up our application in order for Python to find these at execution time. Set the PYTHONPATH variable to the location where you cloned the GitHub repository containing the Python application we are going to be working with export PYTHONPATH = ~/refarch-eda-tools/labs/es-cp4i-schema-lab-v10","title":"Schemas and Messages"},{"location":"use-cases/schema-registry-on-ocp/#produce-a-message","text":"In order to produce a message, we execute the ProduceAvroMessage.py . This script, as you could see in the Python Avro Producer section, is sending the event with key {'key': '1'} and value {'message': 'This is a test message'} according to the schemas defined in default_key.avsc and default_value.avsc for the key and value of the event respectively. Make sure you are on the right path where the python scripts live: ~/refarch-eda-tools/labs/es-cp4i-schema-lab-v10/src Change user1 python3 ProduceAvroMessage.py test-schema-user1 @@@ Executing script: ProduceAvroMessage.py The arguments for the script are: [ 'ProduceAvroMessage.py' , 'test-schema-user1' ] Creating event... DONE --- Event to be published: --- { \"key\" : 1 } { \"message\" : \"This is a test message\" } ---------------------------------------- --- This is the configuration for the avro producer: --- { 'bootstrap.servers' : 'es-1-kafka-bootstrap-integration.apps.eda-solutions.gse-ocp.net:443' , 'schema.registry.url' : 'https://my-user1:*****@es-1-ibm-es-schema-external-integration.apps.eda-solutions.gse-ocp.net' , 'group.id' : 'ProduceAvroMessagePython' , 'security.protocol' : 'SASL_SSL' , 'sasl.mechanisms' : 'SCRAM-SHA-512' , 'sasl.username' : 'my-user1' , 'sasl.password' : '*****' , 'ssl.ca.location' : '/home/ALMARAZJ/es-cert.pem' , 'schema.registry.ssl.ca.location' : '/home/ALMARAZJ/es-cert.pem' } --------------------------------------------------- Message delivered to test-schema-user1 [ 0 ] We can see our new message delivered in the test-schema-user1 topic by Go into the topics page in the IBM Event Streams UI Click on the topic and then on the Messages tab at the top. Finally, click on a message to see it displayed on the right hand side of the screen INFO: Mind the message now is not in JSON format as Avro does not repeat every field name with every single record which makes Avro more efficient than JSON for high-volume usage. This is thanks to having Avro schemas. IMPORTANT: As you can see, we got the test-schema-user1 topic auto-created when we produced the message. The reason for this is that Kafka is set out of the box to let applications to auto-create topics. We created the SCRAM credentials for our application to allow the application to create topics. On a production-like environment, you don't want developers creating applications that auto-create topics in your IBM Event Streams instance without any control. For that, we would configure Kafka to forbid topic auto-creation ( https://kafka.apache.org/documentation/#auto.create.topics.enable ) as well as thoroughly created the SCRAM credentials with the most strict but appropriate permissions that our application needs. IMPORTANT: Similar to the auto-creation of topics, we can see below that our application got the Avro data schemas for both the key and value of the message produced auto-registered. This is because many client libraries come with a SerDes property to allow them to auto-register the Avro data schemas ( https://docs.confluent.io/current/clients/confluent-kafka-python/#avroserializer ). However, on a production-like environment we don't want applications to auto-register schemas without any control but yet we can not leave it to the developers to set the auto-registration property off on their libraries. Instead, we would create the SCRAM credentials with the most strict but appropriate permissions that our application needs. If we look now at the schemas our schema registry has: cloudctl es schemas Schema State Latest version Latest version ID Updated demoSchema_CLI_USER1 active 2 .0.0 2 Fri, 24 Jul 2020 14 :09:37 UTC demoSchema_UI_USER1 active 2 .0.0 2 Fri, 24 Jul 2020 14 :06:27 UTC test-schema-user1-key-d89uk active 1 1 Fri, 24 Jul 2020 15 :41:46 UTC test-schema-user1-value-tv5efr active 1 1 Fri, 24 Jul 2020 15 :41:45 UTC OK we see two schemas, test-schema-user1-key-d89uk and test-schema-user1-value-tv5efr , which in fact correspond to the Avro data schema used for the key ( default_key.avsc ) and the value ( default_value.avsc ) of events sent to the test-schema-user1 topic in the ProduceAvroMessage.py as explained before sending the message. To make sure of what we are saying, we can inspect those schemas: cloudctl es schema test-schema-user1-key-d89uk --version 1 { \"type\" : \"record\" , \"name\" : \"defaultKey\" , \"namespace\" : \"ibm.eda.default\" , \"fields\" : [ { \"type\" : \"int\" , \"name\" : \"key\" , \"doc\" : \"We expect any int as the event key\" } ] , \"doc\" : \"Default Message's key Avro data schema\" } cloudctl es schema test-schema-user1-value-tv5efr --version 1 { \"type\" : \"record\" , \"name\" : \"defaultValue\" , \"namespace\" : \"ibm.eda.default\" , \"fields\" : [ { \"type\" : \"string\" , \"name\" : \"message\" , \"doc\" : \"Any string\" } ] , \"doc\" : \"Default Message's value Avro data schema\" } If I now decided that my events should contain another attribute, I would modify the event value schema ( default_value.avsc ) to reflect that as well as ProduceAvroMessage.py to send that new attribute in the event it sends: Change user1 python3 ProduceAvroMessage.py test-schema-user1 @@@ Executing script: ProduceAvroMessage.py The arguments for the script are: [ 'ProduceAvroMessage.py' , 'test-schema-user1' ] Creating event... DONE --- Event to be published: --- { \"key\" : 1 } { \"message\" : \"This is a test message\" , \"anotherAttribute\" : \"Just another test string\" } ---------------------------------------- --- This is the configuration for the avro producer: --- { 'bootstrap.servers' : 'es-1-kafka-bootstrap-integration.apps.eda-solutions.gse-ocp.net:443' , 'schema.registry.url' : 'https://my-user1:*****@es-1-ibm-es-schema-external-integration.apps.eda-solutions.gse-ocp.net' , 'group.id' : 'ProduceAvroMessagePython' , 'security.protocol' : 'SASL_SSL' , 'sasl.mechanisms' : 'SCRAM-SHA-512' , 'sasl.username' : 'my-user1' , 'sasl.password' : '*****' , 'ssl.ca.location' : '/home/ALMARAZJ/es-cert.pem' , 'schema.registry.ssl.ca.location' : '/home/ALMARAZJ/es-cert.pem' } --------------------------------------------------- Message delivered to test-schema-user1 [ 0 ] I can see that an event with a new attribute has been sent: And I can also see that the new shcema has got registered as well: cloudctl es schemas Schema State Latest version Latest version ID Updated demoSchema_CLI_USER1 active 2 .0.0 2 Fri, 24 Jul 2020 14 :09:37 UTC demoSchema_UI_USER1 active 2 .0.0 2 Fri, 24 Jul 2020 14 :06:27 UTC test-schema-user1-key-d89uk active 1 1 Fri, 24 Jul 2020 15 :41:46 UTC test-schema-user1-value-a5bbaa active 1 1 Fri, 24 Jul 2020 15 :54:37 UTC test-schema-user1-value-tv5efr active 1 1 Fri, 24 Jul 2020 15 :41:45 UTC OK If I inspect that new schema, I see my new attribute in it: cloudctl es schema test-schema-user1-value-a5bbaa --version 1 { \"type\" : \"record\" , \"name\" : \"defaultValue\" , \"namespace\" : \"ibm.eda.default\" , \"fields\" : [ { \"type\" : \"string\" , \"name\" : \"message\" , \"doc\" : \"Any string\" } , { \"type\" : \"string\" , \"name\" : \"anotherAttribute\" , \"doc\" : \"Any other string\" } ] , \"doc\" : \"Default Message's value Avro data schema\" } The schema evolution above (test-schema-user1-value-a5bbaa) should have got registered as a new version of the already existing schema (test-schema-user1-value-tv5efr). IBM Event Streams allows schemas to auto-register themselves when these are sent along with a message from a producer application. However, the Schema Registry does not pick \"new\" schemas up as a new version of a previous schema and simply creates a new schema. Anyway, when reading messages off the topic, Schema Registry handles well what schema to return back to the receiver application so messages can get properly deserialized. Will see that in the next section. SECURITY: As some of you may have already thought, having your clients (that is your applications), auto-register the Avro data schemas that are in the end kind of the contracts that your components of your overal solution agree on in order to understand each other and collaborate between them is NOT a good idea. Specially in microservices architectures where you might have hundreds of microservices talking and collaborating among themselsves. We will see in the Security section how we can control schema registration and evolution based on roles at the schema level also.","title":"Produce a message"},{"location":"use-cases/schema-registry-on-ocp/#create-a-non-compliant-message","text":"Let's see what happens if we send a message that does not comply with its Avro data schema. Let's say that I send the following message: key = { \"key\" : 1 } value = { \"message\" : 12345 } and this is the output of that attempt: Change user1 python3 ProduceAvroMessage.py test-schema-user1 @@@ Executing script: ProduceAvroMessage.py The arguments for the script are: [ 'ProduceAvroMessage.py' , 'test-schema-user1' ] Creating event... DONE --- Event to be published: --- { \"key\" : 1 } { \"message\" : 12345 } ---------------------------------------- --- This is the configuration for the avro producer: --- { 'bootstrap.servers' : 'es-1-kafka-bootstrap-integration.apps.eda-solutions.gse-ocp.net:443' , 'schema.registry.url' : 'https://my-user1:*****@es-1-ibm-es-schema-external-integration.apps.eda-solutions.gse-ocp.net' , 'group.id' : 'ProduceAvroMessagePython' , 'security.protocol' : 'SASL_SSL' , 'sasl.mechanisms' : 'SCRAM-SHA-512' , 'sasl.username' : 'my-user1' , 'sasl.password' : '*****' , 'ssl.ca.location' : '/home/ALMARAZJ/es-cert.pem' , 'schema.registry.ssl.ca.location' : '/home/ALMARAZJ/es-cert.pem' } --------------------------------------------------- Traceback ( most recent call last ) : File \"ProduceAvroMessage.py\" , line 81 , in <module> kafka_producer.publishEvent ( TOPIC_NAME,event_value,event_key ) File \"/tmp/lab/kafka/KcAvroProducer.py\" , line 43 , in publishEvent self.producer.produce ( topic = topicName,value = json.loads ( value ) ,key = json.loads ( key ) , callback = self.delivery_report ) File \"/root/.local/lib/python3.7/site-packages/confluent_kafka/avro/__init__.py\" , line 99 , in produce value = self._serializer.encode_record_with_schema ( topic, value_schema, value ) File \"/root/.local/lib/python3.7/site-packages/confluent_kafka/avro/serializer/message_serializer.py\" , line 118 , in encode_record_with_schema return self.encode_record_with_schema_id ( schema_id, record, is_key = is_key ) File \"/root/.local/lib/python3.7/site-packages/confluent_kafka/avro/serializer/message_serializer.py\" , line 152 , in encode_record_with_schema_id writer ( record, outf ) File \"/root/.local/lib/python3.7/site-packages/confluent_kafka/avro/serializer/message_serializer.py\" , line 86 , in <lambda> return lambda record, fp: writer.write ( record, avro.io.BinaryEncoder ( fp )) File \"/root/.local/lib/python3.7/site-packages/avro/io.py\" , line 771 , in write raise AvroTypeException ( self.writer_schema, datum ) avro.io.AvroTypeException: The datum { 'message' : 12345 } is not an example of the schema { \"type\" : \"record\" , \"name\" : \"defaultValue\" , \"namespace\" : \"ibm.eda.default\" , \"fields\" : [ { \"type\" : \"string\" , \"name\" : \"message\" , \"doc\" : \"Any string\" } , { \"type\" : \"string\" , \"name\" : \"anotherAttribute\" , \"doc\" : \"Any other string\" } ] , \"doc\" : \"Default Message's value Avro data schema\" } As we can see, the attempt failed as the Avro producer will check the message against the Avro data schema defined for the topic we want to send the message to and yield that this message does not comply (the message value attribute we are sending is an integer rather than a string and we are missing the second attribute). Therefore, using Avro schemas with IBM Event Streams give us the ability to build our system with robustness protecting downstream data consumers from malformed data, as only valid data will be permitted in the topic.","title":"Create a non-compliant message"},{"location":"use-cases/schema-registry-on-ocp/#consume-a-message","text":"In order to consume a message, we execute the ConsumeAvroMessage.py within the /tmp/lab/src folder in our python demo environment: Change user1 python3 ConsumeAvroMessage.py test-schema-user1 @@@ Executing script: ConsumeAvroMessage.py The arguments for this script are: [ 'ConsumeAvroMessage.py' , 'test-schema-user1' ] --- This is the configuration for the Avro consumer: --- { 'bootstrap.servers' : 'es-1-kafka-bootstrap-integration.apps.eda-solutions.gse-ocp.net:443' , 'group.id' : 'pythonconsumers' , 'auto.offset.reset' : 'earliest' , 'schema.registry.url' : 'https://my-user1:*****@es-1-ibm-es-schema-external-integration.apps.eda-solutions.gse-ocp.net' , 'enable.auto.commit' : True, 'security.protocol' : 'SASL_SSL' , 'sasl.mechanisms' : 'SCRAM-SHA-512' , 'sasl.username' : 'my-user1' , 'sasl.password' : '*****' , 'ssl.ca.location' : '/home/ALMARAZJ/es-cert.pem' , 'schema.registry.ssl.ca.location' : '/home/ALMARAZJ/es-cert.pem' } --------------------------------------------------- [ Message ] - Next message consumed from test-schema-user1 partition: [ 0 ] at offset 0 with key { 'key' : 1 } and value { 'message' : 'This is a test message' } python3 ConsumeAvroMessage.py test-schema-user1 @@@ Executing script: ConsumeAvroMessage.py The arguments for this script are: [ 'ConsumeAvroMessage.py' , 'test-schema-user1' ] --- This is the configuration for the Avro consumer: --- { 'bootstrap.servers' : 'es-1-kafka-bootstrap-integration.apps.eda-solutions.gse-ocp.net:443' , 'group.id' : 'pythonconsumers' , 'auto.offset.reset' : 'earliest' , 'schema.registry.url' : 'https://my-user1:*****@es-1-ibm-es-schema-external-integration.apps.eda-solutions.gse-ocp.net' , 'enable.auto.commit' : True, 'security.protocol' : 'SASL_SSL' , 'sasl.mechanisms' : 'SCRAM-SHA-512' , 'sasl.username' : 'my-user1' , 'sasl.password' : '*****' , 'ssl.ca.location' : '/home/ALMARAZJ/es-cert.pem' , 'schema.registry.ssl.ca.location' : '/home/ALMARAZJ/es-cert.pem' } --------------------------------------------------- [ Message ] - Next message consumed from test-schema-user1 partition: [ 0 ] at offset 1 with key { 'key' : 1 } and value { 'message' : 'This is a test message' , 'anotherAttribute' : 'Just another test string' } As you can see, our script was able to read the Avro messages from the test-schema-user1 topic and map that back to their original structure thanks to the Avro schemas: [ Message ] - Next message consumed from test-schema partition: [ 0 ] at offset 0 with key { 'key' : 1 } and value { 'message' : 'This is a test message' } [ Message ] - Next message consumed from test-schema partition: [ 0 ] at offset 1 with key { 'key' : 1 } and value { 'message' : 'This is a test message' , 'anotherAttribute' : 'Just another test string' }","title":"Consume a message"},{"location":"use-cases/schema-registry-on-ocp/#data-evolution","text":"So far we have more or less seen what Avro is, what an Avro data schema is, what a schema registry is and how this all works together. From creating an Avro data schema for your messages/events to comply with to how the schema registry and Avro data schemas work together. And we have also seen the code for doing all this, from the python code to send and receive Avro encoded messages based on their Avro data schemas to the rich CLI IBM Event Streams provides to interact with. However, we have said little about the need for data to evolve. When you design an Event Driven architecture for your application (by applying Event Storming or Domain Driven Design for example), it is very hard to come up with data structures/schemas that will not need to evolve/change in time. That is, your data, like your use or business cases, may need to evolve. As a result, Avro data schemas must be somehow flexible to allow your data to evolve along with your application and use cases. But it is not as easy as adding or removing data that travels in your events/messages or modifying the type of such data. And one of the reasons for this is that Kafka (or any other type of event backbone) is many times used as the source of truth. That is, a place that you can trust as to what has happened. Hence, Kafka will serve as the source of truth where all the events (that is, data) that happened (which could be bank transactions, communications, etc) will get stored (sometimes up to hundreds of years ) and will be able to be replayed if needed. As a result, there must be a data schema management and data schema evolution put in place that allow the compatibility of old and new data schemas and, in fact, old and new data at the end of the day. The IBM Event Streams Schema Registry enforces full compatibility when creating a new version of a schema . Full compatibility means that old data can be read with the new data schema, and new data can also be read with the last data schema . In data formats like Avro, you can define fields with default values. In that case adding or removing a field with a default value is a fully compatible change. Let's see then how this affects when you want your data to evolve in a way that it needs to add a new attribute or delete an existing attribute. But let's see what that means in terms of adding and removing attributes from your data schema.","title":"Data Evolution"},{"location":"use-cases/schema-registry-on-ocp/#adding-a-new-attribute","text":"Although we have already seen this in the adding a new version of a schema section, let's try to add a new version of our test-schema-value schema where we have a new attribute. Remember, our default_schema.avsc already contains a new attribute than the original one but that it got registered as a new schema rather than as a new version of the original one. Let's reuse that Avro schema file to register it as a new version ( INFO: you might need to copy/download that file to your local workstation in order to be able to then upload it to the IBM Event Streams through its UI) When doing so from the UI, we see the following error: The reason, as alread explained in the add a new version of a schema section, is because full compatibility dictates that you can only add new attributes to a schema if these have a default value. Reason being that a receiver should be able to deserialize messages produced with an older schema using the newer schema. Because old messages were written with an older schema that did not contain our new attribute, those messages won't have that attribute so we need to provide a default value for it in our never version of the schema so that the receiver is able to deserialize those older messages with the newer schema. If we add the default value for the new attribute, we see that our newer version is now compatible: and that it gets registered fine:","title":"Adding a new attribute"},{"location":"use-cases/schema-registry-on-ocp/#removing-an-existing-attribute","text":"What if we now wanted to remove the original message attribute from our schema. Let's remove it from the default_value.avsc file and try to register that new version: We, again, get the same error. And the reason is because receivers must be able to read and deserialize messages produced with the newer schema (that is, without the message attribute) but with the older schema (that is, with the schema version that enforces the existence of the message attribute). In order to work this around, what we need to do is to register first an intermediate schema that defines a default value for the message attribute: Once we have a default value for the message attribute, we can register a new version of the schema that finally removes that attribute:","title":"Removing an existing attribute"},{"location":"use-cases/schema-registry-on-ocp/#security","text":"As we have already mentioned during the this tutorial, we need to pay attention to the permissions we give to users, groups, applications (and thefore the clients they used to interact with IBM Event Streams), etc since we don't want everyone and everything to be, for instance, creating or deleting topics, schemas, etc. You can secure your IBM Event Streams resources in a fine-grained manner by managing the access each user and application has to each resource. Within IBM Event Streams, you can secure access to the following resource types, where the names in parentheses are the resource type names used in Access Control List (ACL) rules: Topics (topic): you can control the ability of users and applications to create, delete, read, and write to a topic. Consumer groups (group): you can control an application\u2019s ability to join a consumer group. Transactional IDs (transactionalId): you can control the ability to use the transaction capability in Kafka. Note: Schemas in the Event Streams Schema Registry are a special case and are secured using the resource type of topic combined with a prefix of __schema_. You can control the ability of users and applications to create, delete, read, and update schemas. You can find more information about how to secure your IBM Event Streams resources in the official documentation at: https://ibm.github.io/event-streams/security/managing-access/","title":"Security"}]}